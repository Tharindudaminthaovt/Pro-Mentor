title,description
Data Engineer,"Company Description
Sagence is a management advisory firm dedicated to helping our clients optimize the value of their data assets. From thinking to doing, Sagence works with leading institutions in the acquisition, evaluation, development and management of their critical data assets and in the application of analytics to discover new insights, shorten time-to-value, and drive competitive advantage.
**All candidates must be currently authorized to work in the US on a full-time basis. Sagence does not provide sponsorship for work visas upon hire or once on board.**
Job Overview
Sagence is looking for experienced, client-facing Data Engineers to help us build and sustain our client’s data capabilities and our competitive advantage. Along with the requirements below, candidates must possess deep, practical experience in one or more of the following Data Management competencies; data warehousing, data lakes, reference data or master data management, data architecture, data modeling, data governance, data analysis, business intelligence.
Skills & Requirements:
MUST BE HANDS ON. Candidates should have system development experience and proficiency with system development methodologies and possess the following skills and knowledge:
Must have significant hands-on experience with various IT concepts of data management/engineering (ETL, data modeling, data warehousing, etc.)
Must have significant hands-on experience with SQL, data profiling, and data discovery
Experience building business intelligence, analytics, or reporting solutions - either front-end consumption mechanisms (e.g., Microsoft, Tableau & Qlik) or supply of data for these purposes
Familiarity with data architecture principles/approaches, data environment infrastructure considerations, and data modeling principles/approaches
Ability to drive out technical requirements with business and IT stakeholders for implementations of data solutions
Hands-on experience with Agile delivery methodology
Prior professional experience in an IT management, management consulting, or client facing role is preferred
Knowledge of the Financial Services, Insurance, Healthcare or Retail industries is preferred
Demonstrated ability in engaging, communicating, and presenting to stakeholders across both business and technology functions
Tools and Technology:
Proficient at leveraging tools and technology to drive value for clients. Examples include the following;
Database Management Tools:
Relational – e.g. Oracle, MySQL, Microsoft SQL Server, PostgreSQL, DB, or similar
NoSQL – e.g. MongoDB, Couchbase, DataStax, Redix, MarkLogic, or similar
Cloud – e.g. AWS, Azure, xxx, xxx, xxx
ETL Tools - e.g. Informatica, Talend, Microsoft SSIS, or similar
Data Modeling tools - e.g., Toad, ERWin, ER/Studio, PowerDesigner, IBM Data Architect, or similar
Industry Leading BI tools - e.g., Business Objects, Microsoft, Cognos, Tableau, OBIEE, Qlickview, or similar
General:
Must be currently authorized to work in the US on a full-time basis. Sagence does not provide sponsorship for work visas
3+ years of professional experience working in a related role
Must be collaborative, innovative, curious, and resourceful, and exhibit a positive attitude
Strong desire to work on interesting projects with smart and creative people
Willingness to travel up to 80% of the week (M-Th)
Chicago or New York area candidates preferred, but will consider candidates in other parts of U.S.
Our Culture
Passionate, diverse, creative, genuine, flexible, hands-on…these are just a few of the words that describe our culture. Our Partners are deeply involved in the client work on a daily basis. We have a high-energy workplace with a focus on producing high-quality, impactful results. We are committed to equality of opportunity, fairness, work and lifestyle balance, and mutual respect. We promote an entrepreneurial spirit by encouraging individual initiative and foster a collaborative culture and work environment which includes open communication and on-going learning. We build teamwork through small, dedicated teams who continuously teach each other and learn from one another. We strongly believe these characteristics enable our employees to develop to their fullest potential. To learn more, please visit us at www.sagenceconsulting.com"
Data Engineer,"Data Engineer

Job Details
Level
Experienced
Job Location
New York (Home Office) - New York, NY
Position Type
Full Time
Education Level
4 Year Degree
Salary Range
Undisclosed
Travel Percentage
Undisclosed
Job Shift
Day
Job Category
Information Technology
Description
Greater New York Mutual Insurance Company (""GNY"") is an A+ rated, financially stable and growing property casualty insurance company with locations throughout the Northeast. We are currently looking for a dynamic and highly motivated Data Engineer for our New York office.

Responsibilities:
Develop, construct, test, and maintain architectures, such as databases and analytic environments and platform required for structured, semi-structured and unstructured data
Design and develop data pipelines that deliver accurate, consistent, and traceable datasets for data science projects
Support regular and ad-hoc data needs for data scientists
Provide recommendations and implement ways to improve data reliability, efficiency, and quality
Qualifications
Bachelor’s or Master’s degree obtained from an accredited institution preferably in Computer Science, Computer Engineering, and Software Engineering, Data Science, or a related field
3-5 years of professional experience in data science or related field
Experience in database deployment and management and proficient in SQL
Experience in data warehousing and ETL (Extract, Transform, and Load)
Proficient in R, Python, VBA, Excel and Word
Excellent oral and written communication skills"
Data Engineer,"Senior Data Engineer

Master’s degree in Information Technology, Computer Science with 2 years experience.

Design, build, & maintain Consumer Identity Management System to match Affinity consumer database using frameworks such MD5 & SHA512.

Build & maintain Analytical Data Platforms using appropriate SQL, NoSQL and NewSQL technologies like MapR, Spark, Hadoop, & Python.

Lead engineering processes to ensure data quality & meta data documentation using tools like Python, Amazon RedShift, Tableau & Confluence.

Perform quantitative analysis of customer data using tools like SaaS & machine learning. Monitor tag transactions to correctly reward consumers based on merchant reward program offerings.

Skills: SQL, NoSQL, NewSQL, MapR, Spark, Hadoop, Python, Amazon RedShift, Tableau, Confluence, SaaS & machine learning.

Send resume to Affinity Solutions, Inc, Attn: HR Department, 875 Avenue of the Americas, 21st Floor, New York, NY 10001."
Data Engineer,"Our client is a leading hedge fund looking to hire a Data Engineer for their Macro Strategies business unit.

Responsibilities:
Driving innovation through product and platform development
Helping to facilitate bespoke custom basket trades for clients in a scalable infrastructure
Developing infrastructure and tools to administer basket rebalances for external clients and internal trading teams
Automation of corporate action adjustments and improvement of work-flow
Providing metrics for basket trades, to drive sales and trading decisions and to grow the business
Both independent and collaborative work, involving several sales/strat/trading teams globally

Requirements:
Expertise in Python
Strong SQL skills
Web scraping experience
Experience with Linux and Windows platform
Strong communication skills, both written and verbal
Exposure to non-relational databases
Exposure to web UI technologies
Data Warehousing and Modeling expertise
Financial knowledge


If you would like to be considered for the position of Data Engineer or wish to discuss the role further then please leave your details below. Your resume will be held in confidence until you connect with a member of our team
Email: info@njfsearch.com or call London (0207 604 4444,) New York (212 400 4845) or Chicago (312 204 72176) to speak to a member of our team. Thank you"
Data Engineer,"Job Description

Data Engineer

At Citadel, data is the core of the investment process. Data Engineers architect and build our data platforms which drive how we source, enrich, and store data that integrates into the investment process. These Data Engineers own the entire data pipeline starting with how we ingest data from the outside world, transforming that information into actionable insights, and ultimately designing the interfaces and APIs that our investment professionals and quantitative researchers use to monetize ideas. Throughout the process, our Data Engineers partner with top investment professionals and data scientists to design systems that solve our most critical problems and answer the most challenging questions in finance.

YOUR OPPORTUNITY:

Develop solutions that enable investment professionals to efficiently extract insights from data. This includes owning the ingestion (web scrapes, S3/FTP sync, sensor collection), transformations (Spark, SQL, Kafka, Python/C++/Java), and interface (API, schema design, events)
Partner with the industry’s top investment professionals, quantitative researchers, and data scientists to design, develop, and deploy solutions that answer fundamental questions about financial markets
Build tools and automation capabilities for data pipelines that improve the efficiency, quality and resiliency of our data platform
Drive the evolution of our data strategy by challenging the status quo and identifying opportunities to enhance our platform
YOUR SKILLS & TALENTS:

Passion for working with data in order to accurately model and analyze complex systems such as a publicly traded company, commodity market, economy, or financial instruments
Strong interest in financial markets and a desire to work directly with investment professionals
Proficiency with one or more programming languages such as Java or C++ or Python
Proficiency with RDBMS, NoSQL, distributed compute platforms such as Spark, Dask or Hadoop
Experience with any of the following systems: Apache Airflow, AWS/GCE/Azure, Jupyter, Kafka, Docker, Kubernetes, or Snowflake
Strong written and verbal communications skills
Bachelor’s, Master’s or PhD degree in Computer Science or equivalent experience
About Citadel


Citadel is a global investment firm built around world-class talent, sound risk management, and innovative leading-edge technology. For a quarter of a century, Citadel’s hedge funds have delivered meaningful and measurable results to top-tier investors around the world, including sovereign wealth funds, public institutions, corporate pensions, endowments and foundations.

With an unparalleled ability to identify and execute on great ideas, Citadel’s team of more than 675 investment professionals, operating from offices including Chicago, New York, San Francisco, London, Hong Kong and Shanghai, deploy capital across all major asset classes, in all major financial markets."
Data Engineer,"PulsePoint Data Engineering team plays a key role in our technology company that's experiencing exponential growth. Our data pipeline processes over 80 billion impressions a day (> 20TB of data, 220 TB uncompressed). This data is used to generate reports, update budgets, and drive our optimization engines. We do all this while running against extremely tight SLAs and provide stats and reports as close to real-time as possible.

The most exciting part about working at PulsePoint is the enormous potential for personal and professional growth. We are always seeking new and better tools to help us meet challenges such as adopting proven open-source technologies to make our data infrastructure more nimble, scalable and robust. Some of the cutting edge technologies we have recently implemented are Kafka, Spark Streaming, Presto, Airflow, and Kubernetes.

What you'll be doing:
Design, build and maintain reliable and scalable enterprise level distributed transactional data processing systems for scaling the existing business and supporting new business initiatives
Optimize jobs to utilize Kafka, Hadoop, Presto, Spark Streaming and Kubernetes resources in the most efficient way
Monitor and provide transparency into data quality across systems (accuracy, consistency, completeness, etc)
Increase accessibility and effectiveness of data (work with analysts, data scientists, and developers to build/deploy tools and datasets that fit their use cases)
Collaborate within a small team with diverse technology backgrounds
Provide mentorship and guidance to junior team members
Team Responsibilities:
Installation, upkeep, maintenance and monitoring of Kafka, Hadoop, Presto, RDBMS
Ingest, validate and process internal & third party data
Create, maintain and monitor data flows in Hive, SQL and Presto for consistency, accuracy and lag time
Maintain and enhance framework for jobs(primarily aggregate jobs in Hive)
Create different consumers for data in Kafka using Spark Streaming for near time aggregation
Train Developers/Analysts on tools to pull data
Tool evaluation/selection/implementation
Backups/Retention/High Availability/Capacity Planning
Review/Approval - DDL for database, Hive Framework jobs and Spark Streaming to make sure they meet our standards
24*7 On call rotation for Production support
Technologies We Use:
Airflow - for job scheduling
Docker - Packaged container image with all dependencies
Graphite/Beacon - for monitoring data flows
Hive - SQL data warehouse layer for data in HDFS
Impala- faster SQL layer on top of Hive
Kafka- distributed commit log storage
Kubernetes - Distributed cluster resource manager
Presto - fast parallel data warehouse and data federation layer
Spark Streaming - Near time aggregation
SQL Server - Reliable OLTP RDBMS
Sqoop - Import/Export data to RDBMS
Required Skills:
BA/BS degree in Computer science or related field
5+ years of software engineering experience
Knowledge and exposure to distributed production systems i.e Hadoop is a huge plus
Knowledge and exposure to Cloud migration is a plus
Proficiency in Linux
Fluency in Python, Experience in Scala/Java is a huge plus
Strong understanding of RDBMS, SQL;
Passion for engineering and computer science around data
Willingness to participate in 24x7 on-call rotation
What we offer:
401(k) Match and free access to a financial advisor
Generous paid vacation/company holidays
Vacation reimbursement (we give you $500 to take vacation), sabbatical, pawternity leave, marriage leave, honeymoon bonus
Comprehensive healthcare with 100%-paid medical, vision, life & disability insurance
$2,000 annual training and development budget
Complimentary annual memberships to One Medical, NY Citi Bike and SF Ford GoBike
Monthly chair massages
Free fitness classes (spin, yoga, boxing)
Gym reimbursement, local gym membership discounts
Onsite flu shots, dental cleanings and vision exams
Annual company retreat
Paid parental leave and a lot of new parent perks
Emergency childcare credits
Volunteer Time Off and Donation Matching, ongoing group volunteer opportunities
Team lunches, Sip & Social Thursdays, Game Nights, Movie Nights
Healthy snacks and drinks
And there's a lot more!"
Data Engineer,"Spotify is looking for a Data Engineer, to join our Premium team in New York. Spotify Premium is the world's largest audio-anchored subscription business, reaching over 100M users globally.We are looking for an ambitious individual to join a stellar team that's leading the efforts to further expand our customer offerings for Spotify Premium, and help us deliver a best-in-class Premium experience to the world's music fans. What does the music landscape look like in 2023? How will people listen to music in a world of voice controlled UI, autonomous cars and AR? With a focus on innovation projects, you will be part of a small, agile, and constantly evolving team, working to identify and execute upon growth opportunities that meet our ambitious commercial and user experience goals.What you'll do:* Ingest and aggregate data from both internal and external data sources to build our world class datasets.* Build large-scale batch and real-time data pipelines with data processing frameworks like Scio, Storm, or Spark on the Google Cloud Platform.* Leverage best practices in continuous integration and delivery.* Help drive optimization, testing, and tooling to improve data quality.* Collaborate with other engineers, ML experts, and stakeholders, taking learning and leadership opportunities that will arise every single day.* Work in cross functional agile teams to continuously experiment, iterate, and deliver on new product objectives.* Work from our offices in New York, with some travel to other Spotify office locations.Who you are:* You know how to work with high volume heterogeneous data, preferably with distributed systems such as Hadoop, BigTable, and Cassandra.* You have experience with one or more higher-level JVM-based data processing frameworks such as Beam, Dataflow, Crunch, Scalding, Storm, Spark, or something we didn't list- but not just Pig/Hive/BigQuery/other SQL-like abstractions* You are knowledgeable about data modeling, data access, and data storage techniques.* You understand the value of collaboration within teams, are excellent communicators, and can build relationships with a diverse set of stakeholders.* Experience with data ingestion via API and/or web scraping/crawling (e.g. Selenium, BeautifulSoup) at scale preferred.* Experience with Google Cloud Platform is a plus.You are welcome at Spotify for who you are, no matter where you come from, what you look like, or what's playing in your headphones. Our platform is for everyone, and so is our workplace. The more voices we have represented and amplified in our business, the more we will all thrive, contribute, and be brilliant. So bring us your personal experience, your perspectives, and your background. It's in our differences that we will find the power to keep revolutionizing the way the world listens.Spotify transformed music listening forever when we launched in 2008. Our mission is to unlock the potential of human creativity by giving a million creative artists the opportunity to live off their art and billions of fans the opportunity to enjoy and be inspired by these creators. Everything we do is driven by our love for music and podcasting. Today, we are the world's most popular audio streaming subscription service with a community of more than 286 million users."
Data Engineer,"About Freestar:

Freestar engineers cutting-edge monetization solutions for websites. By combining industry-leading technology, data, and massive scale, we enable busy site owners to seamlessly maximize revenue while freeing themselves of the hassles of ad operations. Publishers then have more time to do what they do best: create content.

Data Engineer Job Responsibilities:

Joining our data team, you will have an opportunity to learn and work with modern tools like Airflow and Druid to ensure a seamless stream of data where we need it. As we are a startup environment, you'll likely pick up some software engineering skills too, however the primary focus on the role is on data engineering. We're looking for someone who:
Develops and maintains scalable data pipelines and builds out new API integrations to support continuing increases in data volume and complexity.
Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.
Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.
Writes unit/integration tests, contributes to engineering wiki, and documents work.
Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues.
Works closely with a team of frontend and backend engineers, product managers, and analysts.
Defines company data assets (data models), SQL, Airflow to populate data models.
Designs data integrations and data quality framework.
Build dashboards that concisely and succinctly convey business metrics.
Designs and evaluates open source and vendor tools for data lineage.
Works closely with all business units and engineering teams to develop strategy for long term data platform architecture.
Data Engineer Qualifications / Skills:
Knowledge of best practices and IT operations in an always-up, always-available service
Experience with or knowledge of Agile Software Development methodologies
Excellent problem solving and troubleshooting skills
Process oriented with great documentation skills
Excellent oral and written communication skills with a keen sense of customer service
Education, Experience
Ample relevant knowledge and experience. You either have a BS or MS degree in Computer Science or a related technical field, OR certification from a data science bootcamp + 2 years of experience in a role as a data engineer
Proficiency in Python and Java, Scala, or Go development experience
4+ years of SQL experience (Strong SQL required)
Familiarity with BI reporting tools like Tableau, Looker.
4+ years of experience with schema design and dimensional data modeling
Ability in managing and communicating data warehouse plans to internal clients
Experience designing, building, and maintaining data processing systems
Experience working with either a Map Reduce or an MPP system on any size/scale
Experience/knowledge of cloud computing platforms like AWS/GCP would be a plus
We'd also like to see:
Excellent interpersonal and problem solving skills with the ability to communicate with team members to deliver actionable results
Comfortable interacting across multiple teams and management levels within the organization
Previous background in the ad tech or media landscape (linear, digital, or social) is a plus
What you can expect in return:
Full-Time, Salaried Position
Medical, Dental, and Vision benefits
401K with company match, vested immediately
The opportunity to be part of something BIG
Freestar is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.

This role is not eligible for visa sponsorship"
Data Engineer,"Databases are the beating heart of every business in the world.

Cockroach Labs is the team behind CockroachDB, an open source, distributed SQL database. In addition, to the open source version of the DB we offer CockroachCloud, a self-service, fully managed cloud offering of CockroachDB. We aim to build infrastructure that keeps pace with the world, so developers can focus on what matters most: building the best products. Join us on our mission to Make Data Easy.

About the Role

We're looking for an experienced Data Engineer to join our team. We collect data on the usage of CockroachCloud and CockroachDB. This data will feed into our Product, Sales, and Support decisions. You will be the founding member of our Data Engineering team, where you will provide the future roadmap of data collection at Cockroach Labs

You Will
Providing and maintaining the code for the collection and processing of data related to CockroachCloud and CockroachDB.
Implementing the schema design for our Data Warehouse to facilitate business insights and product operations.
Managing ETL pipelines, workflows to collect, process, and load data into tools such as Segment, Snowflake, Looker, Salesforce, and Zendesk.
Collaborating with Product, Marketing, Design, and Engineering to help drive adoption of our product.
The Expectations

In the first month, you will become an integrated member of our engineering team. You'll become familiar with the data we collect to drive our product decisions and run our operations. We believe it is important to understand how people use CockroachCloud and CockroachDB in order to build a product our users and developers love.

After 3 months, you'll be a fully-fledged member of the team. You will be comfortable with the data collection needs of the company and are shaping their best practices. You will be responsible for determining how we can get the data needed to answer questions posed about the product and the future of the product roadmap. You will be the founding member of our Data Engineering team, where you will provide the data processing roadmap at Cockroach Labs.

You Have
A passion for working on complex technical products, have experience collecting and processing large amounts of data and determining its underlying meaning.
Comfort using programming languages like Go, C/C++, Java, and Python. Experience building collaborative relationships with your colleagues. You enjoy being part of the code review process and partnering with your teammates on challenging problems.
Comfort with SQL style query languages.
Experience with tools used to collect telemetry data.
5 or more years experience with data collection pipelines and schema design.
A BS in Computer Science or equivalent experience.
The Team

Reporting to Kendra Curtis - Director of Engineering, CockroachCloud

Kendra has 20+ years of experience in all levels of the software stack, from early days writing firmware for wireless networking products at Wi-LAN, to managing teams of developers building Web Applications at Google. Kendra worked on Google's early data centers. She was a member of the Management team responsible for the integration of DoubleClick (DCLK) into Google. She was a Co-founder and CEO of Scout It Out, a listing service for Rehearsal Spaces. Kendra joined Cockroach Labs because she loves building great teams and values work-life balance. Outside of work she enjoys skiing, acting, and walking in the park with her dog, Lady.

Isaac Wong - VP of Engineering

Isaac is responsible for the health of the engineering organization at Cockroach Labs. He partners closely with teams to ensure we have a balanced culture that promotes quality and innovation in pursuit of our goals. Before joining Cockroach Labs Isaac was in life sciences for 16 years with Medidata Solutions where he had a front row seat on the exciting ride from a 30 person startup to more than 2000 people worldwide. But the lure of distributed, resilient, and consistent SQL databases, along with the amazing technology and culture at Cockroach Labs proved too much. When not working he likes to draw, play the piano and search NYC for cannolis with his wife and kids.

Our Benefits
100% health insurance option (for you and your dependents!)
Paid parental leave (with baby bucks)
Flex Fridays
Flexible time off & flexible hours
Learning and Development budget
Relocation support
Cockroach Labs is proud to be an Equal Opportunity Employer building a diverse and inclusive workforce. If you need additional accommodations to feel comfortable during your interview process, please email us at accessibility@cockroachlabs.com."
Data Engineer,"Foursquare is the leading independent location technology platform, powering business solutions and consumer products through a deep understanding of location. Over 1,000 clients-including more than 50% of the Fortune 100-choose Foursquare.Foursquare's toolkit includes Placed powered by Foursquare, Audiences powered by Factual and Proximity powered by Factual, Analytics Solutions, Places, Pilgrim SDK and Pinpoint. Together, these products empower brands to measure foot traffic lift via multi-touch attribution; deliver proximity-based advertising; analyze and identify trends; drive deeper engagement via Foursquare's industry-leading developer tools, which have been selected by 200,000 developers including AccuWeather, Apple, Samsung, Microsoft, Snapchat, Twitter and Uber; and optimize advertising campaigns across hundreds of audiences. Over 14 billion consumer-verified place visit confirmations help us keep our map and models fresh and up-to-date.Foursquare has more than 400 employees based in New York headquarters with offices in Seattle, Los Angeles, San Francisco, Chicago, Singapore and London. In 2019 Foursquare acquired Placed from Snap Inc, and then in 2020 merged with Factual. Foursquare is proud to be funded by The Raine Group, Simon Ventures, Naver, Union Square Ventures, Andreessen Horowitz, DFJ Growth, Morgan Stanley Alternative Investment Partners and more.About the TeamThe data engineering team owns critical pieces of the machine learning and analytics platforms. This team helps to build data processing infrastructure to derive insights from billions of location data points every day. Help us build and collaborate with Product, Engineering, and Data Science teams to create tools and processes to bring research and machine learning models to production.Responsibilities of the role:* Influence key decisions on architecture and implementation of scalable data processing and analytics structure* Work with the Data Science team to bring machine learning models into production* Build Hadoop MapReduce and Spark processing pipelines using Java, Python, and Ruby* Build REST APIs for data access by systems across our infrastructure* Focus on performance, throughput, and latency, and drive these throughout our architecture* Write test automation, conduct code reviews, and take end-to-end ownership of deployments to production* Mentor junior engineering staffQualifications:* BS/BA in a technical field such as computer science or equivalent experience* 4-7 years of software development experience* Proficiency in Python, Java, C#, and/or Ruby* 3+ years of experience with Hadoop MapReduce and/or Spark data processing pipelines, analytics systems (e.g. OLAP, BI tools), and machine learning technologies* Experience operating systems in AWS* Excellent communication skills, including the ability to identify and communicate data-driven insightsFoursquare is proud to foster an inclusive environment that is free from discrimination. We strongly believe in order to build the best products, we need a diversity of perspectives and backgrounds. This leads to a more delightful experience for our users and team members. We value listening to every voice and we encourage everyone to come be a part of building a company and products we love.Foursquare is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected Veteran status, or any other characteristic protected by law."
Data Engineer,"Data Engineer

Job Details
Level
Experienced
Job Location
New York (Home Office) - New York, NY
Position Type
Full Time
Education Level
4 Year Degree
Salary Range
Undisclosed
Travel Percentage
Undisclosed
Job Shift
Day
Job Category
Information Technology
Description
Greater New York Mutual Insurance Company (""GNY"") is an A+ rated, financially stable and growing property casualty insurance company with locations throughout the Northeast. We are currently looking for a dynamic and highly motivated Data Engineer for our New York office.

Responsibilities:
Develop, construct, test, and maintain architectures, such as databases and analytic environments and platform required for structured, semi-structured and unstructured data
Design and develop data pipelines that deliver accurate, consistent, and traceable datasets for data science projects
Support regular and ad-hoc data needs for data scientists
Provide recommendations and implement ways to improve data reliability, efficiency, and quality
Qualifications
Bachelor’s or Master’s degree obtained from an accredited institution preferably in Computer Science, Computer Engineering, and Software Engineering, Data Science, or a related field
3-5 years of professional experience in data science or related field
Experience in database deployment and management and proficient in SQL
Experience in data warehousing and ETL (Extract, Transform, and Load)
Proficient in R, Python, VBA, Excel and Word
Excellent oral and written communication skills"
Data Engineer,"About Daily HarvestDaily Harvest makes nourishing food built on fruits and vegetables accessible. We do this by delivering thoughtfully sourced, chef-crafted food to customers' doorsteps, all ready to enjoy in minutes. We're on a mission to take care of food, so food can take care of you.Our team is collaborative, driven, and future-thinking. We're constantly learning, experimenting, and iterating, and celebrate failure just as much as success. We take risks, try new things, and we get things done. We love adaptogens and cruciferous vegetables but never say no to cake. Everything we do, we do in the service of our community.Position OverviewDaily Harvest is looking for a Data Engineer to join our Data team. The Data team uses data to create better customer experiences, smarter business decisions, and efficiencies company-wide. We do this by providing accurate, actionable analyses, building sophisticated data science products, and by maintaining a reliable data infrastructure.We are looking for a Data Engineer who is excited about building and maintaining reliable data infrastructure that enables stakeholders to conduct accurate, actionable analysis, and build sophisticated data science products that's ready to scale with a growing business. You will work with every part of the company and a wide variety of data sources. Currently, our main data stack is Alooma / Stitch, dbt, BigQuery, and Looker.What you'll do:* Design and build data models and new features that enable stakeholders to answer questions about our business and processes* Refactor existing data models to fit into our transformation layer while optimizing for cost and performance* Build, maintain, and optimize the backend of our business intelligence tool (we use Looker)* Develop tooling, testing, and processes to ensure our data infrastructure is robust and observable* Build and maintain Python pipelines to load data into our warehouse* Translate business asks into technical requirementsWho you are:* Strong SQL skills (experience using dbt is a plus)* Experience developing LookML* Strong communication and stakeholder management skills* Ability to thrive in a fast-moving, evolving, and high growth environment* Experience using Git and GitHub to collaborate on repositories* Experience working with managed ETL services (e.g. Alooma, FiveTran, Stitch)* Experience using cloud services (we use Google Cloud Platform, BigQuery experience is a plus)* Experience implementing CI / CD to test and deploy repositories* Experience using Airflow to orchestrate data pipelines* Experience using Docker to deploy tasks* Experience using Python to extract, clean, and load dataBenefits* Unlimited Daily Harvest to keep you hustling, not hangry + cold brew (...always stocked)* Flexible time-off policy + flexible working hours (Unlimited PTO Plan)* Competitive medical, dental, and vision benefits, 401K + equity participation* Ancillary benefits: Commuter, Gym membership + Citi Bike discounts* Access to everything we make (including recipes in development)* Annual company retreat* Quarterly team outings, weekly onsite happy hours, + regular DH team gatherings to celebrate our co-workers* Book club + running club* Showers for post-morning or mid-workday workouts* A dynamic, ambitious, and fun work environment + dog friendly!!At Daily Harvest, our mission is to take care of food, so that food can take care of you. And it wouldn't be possible without our team. We celebrate the unique POV that each person brings to the table and believe in a collaborative and inclusive environment. As an equal opportunity employer, we prohibit any unlawful discrimination on the basis of race, color, religion, military or veteran status, sex, gender, gender identity or expression, sexual orientation, national origin, age, disability or genetic information. These are our guiding principles and apply across all aspects of employment."
Data Engineer,"Job Description
We are passionate about data. We collaborate to build elegant, effective, scalable and highly reliable solutions to empower predictive modelling in finance.
Cubist’s data services group is looking for a Data Engineer to join our dedicated team. Our group is responsible for the timely delivery of comprehensive and error-free data to some of the most demanding and successful systematic and discretionary Portfolio Managers in the world.
This exceptional individual will be a member of a small team of Data Engineers, Data Scientists, and Data Analysts who play a vital role in ensuring the smooth day-to-day implementation of a large research infrastructure, and the live production trading of billions of dollars of capital across global capital markets, including equities, futures, options and other financial instruments.
Job Responsibilities
Building processes and technology tools to deliver data to and support data for discretionary portfolio managers and other teams.
Data onboarding project management for discretionary portfolio managers.
Designing, developing, maintaining, and supporting datasets used firm wide, including Security Master, Risk Models, Pricing, and Corporate Actions.
Monitoring and enhancing the automated data collection and cleansing infrastructure.
Researching new technologies for improved data management and efficient data retrieval.
Desirable Candidates
Ph.D. or Masters in computer science, mathematics, physics, statistics, or other discipline involving rigorous fundamental and/or quantitative analysis techniques.
Ideal candidate will have at least 1 year of experience as an Analyst for a discretionary portfolio manager or in a similar role.
Experience working with large data sets, including classification, regression, and distribution analysis.
Experience applying statistical tests to large data sets.
Programming skills in SQL, TSQL, SQL Server, or PL-SQL.
Programming skills in Python and at least one of C#, C++, or Java.
Web/GUI development experience using Microsoft technologies is a plus.
Experience dealing with intraday, tick and order book data is a plus.
Strong problem solving skills.
Intellectual curiosity and a love of learning.
Attention to detail and a love of process.
Strong oral and written communication skills."
Data Engineer,"Join SADA as a Data Engineer!

Your Mission

As a Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data issues facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring a combination of batch or streaming data pipelines, data lakes and data warehouses.

You will be recognized as an established contributor by your team. You will contribute design and implementation components for multiple projects. You will work mostly independently with limited oversight. You will also participate in client-facing discussions in areas of expertise.
Pathway to Success

#BeOneStepAhead: At SADA we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage adaptability. This means that not only do our engineers understand that change is inevitable, but they embrace this change to continuously broaden their skills, preparing for future customer needs.

Your success comes from your enthusiasm, insight, and positive impact. You will be given direct feedback quarterly with respect to the scope and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, your collaboration with your peers, and the consultative skill you demonstrate in customer interactions.

As you continue to execute successfully, we will build a personalized development plan together that leads you through the engineering or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with a first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:
Google Professional Data Engineer Certified or able to complete within the first 45 days of employment

Required Qualifications:
Expertise in at least one of the following domain areas:
Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or other customer-facing role

Useful Qualifications:
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Applied experience operationalizing machine learning models on large datasets
Knowledge and understanding of industry trends and new technologies and ability to apply trends to architectural needs
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem

About SADA

Values: We built our core values on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing

Work with the best: SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the 2018 Global Partner of the Year. SADA has also been awarded Best Place to Work by Inc. as well as LA Business Journal!

Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, RRSP, professional development reimbursement program as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."
Data Engineer,Job Description Bachelorrsquos Degree in Computer Science or a related discipline 5+ years of applicable engineering experience Strong proficiency in Python with an emphasis in building data pipelines Ability to write complex SQL to perform common types of analysis and aggregations Experience with Apache Airflow or Google Composer Detail-oriented and document all the work Ability to work with others from diverse skill-sets and backgrounds Nice to have Experience with version control systems (Git and Bitbucket) Experience with Atlassian products Jira and Confluence Experience with Docker containerization knowledge of Application Programming Interfaces
Data Engineer,"Peloton is looking for a Data Engineer to build our Data Warehouse and Data Pipelines. You will work with multiple teams of passionate and skilled data engineers, architects, and analysts responsible for building batch and streaming data pipelines that process terabytes of data daily and support all of the analytics, business intelligence, data science and reporting data needs across the organization.Peloton is a cloud first engineering organization with all of our data infrastructure in AWS leveraging EMR, AWS Glue, Redshift, S3, Spark. You will be interacting with many business teams including marketing, sales, supply chain, logistics, finance and partner to scale Peloton's data infrastructure for future strategic needs.Responsibilities* Understand the data needs of different stakeholders across multiple business verticals including Finance, Marketing, Logistics, Product etc.* Develop the vision and map strategy to provide proactive solutions and enable stakeholders to extract insights and value from data.* Understand end to end data interactions and dependencies across complex data pipelines and data transformation and how they impact business decisions.* Design best practices for big data processing, data modeling and warehouse development throughout the company.Requirements* Familiar with at least one of the programming languages: Python, Java.* Comfortable with Linux operating system and command line tools such as Bash.* Familiar with REST for accessing cloud based services.* Excellent knowledge about databases, such as PostgreSQL and Redshift.* Has experiences with GIT, Github, JIRA and SCRUM.* 2+ years in building a data warehouse and data pipelines. Or, 3+ years in data intensive engineering roles.* Experience with big data architectures and data modeling to efficiently process large volumes of data.* Background in ETL and data processing, know how to transform data to meet business goals.* Experience developing large data processing pipelines on Apache Spark.* Experience with Python or Java programming languages.* Strong understanding of SQL and working knowledge of using SQL(prefer PostgreSQL and Redshift) for various reporting and transformation needs.* Excellent communication, adaptability and collaboration skills.* Experience running Agile methodology and applying Agile to data engineering.* Experience with Java, JDBC, AWS, SDKNice to have* Familiar with AWS ecosystem, including RDS, Glue, Athena, etc.* Has experiences with Apache Hadoop, Hive, Spark and PySpark.ABOUT PELOTON:Founded in 2012, Peloton is a global interactive fitness platform that brings the energy and benefits of studio-style workouts to the convenience and comfort of home. We use technology and design to bring our Members immersive content through the Peloton Bike, the Peloton Tread, and Peloton Digital, which provide comprehensive, socially-connected fitness offerings anytime, anywhere. We believe in taking risks and challenging the status quo by continuously innovating and improving. Our team is made up of passionate brand ambassadors, and we know that together, we go far.Headquartered in New York City, with offices, warehouses and retail showrooms in the US, UK and Canada, Peloton is changing the way people get fit. Peloton has been named to many prestigious industry lists, including Fast Company's Most Innovative Companies, CNBC's Disruptor 50, Crain's New York Business' Tech25 and Fast50, as well as TIME's Genius Companies. Visit www.onepeloton.com/careers to learn more about joining our team."
Data Engineer,"Data profiling - Create physical Database design Big Data Implementing Change data capture Python Development Spark, Scala AWS"
Data Engineer,"Requisition no: 503035
Work type: Full Time
Location: Medical Center
School/Department: Biomedical Informatics
Grade: Grade 105
Categories: Information Technology, Research (Lab and Non-Lab)
Position Summary

The Department of Biomedical Informatics (DBMI) at Columbia University is revolutionizing the clinical research enterprise with the help of information technology. At DBMI, we are building the infrastructure of the future to support and enable better research and dissemination. We have an immediate opening for a talented and self-motivated data engineer developer who can succeed in a collaborative work environment. The ideal candidate will have experience with data pipelines and cloud environments. The candidate will be responsible for data processing, data exchange/transfer/load (ETL), data visualization, DevOps, and software architecture. The ideal candidate will have professional experience in a number of programming languages, databases, and development environments. The candidate should be able to contribute to improving the reliability and quality of data. Experience in clinical medicine, clinical vocabulary, and cloud development are not required but preferred. The successful candidate will contribute to the development of open source solutions together with a community of international researchers.

Current available position is grant-funded.

Columbia University's Department of Biomedical Informatics is internationally recognized as one of the best programs of its kind. Our mission is to improve health for society by focusing on discovery and impact: we develop new informatics methods, enrich the biomedical knowledge base, and enhance the health of the population.

Responsibilities
Software and system design, implementation, and testing (75%)
Application deployment and configuration (10%)
Communicate with technical individuals at various grant sites (10%)
Software requirements specification (5%)
Minimum Qualifications

Bachelor's degree or equivalent in education and experience (computer science, biomedical informatics, information science), plus four years of related experience.

Other Requirements

Great communication skills; Experience with one or more compiled programming languages (e.g. Java, Scala, C#, C++, etc.) and one or more interpreted programming languages (Python, JavaScript, Perl, bash, etc.)

Working knowledge of SQL; Experience with big data, NoSQL databases, and health care data a plus.

Equal Opportunity Employer / Disability / Veteran

Columbia University is committed to the hiring of qualified local residents.

Applications open: Sep 12 2019 Eastern Daylight Time Applications close:

Back Apply Share"
Data Engineer,"ABOUT BARKBARK is a company building products, experiences, and entertainment for dogs and the people who love them. The lasting brand that Disney has built for kids and families, BARK is building for the fast-growing market of dog people.We launched in 2011 with BarkBox, a monthly themed subscription of all-natural treats and clever toys. Since then, we've become the fastest growing pet company and have shipped more than 70 million toys and treats to the dogs across the world. We have also expanded to new areas including preventative health, and food. Our ambition-level is high, the opportunity is huge, and our love for dogs is through the roof!WHO WE'RE SNIFFIN' FOR:We are currently looking for a Data Engineer to help design, build and grow our systems for ingesting and processing data that drives decisions for our cross functional teams. This person will work closely with our data analysts and data scientists and ensure the pipelines and datasets are secure, scalable, correct and complete.DOODIES:Design, create and maintain reliable data pipelines relating to financial eventsImplement solutions required for optimal extraction, transformation and loading of data from various internal and external data sources such as Databases, APIs, SFTP, S3, etc.Identify and improve existing processes with considerations to automation, accuracy, security and delivery.Interpret and visualize data, identify issues & complexities, translate nuances to production quality code, and report key findings to inform the direction of projectsStrong competency in working collaboratively and communicating effectively with various departments across the organization, including the finance, engineering and technology teams, in order to create alignment on processes that impact dataPAWFERRED QUALIFICATIONS:4+ years'experience writing readable, modular, production quality Python code to handle varied and complex data2+ years'experience writing SQL; Redshift or Postgres preferredProfessional experience working with Financial DataExperience with Workload Management Systems, such as Airflow or LuigiModerate knowledge of AWS cloud services: EC2, EMR, RDS, RedshiftFamiliarity with Kubernetes & DockerModerate grasp of bash / LinuxProficient in using GitCreative problem solver who focuses on improving business and productStart up experience at an internet-based company a plus!Must love dogs"
Data Engineer,"Job Description


The New York Times is seeking inventive and motivated data engineers at all levels of experience to join the Data Engineering group. In this role, you will build critical data infrastructure that surfaces data and insights across the company.

About Us

Our Data Engineering teams are at the intersection of business analytics, data warehousing, and software engineering. As Maxime Beauchemin wrote in “The Rise of Data Engineering”, ETL and data modeling have evolved, and the changes are about distributed systems, stream processing, and computation at scale. They’re about working with data using the same practices that guide software engineering at large.A strong data foundation is essential for The New York Times and we’re responsible for it. We use our data infrastructure to power analytics and data products and to deliver relevant experiences to our customers in real-time. We enable our company to validate strategic decisions, make smarter choices, and react to the fast changing world. We are part of a New York based technology organization with a remote-friendly workplace that includes engineers around the world. We value transparency and openness, learning, community, and continuous improvement. Check out the Times Open blog, which is written by engineers and other technical team members, and follow @nytdevs on Twitter to see what we’re up to.

About the Job

We focus on the software engineering related to data replication, storage, centralized computation, and data API’s. We provide customers and partners with data tools, shared frameworks, and data services. These are the foundational core of our group which enables ourselves and others to work with data from a common underpinning. Our tools and services enable our group to scale and avoid blocking others.We reduce data redundancy by creating systems and datasets that serve as sources of record. We enable discovery and governance of our data. We support key business goals like growing our digital subscriber base, understanding how our customers use our products, and retaining our print subscribers.

As a data engineer, you will:
Run and support a production enterprise data platform
Design and develop data models
Work with languages like Java, Python, Go, Bash, and SQL
Build batch and streaming data pipelines with tools such as Spark, Airflow, and cloud-based data services like Google’s BigQuery, Dataproc, and Pub/Sub
Develop processes for automating, testing, and deploying your work
About You

To thrive in this role, you are excited about data and motivated to learn new technologies. You are comfortable collaborating with engineers from other teams, product owners, business teams, and data analysts and data scientists. You are own and shape your technical domain area and move the related business goals forward. You are eager to resolve upstream data issues at the source instead of applying workarounds. You analyze and test changes to our data architectures and processes, and determine what the possible downstream effects and potential impacts to data consumers will be.

Benefits and Perks:
Make an impact by supporting our original, independent and deeply reported journalism.
We provide competitive health, dental, vision and life insurance for employees and their families
We support responsible retirement planning with a generous 401(k) company match.
We offer a generous parental-leave policy, which we recently expanded in response to employee feedback. Birth mothers receive 16 weeks fully paid; non gestational parents receive 10 weeks, also fully paid.
We are committed to career development, supported by a formal mentoring program and $8,000 annual tuition reimbursement.
We have frequent panel discussions and talks by a wide variety of news makers and industry leaders.
Join a community committed to the richness of diversity, experiences and talents in the world we cover, supported by a variety of employee resource groups.
#LI-AM1

The New York Times is committed to a diverse and inclusive workforce, one that reflects the varied global community we serve. Our journalism and the products we build in the service of that journalism greatly benefit from a range of perspectives, which can only come from diversity of all types, across our ranks, at all levels of the organization. Achieving true diversity and inclusion is the right thing to do. It is also the smart thing for our business. So we strongly encourage women, veterans, people with disabilities, people of color and gender nonconforming candidates to apply.

The New York Times Company is an Equal Opportunity Employer and does not discriminate on the basis of an individual's sex, age, race, color, creed, national origin, alienage, religion, marital status, pregnancy, sexual orientation or affectional preference, gender identity and expression, disability, genetic trait or predisposition, carrier status, citizenship, veteran or military status and other personal characteristics protected by law. All applications will receive consideration for employment without regard to legally protected characteristics. The New York Times Company will consider qualified applicants, including those with criminal histories, in a manner consistent with the requirements of applicable state and local \""Fair Chance\"" laws."
Data Engineer,"Overview:

We're hiring an experienced Data Engineer to join our growing BI Engineering team to elevate Glossier as an inviting, fun way to buy beauty products. You'll join a team with the mission of empowering decision-making across the organization with self-service data, working closely with our Analytics and Data Science teams.

As a Data Engineer, you are responsible for delivering and maintaining highly available computing platforms and creating data integration services. You will own the systems that collect and transform data from a number of sources, storing data in highly optimized data warehouses. You will also maintain ongoing reliability, performance and support of the infrastructure. This includes monitoring the computing environments and providing solutions based on application needs and anticipated growth. A successful candidate will combine strong technical skills, a passion for creative problem solving, and an intense curiosity.

At Glossier we are also mindful of building an inclusive culture, where decisions are made transparently and we support each other's learning and growth. Data is a critical component at Glossier and ensuring consistent, reliable access to our data is a significant strategic priority. We expect you to have good communication skills and collaborate across functions to deliver robust solutions in correspondence with the business needs of the company.

Our Data Stack:
Data Warehousing with Snowflake, AWS Aurora, Redshift
AWS for serving infrastructure
Python, JavaScript, and TypeScript
DBT and Luigi for ETLs
Fivetran & Stitch
Segment
Looker
Docker
6 Month Expectations:
Have contributed to the data pipeline process, creating new custom integrations that bring data into our systems
Are comfortable working with our data stack
Have taken an active role in designing the future of our data engineering system
Collaborate with team on best practices and overall business strategy
12 Month Expectations:
Deliver data engineering system-including data pipeline and data warehousing components-that is simple, reliant, and performant
Have made numerous contributions to our data pipelining system and data monitoring
Have taken a lead role in managing the development and architecture design of our data infrastructure
Preferred Qualifications:
3 - 5 years of experience working in software development, data engineering, or related STEM fields
3+ years of working experience with various relational databases and data warehousing
Strong programming skills in Python and SQL
Practical experience in best practices for developing data pipelining frameworks
Experience in Linux
Ability to learn autonomously and quickly
Analytical, creative and commercial mindset
Extremely organized and detail-oriented with effective multitasking and prioritization skills
Highly motivated, willing to take ownership of work, drive to solve problems and work effectively under pressure
Excellent written and verbal communication skills, willing to proactively engage other team members in fostering a strong collaborative team-oriented environment
About Glossier

Glossier is a beauty company that lives in NYC, is sold on the internet, and promotes a skincare first philosophy that celebrates beauty in real life.

We are an Equal Employment Opportunity (""EEO"") Employer. It has been and will continue to be a fundamental policy of Glossier not to discriminate on the basis of race, color, creed, religion, gender, gender identity, pregnancy, marital status, partnership status, domestic violence victim status, sexual orientation, age, national origin, alienage or citizenship status, veteran or military status, disability, medical condition, genetic information, caregiver status, unemployment status or any other characteristic prohibited by federal, state and/or local laws. This policy applies to all aspects of employment, including hiring, promotion, demotion, compensation, training, working conditions, transfer, job assignment, benefits, layoff, and termination.

Global Job Applicant Privacy Policy

Last Updated: November 25, 2019

This Global Job Applicant Privacy Policy (""Policy"") describes how Glossier, Inc. and our subsidiaries and affiliates (collectively, ""Glossier,"" ""we"" and/or ""our"") collect, use, disclose, transfer and store (collectively, ""process"") personal information about you in connection with our recruitment activities.

While this Policy is intended to describe the broadest range of our processing activities globally, those activities may be more limited in some jurisdictions based on local laws. For example, the laws of a particular country may limit the types of personal information we can collect or the manner in which we use that information. In those instances, we adjust our internal policies and practices to reflect the requirements of local law. The data controller in each case will be the Glossier entity to which the applicant submits his or her application, as specified below.

I. APPLICABILITY OF OTHER POLICIES

This Policy does not cover our processing of information collected from you as a Glossier customer or as a visitor to Glossier-affiliated websites. To learn more about Glossier's data collection practices in these cases, please see our Glossier Privacy Policy.

II. INFORMATION WE COLLECT

We collect information in connection with your application to work with us, the categories of personal information we may process about you include:
Information you provide on our application forms, including full name, telephone number, personal email address, gender, location, availability, employment history (including whether you have previously worked for Glossier), qualifications, references, LinkedIn profile and website (if provided voluntarily), work authorization status, and how you heard about the job;
Information you provide to us in your resumé, cover letter and any other files you choose to upload or share with us regarding your qualifications, such as design portfolios;
Information you provide to us during an interview or that we collect through the recruitment process (g. work authorization status, willingness to relocate, salary expectations, type of employment contract, interview notes, results of any assessment);
Reference information and/or information received from background checks if you are offered a job (where applicable), including information provided by third parties such as past employers, educational institutions and references; and
Information about your educational and professional background from publicly available sources, including online, that we believe is relevant to your application (g. your LinkedIn profile).
Your decision to apply for a position and provide your personal information to us is voluntary. We will tell you if information is required to move forward with your application.

Sensitive Information: In certain countries, where permitted by law and on a voluntary basis, we may ask questions about race or ethnicity, veteran status and disabilities for specific purposes, such as to accommodate a disability or illness and to comply with legal obligations relating to diversity and anti-discrimination. You are entirely free to decide whether or not to provide such information and your application will not be affected either way. Except as specifically requested, we ask that you avoid submitting information which may qualify as sensitive information under applicable law, including race, religion, ethnicity, nationality, age, gender identity, sexual life or sexual orientation, medical or health information, genetic or biometric data, political opinions, political party or trade union membership and judicial data such as criminal records.

Information About Others: If you provide us with personal information of a reference or any other individual as part of your application, it is your responsibility to obtain consent from that individual prior to providing such information to us.

III. HOW WE COLLECT YOUR INFORMATION

Most of the personal data we process is obtained directly from you, such as when you submit a job application or when we conduct a phone or in-person interview. We may also receive information about you from other sources, such as from your named references, persons who referred you for a position, from background checks (if applicable), recruiting agencies, third party recruitment sources and websites and publicly available sources such as your LinkedIn profile.

When you visit our sites, including our Careers webpage, we collect certain information automatically. To collect this information, we may use cookies, web beacons, and similar technologies. A ""cookie"" is a text file that websites send to a visitor's computer or other internet-connected device to uniquely identify the visitor's browser or to store information or settings in the browser. A ""web beacon,"" also known as a pixel tag or clear GIF, is used to transmit information back to a web server. We may also collect information about your online activities over time and across third-party websites. The information we collect automatically may include:
URLs that refer visitors to our websites;
Search terms used to reach our websites;
Details about the devices that are used to access our websites (such as IP address, browser information, device information, and operating system information);
Details about your interaction with our websites (such as the date, time, length of stay, and specific pages accessed during your visits to our websites, and which emails you may have opened); and
Usage information (such as the number and frequency of visitors to our websites).
We may associate this information with your Glossier account if you have one, the device you use to connect to our Services, or email or social media accounts that you use to engage with Glossier.

For more information about how we use cookies click here.

IV. HOW WE USE YOUR INFORMATION

We use your personal information to evaluate a potential employment relationship with you and for other business purposes. Such uses include:
Assessment of your skills, qualifications, and suitability for the role;
Communication with you about the recruitment process;
Verification of your information and completion of reference and/or background checks (where applicable) if we offer you a position;
Retention of records related to our hiring processes, including a record of the name of unsuccessful applicants, the date of their application and the reason that their application was not successful, in order to streamline future hiring processes;
Legal and compliance purposes, such as responding to suspected fraud, security incidents, or other illegal activity, protecting Glossier's and others' rights and property, exercising a legal claim, cooperating with law enforcement investigations and complying with applicable laws, regulations, legal processes or governmental requests;
Other uses with your consent, which you may withdraw at any time; and
Other legitimate interests, including our interests in considering candidates for current and future employment opportunities and in managing and improving our recruitment and hiring process.
If we hire you, information we collect in connection with your application will become part of your employment record and used to manage the onboarding process and for other employment-related purposes in accordance with our internal employee privacy policy.

V. WHO MAY HAVE ACCESS TO YOUR INFORMATION

Within Glossier: We may disclose your personal data to Glossier personnel and affiliates who need to know the information, including personnel in the recruiting, human resources and information technology departments, and in the department responsible for the position for which you are applying.

Third-Party Service Providers: We may use third party service providers acting on Glossier's behalf to perform some of the services described above. For example, we share certain information with service providers who facilitate our applicant tracking system, video interviews, travel booking and expenses, reporting and analytics and verification/background checking services. We also may share information about you with recruitment agencies working with us in relation to your recruitment as well as with our professional advisors, including accountants, auditors, lawyers, insurers and bankers. These service providers may change over time, but we will always use trusted service providers who we require to take appropriate security measures to protect your personal information in line with our policies. We only permit them to process your personal information for specified purposes and, as appropriate, in accordance with our instructions and the provisions of this Policy and applicable law.

Other Third Parties: In certain limited circumstance, we share and/or are obligated to share your personal information with other third parties, including (a) to comply with our obligations, to protect the rights and property of Glossier, our customers and the public, to cooperate with law enforcement investigations, and to detect and respond to suspected illegal activity and threats to the health or safety of any person or of our systems or services; (b) in connection with, or during negotiations of, any merger, joint venture, sale of company assets, financing, or acquisition of all or a portion of our business, assets or stock by another company (including in connection with any bankruptcy or similar proceedings); and/or (c) with your consent and at your direction.

We may also share aggregated or de-identified information, which cannot reasonably be used to identify you.

IV. DATA RETENTION

If your application for employment is unsuccessful (or you withdraw from the process or decline our offer), we will retain your information for a reasonable period of time beyond the end of the application process for the purposes described above, including complying with our legal obligations, resolving disputes and as necessary for our legitimate interests, such as to consider you for other current and future employment opportunities at Glossier. If you do not want us to contact you regarding other roles, please contact recruiting@glossier.com. After this period, we will securely destroy your personal information in accordance with applicable laws and regulations.

If your application for employment is successful, personal information gathered during the recruitment process will be retained during your employment in accordance with our internal employee privacy policy and retention policies.

VII. YOUR RIGHTS

You may have certain rights under U.S. and international privacy laws in relation to your personal information. This may include the right to access, rectify, port or erase certain personal information we have about you. You may also have the right to object to and restrict certain processing of your data. Certain information may be exempt from such requests pursuant to applicable data protection laws. You can contact privacy@glossier.com to exercise your rights in relation to your personal information. We will respond to your request consistent with applicable law.

VIII. CALIFORNIA RESIDENTS

If you are a California resident, the California Consumer Privacy Act (""CCPA"") requires us to disclose the following information with respect to our collection and use of personal information.

Categories of Personal Information Collected: Over the preceding 12 months, we have collected the following categories of personal information: (1) identifiers, (2) characteristics of protected classifications under California or U.S. law, (3) internet or other electronic activity information, (4) audio, electronic, visual, thermal, olfactory, or similar information, (5) professional or employment-related information, (6) education information, (7) inferences, and (8) other information that identifies, relates, to, describes, or is otherwise reasonably capable of being associated with you. For examples of the precise data points we collect, please see ""Information We Collect"" [link] above.

Business Purposes for Collecting and Disclosing Information: We collect each category of personal information for the business purposes in the ""How We Use Your Information"" section above.

IX. EUROPEAN RESIDENTS

If European privacy laws apply to you, our processing of personal information for the purposes mentioned above is based on the following legal grounds:
As necessary to evaluate and potentially enter into an employment relationship with you;
With your consent, which you may withdraw at any time;
To comply with our legal obligations;
Where necessary to protect your vital interests or those of others; and
For our (or others') legitimate interests, including our interests in considering candidates for current and future employment opportunities and in managing and improving our recruitment and hiring process, unless those interests are overridden by your interests or fundamental rights and freedoms.
We will only use your personal information for the purposes for which we collected it, unless we reasonably consider that we need to use it for another reason and that reason is compatible with the original purpose. If we need to use personal information for an unrelated purpose, we will notify the relevant individual and we will explain the legal basis which allows us to do so. Where the collection or processing of personal information is based on your consent, you may withdraw your consent at any time to the extent permitted by applicable law.

X. INTERNATIONAL DATA TRANSFERS

Due to the global nature of our business, Glossier may transfer your personal information across international borders, consistent with applicable data protection laws, including to the U.S., Canada and European Economic Area (""EEA""). Where personal information is transferred within Glossier to countries outside of the EEA that are not recognized as providing an adequate level of protection under European privacy laws, we do so through a series of intercompany agreements that implement the Standard Contractual Clauses authorized under European privacy laws. We also use a variety of safeguards to ensure that your personal information is adequately protected when processed by our third-party service providers operating in the U.S. or another country outside of the EEA including by signing EU standard contractual clauses or verifying the recipient adheres to the EU-U.S. Privacy Shield and Swiss-U.S. Privacy Shield Framework. You may request additional information concerning such safeguards from the Privacy team by contacting privacy@glossier.com.

XI. SECURITY

Glossier is committed to protecting the security of your personal information and ensuring a level of security appropriate to the risk our data processing presents. Taking into account the costs of implementation, the sensitivity of the data and nature of the data processing, Glossier has implemented organizational, technical and administrative measures to prevent the unauthorized access, destruction, loss, alteration or misuse of personal information.

XII. DATA CONTROLLER

If you apply to a position in the U.S., Glossier, Inc. will be the data controller of your personal information. If you reside in the United Kingdom or EEA, or apply to a position in the EEA, Phase EU Limited will be the data controller. If you reside in Canada, or apply to a position in Canada, Glossier Canada, Inc. will be the data controller.

XIII. CONTACTING GLOSSIER

If you have questions or concerns regarding this Policy, please contact us using the information provided below.

Glossier, Inc. Phase EU Limited

233 Spring Street 5 New Street Square

East 10th Floor London EC 4A 3TW

New York, NY 10012 United Kingdom

United States

Attn: Legal Attn: Legal

privacy@glossier.com priv"
Data Engineer,"CommonBond is building the leading values-driven, customer-centric financial services company and is looking for people who want to help our company grow. Our mission starts by tackling the broken student lending market and changing the way people think about student loans. We accomplish this with lower interest rates for our borrowers, a state-of-the art technology platform, and high-touch customer service. Additionally, our 1-for-1 model is the first of its kind in financial services: for every degree fully funded on the CommonBond platform, we fund the education of a child in need for a full year through a partnership with Pencils of Promise. We are backed by great investors, have a killer team, and are growing rapidly!Creating a robust tech-powered organization is essential to our success and we are looking for someone who will help build and maintain the foundation that supports our infrastructure across all existing and growth avenues.In joining our Software Engineering team, you can expect to have ownership over building new products, solving for challenges we face as a company, creating products that help people, and supporting an incredible social mission.Functions* Maintaining and developing scripts/tool chains for Continuous Integration / Continuous Delivery.* Building and deploying web, app, api, and backend code to dev/qa/staging/production systems.* Release based branching and Vendor based Branching.* Tagging, Versioning, and Release Note maintenance.Desired Qualifications* 2-3 years' experience in System Administration, Build/Release Engineering, DevOps required.* System Admin level experience with Atlassian tools such as Jira, Confluence, Fisheye/Crucible, Bitbucket, Bamboo.* Experience with Build/Release Engineering and automation using Jenkins/Bamboo, Git, Nexus/Artifactory, Ansible/SaltStack.* Experience with Docker, Kubernetes, and SaltStack for containerization.* Knowledge of various programming languages such as Python, NodeJS, and Java.* Scripting experience (Bash, Python).* System Admin level experience with both Windows and Linux operating systems.* Background in Dev-Tool integration, LDAP, Load Balancing is a plus.* Excellent troubleshooting and analytical skills.* Ability to work independently and on a team."
Data Engineer,"Stash is pioneering the future of personal finance with the first financial subscription that helps people create better lives. From budgeting to saving for retirement, Stash unites banking, investing, and advice all in one app that has helped more than 5M people reach their financial goals and make progress towards financial freedom

At Stash, data is at the core of how we make decisions and build great products for millions of users. As a Data Engineer you will be a part of our Data Platform Team which is leading the architectural design decisions and implementation of a modern data infrastructure at scale. You will build distributed services and large scale processing systems that will support various teams to work faster and smarter. You will partner with Data Science to help productionize machine learning models and algorithms into actual data driven products that will help make smarter products for our users.

Tools and technologies in our tech stack (evolving):
Hadoop, Yarn, Spark, MongoDB, Hive
AWS EMR/EC2/Lambda/kinesis/S3/Glue/DynamoDB/API Gateway, Redshift
ElasticSearch, Airflow, and Terraform.
Scala, Python
What you'll do:


Build core components of data platform which will serve various types of consumers including but not limited to data science, engineers, product, qa
Build various data ingestion and transformation job/s as and when they are needed
Productionize our machine learning models and algorithms into data-driven feature MVPs that scale
Leverage best practices in continuous integration and deployment to our cloud-based infrastructure
Build scalable data services to bridge the gap between analytics and application space
Optimize data access and consumption for our business and product colleagues
Develop an understanding of key product, user, and business questions
Who we're looking for:


3+ years of professional experience working in data engineering
BS / MS in Computer Science, Engineering, Mathematics, or a related field
You have built large-scale data products and understand the tradeoffs made when building these features
You have a deep understanding of system design, data structures, and algorithms
Experience (or a strong interest in) working with Python or Scala
Experience with working with a cluster manager (YARN / Mesos / Kubernetes)
Experience with distributed computing and working with Spark, Hadoop, or MapReduce Framework
Experience working on a cloud platform such as AWS
Experience with ETL in general
Gold stars:
Experience working with Apache Airflow
Experience working with AWS Glue
Experience in Machine Learning and Information Retrieval
_________________________________________


We believe that diversity and inclusion are essential to living our values, promoting innovation, and building the best products out there. Our success is directly related to the employees that we hire, grow and retain and we believe that our team should reflect the diversity of the customers that we serve.

As an Equal Opportunity Employer, Stash is committed to building an inclusive environment for people of all backgrounds. We do not discriminate on the basis of race, color, gender, sexual orientation, gender identity or expression, religion, disability, national origin, protected veteran status, age, or any other status protected by law. Everyone is encouraged to apply.

Benefits & Perks:
Equity in Stash
Flexible Vacation
Family-Friendly Medical, Dental, and Vision Insurance Plans
401k
Learning & Development Stipend
Commuter Benefits and Flexible Spending Account (FSA)
Employee referral bonuses
Stocked fridges & kitchens and catered lunch on Fridays
Thursday happy hours
Team outings that do not involve trust falls...
Awards & Recognition:
Forbes Fintech 50 (2019)
LendIt Fintech Innovator of the Year (2019)
Built in NYC's Best Places to work (2019)
Built in NYC's Startups to Watch (2018)
Wall Street Journal's ""Top 25 Tech Companies To Watch"" (2018)
MarCom Awards Double Gold & Platinum Winner (2018)
Webby Award Winner for Best Mobile Sites & Apps in the Financial Services and Banking category (2017)
W3 Awards Winner for Best User Experience (2017)
**No recruiters, please."
Data Engineer,"Mark43's mission is to empower communities and their governments with new technologies that improve the safety and quality of life for all. We build powerful, scalable, and elegant software that sets a new standard for the tools upon which our first responders rely. Our users are diverse, and we are therefore committed to embracing diversity of thought and experience within our team.

Mark43 is looking for a Data Engineer to help us build a next generation real-time crime analysis platform. Our Analytics product helps police departments draw better insights from their data, detectives to solve cases, and agency leadership to allocate department resources more efficiently. Working at Mark43, you'll be helping first responders save lives!

We are looking for someone passionate about data - best ways to collect, organize, transform and present it. You have extensive experience architecting complex, highly available and optimized real-time data pipelines, know the pros and cons of different tools, and when to build your own vs use an off-the-shelf solution.

What you can expect to work on
Plan, design and build distributed, real time data pipelines for the next generation of our Analytics product
Work with analytics product engineers to ensure performance, stability and availability of our MS SQL Server analytics databases
Evangelize best data practices among the engineering team, help internal education
Work together with DevOps engineers to improve our AWS data infrastructure
What we expect from you
BA/BS degree in Computer Science or a related technical field or equivalent practical experience
Knowledge and at least a year of production experience working with streaming platforms such as AWS Kinesis, Kafka, and/or Kafka Connect
Experience working with both MySQL and Microsoft SQL Server databases
Data warehouse design and implementation experience
Good knowledge of at least one scripting language
Excellent communication skills, verbal and written
Experience in a fast paced, agile startup environment
What you can expect from us:


Opportunities to learn and grow personally and professionally
Building mission critical and socially responsible software to enable first responders to better serve their communities
A workplace dedicated to supporting and bettering law enforcement, first responders, and other government agencies via mission critical software products
Working towards a worthwhile mission with a team of friendly and intelligent coworkers"
Data Engineer,"Data Engineering at B12

B12's engineering team views software as a craft, but improving the world as the reason to practice it. Our engineers are responsible for prioritizing, conceptualizing, co-designing, building, testing, and engaging users for any concept we are building out. We're generalists in encouraging each other to experience the full stack, but we're also aware of each other's preferences in the stack. We mentor and teach where we can, both inside and outside of the company.

We value sharing our work with the outside world. Our team has published papers on forming expert flash teams and machine-mediated worker hierarchies. We've baked our research into Orchestra, the system that coordinates our expert and machine teams, and released Orchestra into open source to contribute our software back to the community.

We're looking for a Data Engineer to help us answer critical questions our business faces while improving our data systems and architecture to support greater variety, volume, and velocity of data and data sources. We hope our engineers have more longevity than any one tool we use, but here is a sampling of our current thoughts about technology:
We build our product on Python/Django and JavaScript/React.
We store blobs in Amazon's S3, munch on them in Amazon's EC2, develop in Docker, and deploy containers to Amazon's Elastic Beanstalk.
We believe Postgres should be the first system you consider when you think about persisting structured data.
We religiously clean and centralize data in Amazon's Redshift, and are able to answer most any question in SQL. We recently wrote our 1000th query in Metabase!
Before building complex statistical machine learning models, we build simple ones we can understand. Rarely, we build complex models.
We have near-full test coverage on the backend, and are making progress on our frontend and integration tests.
We set up continuous integration and deployment because, while this model comes with its own pains, we've disliked being on fixed release schedules on previous projects.
We like to move fast and support point-in-time recovery :).
As a Data Engineer, you will
Collaborate with operational teams including sales, marketing, and customer success.
Contribute to infrastructure that enables and informs B12's analytical efforts.
Write SQL queries and reusable views that enable various analyses including funnel, retention, and performance reporting.
Use Python to clean data, send it to various systems including our data warehouse and operational services, and perform feature engineering to power the creation of predictive models.
Build rules-based models and statistical machine learning models in Python using packages like scikit-learn.
You'd be a good fit if
You are fluent in SQL and Python.
You have experience building and using data infrastructure, including systems like Postgres and Redshift.
You've used reporting tools like Metabase, Tableau, or Looker in the past.
You know that no dataset is ever pristine, but love to interrogate, structure, and clean data.
You've contributed to extract-transform-load pipelines to collect data from disparate sources and centralize them in a data warehouse.
You feel comfortable managing your time and deciding amongst competing priorities.
You have worked with non-engineering teams and are comfortable explaining technical solutions to them.
You are passionate about the future of work.
You enjoy learning and teaching.
You have strong written and verbal communication skills in English.
You care about and want to contribute to our mission of helping people do meaningful work.
Don't fear
We don't have a minimum number of years of experience for this role. We highly favor talent and interest.
Some candidates may see this list and feel discouraged because they don't match all the items. Please apply anyway: there's a good chance you're more wonderful than you think you are.
B12 is a safe place for human beings. We are dedicated to building a diverse and inclusive team with a wide range of backgrounds and experiences, each helping us understand our customers better, and strengthen our team. We particularly encourage you to apply if you identify as a woman, are a person of color or other underrepresented minority, or are a member of the LGBTQIA community.
How to apply


Please provide:
A pointer to your CV, resume, LinkedIn profile, or any other summary of your career so far.
Some informal text introducing yourself and what you are excited about.
If you have a profile on websites like GitHub or other repositories of open source software, you can provide that as well. If you don't have one, it's still very possible for us to get along just fine!"
Data Engineer,"At Clear Street, we are disrupting the institutional brokerage and clearing market by modernizing archaic industry segments with brand new technology. We're changing the way institutional investors interact with the markets; offering an alternative to working with big banks. Our cloud-based API technology will empower clients to clear, settle, and finance trading activity while accessing real-time risk and position information. Our platform is built on an incredibly modern tech-stack, by pragmatic engineers focused on building intuitive and frictionless user experiences. Our tech-infused suite of customer experience-oriented prime service offerings will increase our clients' efficiency and provide real-time insights they've never previously experienced.

As a Data Engineer, you will play a critical role in the Data Ops team's expansion as our business continues to grow. You will collaborate with a team of engineers, data specialists, and business analysts to build scalable and performant data pipelines. You've turned your passion for all things data (file formats, storage, databases, and DAGs) into your calling by combining it with deep computer science fundamentals. You will architect data pipelines that ingest, process, store, cleanse, transform, and route massive amounts of financial data to support multiple business initiatives. This is a unique opportunity to join a growing team of passionate engineers and data specialists who are delivering the future of Clear Street's data platform and analytics.

Data Engineer Characteristics:
You have ~5 years of data engineering experience focused on delivering highly scalable data pipelines. Your code has handled the full ETL process used for large scale analytics across huge datasets. Your pipelines have ingested data from multiple data sources (proprietary and vendor datasets).
You have a strong command over object-oriented design patterns, data structures, and algorithms.
You communicate technical ideas with ease and always look to collaborate to deliver high quality products.
Your experience will help you mentor team members and accelerate the development of both our system and our team's growth.
You grasp product specifications and effectively map them to technical requirements. You thoughtfully and successfully incorporate these requirements into your system design and implementation.
You are a collaborator by nature who works effectively with product teams to understand the scope, cost, and requirements of new product feature development.

DataOps Team Stack: Python, PostgreSQL , Snowflake, Airflow, Redis, Celery/Dask, Kubernetes, Apache Arrow, Pandas, Sklearn, Kafka, Docker

We offer:
The opportunity to join a small and growing team of good people, where you can make a difference
A new, high-quality code base with little technical debt and room to build new services and features
An environment that embraces the utility of a DevOps oriented culture and combines it with a focus on CI/CD methodology
A meritocratic philosophy that champions collaboration
Competitive compensation, benefits, and perks"
Data Engineer,"Job Description
Data Engineer
New York (Headquarters)
Latch's data team is continuing to be built from ground up, and we are still actively hiring. We're covering all aspects of the Latch's data strategy including: analysis, engineering, science, & more. We are firm believers of bringing in smart people that can define the roles for themselves, so come join us and start creating greatness.

Smart access isnt about locking doors, its about opening up new possibilities. Latch is the worlds first fully integrated hardware and software system dedicated to bringing seamless access to every door in a modern building. Were looking for the curious and the creative to join our team and help us change the way we access our most valued spaces.

Responsibilities
Understand the data gathered across the entire Latch organization
Design and implement data pipelines, building scalable and optimized enterprise level data systems
Collaborate with other teams in the company, both engineering and business counterparts
Transform raw data into meaningful sets that are query-able and visualizable.
Work closely with Data Analysts and Data Scientists to implement production ready systems
Be a helping hand with tools used by other teams such as Sales CRMs, Ops Customer Success tools, Marketing automation or Finance ERP. Data from these tools are very important to us.
Requirements
BS in Computer Science, Math, related technical field or equivalent practical experience
3+ years of general software programming experience in Java or similar languages
Excellent grasp of data structures and algorithms
Solid level of understanding in SQL
Knowledge of database technology, schema design, and query optimization techniques
Experience in ETL pipelines and data transformations.
Excellent communication skills
Preferred Qualifications
MS in Computer Science, Mathematics, or related technical field
Experience with Map-Reduce technologies such as Spark or Hadoop.
Understanding of basic data science concepts
Experiencing in productionizing machine learning models.
Acute sense of data analysis: being able to make sense out of many seemingly unrelated data sets.
Founded in 2014, Latch is a venture-backed, high-growth organization that's on a mission to change the way people open, manage, and share their spaces. Today, 1 in 10 new developments in the U.S. depend on our full-building smart access solution to meet the needs of residents and property managers.

We are a team of just over 200 employees, all of whom are passionate self starters with unique backgrounds and unexpected stories. We offer unlimited time off, a competitive health package, and the opportunity to work in a creative, dynamic, and fast-paced office environment. We are located just a quick walk from both Hudson Yards and Penn Station in New York City."
Data Engineer,"Us and the role

Seated is an app that rewards you for dining out. Users can browse restaurants based on their location, preferences or mood and book either as walk-in or reservation. The seated mobile app, powered by robust microservices architecture, provides a convenient way to upload receipts, verify spend and reward users. The Seated platform further allows users to spend rewards on in-person experiences such as wine tasting & cooking classes or redeem rewards for gift cards from a wide variety of brands such as Uber, Amazon, Target or SoulCycle.

We are fast-paced, innovative & metric-driven, with a team who are passionate about delighting our customers. We are looking for a Data Engineer with proven experience in producing reporting, dashboards, visualizing insights and expertise in data analytics to join this newly data analytics team. The role requires both a broad knowledge of existing data modeling and processing along with the creativity to invent and customize when necessary using programming and technology platforms.

You will work with data scientists, engineers & product managers hand in hand to build insightful and efficient reporting solutions & data analysis. In your role, you will be a key player in a multi-functional team that delivers insights, having direct and measurable impact on Seated's platforms & consumer applications.

What you'll do
Work with product, engineering & business teams to deliver complex data analysis requests
Visualize datasets across multiple databases & warehouses using tools such as Tableau, D3, Looker, etc.
Build financial models & growth projections for new products and business initiatives
Build ETL pipelines for regular reporting on business and operational KPIs
Help business understand key trends by executing complex analysis via Tableau or ad-hoc SQL queries
Coordinate within cross-functional teams such as engineering, product, marketing, customer experience for various data analysis needs
Proactively build data and event-driven dashboard for real-time business operations and consumer insights
What you'll bring
Bachelors in CS, Statistics, Economics or Engineering, Masters preferred
3+ years of hands-on SQL experience
2+ years of experience in using data visualization tools such as Tableau, Looker, PowerBI
2+ years of experience in building financial models, growth projections & ETL data pipelines
experience either in R or Python and working with data warehousing solution such as AWS Redshift or Google BigQuery
What you'll get
Comprehensive Healthcare, Dental, and Vision
Generous 401(k) Matching
Stock options
Unlimited PTO
Pre-Tax Flexible healthcare spending account (FSA), Dependent Care FSA and Commuter Benefits
Paid Family Leave
$100 monthly Seated allowance (dine on us)
Stocked fridges, coffee, soda, and lots of treats
Collaborative, dynamic work environment within a fast-paced, mission-driven company
At Seated we welcome passionate people from all backgrounds, helping us make dining experiences more accessible & rewarding. If you have the curiosity & passion to drive our mission together, we would love to hear from you."
Data Engineer,"Company Description

Thinknum is one of the fastest growing web data software startups in the world.

Thinknum creates datasets from a broad array of public online sources, capturing ephemeral information on the products, operating markets and labor markets of 400,000+ global companies across sectors, and provides rich toolsets for extracting intelligence. Thinknum has 150+ clients across major corporations and investment firms.

Thinknum's founders met at Princeton University and worked at Goldman Sachs and a hedge fund respectively before starting the company.

Job Description

You will join a fast-growing startup which offers exciting opportunities to those that are willing to put in the work.

This is a salaried position with full benefits. Visa sponsorship is offered.

Qualifications
Passionate about Web Scraping
Expertise in Python
Experience with Selenium, BeautifulSoup
Experience with HTML, Javascript, CSS
Understanding of the DOM, ORMs
Additional Information

All your information will be kept confidential according to EEO guidelines.

Perks and Benefits
Dental & health coverage
Fully stocked kitchen, weekly team breakfast outings
Transitcheck"
Data Engineer,"The AWS Well-Architected Tool team is hiring Data Engineers!!
Imagine if you could help shape the future of architecture, and go on a journey where few have tread before. AWS Well-Architected aims to help our customers develop technical expertise in AWS services, learn how to architect their cloud applications, and provide a great experience for customers and partners.
AWS is one of Amazons fastest growing businesses. More than a million active customers, from Airbnb to SAP, use AWS Cloud solutions to deliver flexibility, scalability, and reliability. As a Data Engineer at Amazon, your Primary responsibilities will be
· Design, implement and support an analytical data infrastructure providing ad-hoc access to large datasets and computing power.
· Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL and AWS big data technologies.
· Creation and support of real-time data pipelines built on AWS technologies including Glue, Redshift/Spectrum, Kinesis, EMR and Athena
· Continual research of the latest big data and visualization technologies to provide new capabilities and increase efficiency
· Working closely with team members to drive real-time model implementations for monitoring and alerting of risk systems.
· Collaborate with other tech teams to implement advanced analytics algorithms that exploit our rich datasets for statistical analysis, prediction, clustering and machine learning
· Help continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers


Basic Qualifications

· 2+ years of industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets
· Demonstrated strength in data modeling, ETL development, and data warehousing
· Experience using big data technologies (Hadoop, Hive, Hbase, Spark etc.)
· Knowledge of data management fundamentals and data storage principles
· Experience using business intelligence reporting tools (Tableau, Business Objects, Cognos etc.)

Preferred Qualifications

· Degree/Diploma in computer science, engineering, mathematics, or a related technical discipline
· Experience working with AWS big data technologies (Redshift, S3, EMR)
· Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets
· Experience working with distributed systems as it pertains to data storage and computing
· Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations"
Data Engineer,"You will join a successful and growing group engaged in
multi-asset automated stat arb trading. We build
a complex suite of research, data, and trading systems.
You'll plan and execute updates to our infrastructure and tools.
You'll take ownership of our Python development environment,
build tools, automated testing, and deployment setup. You'll also
lead
migration of our research workflows and data to a public cloud.
You'll evaluate and implement new tools
and technologies to take our production trading setup to the next
level. You'll improve our research
platform, developer experience, test environment, and config
management.
This is a front office role. You will work directly within an
extremely experienced team of about twenty
quants, engineers, and portfolio managers. We are all technical
and hands-on builders. We'll expect you to
learn about trading and technology from us, and to teach us how
to manage and improve complex
software systems. Each of us wears many hats, and so will you.
This is an ideal role for an experienced developer interested in
quant finance, for a financial developer
looking for systems and infrastructure responsibilities, or a
strong operational engineer looking to
transition to software and infrastructure engineering.
Qualifications
We'll expect you to arrive with these skillsand then level-up
all of them.
Python development skills, not just scripting.
Familiarity with Python tools and data science ecosystem.
Experience with test and CI tools (any).
Solid understanding of git and git workflows.
Basic familiarity with Linux, storage, and networking.
If you are not experienced with Python but have demonstrated
development skills in other languages, we
are open to discussion.
The following further qualifications would help you succeed,
though we expect a talented individual will
be able to acquire them as needed.
Experience on AWS and/or GCP.
Experience with Ansible, Puppet, Chef, or other config
management systems.
Knowledge of testing best practice.
Software and infrastructure security skills.
Experience with relational and/or NoSQL databases.
Familiarity with Docker or another container technolog"
Data Engineer,"Senior Data Engineer

Master’s degree in Information Technology, Computer Science with 2 years experience.

Design, build, & maintain Consumer Identity Management System to match Affinity consumer database using frameworks such MD5 & SHA512.

Build & maintain Analytical Data Platforms using appropriate SQL, NoSQL and NewSQL technologies like MapR, Spark, Hadoop, & Python.

Lead engineering processes to ensure data quality & meta data documentation using tools like Python, Amazon RedShift, Tableau & Confluence.

Perform quantitative analysis of customer data using tools like SaaS & machine learning. Monitor tag transactions to correctly reward consumers based on merchant reward program offerings.

Skills: SQL, NoSQL, NewSQL, MapR, Spark, Hadoop, Python, Amazon RedShift, Tableau, Confluence, SaaS & machine learning.

Send resume to Affinity Solutions, Inc, Attn: HR Department, 875 Avenue of the Americas, 21st Floor, New York, NY 10001."
Data Engineer,"Our client is a leading hedge fund looking to hire a Data Engineer for their Macro Strategies business unit.

Responsibilities:
Driving innovation through product and platform development
Helping to facilitate bespoke custom basket trades for clients in a scalable infrastructure
Developing infrastructure and tools to administer basket rebalances for external clients and internal trading teams
Automation of corporate action adjustments and improvement of work-flow
Providing metrics for basket trades, to drive sales and trading decisions and to grow the business
Both independent and collaborative work, involving several sales/strat/trading teams globally

Requirements:
Expertise in Python
Strong SQL skills
Web scraping experience
Experience with Linux and Windows platform
Strong communication skills, both written and verbal
Exposure to non-relational databases
Exposure to web UI technologies
Data Warehousing and Modeling expertise
Financial knowledge


If you would like to be considered for the position of Data Engineer or wish to discuss the role further then please leave your details below. Your resume will be held in confidence until you connect with a member of our team
Email: info@njfsearch.com or call London (0207 604 4444,) New York (212 400 4845) or Chicago (312 204 72176) to speak to a member of our team. Thank you"
Data Engineer,"Company Description

Pinto is building the world’s smartest food data platform, with a mission of making food data fully personalized, transparent, and easily accessible for both consumers and businesses alike. Over 40% of Americans now follow a specific dietary eating pattern, and our data platform is organizing the world of food at scale across every dimension that matters to these modern consumers (diets, health conditions, ingredients, allergies/intolerances, personalized health needs, etc.). The era of personalization is here and we’re building Pinto to power it .

We’re now looking for a Data Engineer to join our growing engineering team. In this role you’ll envision and build critical features for our data platform, design and implement API methods, and improve the overall performance and reliability of our data processing platform as we scale.

This is an amazing opportunity for someone looking to join in the early stages of a rapidly growing technology startup solving a meaningful problem in food and the broader world of personalization in nutrition and consumer products.

Job Description

What you will be doing
You’ll make key decisions on the architecture and implementation of scalable data processing and analytics structure
You’ll help build data processes and pipelines, and craft the tools to make them efficient
You’ll work on the backend of the Pinto stack using Javascript, Node.js, MongoDB, GraphQL
You’ll work with our Research, Nutrition & Data Analyst teams to gain a deep knowledge of underlying use cases in food data and nutrition
You’ll design systems that improve the accessibility and scalability of our data platform
You’ll automate and handle the life-cycle of systems and platforms that process our data
You’ll build and scale data infrastructure that powers batch and real-time data processing across our data platform
You’ll evolve the maturity of our monitoring systems and processes to improve visibility and failure detection in our infrastructure
You’ll use your skills to help solve real-word problems, while learning about the food, nutrition, and consumer product goods (CPG) ecosystem
Qualifications
4+ years of software engineering and/or Big Data Engineering experience
Degree in Computer Science, Computer Engineering, or Statistics (or commensurate experience)
Experience working with large, complex data sets from a variety of sources
Proven track record of building and shipping large-scale engineering products
3+ years experience with Node.js and the related ecosystem
Experience with MongoDB, GraphQL, Redis, REST APIs in general
Ability to collaborate cross-team with engineers, data analysts, product managers and business analysts
You are a strong communicator. Explaining complex technical concepts to designers, support, and other engineers is no problem for you.
Self-awareness and a desire to continually learn and improve
Bonus points for
An open GitHub profile to showcase some of your work (or code samples)
Have built a deployment pipeline
Have built a full stack application
Experience with or strong interest in data science and/or machine learning
Additional Information

About Us

Pinto is a smarter food data platform, designed for the needs of today's consumers in the world of personalized diet. To do this, we're rapidly bringing the world of food online and mapping it across every dietary need and preference — everything from general diets like Vegan, Paleo, and Keto, to health conditions like Diabetes Management, Heart Health, and Kidney health, to the long tail of ingredient and allergen considerations like Lactose Free, Gluten Free, Nut Free, and No Added Sugars. Over 40% of Americans now follow a specific dietary eating pattern, and our data platform is organizing the world of food at scale with partners that include Whole Foods and Kroger.

Current retail partners include Whole Foods, Kroger, and more. Learn more about Pinto and the technologies we power: http://business.pinto.co

See some examples searches:

-- Vegan meat substitutes: https://bit.ly/2m14P0P

-- Paleo snack bars: https://bit.ly/2lCOcYS

-- Gluten-free cereals that are also kosher: https://bit.ly/2m1510

Come join us on our mission as we build the future of smart, personalized product data!

All your information will be kept confidential according to EEO guidelines."
Data Engineer,"Data Engineer

We are looking for a Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. You will take ownership of expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for multi-functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.

In this role you will build and maintain an efficient data pipeline architecture, while assembling large, complex data sets that meet functional / non-functional business requirements. We are looking for you to Identify, design, and implement internal process improvements, such as automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability. You will build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.

Principal Responsibilities
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Collaborate with partners from both the business and technology to assist with data-related technical issues and support their data infrastructure needs.
Build data and analytical tools for internal customers that assist them in building and optimizing our data organization into an innovative industry leader.
Work with data and analytics specialists to strive for greater functionality in our data systems.
Qualifications/Skills Required

5 + years of experience with:
Object-oriented/object-function languages: Python, Java, Scala, etc.
Hadoop, Spark, Presto, Kafka, etc.
Relational SQL and NoSQL databases, including SQL Server, MySQL and Cassandra.
Data pipeline and workflow management tools: Airflow, Luigi, etc.
AWS cloud services: EC2, EMR, RDS, Redshift, Athena, SQS, etc.
Savvy with data science stack (Pandas, NumPy, SciPy)
Data Science/Analysis background; Proficient at working with large datasets
Unix/Linux command-line experience
Experience working with AWS, GCP or Azure
Broad understanding of equities, derivatives, futures, FX, or other financial-services instruments"
Data Engineer,"At Bloomberg, our products are fueled by powerful data. Our Global Data department combines technology with product expertise to bring unequalled value to the world's information. This information comes from a variety of sources both structured and unstructured, including company filings, web scraped content and alternative data. We collect, analyze, process and publish the data which is the backbone of our iconic Bloomberg Terminal & data products that ultimately drive the financial markets.

Our department arose from a need to transform our business from largely manual processes to more scalable automated solutions. In order to continue the evolution of our data management operations, we are creating systems to manage the relentless flow of information. This not only saves us time and money but helps us deliver richer more actionable content. We're growing our team in order to scale our technology capabilities and expand the range of acquisition, extraction and enrichment techniques we use for automation.

What's in it for you?


Our department supports Global Data's software development and technical operations to shorten the development lifecycle enabling us to provide data quickly and efficiently. We are leveraging advanced data processing and NLP as part of our pre-processing strategy. You will get exposure to our proprietary data pipelining, information extraction, and enrichment operations as part of our data processing paradigm.

Utilizing our modular tech stack, we are excited to expand both the data we process and the insights we provide. We are focused on combining and contributing to existing systems to build larger, more useful pipelines and services. We will work together to identify future projects and areas where we can have an impact. You'll be part of a team building data-driven systems supporting our internal businesses across Data Analytics, News, Research & Enterprise Data.

Who are you?
An autonomous, creative engineer who loves technology, is endlessly curious, aggressively inquisitive, and an advocate for best practices
You're a systems thinker who can see the big picture and understand how to break a larger problem down into smaller pieces that can be solved independently
Someone who is aligned to and excited by our departments core mission to build community, evolve and drive impact
You can build a network and find new opportunities for improvement
We'll trust you to:


Apply your python coding skills to help advance our goal of automating the influx of data and creating ETL solutions for our internal teams
Evangelize technical solutions and innovations among teams for further application
Identify strategic department level technical gaps in our ecosystem and advocate for new technologies and techniques
Leverage your development experience to build systems that are complete and robust enough to weave the data between automation and human expertise
Help build systems that utilize Machine Learning techniques to learn from human SME knowledge and actions
You'll need to have:


4+ years experience programming and scripting in Python within a production environment
2+ years experience with data modeling within SQL and NoSQL databases
2+ years experience working with restful APIs
Deep understanding of large-scale, distributed systems with an ability to understand large systems with minimal documentation
Legal authorization to work full-time in the United States and will not require visa sponsorship
We'd love to see:


Systems thinkers an interest or a natural preference to think about system architecture and how things work together
If this sounds like you apply!


We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status"
Data Engineer,"Work from Home (East Coast) or NYC

About us

Nasdaq (Nasdaq:NDAQ) is a leading global provider of trading, clearing, exchange technology, listing, information and public company services. Through its diverse portfolio of solutions, Nasdaq enables customers to plan, optimize and execute their business vision with confidence, using proven technologies that provide transparency and insight for navigating today's global capital markets. As the creator of the world's first electronic stock market, its technology powers more than 100 marketplaces in 50 countries, and 1 in 10 of the world's securities transactions. Nasdaq is home to approximately 4,000 total listings with a market value of approximately $15 trillion. To learn more, visit: http://www.nasdaq.com

The Team

The financial risk management team utilizes automated processes to manage workflow, risk monitoring, risk evaluation, and risk alerting functions. You will be part of a small, multi-disciplinary team which is united by our goal to utilize the latest technology tools to help us assess financial risks across Nasdaq. We focus on clearing and trading operations, investments, and trade receivables counterparty risk.

Business Unit

The Risk Management group at Nasdaq is tasked with surveillance and risk controls across the company.

Your role and responsibilities

You will be managing the data management platform for counterparty risk, which consists of an automated system of data flows with external and internal data source transformations which occur on a daily basis, and form the backbone of the team. You will work with risk managers, quantitative analysts, data scientists, Finance, and full suite of IT support teams to maintain and develop new features and functionality to our risk management technologies.

You will need the following skills and experience
Data integration software experience such as Informatica, Talend, or Adeptia
Extensive database experience, especially MS SQL
Project management
Good communication skills and time management
And it would be great if you have experience with
Big data platforms such as Apache Spark
Unstructured and specialty databases such as MongoDB & InfluxDB.
AWS resources for data management such as Redshift & Glue.
Business intelligence software such as Tableau, Power BI, or Grafana.
Sounds like you? Please follow through by clicking the “Apply” link and submitting your application. If your skills and experience are a match, we will be in touch soon, and in the meantime please visit our website and social media channels to learn more about our innovative business, inclusive culture and where a career at Nasdaq can take you.

Nasdaq is an equal opportunity employer. We positively encourage applications from suitably qualified and eligible candidates regardless of age, color, disability, national origin, ancestry, race, religion, gender, sexual orientation, gender identity and/or expression, veteran status, genetic information or any other status protected by applicable law."
Data Engineer,"At the NBA, we're passionate about growing and celebrating the game of basketball. Through the intensity of the game and the amazing athletic skill of our players, we deliver excitement to hundreds of millions of fans around the world.As a global sports and media business, the NBA is so much more. While Basketball Operations runs the league's on-court activities, other departments manage relationships with television and digital media partners, develop marketing partnerships with some of the world's most recognizable companies, oversee the licensing of NBA merchandise, and handle a wide range of responsibilities that drive the NBA's success.Position Summary:You will be part of a growing Data Engineering team that handles NBA data for Internal and External Users. The IT department services over 10 internal groups and the Data Engineer will be a seasoned Technologist comfortable with a variety of data technologies. Data Engineering Group handles a Datawarehouse that sources data out of over 15 sources and services over 10 internal groups. The current data technology stack consists primarily of Data warehouses using SQL Technologies, Neo4J based Graph DBs along with the use of a variety of NoSQL and AI/ML frameworks and languages like R, Python etc. Current plans have us moving our technologies to the cloud, specifically Microsoft Azure.The ideal candidate will be a technologist who is able to balance the rapid pace of technology change with an authoritative ability to handle client relationships - including working closely with the business and technical teams/vendors. Assists in the ongoing support of applications. We're looking for someone who is laser-focused on operational excellence and customer satisfaction. You'll need to wear many hats, so flexibility and a can-do attitude are critical!We are looking for a dynamic, collaborative personality that can champion the cause of Agile within the organization. The individual needs to have a consistent track record of successfully delivering value for their organization in a fast-paced environment along with successful management of customer expectations. A passionate engineer who strives for automation would be ideal for this position. Experience mentoring and leading other staff, both onsite and offshore would be desired for this role to be successful.As a Data Engineer, you will lead NBA's data efforts across all products and lines of business. You are a pioneer, building new capabilities that will help unlock new possibilities for our businesses. You will coordinate with external and internal resources to be a part of a Data Engineering practice. You will play a fundamental role in achieving our ambitious growth objectives. You must be comfortable switching between multiple projects, contributing as an individual and working with both business teams and technology teams to translate business requirements into finished products. You possess strategic vision and tactical mastery and combine it with an entrepreneurial spirit to get it done. You will collaborate closely with stakeholders across the company to design innovative solutions and balance challenging priorities and resource demands.The right candidate is passionate about data technologies. We're looking for someone who welcomes challenges and is hyper-focused on delivering exceptional results to internal business customers while creating a rewarding team environment. This position reports into the Head of Data Engineering, ITMajor Responsibility:* Understands business needs and develop solutions that delight consumers and customers* Understands Agile artifacts and develops applications based upon business priority.* Collaborate with project partners to ensure all requirements are met.* Handles relationships with end-user communities. Interacts regularly with users to gather feedback, listen to their issues and concerns, recommend solutions.* Build scalable, fault-tolerant batch and real-time data pipelines to power internal applications, operational workflows, and business intelligence platforms* Create and maintain data-driven APIs to support a wide range of integration with NBA partners* Recommend and implement best practices for data management and governance* Demonstrate your technical abilities and contribute to our overall architecture* Help implement the Enterprise Data Architecture for NBA and help implement it in multi-functional alignment with the Data teams that exist across functions like Marketing, Finance, HR etc.* Provide insights during application design and development for highly complex or critical machine learning projects across numerous lines of business and shared technology.* Ensure alignment to enterprise architecture and usage of enterprise platforms when delivering projects* Continuously improve the quality of deliverables and SDLC processesRequired Skills/Knowledge:* Master's Degree in Computer Science, Engineering, or Management of Info Systems/Technology preferred* Advanced Education in Statistics or Mathematics would be a plus 3+ year of experience in developing ETL and ELT pipelines using SQL and MSFT SSIS 3+ years of experience in developing BigData and/or machine learning solutions 3+ years of experience in a highly regulated industry* 1+ Years of experience defining and/or designing data architectures* 1+ Years of experience leading and/or managing product engineering teams* Experience with the MS Cloud stack (Azure) or AWS* Experience with SQL, NoSQL, BigData and Graph Technologies along with Programming languages like R, Python, Kafka, Storm etc.* Experience building microservices* Background in agile SW development and Scaled Agile Frameworks* A true believer in measuring success based on working software and in quick prototyping* Someone who is a passionate coder and can spin up a snippet of code quickly* Strategic thinker with the ability to build and execute innovative digital product, combined with tactical ability to execute simultaneously against multiple contending priorities* Someone with an iterative approach, drive to move fast and think big* Experience working with and/or managing internal and external teams at the same time, working with multiple brands and digital properties of varying maturities* Demonstrated ability to partner and communicate effectively with non-technical team members, resolving contending or contradictory objectives, and unifying disparate ideas into a homogenized solution* Ability to be versatile and handle multiple projects and re-prioritizations* Possess the ability to influence others, implement change, and standardize processes in a complex business environment* A passion for data and growing in your current role* Ability to effectively and appropriately interview technical candidates* Passion for Automation and Hunger for Acceleration* Keen knowledge of Devops as well as RPA is a big plus* Experience with Architecting Applications (e.g. Design Patterns, distributed applications etc.) with the aim of reuse would be a big plus* Superb communication skills (both written and verbal)* Great teammate - should be ready to go beyond to help immediate team and do not be averse to not shy away from asking for help if needed.* Ability to translate ideas into solutions based on user and business needs* Open Eagerness to learn new technologies and bring new ideas to the tableEducation:Bachelors Degree or equivalent. Masters would be a plus.We Consider Applicants For All Positions On The Basis Of Merit, Qualifications And Business Needs, And Without Regard To Race, Color, National Origin, Religion, Sex, Gender Identity, Age, Disability, Alienage Or Citizenship Status, Ancestry, Marital Status, Creed, Genetic Predisposition Or Carrier Status, Sexual Orientation, Veteran Status, Familial Status, Status As A Victim Of Domestic Violence Or Any Other Status Or Characteristic Protected By Applicable Federal, State, Or Local Law.Nearest Major Market: New York CityNearest Secondary Market: NewarkJob Segment: Database, Engineer, Computer Science, Data Management, SQL, Technology, Engineering, Data"
Data Engineer,"About Attentive:
Attentive is a mobile messaging platform changing the way consumers interact with businesses and organizations. The company is one of the fastest growing startups in New York City and recently raised a $110 million Series C investment led by Sequoia and IVP—two of the world’s leading venture firms—in early 2020, less than 8 months after its Series B round due to strong customer traction. We’ve seen 347% customer growth in just one year, and now work with 1000+ of the most innovative brands like Coach, Urban Outfitters, CB2, PacSun, Lulus, Party City, and Jack in the Box. Attentive was founded in 2016 by the co-founders of TapCommerce, a mobile marketing platform that was acquired by Twitter in 2014.


Role Background:
We’re looking for a great Data Engineer for our growing Analytics team. You’ll be responsible for all things data: building out our streaming and batch ETL pipelines, curating and anonymizing data that’s being generated from various sources, designing and building the data warehouse, and so on.

We’ll expect you to have an in-depth knowledge of distributed systems and data flows. Combined with an understanding of business intelligence and performance requirements, you’ll breathe life into Attentive data and help make it an invaluable part of the platform and business.

If you are a self-starter, excited about building a culture around data-driven decisions, motivated by making an impact, and pushing the boundaries of your knowledge, you will excel here and do great things!
You at Attentive:
Design, implement, and maintain an ever-growing ETL pipeline using state-of-the-art technology
Use best practices and standards for managing large collections of data for analytics
Discover and integrate new heterogeneous data sources
Work closely with data analysts, data scientists, and product managers enabling them to provide insight into key performance metrics of the business
Help to improve data reliability, efficiency, and quality
Your Qualifications:
4+ years of experience designing and developing a data warehouse on a distributed database platforms, such as Snowflake or Redshift
Experience designing, developing, and maintaining high-throughput and low-latency ETL pipelines
Experience with data modeling, data access, and data storage techniques
Experience with big data tools such as Apache Spark
Proficient in SQL and Python
Proficient with at least one RDBMS (MySQL or Postgres preferred)
Successfully implemented data pipelines in the public cloud, especially Amazon Web Services
Strong analytical and interpersonal skills
Enthusiastic, highly motivated and ability to learn quick
Benefits & Perks
Robust health benefits packages including access to a 401k and various medical, dental and vision plans, and $100/month fitness reimbursement
Full support for remote work during COVID-19
Daily lunch delivery credit and other goodies sent to home
Regular company-wide social events (even virtually!)
Generous annual education stipend toward job-related external learning opportunities
An extremely enthusiastic team that appreciates collaboration
Attentive is an Equal Opportunity Employer. We’re committed to diversity and maintaining a work environment that is free from harassment and discrimination. We’re committed to them because our core values demand it. Values like Integrity First, Listening & Cultivating Discussion, and Default to Action. Applicants from all backgrounds are encouraged to apply, and will not be discriminated against on the basis of any protected status under federal, state, or local law."
Data Engineer,"About Datadog:

At Datadog, we’re on a mission to build the best monitoring platform in the world. We operate at high scale—trillions of data points per day—and high availability, providing always-on alerting, visualization, and tracing for our customers' infrastructure and applications around the globe.

The team:

We are building a first-class Internal Analytics team composed of Data Engineers and Data Analysts. If you’re excited to work on a fast-moving team with cutting-edge open-source data collection, transformation and analysis tools, we want to meet you.

You will:
Collect data from a wide range of sources: AWS S3, Redshift, PostgreSQL, and various APIs
Build data ETL pipelines using Spark, Luigi and other open-source technologies, with programming languages like Scala, Python, and SQL
Tune Spark jobs to improve performance
Work closely with product managers, designers, and engineers in order to collect the right data that will help them better understand our customers, product usage, or our own operations
Work with Data Analysts to build the right analytics reports
Have a meaningful impact on many teams at Datadog thanks to data
Join a tightly knit team solving hard problems the right way
Grow with the company
Requirements:
You are fluent in several programming languages such as Python, R, or Scala
You have 2+ years of work experience in building ETL pipelines in production
You value code simplicity and performance
You have work experience with data storage such as AWS S3, Redshift or similar.
Being a SQL expert is a minimum for this position
You are fluent with command line
You enjoy wrangling huge amounts of data and exploring new data sets
You have a natural curiosity and investigative mindset - driven to know “why”.
You can explain complex datasets in very clear ways
You want to work in a fast, high-growth startup environment and thrive on autonomy
Bonus points:
You are familiar with Spark and/or Hadoop
Experience with AWS Redshift and S3"
Data Engineer,"Job Description
BigQuery
SQL
R
Python
Partykit
ETL
DataStage
API
Skill mapping
Primary Skills - Python, R, SQL, Datastage, Linux scripting, API knowledge, SSIS
Platform Knowledge - GCP Big Query
Supporting skills - Azure platform knowledge and Partykit"
Data Engineer,"Data is at the core of everything we do here at HRT. We excel at deriving deep insights from all types of data, allowing us to achieve consistent success in a competitive market. As we think about exciting new avenues for growth, we look forward to simultaneously expanding the scope of data at HRT to new frontiers.

Being a data engineer at HRT means being a pioneer. In this role, you'll have the opportunity to work on problems that are pretty brand new to us. You'll help design tools, processes, and an entirely new system for managing, storing, accessing, and exploring data at HRT. We're looking for someone who loves data (maybe even dreams about data) and wants to be a critical part of bringing a world-class data warehouse to life.

The Skills:
Strong programming experience in Python
Demonstrated ability to work with data
Data infrastructure experience - you know how to really store data; and no, we don't mean saving an Excel file on your desktop (everyone knows that's messy and it should be in a nicely named folder)
Track record of working successfully in a collaborative environment
Top-notch communication skills
The Profile:
You have a minimum of 2-3 years of experience working in data infrastructure
Bachelor's degree in computer science, math or a related field
You enjoy being part of an amazing team but don't mind working alone on a difficult problem
You can analyze and fix problems quickly
You really like to work with people who motivate you and make you better
In your spare time you: code, tinker, read, explore, break things, and have an insatiable curiosity for all things computer related
Culture:

Hudson River Trading (HRT) brings a scientific approach to trading financial products. We have built one of the world's most sophisticated computing environments for research and development. Our researchers are at the forefront of innovation in the world of algorithmic trading.

At HRT we come from all sorts of backgrounds: mathematics, computer science, statistics, physics, and engineering. We're a community of self-starters who are motivated by the excitement of being at the cutting edge of automated trading. Our culture celebrates great ideas whether they come from HRT veterans or new hires. At HRT we're friends and colleagues, whether we are sharing a meal, playing the latest board game, or writing elegant code. We embrace a culture of togetherness that extends far beyond the walls of our office.

Seem like something you might be interested in? Our goal is to find the best people and bring them together to do great work in a place where everyone is valued. HRT is proud of our diverse staff; we have offices all over the globe and benefit from our varied and unique perspectives. HRT is an equal opportunity employer; so whoever you are we'd love to get to know you."
Data Engineer,"Job Description
We are looking for a data engineer to manage and further build out our data infrastructure pipelines and data visualizations. The work will support core business decisions and models that serve as the basis for core product and growth strategy. Candidates should be able to choose the right tool for the job and learn how to use it if they don't know how to already. Candidates should prioritize robustness, scaling considerations, and simplicity in architecture decisions.

JOB REQUIRED SKILLS: Python, PostgreSQL, Redis, AWS, Periscope, Looker"
Data Engineer,"A career at New York Life offers many opportunities. To be part of a growing and successful business. To reach your full potential, whatever your specialty. Above all, to make a difference in the world by helping people achieve financial security. It’s a career journey you can be proud of, and you’ll find plenty of support along the way. Our development programs range from skill-building to management training, and we value our diverse and inclusive workplace where all voices can be heard. Recognized as one of Fortune’s World’s Most Admired Companies, New York Life is committed to improving local communities through a culture of employee giving and service, supported by our Foundation. It all adds up to a rewarding career at a company where doing right by our customers is part of who we are, as a mutual company without outside shareholders. We invite you to bring your talents to New York Life, so we can continue to help families and businesses “Be Good At Life.” To learn more, please visit LinkedIn, our Newsroom and the Careers page of www.NewYorkLife.com.

Position Summary

The Senior Associate is a member of the Risk Insights Team within the Underwriting Department of Strategic Capabilities. Their primary focus is to support Underwriting by providing necessary data and reporting elements as needed. The Senior Associate is expected to design reporting queries and reports to inform management decisions. The individual is required to have a strong skillset in data extraction as well as in analytical tools such as Spotfire or Tableau.

This individual will work independently with other members of the Risk Insights team and Underwriting at large and may serve as a liaison with management or other business units as needed. He/she will frequently work closely with adjacent teams (e.g. Data Solutions and Governance) and functions (e.g. Technology).

Responsibilities

• Write and optimize data extraction queries in order to provide data and/or reports that inform business decisions

• Provide technical expertise in the development of cross-functional or cross-team business activities, specifically around the availability and optimal utilization of available underwriting/application data

• Navigate independently through large amounts of data/data sources and develop reports for key strategic initiatives and Underwriting management

• Identify and interpret patterns and trends, assess data quality and eliminate irrelevant data

• Clearly and concisely articulating in written and verbal communication the findings of the analysis and investigations performed

• Consult with internal customers to develop analyses that lead to actionable insights

• Lead or participate in special projects as assigned related to underwriting/application data

• Gain a thorough understanding of the business, to make recommendations for additional reviews where there may be gaps

Experience

• Experience extracting data from various data repository systems.

• Strong SQL optimization skills

• Experience with and understanding of New York Life products, services, and administrative systems preferred.

• Ability to work collaboratively with technical and business experts to develop reporting elements to assist in business decision making process.

• Strong management skills to prioritize and implement multiple medium to large scale complex initiatives simultaneously.

• Ability to work with little supervision.

• Advanced Excel knowledge. Spotfire or Tableau knowledge also preferred, but not required.

• Must have the ability to multitask under tight deadlines.

• Prior business experience with Underwriting, Inforce Service, or Agency is preferred.

Qualifications

• Bachelor’s Degree preferred

• 5+ years’ experience required

• 5+ years’ experience with SQL or other data extraction

• 1+ years prior insurance knowledge required, preferably Underwriting, Product, Inforce Service or Agency experience

#LI-JP1

EOE M/F/D/V

If you have difficulty using or interacting with any portions of this Web site due to incompatibility with an Assistive Technology, if you need the information in an alternative format, or if you have suggestions on how we can make this site more accessible, please contact us at: (212) 576-5811."
Data Engineer,"BrainPOP is an online K-12 educational solution that makes rigorous learning experiences accessible and engaging for all. Proven to raise academic achievement, BrainPOP has been a trusted resource to more than six million educators, and engages the hearts and challenged the minds of over 300 million learners worldwide.

BrainPOP provides endless opportunities for kids to take agency over their learning through playful, knowledge-building content and learner-driven projects, preparing them for success in the classroom and beyond.

We are seeking a Data Engineer to join our growing Data team. We believe that information is most useful when delivered accurately and on-time. As a Data Engineer, you will focus on building data pipelines that feed data from various sources into our new data warehouse, so that other teams may utilize the datasets that you curate to generate insights of their own. You will be working collaboratively alongside other passionate Data Engineers, Data Analysts, and Software Engineers to support and maintain our data infrastructure.

In this role, you will:
Build and maintain data pipelines that ingest and process large and complex data.
Curate and prep datasets for other teams to use.
Work with various teams to ensure data is collected properly and accurately.
Identify areas of improvement in our current pipeline architecture.
Reinforce best practices for ETL processes and data storage.
On your resume:
Experience working with large-scale data sets.
Experience working with a cloud-based data warehouse such as Snowflake or Redshift.
Experience working with data pipeline tools such as an in-house Airflow pipeline or commercial applications like Segment.
Fluency with SQL and a scripting language like Python, Shell, or Java.
Excellent understanding of data structures and algorithms.
Our ideal candidate has:
A passion for improving education.
Experience working with Looker, AWS, Snowflake, and Google Analytics.
Strong organizational skills.
BS, BA, MS, MA, or PhD in Computer Science, Mathematics, or a related technical field.
3+ years of work experience.
Familiarity with basic data science concepts and statistics.
Life at BrainPOP

Our commitment to supporting and empowering teachers and students is reflected in our dedication to enhancing the lives of our employees—in and out of the office.

Our team is made up of educators, data scientists, published authors, engineers, artists, bakers, film buffs, cyclists, dual-citizens, and so much more. We value collaboration and learning from multiple perspectives.

Besides offering a comprehensive benefits package and putting an emphasis on work-life balance, we make it a point to integrate fun and play into the workplace.

We offer:
Monthly Wellness Activities
Catered Wednesday community lunches
401K with a company match
ClassPass Corporate Membership
Learning & Development Opportunities
Seasonal In-Office Activities
Friends & Family BrainPOP Subscriptions
What we do today directly impacts how teachers teach, and students learn. We continue to be inspired because we can see the difference we’re making and we’re proud to be a creative, collaborative, always-teaching and always-learning community.

We believe that a diverse organization is a more effective organization. BrainPOP is an Equal Opportunity/Affirmative Action Employer."
Data Engineer,"Quadpay was founded in 2018 with the goal to transform the way shoppers pay for their purchases. Would you believe that more than 60% of millennials don’t own a credit card and over 57% of Americans have less than $1,000 in savings? Believe it. Quadpay is helping people make larger purchases accessible with responsible budgeting. We believe in choice and in giving shoppers the freedom and flexibility to manage their purchases in the way that best suits their finances.

We are proud that QuadPay is one of the fastest growing fintech start-ups. We use GSuite to communicate when we aren’t chatting (or often, sending pictures of our pets) on Slack. We’ve got a stocked kitchen and beautiful loft office near Madison Square Park. Our team loves coming to work and has fun during our regular all hands meetings, happy hours, and trivia nights. Come join us!


We’re looking for data fanatics to join the A Team of engineers and analysts that are responsible for building Quadpay’s data infrastructure. We need people who understand data at a granular level – they don’t rely on any one tool but rather understand them all, and each of their pros and cons. This is a unique opportunity to build the data strategy and architecture at a data driven startup that’s changing the world of finance, shoulder to shoulder with a tight team that has your back.
You dream in code:
SQL. Teach us something new, show us what you’ve got
Python/Scala/C# You’ve coded before, you know the importance of efficient, clean code
You thrive in any data environment:
Cloud – we use Azure
Someone who understands complex data and the challenges of accessing it
A real bottom-line person, not someone who throws terms like “big data” around because it’s popular
Traditional/relational databases, Lakes, or Pub/Subs make no difference to you
You know databases inside and out:
Database concepts – indexes, execution engines, etc
Database Administration experience (Azure DWH, SqlServer, Postgresql)
You understand that databases are an integral part of being a Data Engineer
You enjoy looking at and solving big picture problems:
You like to ask questions and devise a complete solution
You want to understand the data (not only the pipes) and you can definitely perform some analytics and build dashboards because you like it.
You love learning new things:
You know that you don’t know enough, and it bothers you that there isn’t enough time in the day to learn about the next topic.
You’re up-to-date on new trends in data – you know who’s using what to solve various problems and are excited for the next release of your favorite tool
If you like being thrown in the deep end of the pool, this team’s for you.
You’re a culture fit:
You believe and want to participate in a blameless culture which focuses on process and technology
You don’t sleep well at night when you leave work with a question unanswered
You feel accountable for everything you do and that sense of urgency has been driving you your entire life
You like to have a good time while getting things done
When we say a “team player” we mean it - you have a crisp high-five and funny stories to tell
You have your team’s back. And the team has yours.
Sense of humor is hugely preferred
Technology is changing the way people interact with the world and we’re bringing that revolution to the way people shop and transact. We allow shoppers to buy anywhere, at anytime - and pay in 4 interest-free, automatic installments over 6 weeks. You get the product right away and QuadPay will pay the merchant upfront.

We're looking for someone who can join our high-functioning team of passionate support professionals and we value a range of diverse backgrounds, experiences, and ideas. We pride ourselves on diversity and creating an inclusive workplace that provides equal opportunities to all persons regardless of age, race, color, religion, sex, sexual orientation, gender identity and expression, national origin, disability, military and/or veteran status, or any other protected classes.

We're growing very quickly and always looking for talented people to join our team and help transform the way consumers shop and pay!"
Data Engineer,"At Caserta, we work with leading organizations to deliver innovative data & analytics solutions. A project-based approach to solving our clients’ toughest data challenges with the fastest time-to-value is a hallmark of our firm. Our consultants are force multipliers that strengthen our clients’ teams with deep domain expertise in data architecture, data engineering, and data science.

Our team is looking for creative, entrepreneurial, and highly-motivated people to carry out our mission of designing, architecting, and implementing the most innovative, forward-looking data-driven solutions available to our clients.

The Role:

As a Data Engineer at Caserta, you will work in teams to deliver innovative solutions on Amazon Web Services, Azure, and Google Cloud using core cloud data warehouse tools like Spark, Event Stream platforms, and other Big Data related technologies. In addition to building the next generation of data platforms, you will be working with some of the most forward-thinking organizations in data and analytics.

Responsibilities
Develop Cloud-enabled Data and Analytics solutions
Participate in the development of Cloud-based and hybrid data warehouses & business intelligence platforms
Build Data Pipelines to ingest structured and unstructured Data.
Gain hands-on experience with new data platforms and programming languages
Requirements
2+ years of experience working in Data Engineering or Data Warehousing
Hands-on experience with leading commercial Cloud platforms, including AWS, Azure, or Google
Experience building data warehousing, data ingestion, and data profiling solutions
SQL & Python skills
Strong aptitude for learning new technologies and analytics techniques
Highly self-motivated and able to work independently as well as in a team environment
Experience building and migrating complex ETL pipelines
Familiarity with or strong desire to learn quantitative analysis techniques (e.g., predictive modeling, machine learning, segmentation, optimization, clustering, regression)
Bachelor’s degree in Business Analytics, Computer Science or a closely related field required
Benefits
Profit Sharing
Health/Medical Benefits
Dental & Vision benefits
Highly competitive PTO/Holiday package
401-k
And more
See what people are saying on Glassdoor and Follow Us on LinkedIn for company updates and valuable data insights."
Data Engineer,"Position Summary

The Department of Biomedical Informatics (DBMI) at Columbia University is revolutionizing the clinical research enterprise with the help of information technology. At DBMI, we are building the infrastructure of the future to support and enable better research and dissemination. We have an immediate opening for a talented and self-motivated data engineer developer who can succeed in a collaborative work environment. The ideal candidate will have experience with data pipelines and cloud environments. The candidate will be responsible for data processing, data exchange/transfer/load (ETL), data visualization, DevOps, and software architecture. The ideal candidate will have professional experience in a number of programming languages, databases, and development environments. The candidate should be able to contribute to improving the reliability and quality of data. Experience in clinical medicine, clinical vocabulary, and cloud development are not required but preferred. The successful candidate will contribute to the development of open source solutions together with a community of international researchers.

Current available position is grant-funded.

Columbia University's Department of Biomedical Informatics is internationally recognized as one of the best programs of its kind. Our mission is to improve health for society by focusing on discovery and impact: we develop new informatics methods, enrich the biomedical knowledge base, and enhance the health of the population.

Responsibilities

1. Software and system design, implementation, and testing (75%)

2. Application deployment and configuration (10%)

3. Communicate with technical individuals at various grant sites (10%)

4. Software requirements specification (5%)

Minimum Qualifications

Bachelor's degree or equivalent in education and experience (computer science, biomedical informatics, information science), plus four years of related experience.

Other Requirements

Great communication skills; Experience with one or more compiled programming languages (e.g. Java, Scala, C#, C++, etc.) and one or more interpreted programming languages (Python, JavaScript, Perl, bash, etc.)

Working knowledge of SQL; Experience with big data, NoSQL databases, and health care data a plus.

Equal Opportunity Employer / Disability / Veteran

Columbia University is committed to the hiring of qualified local residents."
Data Engineer,"The role

As a Data Engineer at Blue State, youll play an integral role on a smart and vibrant analytics team servicing a wide range of progressive organizations. Youll design, build, and manage the systems and processes which form the underpinning of Blue States analytics work, supporting and working alongside data analysts and campaign strategists. But youll also work directly with Blue States clients to help solve their data integrity and integration challenges, serving as a trusted advisor to your counterparts within client organizations.

Day-to-day responsibilities:

Create and support systems and processes for managing, compiling, manipulating, and analyzing data for client and internal projects

Work with Blue States client organizations to solve difficult data migration, management, and integration challenges

Build data pipelines, data warehouses, reporting dashboards, automated exports, and synchronization processes

Automate workflows and look for further opportunities to improve efficiency in our work

Always maintain a high level of data security and privacy

The team

You will be a part of the globalData & Technology team working primarily with our creative agency on client projects. Youll work in either the NY or DC office.

Top things were looking for

Good foundational understanding of statistical analysis

Extensive experience working with SQL databases in an analytics or business intelligence context

Familiarity with common marketing technology platforms like Google Analytics, Google Ads, Facebook Ads, email marketing tools, and other marketing automation tools

Experience with ETL/ELT tools, processes, and best practices

Strong Python experience:

Python should be your go-to tool for solving problems. If the first thing you want to do when you have to do the same thing twice is write a Python script to automate it - we want you!

Experience with task automation in a Python context - experience with AirFlow, Prefect, Dask a big plus

Experience working with restful APIs - you can competently navigate unfamiliar API documentation and figure out how to accomplish tasks

Strong working knowledge of Google BigQuery and the Google Cloud Platform data product ecosystem including:

Designing data warehouse schemas for cross-channel marketing analytics

Utilizing the suite of Google Cloud Platform tools for the purposes of extracting, processing, manipulating and analysing data

Building and running automated tasks within the GCP environment - e.g. Cloud Compute, Cloud Functions, Cloud Run, Cloud Scheduler

Comfortable managing GCP IAM policies across projects and teams

Comfortable working within a spreadsheet (even if you prefer a database) - preferably in Google Sheets - bonus points if youve extended Google Sheets using Google Apps Script

Familiarity with Git and maintains good habits around code maintenance

Able to build repeatable and well-documented processes and tools that can be used by other technically-savvy but non-Python developer analytics team members (think easy to use command-line scripts - not GUIs)

Good at teaching others what you know.

The company

Blue State is the purpose-driven creative and tech agency for brands and causes looking to inspire people to take action. With clients including the AARP, Google, UNICEF, JDRF, and Colgate. Blue State cultivates communities, builds platforms, and transforms how organizations engage their most important people. Led by some of the most creative and analytical minds from the political, nonprofit, and brand worlds, Blue State is a part of WPP and has more than 150 employees across five offices.

Apply Now!"
Data Engineer,"Recognition Inc. 500 5000 Honoree Company for 2012,2013,2014,2015 and 2016 Atyeti Ranks No. 270 on the 2012 Inc. 500 List 2012,2016 and 2017 NJ 50 Fastest Growing Companies Global Investment Banking is looking for a Big Data Engineer for its team in New York.Long Term Contract Please contact Veena.Mahesh(at)Atyeti.com609-480-1642 Role Big Data Engineer I AM LOOKING FOR SENIOR ENGINEER FROM INVESTMENT BANKING BACKGROUND. 10+ YRS OF EXPERIENCE Responsibilities Design, architect and support new and existing data and ETL pipelines and recommend improvements and modifications. Create optimal data pipeline architecture and systems using Apache Airflow Assemble large, complex data sets that meet functional and non-functional business requirements. Be responsible for ingesting data into our data lake and providing frameworks and services for operating on that data including the use of Spark Databricks. Analyze, debug and correct issues with data pipelines Operate on or build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Spark and Azure technologies. Required Skills 8+ years of experience building high performance scalable enterprise analytics or data centric solutions At least 5 years of experience implementing complex ETL pipelines preferably in connection with Hadoop or Spark Experience working with Spark data pipeline and or streaming experience Exceptional coding and design skills in JavaScala or C. Expert in Python, or can demonstrate ability to readily learn new languages and affinity for them. Understand the nature of distributed systems design. Hands-on experience with Azure. Strong understanding usage of algorithms and data structures. Work well in a team environment, and are a self-starter. Ability to lead, or have led teams before. Expertise building and managing python libraries Nice to have Skills Azure Data Lake experience Machine Learning expertise Experience with Apache Airflow Experience with C .NET Core Experience with traditional data martswarehouses Education BS in Computer Science Software Engineering or equivalent experience."
Data Engineer,"Job DescriptionDesign, build and maintain high-performance, fault-tolerant, and scalable data pipelines for business reporting using Microsoft Azure Cloud Services and Microsoft DevOps using Agile software. Build a system that builds and evaluates machine learning models at scale. Work with technologies such as SQL, Azure and/or Kubernetes to build a data platform for projects. Work with data scientists to design and implement advanced statistical models and machine learning pipelines. Build automated tools to help answer questions about impact and design and manage queries from stakeholders. Translate end user requirements into Power BI reports and dashboards and analyze impact of product offerings using Power BI. Collect data from sources such as API, internal data source, and third party data source. Investigate data interactions and dependencies across complex data pipelines and transformation to validate assumptions and find sources of problems.QualificationsRequires: Bachelor's degree in Computer Science or Electrical or Electronic Engineering and 5 years' experience as above or as an EDI Administrator or Programmer Analyst using similar skills as above.Not available to persons needing sponsorship for employment.Employment TypeFull Time"
Data Engineer,"Help us create better connected futures
Make your mark. See the difference. Be recognized.
Work with aligned engaged team members.




About us
At Transit Wireless we create even better connected futures.
Ours is a story of being big enough to deliver and maintain large-scale operations but being nimble enough to make things happen.
Our future-makers thrive on rewriting yesterdays rules. So we put Wi-Fi, fiber and wireless connectivity on subways and we create new services for our customers and communities every day.
We build technology and teams that people want to be part of and we have the courage to not only imagine, but to do what really matters.

About the role

Design, implement and support an analytical data infrastructure.
• Managing AWS resources including EC2, S3, Glue, Redshift, etc.
• Working with AWS services including Lambda, Athena, and API Gateway
• Working with the Elastic Stack
• Interface with engineering teams to extract, transform, and load data from network data sources using SQL and AWS big data technologies
• Explore and learn the latest data management technologies to provide new capabilities and increase efficiency2
• Collaborate with Data Scientists, Network Engineers and Product Managers to recognize and help adopt best practices in reporting and analysis: data integrity, test design, analysis,validation, and documentation
• Help continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers
About you
Bachelor’s degree in Data Science, Computer Science, Engineering, Mathematics, or related technical discipline.
• 3 years of industry experience in a data engineering or data science role (development,data engineering, data science, or related field with experience in manipulating, processing,and extracting value from large datasets).
• Experience with management of relational databases such as PostgreSQL, MySQL or Microsoft SQL Server (Experience in data modeling, ETL development, and datawarehousing).
• Experience with AWS S3, EC2 and Lambda.
• Experience with shell scripting and working in an UNIX environment.
• Expertise in Python.
• Familiarity with engagement with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy
• Experience providing technical leadership and mentoring other engineers for best practices on data engineering
Preferred Qualifications / Bonus Points

• 5 years of industry experience in a data engineering or data science role.
• Master’s degree in Data Science, Computer Science, Engineering, Mathematics, or related technical discipline.
• Experience with analysis or engineering of time series datasets.
• Experience with the TICK Stack (InfluxDB, Chronograph).
• Experience with AWS big data technologies (Redshift, EMR).
Why work with us

We are a courageous culture and we do what really matters, putting people, customers and communities at the heart of every decision.
We believe there is strength in diversity and opportunity through inclusion.
And we provide a learning environment, meaningful recognition for your contribution and competitive compensation.
Help us imagine and create what’s next. Join us."
Data Engineer,"Data EngineerNew York, NYData Engineer New York, NY

Data Engineer for Leading Technology company in NYC

*This is remote to start then onsite. Sorry, our client is unable to consider candidates with a need for work visa transfer/sponsorship at anytime in the future.*

Our client is looking to hire a Data Engineer to join their growing team of analytics experts. This key role is closely aligned with companys software developers, database architects, data analysts and data scientists, and is responsible for ensuring that optimal data delivery architecture is consistent throughout ongoing projects. The successful candidate must be independent, self-directed and comfortable supporting the data needs of multiple teams, systems and products.

He/she will be responsible for expanding and optimizing the data and data pipeline architecture (in addition to optimizing data flow and collection for cross functional teams).

Required experience:
5+ years of experience expanding and optimizing analytics including the use of Oracle Utilities Analytics, Work Management Analytics, EBS Analytics and Enterprise Data Analytics platforms (EDAP).
5+ years of experience creating and maintaining optimal data pipeline architecture.
Demonstrated ability to assemble large, complex data sets that meet functional / non-functional business requirements.
Experience with the identification, design, and implementation of internal process improvements: automation of manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Experience driving the analysis of business processes, functions and procedures to identify opportunities, defining business requirements, and design of solutions to improve business efficiency.
5+ years experience with system/data integration, development or implementation of enterprise and/or cloud software (Oracle Cloud or AWS).
5+ years experience ETL development
5+ years SQL and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Knowledge of web services/ REST
Data modeling experience to support analytics (ie dimensional modeling)
Bachelor's Degree Computer Science, Information Technology or Related discipline"
Data Engineer,"Data EngineerWe are looking for a Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. You will take ownership of expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for multi-functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.In this role you will build and maintain an efficient data pipeline architecture, while assembling large, complex data sets that meet functional / non-functional business requirements. We are looking for you to Identify, design, and implement internal process improvements, such as automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability. You will build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS 'big data' technologies.Principal Responsibilities* Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.* Collaborate with partners from both the business and technology to assist with data-related technical issues and support their data infrastructure needs.* Build data and analytical tools for internal customers that assist them in building and optimizing our data organization into an innovative industry leader.* Work with data and analytics specialists to strive for greater functionality in our data systems.Qualifications/Skills Required* 5 + years of experience with:* Object-oriented/object-function languages: Python, Java, Scala, etc.* Hadoop, Spark, Presto, Kafka, etc.* Relational SQL and NoSQL databases, including SQL Server, MySQL and Cassandra.* Data pipeline and workflow management tools: Airflow, Luigi, etc.* AWS cloud services: EC2, EMR, RDS, Redshift, Athena, SQS, etc.* Savvy with data science stack (Pandas, NumPy, SciPy)* Data Science/Analysis background; Proficient at working with large datasets* Unix/Linux command-line experience* Experience working with AWS, GCP or Azure* Broad understanding of equities, derivatives, futures, FX, or other financial-services instruments"
Data Engineer,"Responsibilities

We find that the most successful candidates for the Data Engineer position are natural and relentless problem solvers who are passionate about working with data to solve business problems. These individuals demonstrate ease with quantitative analysis, can work well as part of small multi-disciplinary teams, have a keen interest in software engineering and are passionate about developing leading edge analytical business applications.

Our Data Engineers are part of the engineering team responsible for data architecture, back-end development and maintenance of our proprietary decision support software applications. They work closely with product management and the front-end development team to ensure that our products are constantly improving and have leading edge functionality.

You can expect the following responsibilities:
Collaborate with Product Management and other engineers on the team to make product improvements and develop new features
Hands-on SQL Server development (Stored Procedures, Functions etc)
Migrate complex data processing from SQL Server to Big Data using Spark, Scala in the near future.
Create and maintain detailed documentation and functional design specifications
Perform ongoing backend Database maintenance of existing applications
Provide technical information to assist in the development of client facing product documentation
Author and participate in software design and code reviews
Adhere to change management protocols and version control
Present demonstrations of new features to internal product teams as well as high level firm leadership and partners
Desired Skills& Expertise

Aspiring candidates should have the following background, skills and characteristics:
Bachelor’s degree, preferably in computer science, or engineering field
3+ years of experience in SQL Server development
1+ years of ETL experience using flat files and other RDBMS
Experience in troubleshooting and debugging SQL code
Familiarity with Tableau
Familiarity with the design, development and maintenance of best-in-class BI capabilities, including data warehouse data structures and data pipelines spanning Spark/Hadoop and RDBMS worlds
Expert-level database development experience in SQL Server, preferably for reporting data marts and business intelligence solutions, including writing stored procedures for complex business logic in T-SQL
Familiarity of architectural design patterns for micro-services leveraging relational and big data technologies is an added plus
Familiarity with Agile development process, SVN or other change management protocols would be a plus
Proven track record of academic and/or professional success
Exceptional analytical thinking ability, ease with quantitative analysis, and excellent problem-solving skills
Self–discipline and willingness to learn
Ability to work well with others in a high-pressure environment
Excellent verbal and written communication skills
All candidates must possess work authorization which does not (and will not in the future) require work sponsorship by an employer.

We are proud to be an Equal Opportunity Employer. Please visitwww.novantas.com for more information."
Data Engineer,"Entera, where residential real estate investing is made simple

At Entera, we are on a mission to transform the way investors find and buy properties. Powered by machine-learning, Entera's end-to-end residential real estate platform modernizes the real estate buying process. Entera's property source aggregation platform, discovery algorithms, intelligent tools and expert real estate service team help our clients access and evaluate more properties, make data-driven investment decisions, and win more - 100% online.

Entera is based in San Francisco, New York & Houston, with satellite service offices in 12 additional markets across the US. We're always looking for talented, creative and passionate people to join our team. If you're interested in opportunities at Entera, we'd love to hear from you!

Job Description

As a Data Engineer at Entera, you'll contribute to our best-in-class data pipeline and data-driven culture. You'll work with multi-discipline experts with hard-science backgrounds in a tight knit team to deliver on our efforts around data curation and management. You'll work with modern frameworks to ETL and massage data for preparation, and then utilize BI tools to develop visualizations and deliver data to both our internal business users and customers as you surface brand new depths of our vast dataset. You'll write Python, SQL, and R in shared notebooks that you and the data science team have ownership of. Within our team, you'll be able to further develop your skills and work with a team of experts to deliver on massive improvements to our data pipe and associated systems.

Successful candidates will thrive in Entera's unique operating environment and culture: high-growth, innovative, lean, and values-driven. As such, successful candidates must be highly capable in each of the following dimensions (among others): adaptability, curiosity, resourcefulness, analytical thinking/problem solving, pro-activity, collaboration, technological savvy, and operating in a dynamic environment.

Job Responsibilities
Use Python, SQL, and R to improve upon a best-in-class data pipeline and develop our workflows
Contribute to cloud-first services that improve our reporting, analysis, and metrics collection efforts
Use agile software development processes to iteratively make improvements to our back-end systems
Mold front-end and back-end data sources to help draw a more comprehensive picture of user flows throughout our system
Deliver on detailed specifications for business intelligence and reporting needs
Contribute and further develop our data-driven culture
Work with product and engineering in cross-functional teams to deliver on improvements to our systems
Preferred Qualifications:
MS or PhD in Computer Science, Mathematics, Statistics, Physics, Economics, or similar hard-science
3+ years hands-on experience in Data + Analytics at growing product-driven tech companies
Proficiency in cloud services and modern ETL workflows
Advanced capabilities across Python, R, and SQL
Understanding of Spark
Strong analytical and problem solving skills
Working knowledge of Python web frameworks like Flask
Software development background"
Data Engineer,"Hello. We’re Stadium Goods. We are the world’s premier sneaker + streetwear marketplace selling only the most sought-after footwear, apparel and other hard-to-find items on behalf of our sellers. We are driven by our principles and committed to providing the best consumer and consignment experience there is. In January 2019 Stadium Goods became part of the Farfetch family of companies, which will help guide our evolution as we usher in the next chapter of scale and growth.

Our fast-growing team is looking for a Data Engineer to be based in our New York City headquarters.

ROLE & IMPACT

Working on Data Engineering your role will be focused on creating access to critical data for a variety of teams, spread across the business and engineering groups. Stadium Goods is an international organization with diverse data from across multiple platforms (including stores) and your work will directly help the organization create value from that data. You will use a variety of techniques, ranging from structuring data for Tableau and creating the perfect report for broad consumption or writing APIs for the engineering team to consume data in a structured way.

You will join other data engineers who are building out the practice and you’ll have the freedom to explore new ways of thinking about these challenges and have the potential to make a significant impact on the way the business operates using data.

WHO YOU ARE
You understand the importance of process and structure but also know how to get things done in a scrappy way.
You’ve worked with a variety of types of data and know how to make things come together to form a cohesive dataset.
You are ready to answer the big questions about what’s working and what’s not, but also have the patience and focus to dig in on why a certain piece of data may or may not be correct due to technical issues.
You are ready to work closely with a variety of stakeholders to get things done and can explain problems from outside people’s experience in a way that helps them understand and feel engaged.

RESPONSIBILITIES
Create and maintain optimal data pipeline architecture
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build systems that allow for validation of our data product, ensuring accuracy of these data sets
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights
Requirements
2-5 years’ experience working in Data Engineering
A foundation in Computer Science, Engineering, Stats, or Math, through education and/or work
Advanced SQL skills
Strong Python or R, and Tableau skills
Experience with AWS or Azure clouds
Linux/Unix experience preferred
Prefer experience working in ecommerce (bonus points if you’ve worked with eBay, Amazon, or Tmall)
Experience with Airflow, DBT, or similar tools for managing ETL pipelines
Bonus for working on international commerce
Ability to make thoughtful trade-offs among a diverse set of teams and priorities, filling the gap between business requirements and engineering, and suggesting reasonable workarounds when necessary
Bonus points if you have a passion for the sneaker and streetwear industry
Benefits
Comprehensive total rewards package that includes competitive salary, company equity, health benefits and 401k
Paid time off & work from home policy
Dynamic career growth opportunities
A fun, creative and mission-driven work environment
Team outings and afterwork events
Stadium Goods is committed to being an inclusive workplace where diversity in all its forms is celebrated. We make employment decisions without regard to race, religious creed, color, age, sex, sexual orientation, gender, gender identity, gender expression, national origin, ancestry, marital status, medical condition as defined by state law, physical or mental disability, military service or veteran status, pregnancy, childbirth and related medical conditions, genetic information or any other classification protected by applicable federal, state or local laws or ordinances. If you require special accommodation, please let us know.

LOCAL CANDIDATES ONLY

ABSOLUTELY NO RECRUITING AGENCIES PLEASE"
Data Engineer,"FanDuel Group is an innovative sports-tech entertainment company that is changing the way consumers engage with their favorite sports, teams, and leagues. The premier gaming destination in the United States, FanDuel Group consists of a portfolio of leading brands across gaming, sports betting, daily fantasy sports, advance-deposit wagering, and TV/media, including FanDuel, Betfair US,and TVG. FanDuel Group has a presence across 45 states and 8 million customers. The company is based in New York with offices in California, New Jersey, Florida, Oregon, and Scotland.

Our competitive edge comes from making decisions based on accurate and timely data. As a Data Engineer, you will help us build scalable systems to provide access to that data across the company.

What we're looking for

We are looking for an experienced Data Engineer, ideally well versed in Python, with a deep understanding of large scale data handling and processing best practices in a cloud environment. You should be comfortable building complex yet performant SQL queries on large data sets. Our current stack is built on AWS with Spark and Hive on Amazon EMR for batch processing and Redshift for the data warehouse. Experience working with and tuning these for large scale workloads would be a plus.

Data is a key component of the business used by almost every facet of the company including product development, marketing, operations and finance. It is vital that we deliver robust solutions that ensure reliable access to data with a focus on quality and availability. We operate a rigorous code review process, so you need to be able to continuously give and take feedback and act on it.

As our data is always growing it is important that we have a cost effective data warehouse with data that is modelled to suit our users needs.

Looking ahead to the next phase of our data platform we are keen to do more more with real time data processing and working with our data scientists to create machine learning pipelines. We would love to hear how you have tackled these before.

What you get in return

Beyond working with such a great team?
An exciting environment with real growth
Contribute to exciting products used by a highly passionate user base
Personal learning and development opportunities
Flexible holiday allowance
401K plan with company match
Attractive health insurance premiums
There's more, but we don't want to go on and on.

FanDuel is an equal opportunities employer. Diversity and inclusion in FanDuel means that we respect and value everyone as individuals. We don't tolerate bias, judgment or harassment. Our focus is on developing employees so that they reach their full potential."
Data Engineer,"Title
Data Engineer, #3969

10-Jul-2020

Job Description
Design, build and maintain high-performance, fault-tolerant, and scalable data pipelines for business reporting using Microsoft Azure Cloud Services and Microsoft DevOps using Agile software. Build a system that builds and evaluates machine learning models at scale. Work with technologies such as SQL, Azure and/or Kubernetes to build a data platform for projects. Work with data scientists to design and implement advanced statistical models and machine learning pipelines. Build automated tools to help answer questions about impact and design and manage queries from stakeholders. Translate end user requirements into Power BI reports and dashboards and analyze impact of product offerings using Power BI. Collect data from sources such as API, internal data source, and third party data source. Investigate data interactions and dependencies across complex data pipelines and transformation to validate assumptions and find sources of problems.

Requisition Number
3969BR

Qualifications
Requires: Bachelor’s degree in Computer Science or Electrical or Electronic Engineering and 5 years’ experience as above or as an EDI Administrator or Programmer Analyst using similar skills as above.

Not available to persons needing sponsorship for employment.

State
New York

Employment Type
Full Time

City
New York

Company Name
Munich Re America Services

Country
United States of America"
Data Engineer,"Ladders is seeking an experienced Data Engineer to join our engineering-led organization. Come join the team responsible for the flow of data from ingestion to consumption. This role is highly visible and works with a variety of stakeholders. We currently run primarily RDBMS in the cloud but are looking to expand our use of cloud native technologies. The ideal candidate would understand both RDBMS and newer technologies (like RedShift, BigQuery, etc.). We seek someone with a passion for data while being grounded in solid engineering fundamentals (version control, monitoring, automated testing, etc.). The right person will have ample opportunity to make an impact on the organization.
RESPONSIBILITIES
Participate in the architecture and development of our cloud-based data pipeline
Practice and promote craftsmanship in data infrastructure components (monitoring, testing, code reviews, documentation, scalability, performance, etc.)
Own long-term impacts of key design decisions and balance technical debt with business needs
Break down requirements, estimate tasks, and assist in planning roadmap accurately
Help users get value from analytic data
Grow engineering teams through mentorship, recruiting, and interviewing
Focus on team over individual achievements
Lead definition of team objectives and drive team towards them
QUALIFICATIONS
Previous success in a Data Engineering role, working knowledge of data concepts (optimization, integrity, policies, etc.)
Strong SQL and Relational Skills
Working knowledge of Cloud Data Platforms (RedShift, BigQuery)
Ability to collaborate cross functionally with other engineering and non-engineering departments
Architected non-trivial solutions for a company at scale
Curious, self-motivated, and an empathetic drive to solve problems
BONUSES
Project/Product management experience
Application development experience
Previous experience working in a startup environment"
Data Engineer,"Role Description

J.P. Morgan ' s Corporate & Investment Bank (CIB) is a global leader across banking, markets and investor services. The world ' s most important corporations, governments and institutions entrust us with their business in more than 100 countries. With $23.5 trillion of assets under custody and ranked #1 for Global Investment Banking fees, the Corporate & Investment Bank provides strategic advice, raises capital, manages risk and extends liquidity in markets around the world.

Department Description

The Cross Product Change Services (CPCS) Team within Corporate and Investment Bank serves multiple internal and external clients in providing change services such as Business Analysis, Minimum Viable Product Assessment, Reference Data Strategy definition and implementation, Process assessment and re-engineering, Automation, Implementation support, Test strategy and delivery.

CPCS is currently looking for an experienced Data Engineer to join the organization and play an integral role in the build out of the instrumentation required for measuring and impacting the transformation of the Operations workforce collaborating with the Business and Technology.

Key Responsibilities:
Responsible for delivering insights and reporting to the management team and stakeholders, based on the data gathered from diverse applications , processes and manual activities
Enable the management team and stakeholders to make timely, well-informed and data-driven decisions
Continuous innovation - seek out and adopt cutting edge techniques to improve the storage, collection, visualization and analysis of data
Development of detailed data model, in line with detailed business requirements, to ensure that the business is able to meet their business objectives
Use your experience in engineering, optimizing and debugging high volume data pipelines to contribute to the creation of reporting platforms.
Strive for the highest standards of data quality and governance
The successful candidate will join a team focused on change delivery across products, help accelerate pace of delivery, provide metrics, KPIs to enable data based decision making and be responsible for supporting the strategy and execution of Cross Product Change Services objectives and initiatives.

The role will require working with stakeholders across the CIB, looking to leverage best practice across the industry and other J.P. Morgan business units. Lateral contacts in business, technology, middle offices, product and client facing organizations firm wide should be developed and mined for best practices.

Qualifications:
2+ years of experience working in Business Intelligence / MIS Reporting on an enterprise level
Understanding and related experience in the integration of data from multiple data sources and automation of data processes
Understanding of data warehousing with coding abilities in the field of data manipulation
Depicts strong knowledge and experience in querying, manipulating and summarizing large amount of data through SQL, in addition to experience in scripting languages
Knowledge and build experience in Alteryx, Xceptor and Tableau.
Excellent analytical skills and ability to understand and build complex data models
Strong understanding of SDLC in an enterprise environment and experience working as an agile developer
Solid experience of and enthusiasm for agile development methodologies
Thoughtful and comfortable communicator (in person or on paper) with both technical and business stakeholders, with the ability to facilitate discussions and conduct training
J.P. Morgan is a global leader in financial services, providing strategic advice and products to the world's most prominent corporations, governments, wealthy individuals and institutional investors. Our first-class business in a first-class way approach to serving clients drives everything we do. We strive to build trusted, long-term partnerships to help our clients achieve their business objectives.

We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs."
Data Engineer,"The Data Engineer role, as a member of the Technology team, will report to the Director of Data Engineering with the goal of supporting the firm's progress towards simplifying and streamlining how it consumes, manages, and distributes information across its growing data universe. Central to these efforts is a multi-pronged approach to database stewardship and governance:* Fielding and satisfying the inflow of Ad-Hoc requests of varying size, scope and frequency that originate from all functional areas of the firm* Developing and implementing sustainable data-driven solutions relevant to projects at the department level* Implementing a ""full-stack"" approach to each solution with the entire application life-cycle in mind (work Upstream, think Downstream)"
Data Engineer,"Job Description
Not ready to apply? Message with the recruiter of this job to learn more:
https://strike.chat/s/schedule-chat/bcg_platinion/data_engineering/e677c?src=job-board
At BCG Platinion, we are thinkers, makers, and doers who love solving hard problems. We aren’t afraid to roll up our sleeves and dive into the weeds of complexity—in fact, that’s our job. When companies are tasked with understanding and managing their data, we come in and develop the products, tools, and platforms they need to harness that information for real impact.

(Senior) Data Engineers at BCG Platinion are:
Iterative. They are excited to prototype at all levels of fidelity—and have the humility to walk away from ideas when they fail.
Collaborative. They have the ability and enthusiasm to work with researchers, engineers, business consultants, and other designers who will challenge and support one another.
Comfortable with ambiguity. They know projects and businesses move fast. That means the path forward isn’t always well-defined. They are comfortable and collaborative through our process.
Interdisciplinary. They deliver data products for digital solutions, deploy analytical models into production, fix existing data platforms, or coach and enable other teams in best practices depending on need.
You’re Good At:
Working with a diverse set of clients across domains and industries
Implementing data orchestration pipelines, data sourcing, cleansing, and augmentation and quality

control processes
Deploying machine learning models in production
Supporting data architects in designing data architectures
Assisting in mentoring data engineers to further their personal and professional growth
Supporting project management operations of a project
Translating business needs into solutions
Contributing to overall solution, integration, and enterprise architecture
You’ll Bring:
2+ years of experience working on large scale, full lifecycle data implementation projects
BS/BA in data engineering, software engineering, data science, computer science, applied mathematics, or equivalent experience
2+ years professional development experience with some of the AWS/Azure/GCP data stack: S3, Redshift, AWS glue, EMR, Azure Data Warehouse, Azure Blob Store, Google Big Query
A deep knowledge of performant SQL and understanding of relational database technology
Hands-on RDBMS experience (data modeling, analysis, programming, stored procedures)
Expertise in developing ETL/ELT workflows with one or more of the following: Python, Scala, Java
Deployment of data pipelines in the Cloud in at least AWS, Azure, or GCP
A deep understanding of relational and warehousing database technology,

working with at least one of the major databases platforms (Oracle, SQLServer, Teradata, MySQL, Postgres)
Additional consideration to candidates who possess some of the following criteria:
Experience working with Big Data technologies such as Spark, Hive, Impala, Druid, or Presto
A solid foundation in data structures, algorithms, and OO Design with fundamentally strong programming skills
Proven success working in and promoting a rapidly changing, collaborative, and iterative product development environment
Strong interpersonal and analytical skills
Intellectual curiosity and an ability to execute projects
An understanding of “big picture” business requirements that drive architecture

and design decisions
DevOps and DataOps skills including “infrastructure as code” systems like

CloudFormation or Terraform
Data system performance tuning
Implementation of predictive analytics and machine learning models (MLlib,

scikit-learn, etc)
Willingness to travel around the globe to work with clients and BCG teams. At

times, this role involves significant travel to client sites. The amount of travel will depend on client needs and nature of projects
Not ready to apply? Message with the recruiter of this job to learn more:
https://strike.chat/s/schedule-chat/bcg_platinion/data_engineering/e677c?src=job-board"
Data Engineer,"Data Engineer, #3969

Munich Re America Services, New York, United States

Entry level

Professional

Type of job

Full-time

Area of Expertise

IT/Cyber

Your job


Design, build and maintain high-performance, fault-tolerant, and scalable data pipelines for business reporting using Microsoft Azure Cloud Services and Microsoft DevOps using Agile software. Build a system that builds and evaluates machine learning ...
Apply now
Open job details

Published on 07/10/2020"
Data Engineer,"We are looking for a data engineer to manage and further build out our data infrastructure pipelines and data visualizations. The work will support core business decisions and models that serve as the basis for core product and growth strategy. Candidates should be able to choose the right tool for the job and learn how to use it if they don’t know how to already. Candidates should prioritize robustness, scaling considerations, and simplicity in architecture decisions.

Requirements

You have at least 2 years in industry. You are experienced with:

- SQL (Postgres in particular)

- Python

- Redis

- AWS

- Periscope or Looker

Optional Skills and Experience:

We will prioritize candidates that have the following additional experience:

- Experience with Segment or another CDP

- Familiarity with Node

- Machine learning

- Data visualization, web and/or mobile (eg, D3.js)

- Spark, Hadoop, or Hive"
Data Engineer,"We are looking for a Data Engineer to join our team. You will be responsible for all aspects of the design, development and delivery of data and database solutions.

What you'll do...
Design, develop and implement data solutions. This may include: architecture design, prototyping of concepts to proof of concept, development of standards, design and development of test plans, code and module design, development and testing, data solution debugging, design and implementation of a solution that follows efficient design techniques and development that meets and exceeds the intent of the design of the data solution.
Develop, construct, test and maintain optimal data pipeline architecture
Support and maintain data and database systems to meet business delivery specifications and needs.
Partner with a team of developers to effectively meet the deliverables and schedule of a data solution component within a larger application project.
Lend support to various business and technology teams as necessary to ensure solid, scalable, robust solutions.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using MySQL, Python and ETL Tools
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Prepare data for predictive analysis & use data to discover tasks that can be automated
Partner with internal stakeholders including the BI & Finance teams to assist with data-related technical issues and support their data infrastructure needs.
About you...
3-5 years of experience in a Data Engineer role
Degree in Computer Science, Statistics, Information Systems or equivalent field
Advanced working SQL knowledge and experience working with relational databases
Proven experience with Data Transformations & Integrity testing by using test-driven development methodologies.
Demonstrated experience with EXPLAIN, indexing and query optimizations
Prior experience with object-oriented/object function scripting languages: Python, Java, C++
Prior experience with Amazon AWS RDS & Aurora
Working knowledge of with Docker, Git & SDLC Methodologies
Knowledge of Salesforce Data APIs and SOQL
Experience building (or redesigning) and optimizing data pipelines, architectures and data sets.
Knowledge of non-SQL data sources: such as JSON, MongoDB, and consuming data from APIs
Prior experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Proven experience building processes supporting data transformation, data structures, metadata, dependency and workload management.
Demonstrated experience with data security, encryption, HIPAA
About us...

Progyny is a leading fertility benefits management company in the US. We are redefining fertility and family building benefits, proving that a comprehensive and inclusive fertility solution can simultaneously benefit employers, patients, and physicians.Our benefits solution empowers patients with education and guidance from a dedicated Patient Care Advocate (PCA), provides access to a premier network of fertility specialists using the latest science and technologies, reduces healthcare costs for the nation’s leading employers, and drives optimal clinical outcomes. We envision a world where anyone who wants to have a child can do so.

Our mission is to make any member’s dream of parenthood come true through a healthy, timely, and supported fertility and family building journey.

Come join a company that’s been recognized by Modern Healthcare as one of the Best Places to Work in Healthcare.

Our perks:
Family friendly benefits:
Paid maternity and paternity
Fertility benefits (including egg freezing and IVF)
Emergency childcare program
Parent’s group
Health, dental, vision and life insurance options for employees and family
Paid vacation and summer flex time
Company equity
Bonus program
401K Match
Monday breakfasts/ Friday lunches/ healthy snacks
Company social events
Sit/ stand desks
Progyny is an Equal Opportunity Employer. We respect and seek to empower each individual and support the diverse cultures, perspectives, skills and experiences within our workforce."
Data Engineer,"Â

Â

4+ years' work experience as a Data Engineer or similar role
2+ years of hands-on development experience in AWS
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing 'big data' data pipelines, architectures and data sets.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Hands-on implementation experience in manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores.
Strong engineer with ability to abstract our core/repeated requirements into a build-once, configurable framework.
Experience with Dev/OPS architecture, implementation and operationÂ
3+ years software development experience with object-oriented programming languages, preferably Python
Knowledge of version control systems: Git, Azure Devops, Bitbucket
Knowledge of AWS cloud services: EC2, EMR, S3, RDS, Glue
Knowledge of data pipeline management tools: Airflow, Luigi, Tidal
Knowledge of data blending tools: Alteryx, Matillion, Attunity
Previous experience with the Snowflake database management system
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Jenkins + CI/CD, Kafka, Containerization (Docker), DBT
AWS certified
Â

Pavan Kumar | Sr IT Recruiter
E: pavan@conchtech.com | T: 9018081426
Conch Technologies, Inc | www.conchtech.com"
Data Engineer,"Come to Insider Inc to build great data products! We are looking for a new team member to join data engineering, where you will be responsible for identifying the data we need to collect, building great ETLs, writing superb SQL against our warehouse, and then working with data science and analytics teams to make the business work better and go faster.Responsibilities:* Collaborate with analysts and business stakeholders to understand data needs and scope out technical requirements for data products* Design and implement data architecture, and support feature enhancements to the data warehouse* Develop and maintain scalable and fault-tolerant data pipelines connecting multiple internal data sources and third-party data through APIs as needed* Discover new opportunities to automate ETL, reporting, and other analytics tasks to drive efficiency and optimize business operations company-wide* Work with stakeholders across divisions to ensure data quality and integrity from data collecting, processing, and presentation* Develop strong subject matter expertise in the data sets that flow into the infrastructure, assess and evaluate the value and quality of new data sources* Explore and identify next-generation reporting options that meet the company's future analytics needsWhat we are looking for:* 2-5 years of experience building data products* Proficiency in Python and SQL* Ability to communicate excellently with both business and technical audiences* Experience translating data needs into deliverables to meet business goals* Self-directed, supremely curious, and detail-oriented* Team player who's able to collaborate across functions* Experience with Google Tag Manager is a plusAbout us:Insider Inc. is a global news and lifestyle publication with teams in New York, London, San Francisco, Los Angeles, and beyond. Our mission is to inform and inspire the digital generation and become the most influential and loved journalism brand in the world. To do this, we delight our audience, clients, and colleagues and we get better every day (you'll hear that mantra a lot). Our employees are world-class storytellers, excellent communicators, ""good eggs"", and-most of all-effective. We make things happen, love what we do, and have fun while we do it. You can read more about our Mission, Values, and Culture here and more about our company and brands here.Our newsroom has three divisions: Business Insider, Insider News, and Insider Life. As the 4th largest digital media brand in the U.S, we tell stories that command attention and inspire action. We equip a curious and action-oriented audience of more than 375 million, including half of all U.S. millennials on the internet, with the information and inspiration they need to keep the world, and their lives, moving forward. The company started by building the world's largest digital business news site which and has continued to grow and evolve to match the changing passions and interests of its audience. Insider Inc. is home to 50+ content verticals, the #1 daily news show on Facebook, and we publish 16 global editions in 8 languages.We're always growing, and we're looking for talented, curious, and motivated individuals to join our team. We'd love to hear from you - even if you don't meet 100% of our requirements."
Data Engineer,"Our client, a Fortune 500 Company is looking to fill multiple roles for Data Engineers of all levels with a variety of skills. The right candidate(s) will have one of or multiple skills in each category Programming (Java, Scala or Python) Computer Science Fundamentals (Algorithms and Data Structures) Distributed Computing (Spark or Parquet) Proficiency in DatabasesSQL, performance tuning, NoSQL, MongoElastic Web Application Architectures (HTTPHTMLCSS, JavaScript) FinancialFixed income Knowledge is a plus Spark, Scala and Functional Programming Event Driven Programming (Kafka, Akka, Flink or Spark Streaming) FrontEnd Development (Angular, Java Scripts, Reactive Programming) Kubernetes, Docker, AWS Infrastructure and Process Management What You Will Do Build complex data ingestion pipelines using Scala, Spark, Parquet and S3 Design scalable processes in event driven architecture to support Fixed Income applications Develop near real time streaming analytics using KafkaKinesis Act as Subject Matter Expert and help rest of the team in leveraging the platform and migrating applications to it Establish end to end data lineage and data catalogue. Work with data governance team to setup data quality checks and metrics Create self-service notebook environment with ZeppelinJupyter for exploratory data analytics and rapid interactive development Troubleshoot any performance issues and ensure efficient data organization Build efficient web-based tools for monitoring and tracking What You Will Need Bachelorrsquos degree in computer science or related field required. Masterrsquos degree preferred Knowledge of data structures, algorithms and functional programming Passion to learn new things, experiment with new ideas and build world class data platform 5+ years of experience in programming with Scala, Python or Java 2+ years of experience in Scala, Spark and functional programming. Deep knowledge of spark internals such as partitioning, DAG, lazy evaluation. Strong experience with relational data bases, SQL, and query optimization. Knowledge of data warehousing, dimensional data model and business intelligence is a plus. Knowledgeexperience with event driven programming and Akka actor model"
Data Engineer,"About TelTech

A member of the IAC family, TelTech builds innovative communication apps that help people better enjoy their mobile-connected lives. From RoboKiller, an app that stops and gets even with telemarketers, to TapeACall, which records iPhone conversations, we build apps that protect our customers every day. We are looking to expand our team with creative, talented, and passionate people who are committed to the growth of our existing products as well as new products we intend to launch. TelTech's sister companies in Mosaic Group include iTranslate (Apple Design Award Winner 2018), Daily Burn (top-rated health and fitness company), and Apalon.

Position Summary

This position will serve as the primary data engineer for TelTech. A successful data engineer at Teltech will have significant data warehousing experience, Python development experience, and mastery of SQL. The data engineer will also be proactive in maintaining relationships with product, marketing, and analytics to ensure all data-related business needs are met.

Responsibilities
Gather requirements from business stakeholders (product, marketing, analytics) and effectively translate them into a data environment that supports business needs
Design and maintain data warehouse (PostgreSQL and BigQuery) and data lake (GCS)
Build batch and real-time data pipelines to ingest and process data from various sources such as product backends, third party platforms, etc.
Build internal tools and services to support data analytics and engineering
Help product, marketing, analytics, and engineering with diagnosing data-related issues
Document, profile, and manage data across multiple products and platforms for data governance (consistent naming conventions, data validation, data retention)
Maintain a line of communication between developers and data team to ensure all stakeholders are up to speed on changes to source database schema for analytics purposes
Work closely with other data engineering teams across Mosaic to ensure alignment of methodologies and best practices
Qualifications
2 - 4 years experience programming and scripting in Python within a production environment
Hands-on experience with relational databases (MySQL and PostgreSQL)
Familiarity with creating ETL processes for large-scale data warehouses
Must have familiarity with GCP and its offered services (BigQuery, CloudSQL, Dataprep, etc)
Knowledge of PubSub, or relevant messaging queue like Kafka, is a must
Distributed file systems (GCS, HDFS) knowledge is a must
Familiarity with Java, JavaScript, Airflow, Docker, Scala, and Spark are preferred
Familiarity with NoSQL is a plus
Prior experience with CI/CD is a plus
Why TelTech?
An amazing working environment with a lot of perks including but not limited to:
Unlimited PTO!
Matching 401k!
Company Contribution towards Commuter Benefits!
Fully stocked kitchen!
Environment where you can mentor and learn from others.
PRIVACY STATEMENT"
Data Engineer,"Come see what's cookin' at HelloFresh!At HelloFresh, we want to revolutionize the way we eat by making it more convenient and exciting to cook meals from scratch. We have offices all over the world and we deliver delicious meals to millions of people.We are the industry leader in meal-kit subscription services and we're growing all the time. We have distinct meal-kit services that cater to everyone with the most menu variety in the market, which allows us to reach an incredibly wide population of people.The HelloFresh team is diverse, high-performing, and international, and our work environment is an inspiring space where you can thrive as a result.Job Description:We are growing our Data Engineering team to take our data modeling and automation to the next level. The Data Engineering team is part of the broader Growth Organization (Data Science, Digital Product, Marketing) and works closely with Data Scientists to build and maintain best-in-class data products to improve HelloFresh's user-experience and marketing effectiveness. The Data Engineering department is focused on designing scalable and automated data flows using a variety of big data tools and platforms (AWS Glue, Airflow, Spark, Databricks Cloud etc..)We are hiring a Senior Data Engineer to lead HelloFresh's Data Infrastructure & Automation workstreams as a key member of the Data Engineering team.Our vision is to maintain a best-in-class automated data platform directing our marketing initiatives on delivering the right food box at the desired price point to the front door of all our customers. We are looking for someone who can head up our engineering key projects and mentor our more junior engineers.You will ...* Data modeling: design, extend and update the HelloFresh's data model (create new schemas, fact tables, mat views, joins, etc.)* Data cleaning/enrichment: keeping data clean and consistent with production systems (e.g. bug fixes, backfills )* Design and implement end-to-end data products and marketing automation flows: from data ingestions for data science modeling to creation of automated pipelines to external software (Salesforce, etc.)* Data infrastructure: maintaining and deploying ETLs on Databricks cloud* Design automated Monitoring of ETLs in production* Data Transformations: implement the logic of the data pipeline (aggregations, projections, selections, etc )* Data Cleaning: keeping data clean and accurate with data sources (DWH, 3rd Party)You are...* An active, solution-oriented member of autonomous, cross-functional agile teams collaborating with Product Owners, Data Scientists, and Business Intelligence teams* Able to develop an in-depth understanding of HelloFresh's core product and architecture, and act as an ambassador for state of the art software solutions and industry best practices offering support and mentorship to colleaguesAt a minimum, you have...* MSc in a STEM discipline* 5+ years' data engineering experience is required* Advanced Python (functional and OOP) and SQL skills (DDL, DML, CTEs, query optimization, ...) experience with Apache Spark required* Knowledge of data structures (DataFrames, RDDs, Dataclasses) and data formats (CSV, JSON, Parquet, Avro, ORC)* Required prior experience installing data architectures on Cloud providers (e.g. AWS), using DevOps tools and automating data pipelines.* The ability to design, implement and deliver maintainable and high-quality code using best practices (e.g. git/github, secrets, configurations, yaml/json)* Experience with data modeling, design patterns, building highly scalable and secured solutions preferred* Min 2 years experience with job orchestration tools like Airflow, Luigi or similar* Experience with end-to-end testing of data pipelines and implementing Data Quality checks (e.g. using frameworks AWS Deequ) is preferred.You'll get* Competitive Salary & 401k company match that vests immediately upon participation* Generous parental leave of 16 weeks & PTO policy* $0 monthly premium and other flexible health plans effective first day of employment* 75% discount on your subscription to HelloFresh (as well as other product initiatives)* Snacks, cold brew on tap & monthly catered lunches* Company sponsored outings & Employee Resource Groups* Collaborative, dynamic work environment within a fast-paced, mission-driven companyIt is the policy of HelloFresh not to discriminate against any employee or applicant for employment because of race, color, religion, sex, sexual orientation, gender identity, national origin, age, marital status, genetic information, disability or because they are a protected veteran."
Data Engineer,"Are you interested in developing a large-scale data warehouse while implementing KPIs to measure the impact on end-user productivity? Are you proficient in utilizing programming languages and have a strong data analytics experience? Do you want to work for a highly skilled team of engineers that are doing both? Then you could be Fivesky's next Data Engineer.Who you are:* 7+ years of relevant experience in an enterprise environment.* Bachelor's in Information Technology, Computer Science or a related discipline.What you've done:* Deep proficiency in programming languages such as Java, C++, Python, C# or similar languages* Experience in creating data visualizations and developing a large-scale data warehouse* Ability to work across the stack from backend system development and data engineering to front end user interface design* You have worked with a big data platform like HadoopWhat you will do:* Design and develop data warehouses to inform applications* Update platform functionalities by creating external tools* Create a data science platform for enabling the calculation of dynamic KPIs* Collaborate with teams across the firm to deliver integrated solutions and quantify the organization's impact* Provide technical support to the operations in order to deliver a dependable and highly-functioning production platform* Construct visualizations and dashboards in order to drive key management decisions* Improve and maintain processing architecture/systems and existing log streaming* Translate requirements from product managers into technical implementations* Analyze new data sources and create data visualizations to support business use cases* Explore and evaluate cutting-edge technologies, suggest improvements based on best practices, trends, and industry standards and choose appropriate solutions to solve problemsAt Fivesky, our employees are our greatest asset and the focal point around which we operate; therefore, we always want the best for our employees. In addition to offering competitive compensation plans and long-term career opportunities, we offer an attractive mix of benefit plans to our employees that include provisions for vacation, holiday pay, and sick days.Fivesky is an equal opportunity employer and will consider all applications without regard to race, sex, age, color, religion, national origin, veteran status, disability, genetic information, or any other characteristic protected by law.This is a full-time, direct employment opportunity in New York The salary is open based on experience and qualifications."
Data Engineer,"Small Teams; Big Data


At Tapad, we look for individuals who are motivated by complex and challenging work. We want to work with people who share compelling solutions to those challenges, solutions informed by their unique experiences, passions, and expertise.

We are looking for a Senior Data Engineer to join our global engineering team in New York City. We need a person who can work hands-on as an engineer, solving complex problems and building advanced software systems. We face daily challenges that are both unique and engaging, while processing data at petabyte scale. That is over one trillion data points in any given 60-day period — with consumer privacy and data security at the heart of everything we do.

The size and scale of our challenges demand the use of cutting edge, open-source technologies, like Apache Spark, Apache Beam, and Kubernetes. We're proud to have been built on Scala from day one.

Collaboration is at the forefront of our day-to-day at Tapad. You would be working with multiple engineering domains, and with our commercial teams, to guarantee that we are delivering the best products. We believe that our engineers have an obligation to dissent and discuss. A successful Tapad engineer understands that their ideas hold weight, and they contribute freely and regularly. We want someone who is motivated to find large-scale solutions with us. We work with big data on small teams. Every contribution is important.

When you work with us, you matter. We ask our employees to make an impact and feel it is only right to give a lot in return. We believe if you're sick, feel like you're getting sick, or just need a personal day, take that time to get better. We love to develop a sense of community, so we host meetups, share knowledge, and have regular team outings. At Tapad, we make sure our office is full of individuals who can teach and learn from one another.

Technologies we use at Tapad (don't worry, we'll teach you):
Google Cloud Platform (GCP), Google Dataflow/Beam, SQL, BigQuery
Scala, sbt, cats, http4s, fs2
Spark ML, TensorFlow, Kubeflow, Python, PyTorch
Airflow, Prometheus, Kubernetes
We are looking for candidates who meet some of the following qualifications:
5+ years of experience making significant contributions in the form of code
Strong understanding of algorithms and data structures and knowing when to apply them
Deep familiarity with one or more programming languages (Ex: Scala, Java, Python, etc.)
Interest in machine learning techniques to develop better predictive and clustering models
Experience working with high-scale systems
Proficiency in SQL
Interest in creating powerful machine learning tools that facilitate experimentation and productionalization at scale
Interest in data engineering and warehousing to develop ingestion engines, ETL pipelines, and organizing data to expose it in a consumable format
Passionate about helping mentor your teammates grow by providing insightful code reviews and feedback
Lead technical projects. Taken responsibility for the planning, execution, and success of complex technical projects
Bonus Experience:


Experience with building systems for model training and serving using TensorFlow, Keras, PyTorch, Spark ML, or Kubeflow
Working with productionalization of Machine Learning research, and ML project lifecycle management
Understanding and genuine interest of AI and machine learning methods and algorithms, e.g., gradient based optimization and neural networks
Experience with functional programming
Strong understanding of concurrent and parallel programming
A day in the life of a Tapad Engineer:
Solve complex problems building advanced software systems, while processing several petabytes of data
Adapt quickly to utilize software engineering best practices
Demonstrate the ability to deliver quality software collaboratively
Designing, implementing, and running big data pipelines that canvas over petabytes of data
Contributing to real production projects that constitute Tapad's core offering
Collaborate with your team of engineers, and add to product, account, and business development functions to create new products and features
Learn functional programming
Ensure that the software you create is testable and tested
Serve as a senior member of the team by contributing to the architecture, design, and implementation of Tapad's systems
Lead and drive technical projects. Take responsibility for the planning, execution, and success of complex technical projects
Tapad Perks:
Generous PTO - no accruing necessary
401k matching
On-site medical and dental partitioners (we bring the doctors in-house so you can make appointments at your convenience)
Scala School (we'll teach you!), Coursera, LinkedIn learning, peer-lead professional development, and an abundance of resources to help you stay sharp
Unlimited snacks and beverages, collaboration catered lunches
Discounts on gym memberships
Foosball, ping pong, diversity and inclusion group, book club, Tough Mudder, push-up challenges, and tons of other extra-curricular activities that will make you feel like part of the Tapad family
Check out our #TapadLife page to see what our employees have to say
Find more about our engineering culture HERE
About Tapad:


Founded in 2010, Tapad cracked the code on cross-device marketing technology. Our groundbreaking, proprietary technology assimilates trillions of data points to find the relationship between smartphones, desktops, laptops, tablets, and connected TVs. Ten years later, we are processing data at petabyte scale, with an engineering team that comprises roughly half of our entire organization. When you work with us, you matter, and your work matters.

We use Scala, in combination with large-scale data processing and open-source technologies, to build our device graph. Across our engineering teams, we also use Scala, GCP, Spark, Kubernetes, Python, TypeScript, Angular, and anything else that helps us get the job done. We're open-minded about new technologies, we're passionate about what we do, and we make time for everyone to learn and grow as the industry changes. Engineers at Tapad are approachable and ambitious people who think outside the box and solve big problems collaboratively. Are you up for the challenge?

Tapad is proud to be an equal opportunity employer and will consider all qualified applicants regardless of age, sex, race, religion, national origin, sexual orientation, gender identity, marital or family status, disability, or any other legally protected status. Tapad does not accept resumes from unsolicited search firms nor recruiters. In no event shall fees be paid to any unsolicited search firms nor recruiters, regardless of whether the candidate is made an offer or accepts a placement at Tapad. All resumes received through any channels will be considered the sole property of Tapad."
Data Engineer,"Data Engineer

INTURN is the first marketplace to help brands efficiently sell their excess inventory to retailers. We are a venture backed NYC start-up addressing a complex section of the retail industry.

Job summary

You have the opportunity to collaborate with application engineers, data analysts, and product managers to store, understand, and allow reporting on INTURN’s data. We seek to manage a collection of data and warehousing. Successful candidates will help INTURN manage the brokerage of millions of dollars of merchandise and complexities of a rapidly growing business. INTURN’s analytics team is looking for someone to support data visualization and reporting tools for internal users.

You may be the one to join an awesome team and be part of iterative development, from compiling, vetting specifications, schema design through integration, testing and deployment If you are passionate about building a product that is solving a real-world problem that buyers and sellers will use every day.

INTURN is looking for a self-motivated, detail oriented, individual with strong problem solving skills who can think outside of the box.

Key Qualifications
Strong CS skills with 3+ years in professional data-oriented engineering experience.
Expert in at least of one or more of the following languages PHP/Ruby/Python/Java
Proven SQL and scripting skills.
Experience with AWS, EC2, Linux command-line.
Willingness to learn and try to technologies
Experience with non-relational and/or column-oriented data stores
Comfort working with large data sets.
Familiarity with Amazon Web Services (AWS)
Love data! Charting, recording, exploring, and finding the underlying causes
Bonus
BA in Computer Science or related field
Experience with CSS Preprocessors (SaSS or less)
Familiarity with R
Previous startup experience
Perks
Benefits include Premium health benefits (including health,vision, dental), competitive salaries, equity, and a chance to do cutting-edge work with a great team"
Data Engineer,"Data Science at Policygenius...

Policygenius continues to disrupt the insurance industry by delivering innovative technology-driven experiences. We are advancing our tech capabilities and learning to leverage our hordes of data to develop innovative machine learning applications. We are relentless in our drive to reliably deliver outstanding products at scale. We are growing fast, but we can go further faster with experienced, collaborative, challenge-seeking data engineers like yourself.

Our data science team builds machine learning applications that are embedded into our consumer facing applications or power our processes to make us more efficient. We partner with Product, Design, Engineering and numerous stakeholders across the company to develop deeper predictors of behavior and build solutions to optimize our internal and external experiences.

In this role, you will…
Take complicated algorithms, code review, optimize for production and integrate into product features or process flows.
Design data architecture that is simple, fault tolerant and requires little overhead.
Design data pipelines utilizing ETL tools, event driven software or cloud functions and other streaming software.
Partner with both data scientists and engineers to bring our amazing concepts to reality. This requires learning to speak the language of statisticians as well as software engineers.
Work directly with stakeholders to ingest new sources of data, but also work to manage our self service data model.
Develop internal data science specific tooling for solutions such as A/B testing, learning sharing and analysis repositories and machine learning components.
Ensure ultimate reliability in data pipelines and enforce data governance, security and protection of our customer's information.
Mentor data engineering team members and even data scientists in architecture and coding techniques.
We'd love to hear from you if…
You have 3-5+ years of experience as a software engineer or data engineer coding in Python and using SQL.
You are acquainted with designing custom machine learning pipelines that integrate into production environments that are customer facing.
You understand life is not all machine learning and simple pipelining is often extremely relevant to add business value.
You are obsessed with reducing lag, building scalable systems, optimizing performance, automating things and solving complex problems!
You have some awareness of machine learning concepts.
You have 3-5+ years of experience working in a consumer facing business.
You have experience working on product teams, but also collaborating with other data science team members.
You have a background in computer science or related.
You can communicate with a team and articulate ideas to both team members and non-technical stakeholders.
You have a drive to learn and master new technologies and techniques.
You have experience with relational cloud databases like Redshift, BigQuery, Snowflake, but also comfortable working with unstructured files and datasets.
You can expect...
Company-paid health, dental, vision, life & disability insurance
401(k) plan, FSA & commuter benefits
Generous PTO
Training, mentorship and coaching from leadership
The opportunity to grow alongside a company shaking up a big, old-fashioned industry
Fun, diverse, open-minded coworkers
Dog companionship!!!
Technologies You Will Use
Python for data pipelining and automation.
Google Cloud Platform: Kubernetes, Cloud SQL, Cloud Functions, PubSub, BigQuery, DataStore, and more: we keep adopting new tools as we grow!
Airflow for data pipelining.
Tableau for data visualization and consumer facing dashboards.
Many more to come!
About Policygenius

Policygenius is America's leading online insurance marketplace. Since 2014, our mission has been to help people get the financial protection they need (and feel good about it). We make it easy for our customers to understand their options, compare quotes, and buy insurance, all in one place. To date, we've helped more than 30 million people shop for all types of insurance and placed over $45 billion in coverage.

At Policygenius, we're proud of building an environment that encourages our teammates to bring their authentic selves to work. Despite rapid growth (we've doubled in size year over year!), we've continuously maintained our inclusive culture through humility, hard-work, and humor, and we're looking for more people with grit, collaborative attitudes, and creative problem-solving skills to join our team. Come see why we've been voted one of Inc. Magazine's ""Best Workplaces"" two years in a row!

Diversity at Policygenius

Policygenius believes differences should be celebrated and is committed to building a team as diverse as the customers we serve. We welcome different perspectives and opinions to foster innovation, authenticity, and excellence across all parts of our company, and are committed to providing employees with a work environment free of discrimination and harassment.

As an Equal Opportunity Employer, Policygenius highly encourages applicants from all walks of life. All employment decisions at Policygenius are based on business needs, job requirements and individual qualifications without regard to actual or perceived race, color, sex, pregnancy, sexual orientation, gender identity or expression, age, national origin, political affiliation or belief, religion, disability, uniformed service, marital status or any other status protected by law.

Come join the team!"
Data Engineer,"About Lokavant
Lokavant is a technology company whose mission is to ensure that no clinical trial fails due to operational error. By integrating and analyzing the disparate data sources within clinical trials, Lokavant provides real-time visualizations and risk alerts to study sponsors and contract research organizations (CROs) to enable data-driven decisions. These insights expedite trial timelines and reduce the costs of development, allowing safe and efficacious treatments to get to patients.

Lokavant centralizes trial data to power a machine learning model that anticipates trial risk, provides data-driven risk mitigation strategies, and predicts the impact of mitigation strategy implementation. Lokavant's anticipatory monitoring capability is grounded in a compendium of data from over 1,000 clinical trials and will improve with each deployment.
About the Opportunity

How often are you given the opportunity to build something from the ground up, with an abundance of resources at your disposal; to be part of a team of people accomplished in diverse scientific and engineering disciplines, focused on using the best of what lies at the forefront of technology to address complex, real-world problems that have a positive impact on potentially millions of peoples' lives? This is that kind of opportunity.

We are seeking a thoughtful, hands-on technology enthusiast with a strong aptitude for data engineering to join the rapidly growing Lokavant team in our New York City headquarters. The Data Engineer will work very closely with our front-end developers, back-end developers, development operations engineers, and data scientists. Our platform is fully cloud-based and is being built around modern tools and frameworks in an incredibly fast-moving agile environment.

Key Responsibilities
Design, develop, and implement data infrastructure and pipelines that ingest and transform data from various external sources, storing it in highly optimized database systems, and making it useful to our application and reporting layers
Create automation systems and tools to configure, monitor, and orchestrate data infrastructure and pipelines
Create data integration services to help onboard new customers as quickly as possible
Maintain ongoing reliability, performance, and support of the data infrastructure, providing solutions based on application needs and anticipated growth
Participate in creating and maintaining strict compliance, data privacy and security measures
Develop robust and production-level code to implement new product features in collaboration with other engineers and subject matter experts
Identify and resolve performance and scalability issues, troubleshoot problems, and improve product quality
Collaborate with the Front-End Development team to thread the right information through to forward-facing applications
Interface with the Development Operations colleagues to evaluate and implement methodologies and workflows to facilitate the frequent and continuous release of high-quality software
Work closely with Data Science colleagues to implement descriptive and predictive algorithms and models using the latest technologies
Keep up to date on emerging technology solutions, particularly those on AWS, for continuous improvements in data engineering
Help recruit highly capable engineers to the team from diverse backgrounds
Mentor and be mentored by engineers of varied experience levels and subject matter areas

Minimum Requirements
3+ years relevant experience with data engineering
Strong proficiency with Python (ideally PySpark) and SQL
Experience with AWS S3, EC2, EMR, or an equivalent cloud-hosted infrastructure
Experience with cloud-hosted database/data warehouse architecture (e.g. Redshift, Snowflake, etc.)
Experience writing and productionizing complex data transformations in SQL and related frameworks
Interest in building distributed computing and orchestration frameworks (e.g. Spark, Kubernetes, Airflow, etc.)
Experience working in an Agile software development environment
Exceptional written and verbal communication skills
Strong attention to detail and highly organized, with effective multi-tasking and prioritization skills
Proactive, self-motivated and self-directed, with the ability to learn quickly and autonomously
Comfortable with ambiguity
Superior problem-solving and troubleshooting skills
Ability to work as part of a collaborative cross-functional team in a fast-paced environment
Sincere interest in working at a rapidly changing start-up and scaling with the company as we grow
Bachelors degree with strong academic performance in Computer Science, Software Engineering, Applied Science, or equivalent field
Preferred (Nice-to-have) Qualifications
Experience building and deploying large-scale data processing pipelines
Experience integrating data from disparate data sources
Experience with continuous integration and automation tools and processes (e.g. Jenkins, Semaphore, etc.)
Experience with healthcare data, ideally clinical/operational clinical trial data
Knowledge of clinical data standards (e.g. CDISC, FHIR, HL7, etc.)
Knowledge of e-clinical systems and technologies (e.g. EDC, CTMS, IRT, etc.)
Employee Benefits
Competitive salary and equity compensation
Full medical, dental, and vision benefits
One Medical membership
401(k) plan
Flexible PTO policy
Generous parental leave
Great NYC office located in the heart of Times Square
Team events and outings
Lokavant is an equal opportunity employer, indiscriminate of race, color, religion, ethnicity, ancestry, national origin, sex, gender, gender identity, sexual orientation, age, marital status, veteran status, disability, medical condition, or any other protected characteristic. We celebrate diversity and are committed to creating an inclusive environment for all employees."
Data Engineer,"Title Data Engineer Type of Employment FTE Location NYC,NY Responsibilities Very strong programmer in Python as their primary language. Java as a secondary language would be helpful. Skills with technologies like Kafka, Airflow, Spark and Scala. Heavy ETL experience is also very desirable. Leadership experience is also preferable and they want the person to be able to mentor more junior resources and possibly lead them in the near future. Candidates must have very strong communication and articulation skills"
Data Engineer,"Job Description

We are looking for an Analyst to join our global Business Intelligence team in New York. This team provides business reporting, tools, and analysis to senior business leaders, driving strategic decisions and profitability maximization throughout the organization. Additionally, the team manages Bernstein’s commission sharing program (CSA), a client-facing function closely aligned with the sales and trading functions.

The group’s responsibilities include:
Design and development of all business metrics and performance reporting
Data Management
Bespoke client analysis
Modeling and scenario analysis
Development and implementation of business-process enhancements
Managerial accounting policy and implementation.
This role provides opportunities to build skills and responsibility. You will interact directly with internal and external clients and work with managers across the Bernstein organization. In addition, you will acquire expertise in the firm’s business practices related to sales, trading and research, and develop rigorous analytical and problem-solving skills. Our team is relatively small, offering you a high level of responsibility and empowering you to quickly make an impactful contribution.

Desired Skills and Experience

Ideal candidates typically possess:
Excellent analytical and problem-solving skill
Strong Interpersonal skills and ability to collaborate across departments to resolve issues
Relentless intellectual curiosity and a drive to excel at the highest level
Facility with Python, SQL, and Excel is highly desired
Bachelor's degree in finance or related field with 2-3 years of experience in financial services is preferred. Familiarity with the brokerage industry is also a plus.
We look for consummate team players who are detail-oriented and have excellent oral and written communication skills and who can perform and maintain a positive attitude in a demanding environment. This role has high growth potential and historically, our associate positions have proven to be excellent platforms for successful professional careers in finance as well as continuing education in related fields.

Company Description

Bernstein is widely recognized as Wall Street's premier sell-side research and brokerage firm, with a global equity trading platform that spans the U.S., Europe, and Asia. Our firm was founded in the U.S. in 1967 and in the U.K. in 1999. Our research and trading capabilities are sought out by leading investment managers around the world, and we are annually ranked at the top of our industry by acknowledged arbiters. In independent surveys of major institutional clients, Bernstein's research is ranked #1 for overall quality, industry knowledge, most trusted, best detailed financial analysis, major company studies, most useful valuation frameworks and best original research. In Institutional Investor’s latest annual client survey, the leading survey by which research analysts in our industry are evaluated, over 90% of our U.S. analysts and the majority of our European Analysts were recognized as among the best in their respective fields – more than any other firm in our industry. We began operations in Hong Kong in 2010, and now cover a range of sectors across Asia. Our European research team is also home to Extel's #1-ranked analyst across Europe.

Research has always been Bernstein's calling card. The brand is defined by our renowned Blackbooks, known for their unbiased, in-depth company and industry forecasts. We have a community of Research Analysts who are acknowledged thought leaders that typically have many years of experience in the industries they cover. Our reputation is for the very highest caliber of independent and disciplined investment and industry analysis. We are a wholly-owned subsidiary of our buy-side parent, AB.

New York City, New York"
Data Engineer,"About TelTech

A member of the IAC family, TelTech builds innovative communication apps that help people better enjoy their mobile-connected lives. From RoboKiller, an app that stops and gets even with telemarketers, to TapeACall, which records iPhone conversations, we build apps that protect our customers every day. We are looking to expand our team with creative, talented, and passionate people who are committed to the growth of our existing products as well as new products we intend to launch. TelTech's sister companies in Mosaic Group include iTranslate (Apple Design Award Winner 2018), Daily Burn (top-rated health and fitness company), and Apalon.

Position Summary

This position will serve as the primary data engineer for TelTech. A successful data engineer at Teltech will have significant data warehousing experience, Python development experience, and mastery of SQL. The data engineer will also be proactive in maintaining relationships with product, marketing, and analytics to ensure all data-related business needs are met.

Responsibilities
Gather requirements from business stakeholders (product, marketing, analytics) and effectively translate them into a data environment that supports business needs
Design and maintain data warehouse (PostgreSQL and BigQuery) and data lake (GCS)
Build batch and real-time data pipelines to ingest and process data from various sources such as product backends, third party platforms, etc.
Build internal tools and services to support data analytics and engineering
Help product, marketing, analytics, and engineering with diagnosing data-related issues
Document, profile, and manage data across multiple products and platforms for data governance (consistent naming conventions, data validation, data retention)
Maintain a line of communication between developers and data team to ensure all stakeholders are up to speed on changes to source database schema for analytics purposes
Work closely with other data engineering teams across Mosaic to ensure alignment of methodologies and best practices
Qualifications
2 - 4 years experience programming and scripting in Python within a production environment
Hands-on experience with relational databases (MySQL and PostgreSQL)
Familiarity with creating ETL processes for large-scale data warehouses
Must have familiarity with GCP and its offered services (BigQuery, CloudSQL, Dataprep, etc)
Knowledge of PubSub, or relevant messaging queue like Kafka, is a must
Distributed file systems (GCS, HDFS) knowledge is a must
Familiarity with Java, JavaScript, Airflow, Docker, Scala, and Spark are preferred
Familiarity with NoSQL is a plus
Prior experience with CI/CD is a plus
Why TelTech?
An amazing working environment with a lot of perks including but not limited to:
Unlimited PTO!
Matching 401k!
Company Contribution towards Commuter Benefits!
Fully stocked kitchen!
Environment where you can mentor and learn from others.
PRIVACY STATEMENT"
Data Engineer,"Motivate LLC is looking for a detail-oriented and resourceful Data Engineer to join the Business Operations team. In this role, you will be designing and developing a data warehouse solution for all aspects of our business to discover insights and develop data-driven resources to guide key decisions.About MotivateMotivate is a best-in-class service operation and logistics company, delivering bikeshare services to some of the largest urban environments in the U.S. We operate in 8 markets across the U.S. including New York, San Francisco, Chicago, and Washington D.C. and will serve over 1.5 million customers this year alone. Led by a group of urban visionaries and seasoned operations experts, our mission is to revolutionize the landscape of our cities. In an effort to completely transform the urban experience, Motivate is leading the way in making cities more accessible, healthier and sustainable. We operate and maintain safe and dependable systems that help make cities great by connecting individuals to the people and places they love. We're offering an opportunity to work with the most passionate, creative and proven team in this emerging, dynamic industry that is transforming cities around the globe.About the Role:* Collecting data from a wide range of sources: AWS S3, Redshift, and various APIs* Working closely with the Business Operations team members and our business partners to design and implement a scalable data warehouse from disparate and transitioning data sources* Working in tandem with data analysts to establish ETL pipelines for efficient and reliable analysis and reporting* Developing a system to monitor data quality, ensuring the accuracy and availability of data used in daily operations.* Collaborating with Business and Field Operations, as well as our business partners, in order to collect the right data to help understand performance, efficiency, productivity, and profitabilityAbout You:* You are a team player, thrive in growth environments, and are excited to tackle the unique challenges facing growing transportation and logistics solutions* You have an undergraduate or graduate background in an analytical field (mathematics, computer science, or similar)* You have worked for 1+ years experience building and maintaining data warehouses as well as constructing ETL data pipelines at scale* You have development experience with warehousing tools, including but not limited to Redshift, BigQuery, or Snowflake* You have a strong understanding of Python and SQL* You have worked with analysts to provide them with both the data they need as well as helping them to arrive at a better understanding of any relevant nuances* You enjoy taking deep dives to reconcile discrepancies between expected and actual results and take pride in building processes to avoid such issues in the future* You have experience translating business needs into technical requirements and then communicating those requirements to consumers at different levels of proficiency* You have a firm understanding of how to optimize and stabilize the flow of data ingestion to ETL to presentation to consumersBonus pointsExperience with:* Dimensional modeling* Airflow/ Luigi, DBT, AWS S3* Panoply* R* Tableau / Domo / Looker* GitHubThe above description is not intended to be an exhaustive list of all duties, responsibilities, or qualifications associated with the job. From time to time, and related to the nature of work performed to accomplish Motivate's Mission, employees may be required to perform duties outside of their normal responsibilities.Motivate is an Equal Opportunity Employer. Applicants are considered for positions and are evaluated without regard to mental or physical disability, race, color, creed, religion, sex, gender, national origin, ancestry, age, genetic information, military or veteran status, sexual orientation, gender identity or expression, marital status, pregnancy or any other legally protected status under applicable law. We also provide reasonable accommodation to qualified individuals with disabilities in accordance with the Americans With Disabilities Act and applicable state and local law, as well as individuals who need accommodation because of pregnancy. If you require assistance or reasonable accommodation during any aspect of the application process, please contact [the Human Resources Department or the hiring manager].We thank all applicants in advance for their interest in this position, however, only those selected for an interview will be contacted.Motivate LLC., does not offer sponsorship of any kind for this position.Other details* Pay Type Salary* New York, NY, USA* Washington, DC, USA"
Data Engineer,"The role

As a Data Engineer at Blue State, youll play an integral role on a smart and vibrant analytics team servicing a wide range of progressive organizations. Youll design, build, and manage the systems and processes which form the underpinning of Blue States analytics work, supporting and working alongside data analysts and campaign strategists. But youll also work directly with Blue States clients to help solve their data integrity and integration challenges, serving as a trusted advisor to your counterparts within client organizations.

Day-to-day responsibilities:

Create and support systems and processes for managing, compiling, manipulating, and analyzing data for client and internal projects

Work with Blue States client organizations to solve difficult data migration, management, and integration challenges

Build data pipelines, data warehouses, reporting dashboards, automated exports, and synchronization processes

Automate workflows and look for further opportunities to improve efficiency in our work

Always maintain a high level of data security and privacy

The team

You will be a part of the globalData & Technology team working primarily with our creative agency on client projects. Youll work in either the NY or DC office.

Top things were looking for

Good foundational understanding of statistical analysis

Extensive experience working with SQL databases in an analytics or business intelligence context

Familiarity with common marketing technology platforms like Google Analytics, Google Ads, Facebook Ads, email marketing tools, and other marketing automation tools

Experience with ETL/ELT tools, processes, and best practices

Strong Python experience:

Python should be your go-to tool for solving problems. If the first thing you want to do when you have to do the same thing twice is write a Python script to automate it - we want you!

Experience with task automation in a Python context - experience with AirFlow, Prefect, Dask a big plus

Experience working with restful APIs - you can competently navigate unfamiliar API documentation and figure out how to accomplish tasks

Strong working knowledge of Google BigQuery and the Google Cloud Platform data product ecosystem including:

Designing data warehouse schemas for cross-channel marketing analytics

Utilizing the suite of Google Cloud Platform tools for the purposes of extracting, processing, manipulating and analysing data

Building and running automated tasks within the GCP environment - e.g. Cloud Compute, Cloud Functions, Cloud Run, Cloud Scheduler

Comfortable managing GCP IAM policies across projects and teams

Comfortable working within a spreadsheet (even if you prefer a database) - preferably in Google Sheets - bonus points if youve extended Google Sheets using Google Apps Script

Familiarity with Git and maintains good habits around code maintenance

Able to build repeatable and well-documented processes and tools that can be used by other technically-savvy but non-Python developer analytics team members (think easy to use command-line scripts - not GUIs)

Good at teaching others what you know.

The company

Blue State is the purpose-driven creative and tech agency for brands and causes looking to inspire people to take action. With clients including the AARP, Google, UNICEF, JDRF, and Colgate. Blue State cultivates communities, builds platforms, and transforms how organizations engage their most important people. Led by some of the most creative and analytical minds from the political, nonprofit, and brand worlds, Blue State is a part of WPP and has more than 150 employees across five offices."
Data Engineer,"Our client, a Fortune 500 Company is looking to fill multiple roles for Data Engineers of all levels with a variety of skills.

The right candidate(s) will have one of or multiple skills in each category:

Programming (Java, Scala or Python)
Computer Science Fundamentals (Algorithms and Data Structures)
Distributed Computing (Spark or Parquet)
Proficiency in Databases/SQL, performance tuning, NoSQL, Mongo/Elastic
Web Application Architectures (HTTP/HTML/CSS, JavaScript)
Financial/Fixed income Knowledge is a plus
Spark, Scala and Functional Programming
Event Driven Programming (Kafka, Akka, Flink or Spark Streaming)
FrontEnd Development (Angular, Java Scripts, Reactive Programming)
Kubernetes, Docker, AWS Infrastructure and Process Management

What You Will Do:
Build complex data ingestion pipelines using Scala, Spark, Parquet and S3
Design scalable processes in event driven architecture to support Fixed Income applications
Develop near real time streaming analytics using Kafka/Kinesis
Act as Subject Matter Expert and help rest of the team in leveraging the platform and migrating applications to it
Establish end to end data lineage and data catalogue. Work with data governance team to setup data quality checks and metrics
Create self-service notebook environment with Zeppelin/Jupyter for exploratory data analytics and rapid interactive development
Troubleshoot any performance issues and ensure efficient data organization
Build efficient web-based tools for monitoring and tracking
What You Will Need:
Bachelors degree in computer science or related field required. Masters degree preferred
Knowledge of data structures, algorithms and functional programming
Passion to learn new things, experiment with new ideas and build world class data platform
5+ years of experience in programming with Scala, Python or Java
2+ years of experience in Scala, Spark and functional programming. Deep knowledge of spark internals such as partitioning, DAG, lazy evaluation.
Strong experience with relational data bases, SQL, and query optimization. Knowledge of data warehousing, dimensional data model and business intelligence is a plus.
Knowledge/experience with event driven programming and Akka actor model"
Data Engineer,"About Us

Simon Data was founded in 2015 by a team of successful serial entrepreneurs. We're a data-first marketing platform startup, and we approach our work seriously; we tackle problems in a scrappy and disruptive fashion, yet we build for scale to support our clients at big data volume.

Simon Data is a data-first customer experience orchestration platform, designed to disrupt the marketing technology and marketing cloud category. Simon's platform empowers businesses to use enterprise-scale big data and machine learning to power customer communications in any channel. Simon's unique approach allows brands to develop incredible personalization capabilities without needing to build and maintain massive bespoke data infrastructure.

Our culture is rooted in organizational transparency, empowering individuals, and an attitude of getting things done. If you want to be a valuable contributor on a team that champions these core values we would love to hear from you.

The Role

As a member of the data engineering team, you will have the opportunity to tackle cool engineering projects and make a significant impact on the trajectory of an early-stage startup. Simon's stack is composed of a wide variety of cutting-edge technologies, and you'll work on a diversity of projects that will accelerate your growth as an engineer.

You are smart and passionate about data. You love designing creative solutions to complex problems and are always thinking about how you can automate processes. You care deeply about your work and always seek to grow your knowledge and skills. If you are excited about growing your technical career and want to be in an environment that fosters growth and mentorship this is the place for you.

What You'll Do
Build custom data pipelines on top of our core platform (Python/Redshift/Snowflake)
Collaborate closely with senior engineers to design and develop new product features
Build out tooling for a data development environment for our clients on our platform
Identify solutions to client requests
Drive technical requirements from high-level marketing and business needs of our end users
Partner with our Customer Success team to deliver quality solutions to our customers
Qualifications
Fluent in at least one programming language
Detailed-oriented and able to work across departments
Motivated self-starter
Strong technical communication skills
Strong analytical skills and a love for all things data
Experience with Python/Django a plus but not expected
Proficiency in SQL is a plus
Visa sponsorship for this role is currently not available.

Diversity

We're proud to be an equal opportunity employer open to all qualified applicants regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or expression, Veteran status, or any other legally protected status."
Data Engineer,"Who We Are


EDO is a data science software firm that develops analytics tools to make data accessible and actionable for the media and entertainment industry. Currently focused on film and TV, we work with many major movie studios and TV networks to help them forecast, market and distribute their content more effectively. Building from this strong base, we are growing into adjacent verticals.

We are a team of world-class engineers and data scientists backed by top leaders in entertainment and technology. Our co-founders and executive leadership have a strong track record with other successful ventures.

What You Will Do
Work on our data pipelines, scaling our data collection and processing jobs that power our analytics
Build robust and scalable tooling in Python to aid in Diagnostics, Analysis and Inference which will lead to pipeline and platform optimizations
Improve our code quality through unit testing and code reviews
Work on a small team and take ownership of high impact projects
Work in a fast paced environment with continuous deployment and rapid product iteration following standard DevSecOps practices
Understand how the futures of media, entertainment and advertising are being defined by the innovative applications of technology and data
Learn about how a startup is built from an early stage
What We Are Looking For
Self-driven individuals who take ownership of their work
Ability to build products quickly and efficiently
Strong understanding of software engineering practices and principles
Interpersonal and communication skills to work on a small team
Willingness to learn about our clients and their unique problems
Experience with Python, MySQL, background job frameworks (such as Airflow), and AWS is a plus
Benefits
Early-stage equity and competitive salary
Medical, dental, and vision insurance
Meals and snacks during work
Movie tickets, fitness discounts, commuter subsidies, and Apple hardware"
Data Engineer,"Stash is pioneering the future of personal finance with the first financial subscription that helps people create better lives. From budgeting to saving for retirement, Stash unites banking, investing, and advice all in one app that has helped more than 5M people reach their financial goals and make progress towards financial freedomAt Stash, data is at the core of how we make decisions and build great products for millions of users. As a Data Engineer you will be a part of our Data Platform Team which is leading the architectural design decisions and implementation of a modern data infrastructure at scale. You will build distributed services and large scale processing systems that will support various teams to work faster and smarter. You will partner with Data Science to help productionize machine learning models and algorithms into actual data driven products that will help make smarter products for our users.Tools and technologies in our tech stack (evolving):* Hadoop, Yarn, Spark, MongoDB, Hive* AWS EMR/EC2/Lambda/kinesis/S3/Glue/DynamoDB/API Gateway, Redshift* ElasticSearch, Airflow, and Terraform.* Scala, PythonWhat you'll do:* Build core components of data platform which will serve various types of consumers including but not limited to data science, engineers, product, qa* Build various data ingestion and transformation job/s as and when they are needed* Productionize our machine learning models and algorithms into data-driven feature MVPs that scale* Leverage best practices in continuous integration and deployment to our cloud-based infrastructure* Build scalable data services to bridge the gap between analytics and application space* Optimize data access and consumption for our business and product colleagues* Develop an understanding of key product, user, and business questionsWho we're looking for:* 3+ years of professional experience working in data engineering* BS / MS in Computer Science, Engineering, Mathematics, or a related field* You have built large-scale data products and understand the tradeoffs made when building these features* You have a deep understanding of system design, data structures, and algorithms* Experience (or a strong interest in) working with Python or Scala* Experience with working with a cluster manager (YARN / Mesos / Kubernetes)* Experience with distributed computing and working with Spark, Hadoop, or MapReduce Framework* Experience working on a cloud platform such as AWS* Experience with ETL in generalGold stars:* Experience working with Apache Airflow* Experience working with AWS Glue* Experience in Machine Learning and Information believe that diversity and inclusion are essential to living our values, promoting innovation, and building the best products out there. Our success is directly related to the employees that we hire, grow and retain and we believe that our team should reflect the diversity of the customers that we serve.As an Equal Opportunity Employer, Stash is committed to building an inclusive environment for people of all backgrounds. We do not discriminate on the basis of race, color, gender, sexual orientation, gender identity or expression, religion, disability, national origin, protected veteran status, age, or any other status protected by law. Everyone is encouraged to apply.Benefits & Perks:* Equity in Stash* Flexible Vacation* Family-Friendly Medical, Dental, and Vision Insurance Plans* 401k* Learning & Development Stipend* Commuter Benefits and Flexible Spending Account (FSA)* Employee referral bonuses* Stocked fridges & kitchens and catered lunch on Fridays* Thursday happy hours* Team outings that do not involve trust falls...Awards & Recognition:* Forbes Fintech 50 (2019)* LendIt Fintech Innovator of the Year (2019)* Built in NYC's Best Places to work (2019)* Built in NYC's Startups to Watch (2018)* Wall Street Journal's ""Top 25 Tech Companies To Watch"" (2018)* MarCom Awards Double Gold & Platinum Winner (2018)* Webby Award Winner for Best Mobile Sites & Apps in the Financial Services and Banking category (2017)* W3 Awards Winner for Best User Experience (2017)No recruiters, please."
Data Engineer,"Want to leverage your experience and development skills in the Healthcare industry as a Data Engineer?

CGI is looking for a strong Data Engineer for Healthcare Client to work in Bloomfield, CT.

Build your career with us.

It is an extraordinary time to be in business. As digital transformation continues to accelerate, CGI is at the center of this changesupporting our clients digital journeys and offering our professionals exciting career opportunities.

At CGI, our success comes from the talent and commitment of our professionals. As one team, we share the challenges and rewards that come from growing our company, which reinforces our culture of ownership. All of our professionals benefit from the value we collectively create.

Be part of building one of the largest independent technology and business services firms in the world.

Learn more about CGI at www.cgi.com.

No unsolicited agency referrals please.

CGI is an equal opportunity employer.

Qualified applicants will receive consideration for employment without regard to their race, ethnicity, ancestry, color, sex, religion, creed, age, national origin, citizenship status, disability, medical condition, military and veteran status, marital status, sexual orientation or perceived sexual orientation, gender, gender identity, and gender expression, familial status, political affiliation, genetic information, or any other legally protected status or characteristics.

CGI provides reasonable accommodations to qualified individuals with disabilities. If you need an accommodation to apply for a job in the U.S., please email the CGI U.S. Employment Compliance mailbox at US_Employment_Compliance@cgi.com. You will need to reference the requisition number of the position in which you are interested. Your message will be routed to the appropriate recruiter who will assist you. Please note, this email address is only to be used for those individuals who need an accommodation to apply for a job. Emails for any other reason or those that do not include a requisition number will not be returned.

We make it easy to translate military experience and skills! Click here to be directed to our site that is dedicated to veterans and transitioning service members.

All CGI offers of employment in the U.S. are contingent upon the ability to successfully complete a background investigation. Background investigation components can vary dependent upon specific assignment and/or level of US government security clearance held.

CGI will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with CGIs legal duty to furnish information.

Your future duties and responsibilities

As a Data Engineer , this team member will have a superior software development background that will facilitate design and development of technical solutions for new and existing Business Intelligence applications. Interacts with Lead Developers, System Analysts, Business Users, Architects, Test Analysts, Project Managers and peer developers to analyze system requirements, design and develop software applications. Technical solutions and supporting deliverables will be constructed to enable our Healthcare IT client to accomplish their business objectives. This position is on a team that is responsible for providing technical Business Intelligence solutions.

If you are looking for a new challenge and want to make a difference in the Healthcare Industry, this role is for you.

Required qualifications to be successful in this role

• The teams scope is to ensure that all the data related Pharmacy is Integrated in the Clients Corporate Warehouse and available for Analytics and reporting capabilities supporting the Business areas across the Enterprise. Data will also be persisted on Big Data HADOOP Platform. Team will be focused on Data Analysis, Unified Data Design, Business and Test driven Development, Deployment with focuses on Continuous Improvement Continuous Delivery (CICD).

• Conduct unit testing and participate with system testing as required. Knowledge with Test Driven Development is strongly preferred.

• Fill out required documentation such as installation instructions and follow Client standards and procedures.

• Anticipate customers analytic needs and proactively conceptualize and champion high-value Business Intelligence solutions.

• Actively support products by providing prompt responses to customer problems and inquiries. Make the customers experience first priority in all.

• Provide on-call support duties for Application Production Support, which could include off hours and weekends.

• Keep the Production Support team management informed of any issues or concerns.

• Participate/Assist with deployments to all environments including Production (which could occur after hours).

New development and support task responsibilities as assigned

REQUIRED QUALIFICATIONS TO BE SUCCESSFUL IN THIS ROLE

This position requires:

Technical Skill set required:
Knowledge of Data Warehousing and Data Integration Concepts is MUST
Strong Data Analysis skills and I Love Data Data Mindset
Understanding Data Modeling concepts
3+ Years in Teradata or Oracle or related DB experience
3+ years writing Advanced SQL knowledge and ability to tune queries
1+ years in Python or strong coding background in 1-language is a must C/C++ or JAVA
Good to Have:
Experience with HADOOP / Big Data
Experience writing ETL
GIT Version control,
UNIX/LINUX and shell scripting is a Plus
Knowledge of Pharmacy and Healthcare experience is a Plus.
Knowledge of Cloud Computing is a Plus.
Soft Skill set required:
Positive can do attitude
Good Team player
Self-learner and strong problem solving skills
Work Collaboratively"
Data Engineer,"Data Engineer– Performance Data Service

We are seeking an experienced Data Engineer to join our Managed Data Services team and work closely with Product Management and Client Support teams to support us in the build out of our Data Management Platform and associated services. The Managed Data Services team supports the full life-cycle of data in T-REX from ingestion, to curation, normalization, and delivery. The team is responsible for ensuring the highest level of data quality, providing unique value to the T-REX platform and our clients workflows.

Our ideal candidate has:
Demonstrable experience working in data management for products in the Financial Services industry
Experience in Structured Finance or related field
Ability to develop, create, test, and maintain data processing pipelines
Experience in recommending and implementing ways to improve data reliability, efficiency and quality
Experience in managing and supporting the acquisition of client and 3rd party data
Experience in working with clients on problem resolution
As part of our Data team you will have an important role in the development and operations of our strategic Performance Data Service offering. You will work closely with our Product and Engineering teams to build out and continuously enhance the service and platform, while working with client development to align client requirements.

Responsibilities:
Define mapping from source documents to T-REX dictionaries, normalizing and automating the process
Ensure our data content and platform are continually advancing, working closely with engineering and product teams
Integrate python wrappers for API queries in the data pipeline
Expand on and automate the data ingestion pipeline
Provide the highest quality data for our users by continuously defining, developing and adhering to a data validation process from ingestion to end user workflows
Ensure the efficiency and effectiveness of our data management capabilities and processes are world class and always improving
Provide client support and resolve client requests as they relate to our data service
Establish and maintain excellent relationships with data sourcing and integration partners
Skills & Qualifications

Required:
3+ years of Data Engineering experience
Proficient in Python and Querying languages
Experience working with APIs
Experience working in Financial markets
Exceptional written and verbal communication skills
Preferred:
Experience working with Structured Finance products and/or in the Structured Finance industry or related field
Experience working with AWS Textract, Xtractor, IvyTools, PdfToText and similar toolsets
What it’s like working at T-REX:
You’re going to be challenged: Building an inter-disciplinary SaaS platform that combines deep knowledge from both the financial and technology industries isn’t easy!
You’re going to make an impact: T-REX offers you the opportunity to have a deep impact our growth, product and team.
You’re an asset we’re going to invest in: We believe in investing in our people so we have generous tuition reimbursement and wherever we can we promote from within.
You’re going to have fun: You’ll get to work with a team of diverse people; we’re an international team who loves everything from boardgames, Pi Day baking competitions and T-REX Trivia Nights
T-REX - People Hiring People:

At T-REX, we are committed to equal employment opportunity regardless of race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression. We are proud of our diversity and strive to reflect to society we are a part of. T-REX is a team of people who love to put in good work and have fun together.

Join us on our mission to Empower Efficient Finance and have fun doing it!"
Data Engineer,"Job Description
We are seeking a senior Data Engineer who will focus on creating and maintaining systems to collect and store data for reporting and analytics. This person will create solutions that compile heterogeneous data in various formats from multiple sources into a normalized data model which is consumed by a processing engine, applies business rules and provides reporting capabilities.
The candidate should have excellent problem solving and communication skills. They should be able to articulate and document designs and concepts that will be utilized to solve complex data architecture problems, working with key stakeholders to prioritize needs.
At Virtual Clarity, we are building sophisticated software tooling unrivaled in the marketplace. The ideal candidate will thrive in a fast-paced environment, driven by innovation, and the creation of tools that have already demonstrated disruption in the marketplace. You will demonstrate a special aptitude for rapidly understanding and solving complex problems and thrive on quickly implementing your solutions into executable code.
Our customers are the worlds largest financial institutions, manufacturers, retail, hospitality providers and retailers. If you enjoy being part of an agile team that develops solutions at the speed of business, creating new software that solves problems now, and getting recognition for your efforts, this job is for you.
Please review the following job requirements carefully. During the interview process, you will be expected to demonstrate mastery of required skills.
Key objectives/Results:
Deliver database solutions as an active (hands-on) part of a small multi-disciplinary team
Develop and maintain databases which include: data repositories, reporting and analytical databases
Deliver analysis of data from multiple sources and provide analytical results to target the business issues and engineer the data to fit the client needs
Design and build robust and scalable solutions for managing structured and unstructured data using traditional databases
Conduct end-to-end analysis which includes data gathering and requirements specification, analysis, ongoing scaled deliverables and presentations
Coordinate, prepare and perform data analysis and transformations to align the data to business rules. Use this data to provide actionable information and business insights
Work through early stages of software life cycle to profile data and to create conceptual, logical and physical data model designs with appropriate structure and relationships for optimal performance
Provide gap analysis of available data against requirements to design future state solutions
Deliver documentation including technical designs, data flows, ERDs, and mapping documents
Responsibilities:
Work with client data owners and functional designers to create technical specifications and execution methods
Translation of business and technical requirements into scalable database solutions
Participate in all aspects of the application lifecycle: requirements analysis & definition, system design, implementation, testing, deployment and sustainment
Collaborate with the team's application architect to improve upon database and application designs and performance
Work with Business SMEs to obtain a greater understanding of the needs of the business and the types of analysis that support business goals
Interface with internal teams to extract, transform and load data from a wide variety of data sources with frequency varying from batch to on demand
Provide technical expertise and capabilities in SQL query writing and use of DBMS technology
Work closely with web development team in the ongoing development and maintenance of databases, applications or tools
Desired Skills & Experience
7+ years of database design and development work experience
Detailed knowledge of development with MS SQL Server 2016 or later, including new database features and T-SQL enhancements
Experience in ETL concepts and tools, including SSIS
Extensive experience in SQL programming: Procedures, Functions, Triggers, and dynamic SQL
Strong Skills in Data Modelling such as Star Schema, Snow Flake Schema, Fact and dimensional tables and loading data from multiple data sources
Normalization/De-normalization concepts, database design methodology
Experience in data analysis, data profiling, data transformation, data mapping from source to target database schemas and data cleansing
Experience creating technical design documentation
Solid understanding of data architecture, efficient database design, writing complex queries and query tuning and troubleshooting
Our Company:
Virtual Clarity is a Technology Consultancy business, with offices in the USA and Europe. Business leaders know that transforming I.T. into a Service is a shift that needs to happen. But many don't know how to get there. Getting our customers there simply, is simply all we do. We untangle their I.T., taking it from complex legacy, to a clear, cloud infrastructure, that will help make them and their business clean, lean and ready for a nimble future.
We not only help customers refine their technology vision, but also help them sell it to the rest of their company, so that everyone can see the need for change, and most importantly, get excited about it. In fact, our remit goes beyond advisory services into the realms of getting things done. We are active participants in the task of creating I.T. as a Service.
If I.T. is keeping customers awake, it's not working. Fundamental change can feel like stepping into the abyss. We know that CIOs and CTOs already have plenty of things keeping them awake at night - no one needs more! Thankfully, the benefits of moving to ITaaS far outweigh any reasons to stay as they are, and our current clients are sleeping all the better for making that move.
Our Culture:
We are an entrepreneurial business, where almost everyone comes into contact with the customer, at some point. We are informal unless required to be otherwise, by our customers, where, much of the time, we work on-site.
We work with game changers, believers and visionaries. We give a damn; not everyone thinks its cool to care, but we do. We care about our customers and about our people. Were obsessed with return on energy and deliver return on investment. We are a collective of change makers and focused thinkers; we actually make it happen.
The VC tone is SMART, RELEVANT & FUN. We are actively NOT corporate. Our intention is to offer all our people opportunities to produce, learn and teach, as well as a platform for taking their career to the next stage."
Data Engineer,"We currently have a 6 month assignment for 4 Position (s) (Data Engineer Developer) open. Location Newark NJ Target Start Date ASAP Years of Experience (4-6 years Intermediate Level) Right to hire No Job Description our technology group is a dynamic, fast-paced environment, with exciting changes on the horizon under new senior leadership. We are looking for a data engineer developer to build next generation data platform from ground up on AWS as part of a small focused team. Our ideal candidate will have strong knowledge of computer science fundamentals with programming experience in Scala Python and Spark. The right candidate for this role will identify this challenge as a unique and valuable opportunity to help drive our global technology transformation, so if this sounds interesting Responsibilities Build complex data ingestion pipelines using Scala, Spark, Parquet and S3 Design scalable processes in event driven architecture to support Fixed Income applications Develop near real time streaming analytics using KafkaKinesis Act as Subject Matter Expert and help rest of the team in leveraging the platform and migrating applications to it Establish end to end data lineage and data catalogue. Work with data governance team to setup data quality checks and metrics Create self-service notebook environment with ZeppelinJupyter for exploratory data analytics and rapid interactive development Troubleshoot any performance issues and ensure efficient data organization Build efficient web-based tools for monitoring and tracking Qualifications (Required) Bachelor's degree in computer science or related field required. Master's degree preferred Knowledge of data structures, algorithms and functional programming Passion to learn new things, experiment with new ideas and build world class data platform 5+ years of experience in programming with Scala, Python or Java 2+ years of experience in Scala, Spark and functional programming. Deep knowledge of spark internals such as partitioning, DAG, lazy evaluation. Strong experience with relational data bases, SQL, and query optimization. Knowledge of data warehousing, dimensional data model and business intelligence is a plus. Knowledgeexperience with event driven programming and Akka actor model Excellent verbal and written communication skills Qualifications (Desired) Experience with AWS infrastructure, docker, ECSEKS, EMR, KafkaKinesis. Front-end development experience with Angular, Java Script, Reactive Programming. Knowledge or desire to learn Investment Management, Fixed Income and Finance Familiarity with NoSQL and ElasticSearch"
Data Engineer,"Youngsoft is one of the fastest growing companies based out of Wixom Michigan providing IT services to various clients across the USA. currently we have a "" Data Engineer "" position at one of our client locations You should be excited to work with data, have the curiosity to dive deeply into issues, and feel empowered to make a meaningful impact at a mission-driven company. We are a team committed to agile value delivery and solid engineering principles, as well as continuously improving our craft. If you love shipping software that delivers deep and meaningful impact to peoplersquos mental and behavioral health, join us! PRIMARY RESPONSIBILITIES Build and maintain large-scale batch and real-time ETL pipelines in a Google Cloud Platform architecture (BigQuery, Dataproc, Firestore, etc.). Design and support Data Lakes and Marts in Google CloudStorage, BigQuery, and NoSQL (Google Firestore MongoDB). Implement high-quality test-driven code, participate in code reviews, and own the development of medium-size features. Support business operational activities including generating marketing leads and helping ensure we are meeting our clientrsquos expectations. Collaborate with data scientists and business analysts to enable self-service analytics and reporting. Support Production systems off hours. POSITION REQUIREMENTS Experience 2+ years of industry experience in software development or data engineering. 2+ years extracting insights from data. 1+ years hands-on experience manipulating data and building data pipelines. Hands-on experience with relational and NOSQL databases. Strong appreciation for test-driven development and developing code standards. Competencies Strong understanding of Python or Scala. We use Python, Pyspark, and SQL. Understanding of workflow orchestrators (ex. Airflow, Oozie, or Azkaban). Understanding of test-driven development techniques. Ability to proactively communicate and collaborate across a growing distributed team. Quality Control Help define and manage overall data quality objectives. Implement high-quality source code that passes tests. Achieve and maintain team defined quality thresholds that mitigate solution risks. Please contact recruiter at 248-675-1196Please respond with your updated resume, contact information"
Data Engineer,"Data Engineer

OVERVIEW OF THE COMPANY

Under the corporate banner, we produce and distribute content
through some of the worlds leading and most valued brands,
including: News, Sports, the Network, and the Television
Stations. We empower a diverse range of creators to imagine and
develop culturally significant content, while building an
organization that thrives on creative ideas, operational
expertise and strategic thinking.

JOB DESCRIPTION
--------------
We are building state of the art software and algorithms to
improve the way that our media company transacts, interact with
consumers and customers, and make vital business decisions with
large revenue impacts. Advertising technology plays a very
important role toward these goals, so as a Data Engineer
supporting the Data Science team in the Ad Ops department, you
will frame, pose and translate business problems to build
AI-powered solutions that directly contribute to data products.
You will analyze the content and advertising data and work on
linking the two together and get the opportunity to work on
complex data pipelines and on the latest cloud-based technologies
like AWS lambda, SageMaker, and Apache Spark.

A SNAPSHOT OF RESPONSIBILITIES
Create and maintain data pipelines and process data to derive
insights form it
Collaborate with product management and engineering departments
to understand company needs and devise solutions
Analyze ad-server logs to keep track of thousands of ad
campaigns on multiple devices and multiple types of digital
content
Implement methods to measure KPIs in presence of noise over
long periods of time
Develop and monitor models
Analyze user behavior on Web, linear and digital TV
Build and maintain cloud-based systems on AWS and other cloud
providers
Reconcile data arriving at high speed from multiple diverse
sources
Optimize models for on-device and multi-modal intelligence
WHAT YOU WILL NEED
Experienced Data Engineer with a BS, MS in a quantitative field
(CS, Engineering, Physics, etc.)
Experience with AWS infrastructure
Python ML packages, BOTO, Postgres/Redshift
Experience with Python. Knowledge of one or two other
programming languages
Strong knowledge of relational and distributed databases
General knowledge and familiarity with digital advertising
ecosystem and stack
Log analysis. Experience with ad server logs a plus
Experience working with third party data APIs
Knowledge of Spark and distributed data systems
Experience with AWS lambda and SageMaker a plus
Knowledge of Alteryx workflows will be a plus
Statistical Analysis/Inference will be a plus
resumes to annie@ingenium.agency"
Data Engineer,"Job Description

Data Engineer

At Citadel, data is the core of the investment process. Data Engineers architect and build our data platforms which drive how we source, enrich, and store data that integrates into the investment process. These Data Engineers own the entire data pipeline starting with how we ingest data from the outside world, transforming that information into actionable insights, and ultimately designing the interfaces and APIs that our investment professionals and quantitative researchers use to monetize ideas. Throughout the process, our Data Engineers partner with top investment professionals and data scientists to design systems that solve our most critical problems and answer the most challenging questions in finance.

YOUR OPPORTUNITY:

Develop solutions that enable investment professionals to efficiently extract insights from data. This includes owning the ingestion (web scrapes, S3/FTP sync, sensor collection), transformations (Spark, SQL, Kafka, Python/C++/Java), and interface (API, schema design, events)
Partner with the industry’s top investment professionals, quantitative researchers, and data scientists to design, develop, and deploy solutions that answer fundamental questions about financial markets
Build tools and automation capabilities for data pipelines that improve the efficiency, quality and resiliency of our data platform
Drive the evolution of our data strategy by challenging the status quo and identifying opportunities to enhance our platform
YOUR SKILLS & TALENTS:

Passion for working with data in order to accurately model and analyze complex systems such as a publicly traded company, commodity market, economy, or financial instruments
Strong interest in financial markets and a desire to work directly with investment professionals
Proficiency with one or more programming languages such as Java or C++ or Python
Proficiency with RDBMS, NoSQL, distributed compute platforms such as Spark, Dask or Hadoop
Experience with any of the following systems: Apache Airflow, AWS/GCE/Azure, Jupyter, Kafka, Docker, Kubernetes, or Snowflake
Strong written and verbal communications skills
Bachelor’s, Master’s or PhD degree in Computer Science or equivalent experience
About Citadel


Citadel is a global investment firm built around world-class talent, sound risk management, and innovative leading-edge technology. For a quarter of a century, Citadel’s hedge funds have delivered meaningful and measurable results to top-tier investors around the world, including sovereign wealth funds, public institutions, corporate pensions, endowments and foundations.

With an unparalleled ability to identify and execute on great ideas, Citadel’s team of more than 675 investment professionals, operating from offices including Chicago, New York, San Francisco, London, Hong Kong and Shanghai, deploy capital across all major asset classes, in all major financial markets."
Data Engineer,"PulsePoint Data Engineering team plays a key role in our technology company that's experiencing exponential growth. Our data pipeline processes over 80 billion impressions a day (> 20TB of data, 220 TB uncompressed). This data is used to generate reports, update budgets, and drive our optimization engines. We do all this while running against extremely tight SLAs and provide stats and reports as close to real-time as possible.

The most exciting part about working at PulsePoint is the enormous potential for personal and professional growth. We are always seeking new and better tools to help us meet challenges such as adopting proven open-source technologies to make our data infrastructure more nimble, scalable and robust. Some of the cutting edge technologies we have recently implemented are Kafka, Spark Streaming, Presto, Airflow, and Kubernetes.

What you'll be doing:
Design, build and maintain reliable and scalable enterprise level distributed transactional data processing systems for scaling the existing business and supporting new business initiatives
Optimize jobs to utilize Kafka, Hadoop, Presto, Spark Streaming and Kubernetes resources in the most efficient way
Monitor and provide transparency into data quality across systems (accuracy, consistency, completeness, etc)
Increase accessibility and effectiveness of data (work with analysts, data scientists, and developers to build/deploy tools and datasets that fit their use cases)
Collaborate within a small team with diverse technology backgrounds
Provide mentorship and guidance to junior team members
Team Responsibilities:
Installation, upkeep, maintenance and monitoring of Kafka, Hadoop, Presto, RDBMS
Ingest, validate and process internal & third party data
Create, maintain and monitor data flows in Hive, SQL and Presto for consistency, accuracy and lag time
Maintain and enhance framework for jobs(primarily aggregate jobs in Hive)
Create different consumers for data in Kafka using Spark Streaming for near time aggregation
Train Developers/Analysts on tools to pull data
Tool evaluation/selection/implementation
Backups/Retention/High Availability/Capacity Planning
Review/Approval - DDL for database, Hive Framework jobs and Spark Streaming to make sure they meet our standards
24*7 On call rotation for Production support
Technologies We Use:
Airflow - for job scheduling
Docker - Packaged container image with all dependencies
Graphite/Beacon - for monitoring data flows
Hive - SQL data warehouse layer for data in HDFS
Impala- faster SQL layer on top of Hive
Kafka- distributed commit log storage
Kubernetes - Distributed cluster resource manager
Presto - fast parallel data warehouse and data federation layer
Spark Streaming - Near time aggregation
SQL Server - Reliable OLTP RDBMS
Sqoop - Import/Export data to RDBMS
Required Skills:
BA/BS degree in Computer science or related field
5+ years of software engineering experience
Knowledge and exposure to distributed production systems i.e Hadoop is a huge plus
Knowledge and exposure to Cloud migration is a plus
Proficiency in Linux
Fluency in Python, Experience in Scala/Java is a huge plus
Strong understanding of RDBMS, SQL;
Passion for engineering and computer science around data
Willingness to participate in 24x7 on-call rotation
What we offer:
401(k) Match and free access to a financial advisor
Generous paid vacation/company holidays
Vacation reimbursement (we give you $500 to take vacation), sabbatical, pawternity leave, marriage leave, honeymoon bonus
Comprehensive healthcare with 100%-paid medical, vision, life & disability insurance
$2,000 annual training and development budget
Complimentary annual memberships to One Medical, NY Citi Bike and SF Ford GoBike
Monthly chair massages
Free fitness classes (spin, yoga, boxing)
Gym reimbursement, local gym membership discounts
Onsite flu shots, dental cleanings and vision exams
Annual company retreat
Paid parental leave and a lot of new parent perks
Emergency childcare credits
Volunteer Time Off and Donation Matching, ongoing group volunteer opportunities
Team lunches, Sip & Social Thursdays, Game Nights, Movie Nights
Healthy snacks and drinks
And there's a lot more!"
Data Engineer,"Job Description
Data Engineer
Duration: Contract
Location: NYC, NY

•Must be able to do ETL (SSIS, Azure Data Factory, Informatica)
•Hands-on experience in Azure Data Factory, Informatica and good understanding on data obfuscation or data masking techniques
•Design, construct, install, test and maintain highly scalable data management systems
•Build automated data delivery pipelines and services to integrate data
•Build and deliver cloud-based deployment and monitoring capabilities consistent with DevOps models
•Develop solutions in agile environment for the overall data domain
•Deep experience with developing SQL
•Must have Deep experience deve"
Data Engineer,"What You'll Do:We are looking for a data engineer to manage and further build out our data infrastructure pipelines and data visualizations. The work will support core business decisions and models that serve as the basis for core product and growth strategy. Candidates should be able to choose the right tool for the job and learn how to use it if they don't know how to already. Candidates should prioritize robustness, scaling considerations, and simplicity in architecture decisions.Who You Are:You have at least 2 years in industry. You are experienced with:* SQL (Postgres in particular)* Python* Redis* AWS* Periscope or LookerOptional Skills and Experience:We will prioritize candidates that have the following additional experience:* Experience with Segment or another CDP* Familiarity with Node* Machine learning* Data visualization, web and/or mobile (eg, D3.js)* Spark, Hadoop, or HiveAtom offers competitive benefits including Health, Dental and Vision plans, 401(k) Matching, a complimentary Equinox gym membership, Commuter Benefits, Flexible hours and PTO.We will be working from home until the COVID-19 virus abates. Until then, we will onboard new employees remotely. We do expect however that new employees will work in the office when the crisis is over."
Data Engineer,"Responsibilities Devise and develop data solutions leveraging various cloud-based data, database, and distributed computing technologies. Build data processes to manage terabytes of financial data Work directly with researchers and analysts to gather requirements, iteratively build solutions, and provide support Create software that is easily supportable, well documented, well understood, and well tested Qualifications Requireemnts 5+ years in data engineering andor architecture Deep expertise in cloud based databases Snowflake, Amazon RedShift, Microsoft Synapse, Google BigQuery, and others Experience migrating from traditional SQL RDMS systems to cloud based databases High level knowledge of SQL and experience writing production queries Python andor C experience Self-starter who can work independently Degree(s) in computer science, mathematics, engineering, or closely related field. Please get back to me with your resume and contact details to satya.ksagarsft.com or you can reach me on 858-371-3588 860-924-5866. Thanks, Satya K"
Data Engineer,"Job Details

Level

Experienced

Job Location

New York - New York, NY

Position Type

Full Time

Education Level

4 Year Degree

Salary Range

Undisclosed

Travel Percentage

Undisclosed

Job Shift

Day

Job Category

Information Technology

Description

Greater New York Mutual Insurance Company (""GNY"") is an A+ rated, financially stable and growing property casualty insurance company with locations throughout the Northeast. We are currently looking for a dynamic and highly motivated Data Engineer for our New York office.

Responsibilities:
Develop, construct, test, and maintain architectures, such as databases and analytic environments and platform required for structured, semi-structured and unstructured data
Design and develop data pipelines that deliver accurate, consistent, and traceable datasets for data science projects
Support regular and ad-hoc data needs for data scientists
Provide recommendations and implement ways to improve data reliability, efficiency, and quality
Qualifications
Bachelor's or Master's degree obtained from an accredited institution preferably in Computer Science, Computer Engineering, and Software Engineering, Data Science, or a related field
3-5 years of professional experience in data science or related field
Experience in database deployment and management and proficient in SQL
Experience in data warehousing and ETL (Extract, Transform, and Load)
Proficient in R, Python, VBA, Excel and Word
Excellent oral and written communication skills"
Data Engineer,"Data Engineer

Global Sports & Entertainment - Secaucus, NJ

Description

Our client, a globally recognized sports and entertainment brand with over 1 billion+ fans is seeking a tenacious data engineer to join their expanding data engineering team, partnering cross-functionally with over 10 internal groups.

Position Summary

You will be part of a growing Data Engineering team that handles our clients data for internal and external users. The IT department services over 10 internal groups and the Data Engineer will be a seasoned technology leader comfortable with a variety of data technologies.The Data Engineering Group handles a data warehouse that sources data out of over 15 sources and services over 10 internal groups. The current data technology stack consists primarily of data warehouses using SQL technologies, Neo4J-based Graph DBs along with the use of a variety of NoSQL and AI/ML frameworks and languages like R, Python etc.The ideal candidate will be a technologist who is able to balance the rapid pace of technology change with an authoritative ability to handle client relationships - including working closely with the business and technical teams/vendors, and assisting in the ongoing support of applications. We're looking for someone who is laser-focused on operational excellence and customer satisfaction. You'll need to wear many hats, so flexibility and a can-do attitude are critical!

We are looking for a dynamic, collaborative personality that can champion the cause of Agile within the organization. The individual needs to have a consistent track record of successfully delivering value for their organization in a fast-paced environment along with successful management of customer expectations. A passionate engineer who strives for automation would be ideal for this position.

Experience mentoring and leading other staff, both onsite and offshore will be required for this role to be successful. As a Data Engineer, you will carry out the data efforts across all products and lines of business. You are a pioneer, building new capabilities that will help unlock new possibilities for our businesses. You will play a fundamental role in achieving our ambitious growth objectives. You must be comfortable switching between multiple projects, and working with both business teams and technology teams to translate business requirements into finished product.You possess strategic vision and tactical mastery and combine it with an entrepreneurial spirit to get it done.

You will collaborate closely with stakeholders across the company to design innovative solutions and balance challenging priorities and resource demands. We’re looking for someone who welcomes challenges and is hyper-focused on delivering exceptional results to internal business customers while creating a rewarding team environment. This position reports into the Head of Data Engineering.

MAJOR RESPONSIBILITIES:
Understand business needs and develop solutions that delight consumers and customers
Understand Agile artifacts and develop applications based upon business priority.
Collaborate with project partners to ensure all requirements are met.
Handle relationships with end user communities.
Interact regularly with users to gather feedback, listen to their issues and concerns, recommend solutions.
Demonstrate your technical abilities and contribute to our overall architecture.
Help define and implement the Enterprise Data Architecture and help implement it in multi- functional alignment with the data teams that exist across functions, such as: Marketing, Finance, HR etc.
Provide leadership during application design and development for highly complex or critical machine learning projects across numerous lines of business and shared technology.
Evaluate readiness of new big/fast data capabilities to be used to support critical operational processes.
Build, maintain and demonstrate partnership with 3rd parties like Stanford Labs or Google Labs to be at the cutting edge of Data and ML technologies, and foster data-driven decision making.
Ensure cross-collaboration with sister teams in other geographies.
Ensure alignment to enterprise architecture and usage of enterprise platforms when delivering projects.
Provide oversight for budgets and project plans, and handle technology risks and issues.
Partner with your peers within the organization and build multi-functional alignment.
REQUIRED SKILLS/KNOWLEDGE
5+ years of experience developing Big Data and/or machine learning solutions
Experience with a modern cloud platform, such as AWS, Microsoft Azure, GCP, etc.
Experience with SQL, NoSQL, BigData and Graph Technologies along with Programming languages like R, Python, Kafka, Storm etc.
Experience building microservices
Background in Agile SW development and scaled Agile Frameworks
A true believer in measuring success based on working software and in quick prototyping
Someone who is a passionate coder and can spin up a snippet of code quickly
Strategic thinker with the ability to build and execute innovative digital products, combined with tactical ability to execute simultaneously against multiple contending priorities
Someone with an iterative approach, drive to move fast and think big
Demonstrated ability to partner and communicate effectively with non-technical team members, resolving contending or contradictory objectives, and unifying disparate ideas into a homogenized solution
Ability to be versatile and handle multiple projects and re-prioritizations
Possess the ability to influence others, implement change, and standardize processes in a complex business environment
A passion for mentoring and growing the potential of others
Ability to effectively and appropriately interview technical candidates
Passion for automation and hunger for acceleration; keen knowledge of DevOps as well as RPA is a big plus
Experience with architecting applications (e.g. design Patterns, distributed applications etc.) with the aim of reuse
Superb communication skills (both written and verbal)
Great teammate - should be ready to go beyond to help immediate team and do not be averse to not shy away from asking for help if needed.
Ability to translate ideas into solutions based on user and business needs
Open Eagerness to learn new technologies and bring new ideas to the table
Education
Bachelors Degree or equivalent. Masters would be a plus"
Data Engineer,"About UsSimon Data was founded in 2015 by a team of successful serial entrepreneurs. We're a data-first marketing platform startup, and we approach our work seriously; we tackle problems in a scrappy and disruptive fashion, yet we build for scale to support our clients at big data volume.Simon Data is a data-first customer experience orchestration platform, designed to disrupt the marketing technology and marketing cloud category. Simon's platform empowers businesses to use enterprise-scale big data and machine learning to power customer communications in any channel. Simon's unique approach allows brands to develop incredible personalization capabilities without needing to build and maintain massive bespoke data infrastructure.Our culture is rooted in organizational transparency, empowering individuals, and an attitude of getting things done. If you want to be a valuable contributor on a team that champions these core values we would love to hear from you.The RoleAs a member of the data engineering team, you will have the opportunity to tackle cool engineering projects and make a significant impact on the trajectory of an early-stage startup. Simon's stack is composed of a wide variety of cutting-edge technologies, and you'll work on a diversity of projects that will accelerate your growth as an engineer.You are smart and passionate about data. You love designing creative solutions to complex problems and are always thinking about how you can automate processes. You care deeply about your work and always seek to grow your knowledge and skills. If you are excited about growing your technical career and want to be in an environment that fosters growth and mentorship this is the place for you.What You'll Do* Build custom data pipelines on top of our core platform (Python/Redshift/Snowflake)* Collaborate closely with senior engineers to design and develop new product features* Build out tooling for a data development environment for our clients on our platform* Identify solutions to client requests* Drive technical requirements from high-level marketing and business needs of our end users* Partner with our Customer Success team to deliver quality solutions to our customersQualifications* Fluent in at least one programming language* Detailed-oriented and able to work across departments* Motivated self-starter* Strong technical communication skills* Strong analytical skills and a love for all things data* Experience with Python/Django a plus but not expected* Proficiency in SQL is a plusVisa sponsorship for this role is currently not available.DiversityWe're proud to be an equal opportunity employer open to all qualified applicants regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or expression, Veteran status, or any other legally protected status."
Data Engineer,"Job Description
Are you intrigued by large scale data sets? Does the opportunity to take on a role in data transformation for a growing company excite you? Are you ready to pivot your strong data engineering background into a consulting position?
Our client is a boutique consulting firm with a focus on data strategies taking on a new engagement to do the data governance implementation within an asset management firm in NYC. This asset management firm is currently looking to develop a data platform to support their rapidly growing business. The consultancy is looking to add a Data Engineer to their team who will help them to implement and develop technology for data ingestion and data cataloguing for the data system.This is a prime opportunity to engage with a consultancy specializing in data strategy. The ideal candidate is highly skilled in Python, SQL, Kafka and Apache Airflow within a financial services firm.
Data Engineer Requirements:
Bachelors degree and 4+ years experience
Strong experience with Kafka and Apache Airflow.
Background in Python and Sql.
Understanding on UNIX/Linux.
Ideally worked with cloud infrastructure like AWS.
Worked with technologies like Streamsets, Control -M for Data Pipeline.
This exciting role is a fulltime assignment with a consulting firm in NYC.
#zr 25448
Company Description
Does the opportunity to work with high-level stakeholders interest you? Are you looking for an impactful role in consulting? Do you want to join an organization with a high growth perspective?"
Data Engineer,"Knowledge is our product, and data is our platform. We need engineers who look at a data set and want to unlock the answers it holds inside. Engineers who look at a data set and think about how to make sure it is correct. Engineers who look at a data set and want to make infrastructure to help build it better, faster, and stronger.As a Data Engineer, you will work closely with oncologists and statisticians to build software that will help our customers discover novel insights into their data. You will design our data infrastructure, and use it to develop extensible, robust data and analytics pipelines, tools, visualizations, and services for accessible and flexible data analysis. You will learn more than you ever thought possible about how cancer is treated in the real world, and your work will directly support oncology research and publications.Who you are:* You hold a BS, MS, or Ph.D. in computer science or related field* You have 2+ years work experience* You have experience with languages like Python, C++, Java, or C#* You are passionate about performance, reliability, and scalability of systems* You are inspired by our mission to improve cancer research through technology* You seek simple approaches to complex problems* You like science and/ or medicine just because it's coolBonus points if you have any of the following:* Have a good understanding of relational databases like PostgreSQL, MySQL or MSSQL* Have real passion for data and a strong understanding of statistics* Have developed distributed data processing systems against large, heterogeneous data sets* Have taken a leading role in delivering complex software systems all the way to production* You almost decided to go to med school Flatiron is proud to be an equal opportunity workplace. We are committed to equal employment opportunity regardless of race, creed, color, religion, gender, sexual orientation, gender identity/expression, national origin, disability, age, genetic information, or Veteran status. If you have a disability or special need that requires accommodation, please let us know."
Data Engineer,"The Data & Analytics team plays a vital role . Data is part of nearly everything that we do. The Data & Analytics team is responsible for storing and organizing data, building algorithms used for patient identification/engagement/treatment, and making useful and actionable reporting for business managers.

· Responsibilities

· Collaborate with the Data Analytics team to ensure the confidentiality, integrity, and availability of data necessary for business analytics and reporting

· Collaborate with client and partner data management teams to defines data transfer requirements and logistics

· Build and validate large-scale batch and real-time healthcare data ingestion pipelines with tools on the AWS platform (Redshift, S3, Airflow, Python)

· Manage the overall quality and security of enterprise data assets

· Manage the day-to-day operational data management needs

· Collaborate with data scientists and other engineers and to develop new data-driven product features and deploy machine learning models

· Collaborate with DevOps team to design and manage data architecture for structured (SQL) and unstructured (NoSQL) data sources

Qualifications

· 2+ year experience working in a healthcare payer data operations role strongly preferred

· Strong experience required in Python ( pandas ) and Linux

· Strong experience with SQL preferred

· Strong experience with commercial and/or open-source ETL tool like Airflow

· Experience designing canonical enterprise analytic & reporting schemas

· Experience working on GCP and/or AWS platforms is a plus

· BA in Computer Science, Mathematics, Operations Research, or other related disciplines

· 4+ years of work experience is strongly preferred"
Data Engineer,"Job Description
Hagerty Consulting (www.hagertyconsulting.com) is one of the nation’s leading emergency management and homeland security consulting firms. Known for its public spirit, innovative thinking, problem-solving, and exceptional people, Hagerty is sought after to work on some of the largest, most complex, crisis and emergency management issues. Our services are focused on creating more resilient/sustainable jurisdictions, developing whole community operational plans, supporting recovery eligibility after disasters, and obtaining billion-dollar federal loans and grants..

We are looking for Data Engineerwith experience in Amazon Redshift SQL or Google BigQuery SQL, Looker or Tableau developer, who are self-starters, and ready to support our clients across the nation.

Responsibilities Include:

• Support existing reporting utilizing Looker or Tableau developer, and Excel
• Continually identify and execute process improvements
• Problem solving / troubleshooting experience /skills to assist Production Support in issue resolution

Basic Qualifications:
Candidates must have 1+ years of experience as a data analyst or BI Developer
Excellent communication skills (must be able to interface with both technical and business leaders in the organization)
Superior written and oral communication skills
Expert knowledge of Amazon Redshift SQL or Google BigQuery SQL, and Excel functions
Bachelors Degree Required

Additional Qualifications:

• Data modeling in LookML and/or dbt

EQUAL EMPLOYMENT OPPORTUNITY
Hagerty Consulting, Inc. is fully committed to a strong equal opportunity program. Hagerty Consulting does not discriminate based on the following protected categories: Race, Color, Creed, Religion, National Origin, Ancestry, Age, Sex, Sexual Orientation, Gender Identity (transgender status), Domestic Partner Status, Marital Status, Disability, AIDS/HIV Status, Height, and/or Weight."
Data Engineer,"Your Objective:u00a0u00a0u00a0u00a0u00a0u00a0u00a0u00a0u00a0 Help build and oversee the data warehouse, data lake and own data quality for allocated areas of ownership, including defining and managing SLA for all data sets.

u00a0u00a0u00a0u00a0u00a0u00a0u00a0u00a0u00a0 Partner with the data science team to investigate and implement advanced statistical models and machine learning pipelines.

u00a0u00a0u00a0u00a0u00a0u00a0u00a0u00a0u00a0 Assist client support and sales with client integrations

What you have in your toolkit:

u00a0u00a0u00a0u00a0u00a0u00a0u00a0u00a0u00a0 1-3 years of experience in the data warehouse field

u00a0u00a0u00a0u00a0u00a0u00a0u00a0u00a0u00a0 1+ years of experience with object oriented programming languages

u00a0u00a0u00a0u00a0u00a0u00a0u00a0u00a0u00a0 1+ years of experience with schema design and dimensional data modeling

u00a0u00a0u00a0u00a0u00a0u00a0u00a0u00a0u00a0 1+ years of experience writing SQL statements

u00a0u00a0u00a0u00a0u00a0u00a0u00a0u00a0u00a0 Bachelors degree or higher in Computer science, electrical engineering or related fields

Job Requirements:"
Data Engineer,"**We are still hiring and interviewing for this position. We are conducting interviews over video. We are currently working 100% remote and following the CDC guidelines of when to return to work.**

COMPANY OVERVIEW

Global Atlantic is one of the fastest growing companies in the Life and Annuity industry. We are focused on the U.S. retirement and life insurance markets with a broad range of annuities and life insurance options, as well as reinsurance solutions. Our success is driven by competitive, innovative product designs, leading investment management and integrated risk management, along with highly experienced leadership.

Global Atlantic is looking for a diverse team of talented individuals who reinforce our culture of collaboration and innovation. We are dedicated to the career development of our people because we know they are critical to our long-term success. Join our team and come grow with us.

POSITION OVERVIEW

Summary: The Senior Data Engineer will expand and optimize the data and data pipeline architecture, as well as optimize data flow and collection for cross functional teams. The Data Engineer will perform data architecture analysis, design, development and testing to deliver data applications, services, interfaces, ETL processes, reporting and other workflow and management initiatives. The role will work closely with the business, data analysts and IT teams to support on data strategy initiatives and will ensure optimal data delivery architecture is consistent throughout the strategy. The role also will follow modern SDLC principles, test driven development and source code reviews and change control standards in order to maintain compliance with policies. This role requires a highly motivated individual with strong technical ability, data capability, excellent communication and collaboration skills including the ability to develop and troubleshoot a diverse range of problems.

Responsibilities
Design and develop enterprise data / data architecture solutions using Hadoop and other data technologies like Spark, Scala, Python, SQL etc.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Create and maintain optimal data pipeline architecture.
Devise and execute continual improvement initiatives in all Data Management Service delivery and technology, with a focus on delivery velocity and quality.
Partner with business leaders to determine and prioritize delivery initiatives.
Define or influence system, technical and application architectures for major areas of development.
Devise and execute in software development life cycle including requirements gathering, development, testing, release management, and maintenance.
Engage with business partners to report (formally and informally) on technology strengths, weaknesses, successes and challenges on a regular basis.
Ability to do analytical programming in EDW architecture to bridge the gap between a traditional DB architecture and a Hadoop centric architecture.
Highly organized and analytic, capable of solving business problems using technology.
Ensure appropriate change management and other technology methodologies are carried out on a consistent basis over time.
Should be an individual with in-depth technical knowledge and hands-on experience in the areas of Data Management, BI Architecture, Product Development, RDBMS and non-RDBMS platforms.
Should have excellent analytical skills, able to recognize data patterns and troubleshoot the data.
Will be responsible for design and delivery of data solutions to empower data migration initiatives, BI initiatives, dashboards development etc.
QUALIFICATIONS
Undergraduate degree required, MBA or other advanced degree preferred.
6+ years of experience as a member of an information technology team.
Minimum of 3 years of relevant experience architecting the complete end to end design of enterprise wide solution using latest technologies – Hadoop, Spring boot/Spring Cloud, Rest API, SQL
Minimum of 3 years experience in Python, Core Java, Hortonworks, AWS, Rest API, Microservices, Spring Boot, Spring cloud etc.
Ideally a strong Life Insurance background.
Experience with data modeling, complex data structures, data processing, and data quality and data lifecycle
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing 'big data' data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to provide solution for specific business requirements.
Experience building solutions for streaming applications.
Should be able to lead critical aspects of the data management and application management.
Experience in UNIX shell scripting, batch scheduling and version control tools.
Experience in large scale server-side application development that includes the design and implementation of high-volume data processing jobs.
Cultural awareness with excellent interpersonal and relationship building skills.
Passion and drive for continuous improvement and transformational change, with a business owner mentality.
Strong written and verbal communication skills.
At Global Atlantic, enjoy a highly competitive compensation and benefits package, with a focus on pay equity and flexibility. We are proud to support work/life balance that meet individual and business needs. A business casual dress code, above-average paid time off, parental and military leave, adoption assistance and tuition reimbursement are just a few of the extra perks that make Global Atlantic a great place to work. We strive to foster a culture of total well-being through community outreach and charitable giving programs.

In New York, we actively partner with the Red Hook Conservancy, Girls who Invest, and The Bowery mission.  Social platforms provide an environment to collaborate with others and participate in friendly competitions towards achieving physical, emotional and financial well-being. Our highly competitive health, retirement, life and disability plans can be tailored to best suit your needs and those of your whole family.

Global Atlantic is committed to creating an inclusive environment where everyone feels like they can meaningfully contribute to our success. We are proud to be an equal opportunity employer and we do not discriminate in employment on any basis that is prohibited by federal, state or local laws. We encourage qualified applicants from all demographics to apply. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, disability, age, or veteran status.

Global Atlantic Financial Company Employee Candidate Privacy Notice"
Data Engineer,"Job Description:
Bachelor's Degree in Computer Science or a related discipline
5+ years of applicable engineering experience
Strong proficiency in Python with an emphasis in building data pipelines
Ability to write complex SQL to perform common types of analysis and aggregations
Experience with Apache Airflow or Google Composer
Detail-oriented and document all the work
Ability to work with others from diverse skill-sets and backgrounds
Nice to have:
Experience with version control systems (Git and Bitbucket)
Experience with Atlassian products Jira and Confluence
Experience with Docker containerization
knowledge of Application Programming Interfaces

Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company."
Data Engineer,"Want to work at the forefront of a fast-growing and award winning fintech company? With an incredible team and partners looking for innovative results, weve doubled in size our first year and doubled the asset classes we serve in our second. As a cloud-based company, we transitioned seamlessly into a socially-distanced workforce in recent months, and with a pace that hasnt slowed, were still hiring and are currently on the lookout for smart and collaborative talent to join our remotely connected team.

Job Description
SIMON is looking for a Data Engineer with 1-5 years of experience. In this role you will utilize a variety of data sets to maximize SIMONs earnings potential. The ideal candidate would be half computer hacker & half number-cruncher. This role will have the ability to directly impact the business bottom line and make a substantial contribution to a growing fintech startup.

Responsibilities

· Analyzing user behavior to maximize adoption of the platform

· Ingesting external data sets into a computational research environment

· Measuring performance of the platform to reduce operational cost

Minimum Qualifications

· Experienced in at least 1 numeric research framework (python/pandas, R/Splus, Octave/Matlab)

· Some background in probability/statistics

· Familiarity with various database designs (Relational, Columnar, nosql)

Preferred Qualifications

· Familiarity with AWS and infrastructure-as-code (terraform or cloud formation)

· Understanding of machine learning techniques

We offer a competitive salary and benefits, the chance to work with a curated team of top-notch, highly creative talent, and a fun and agile work environment with many perks in the heart of New York Citys Chelsea district.

Meet SIMON
SIMON Markets is a fast-growing and award-winning FinTech company on the lookout for smart, collaborative talent to join our team.

Originally founded within Goldman Sachs, SIMON Markets spun-out in December 2018 under the shared ownership and direction of seven leading financial institutions. Our team works to build transparency around complex, risk-managed financial products by way of a single, modern platform. Our platform provides financial advisors with access to multi-issuer solutions, a depth of educational tools, and advanced post-trade analysis that hasnt before been possible.

By combining state-of-the art technology with the leadership of niche industry experts and an agile start-up mentality, SIMON is well-positioned to reshape our industry. We are leading the way in a space we helped create, we are passionate about pushing boundaries, and we are growing like crazy."
Data Engineer,"Job Description
We are looking for a Data Engineer to join our team. This is a unique role that will allow this Data Engineer to work in Singapore for the first few months before coming back to work in NYC.

Qualifications:

Proven demonstrable experience designing and implementing ETL pipelines
Experience with any Data warehouses (Teradata / Vertica / Redshift / Snowflake)
Good working knowledge of RDBMS databases (PostgreSQL, Oracle, MySQL)
Extensive Hands on coding experience in Python/Java/Shell / Spark+scala
Good knowledge of testing tools (JMeter / Juint / Python unittest)
Working knowledge of Apache Spark and/or other distributed computing frameworks (MapRed)
Good at tuning and profiling complex Spark programs
Should have some Hadoop experience
Good to have:
Experience with Presto / SparkSQL
Groovy
Cucumber/Gherkin/Selenium
Some relevant AWS experience (EC2/ S3)
Banking and Financial Services
About Boyle Software:
Boyle Software designs, develops, and delivers custom technology solutions for a wide array of businesses and organizations: Fortune 500 companies, start-ups, non-profits, etc.

Boyle Software is more than the sum of its parts. We are a successful company because we attract and retain the top talent in the industry. Challenging each other with the latest technology packages, frameworks, and platforms, we are constantly evolving and improving ourselves. Boyle Software is a family full of talent and passion for the latest technology with offices in New York City and Kiev, Ukraine.

We want you to join our ever-growing team of smart, passionate engineers and expand your skills while you work!"
Data Engineer,"About Madison Logic:
We are a growing MarTech company that empowers B2B Companies globally to convert their accounts faster (find us on Inc. Magazine's 5000 List for the 6th consecutive year). As a truly global company, we take pride in the diverse backgrounds of our team. We put thought into every hire and we hand pick only the best talent. When you join our team, you are committing to giving 100% and always striving for more. We face challenges head-on and we overcome them together.



What We're Looking For:
We are seeking a Data Engineer with a highly analytical mind who is skilled at understanding data and is able to translate it into actionable insights. In this role you will develop easy to understand reports, dashboards, and tools with the aim of optimizing and streamlining the way data is viewed. In this highly collaborative role, you will work closely with the architecture and data teams to reach business objectives.
What You'll Do:
Use SQL/Python to create reports, dashboards, and visualizations.
Aggregate/Model data and use that data to build reports in Domo
Analyze data to help improve business performance.
Identify the best data sources for a given analysis.
Develop processes for data mining, data modeling, and data production.
Offer insights and recommendations to improve data reliability and quality.

What You'll Need:
Bachelor’s degree in computer science, statistics or mathematics
3+ year’s experience with Python or Node.js
4+ year’s experience with SQL or mySQL
3+ year’s experience with cloud computing services (AWS)
Working understanding of big data analytics
Experience working with data cleaning and standardizing process
Who We Are:
Our Vision: We empower B2B organizations globally to convert their best accounts faster

Our Values: #TEAM #OWNIT #RESPECT #EXCEL #EMPOWER

Our Commitment to Diversity & Inclusion:
Madison Logic is proud to be an equal opportunity employer. We are committed to equal employment opportunity regardless of sex, race, color, religion, national origin, sexual orientation, age, marital status, disability, gender identity or Veteran status.

Privacy Disclosure:
All of the information collected in this form and/or by your application by submission of your online profile is necessary and relevant to the performance of the job applied for. We will process the information provided by you in this form, your CV (including physical and online resume profiles), by the referees you have noted, and by the educational institutions with whom we may undertake to verify your qualifications with, in accordance with our privacy policy and for recruitment purposes only.

For more information on how we process the information you have provided including relevant lawful bases (where relevant) please see our privacy policy which is available on our website (https://www.madisonlogic.com/privacy/)."
Data Engineer,"Andiamo is an Equal Opportunity Employer

Andiamo provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, Andiamo complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.

All qualified candidates are encouraged to apply by submitting their resume as an MS word document including a cover letter with a summary of relevant qualifications, highlighting clearly any special or relevant experience."
Data Engineer,"Vydias Data Engineering & Data Science efforts are an integral part of our success. Through our data, we nurture our artists, creators, partners, and internal users with insights to help them engage their audience.
Our Extract-Load-Transform workflows promote quick analysis and richer, complex investigations all at once, and our data warehouse supports both data science and analytics. Of all the terabytes of data we gather, one-third is well-structured, with the remaining being mostly semi-structured and some wholly unstructured. And, it currently doubles every 5.3 months!
The Role:
As a Sr. Data Engineer, you will own Vydias multitude of data pipelines. You will design and implement our ELT workflows, which originate at partner APIs and conclude in the Warehouse. You will work closely with our data science, BI, and product teams in figuring out current and future needs.
As a leader on the Data team, your responsibilities will include:
Ensuring the availability and timely delivery of data, company-wide
Modeling new data sets and crafting all new ELT workflows and pipelines
Lead the orchestration of the workflows and contribute strongly to infrastructure decisions
Improving on and monitoring of existing pipelines and oversight of our ELT workflow
Maintaining a single version of truth for our data and working with others to implement continuous integration (CI) data quality tests
Mentoring and guiding your junior colleagues and leading with vision, and with respect to the companys data strategy


Requirements:
While we are not married to any tool or technology we also look for those intimately familiar with Python and have previous experience using Airflow, Docker and Kubernetes.
We use AWS S3 & EC2 extensively, our current DW is on Redshift and our app relies mostly on Postgres. We use Looker in-house for BI and Product Engineers work mostly in Ruby and Python, while our data scientists work in R.
About You:
We want to learn more about you. If you feel like you are the right fit for this role, let us know.
In our mind, being a perfect fit means that you have the necessary hard skills and expertise, and the complimentary soft skills.
You are a Python pro and have regularly used AWS or Google Cloud Platform to manage data and move it between applications
Do you love APIs? When you encounter a new one, do you study it inside and out and learn every corner of it, as though you designed it yourself?
Working with deeply-nested, complex JSON is a fun day at the office for you.
You can articulate the merits and pitfalls of the different approaches in designing a pipeline.
You are passionate about data quality control and know how and where to anticipate potential errors.
Working in the cloud is not a point of distinction for you, it is a given.
You understand what it means to work at a tech startup. Hopefully, this is what excites you more than anything else about working here.
You intuitively know how to extract value and insights from data.
You love the idea of building the data scene in NJ and being a leader in this community.
You have orchestrated workflows using Airflow and are familiar with the challenges and how to overcome them.
Critically, you're a person who thinks in data; you relate the real world to data, and vice-versa. You understand that data is not the end goal but a vehicle to help get us where we are going, and you see your role as the person most critical in making that happen."
Data Engineer,"Role Background:

Were looking for agreat Data Engineer for our growing Analytics team. Youll be responsiblefor all things data: building out our streaming and batch ETL pipelines,curating and anonymizing data thats being generated from various sources,designing and building the data warehouse, and so on.

Well expect you to havean in-depth knowledge of distributed systems and data flows. Combinedwith an understanding of business intelligence and performance requirements,youll breathe life into data and help make it an invaluable part of theplatform and business.

If you are aself-starter, excited about building a culture around data-driven decisions,motivated by making an impact, and pushing the boundaries of your knowledge,you will excel here and do great things!

The Role
Design, implement, and maintain an ever-growing ETL pipeline using state-of-the-art technology
Use best practices and standards for managing large collections of data for analytics
Discover and integrate new heterogeneous data sources
Work closely with data analysts, data scientists, and product managers enabling them to provide insight into key performance metrics of the business
Help to improve data reliability, efficiency, and quality
Your Qualifications:
4+ years of experience designing and developing a data warehouse on a distributed database platforms, such as Snowflake or Redshift
Experience designing, developing, and maintaining high-throughput and low-latency ETL pipelines
Experience with data modeling, data access, and data storage techniques
Experience with big data tools such as Apache Spark
Proficient in SQL and Python
Proficient with at least one RDBMS (MySQL or Postgres preferred)
Successfully implemented data pipelines in the public cloud, especially Amazon Web Services
Strong analytical and interpersonal skills
Enthusiastic, highly motivated and ability to learn quick
Benefits & Perks
Robust health benefits packages including access to a 401k and various medical, dental and vision plans, and $100/month fitness reimbursement
Full support for remote work during COVID-19
Daily lunch delivery credit and other goodies sent to home
Regular company-wide social events (even virtually!)
Generous annual education stipend toward job-related external learning opportunities
An extremely enthusiastic team that appreciates collaboration"
Data Engineer,"Skills AWS, Hadoop, Spark, Python
Job Description:
7 years IT experience and 5 years exp in Big Data area
Knowledge on DWH concepts and SQL query skills
Need AWS experience
Experience developing and supporting Data warehouse ETL applications
Need someone who will help them to convert Pig Latin UDFs to Spark. Familiar with UDFs
Good understanding of Spark Structured Streaming Vs RDD streaming
Prior exp to perform operations in Hive and Spark
Good experience in Python
Experience with NoSQL and SQL databases
Excellent analytical and problem solving skills
Excellent oral and written communication skills"
Data Engineer,"Data Engineer
Type - C2C / C2H / W2 / FT
Duration – 12-18 Months
Location – NYC, NY (it could be remote. Post that, location is NYC)
Only GC or USC
We are looking for a self-motivated Data Engineer to join our data platform team. The candidate will be responsible for expanding and optimizing our data platform and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing and/or re-designing our data architecture to support next generation of products and data initiatives.

Responsibilities:

• Gather requirement from Product Owner team; groom the userstories for the requirements and plan the stories for each sprints for project delivery.
• Design the database objects and develop procedures, functions, packages with scalability and high performance
• Extensively work in collections and table type objects with multiple level nested tables
• Work with stakeholders including Managemet team, Product Owners and Architecture teams to assist with data-related technical issues and support thier data infrastructure needs.
• Identify, design and implement internal process improvements: automating manual process and optimizing data delivery.
• Maintain the source codes using version control tools like github and work with DevOps team for seamless deployment of database objects to various environments.
• Prepare Conceptual, Logical and Physical data models for the Databases using tools like Erwin Data Modeler.

Technologies and Tools:

Oracle 11g/12c, Toad for Oracle, Linux, Putty, GitHub, Rally,
Nice to have: Informatica, Python, Aurora, and Hive

Qualifications:

Experience building and optimizing Oracle databse objects like Packages, Procedures, Functions and SQL queries.
Advanced hands-on SQL knowledge and experience working with relational databases for data querying and retrieval.
Experince in performance tuning of DB using Collections and Bulk operations.
Experience in Agile Methodology practice for continuous delivery of the project increments.

Nice to have:
Experience with big data frameworks/tools: Hadoop, AWS, Kafka, Spark, etc.
Experience with relational SQL and NoSQL databases, including Oracle, Hive, HBase.
Experience performing root cause analysis on data and processes to answer specific business questions and identify opportunities for improvement.
Experience with data security.
Experience with Java and/or Python
Working with product owners to review the requirement, analyze the business logic, and translate that to the details tasks of the particular user stories in Rally;
Participate daily scrum meetings to collaborate with other scrum team members;
Working closely with team members to implement the requirement captured in the Rally user stories;
Deliver the commitment made to the program increment (PI)."
Data Engineer,"Are you a Data Engineer that likes to build critical data infrastructure? Do you have an interest in software engineering, data warehousing and business analytics? If so, this exciting Media & Technology organization based out of New York will motivate you to learn new technologies!

What's The Job?

As a Data Engineer on our team you will be designing and developing data models, building batching and streaming data pipelines. You will also develop automation processes using the newest technologies.

Who Are We?

We are a creative media organization that is a pioneer in our industry. We are always thinking about how to deliver the best offerings and experiences to our customers.

What Skills Do You Need?

Experience working with Python & SQL Languages.
Experience using SQL Databases.
Experience using tools such as Spark and Airflow.
Experience with GCP or AWS Cloud.
Prior experience with Java, Go, Bash, Docker or Kubernetes is a major PLUS!


Compensation

$120,000 - $150,000 base salary
Generous vacation and parental leave
Full Medical, Dental and Vision
401(K) Matching
We are big proponents of diversity, and encourage diverse applicants / candidates with diverse backgrounds to apply."
Data Engineer,"Impact
Ever wondered how Amazon offers the Earth's biggest selection and still manages to offer lower prices every day to our customers? Our retail business teams work with a massive array of vendors and business financial performance metrics to expand selection and drive costs lower. Given the rapid growth of our business, this requires our category leaders, financial analysts, site merchandisers and vendor managers to quickly analyze vendors, categories and brands, diving deep into data showing business efficiency down to the unit sale level. The technology that enables this has huge visibility and impact and is critical to Amazon's continued profitability and growth.

Innovation
We're working on the future. If you are seeking an environment where you can drive innovation. If you want to apply state-of-the-art software technologies to solve real world problems. If you want the satisfaction of providing visible benefit to end-users in an iterative fast paced environment, this is your opportunity. The responsibilities of this role will be key in paving the future of Amazon Retail and transforming how we do business.

Opportunity
You will be part of a team of creative, top-notch software developers to work hard, have fun, and make history. Software engineers at Amazon are more than just order takers; they see a problem and leverage innovative technology to address it. You will be working with very large data sets, well beyond the scalability limits of conventional relational databases. We're looking for people who innovate, love solving hard problems, and never take ""no"" for an answer.
Our team is within Retail Financial Intelligence Systems and develops sophisticated tools for the Amazon Retail businesses, supporting deep dive analysis, vendor negotiations and business planning towards enhancement at the bottom line. We also provide financial and operational reports to Amazon retail vendors worldwide.
To learn more about Financial Intelligence Systems, visit our page at http://bit.ly/AmazonFIS

Basic Qualifications

· Bachelors degree or higher in an analytical area such as Computer Science, Physics, Mathematics, Statistics, Engineering or similar.
· 5+ years relevant professional experience in Data Engineering and Business Intelligence
· 3+ years in with Advanced SQL (analytical functions), ETL, DataWarehousing.
· Advanced data analysis skills e.g. pivot table.
· Strong knowledge of data warehousing concepts, including data warehouse technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools and environments, data structures, data modeling and performance tuning.
· Ability to effectively communicate with both business and technical teams.
·


Preferred Qualifications

· Experience on working with Big Data
· Knowledge and experience on working with Hive and the Hadoop ecosystem
· Knowledge of Map Reduce, Spark and Presto"
Data Engineer,"Who We Are

At Cross River, we are building the best banking and technology products for fintechs, all through APIs. As venture-backed pioneers of the platform-based financial services model known as the sponsor, or partner, bank, we work closely with our partners to offer products such as payments rails and loan origination services to the latest in regulatory and compliance solutions. What drives us every day? A desire to innovate in the service of financial inclusion. Giving access to consumers, whether they are looking for credit or for a better way to pay has been in our DNA from day one. Our nimble, adaptive, one-step ahead culture is powered by our people, their creativity, tech-forward thinking, and collaborative spirit, we are experiencing explosive growth.

What we are looking for

Cross River's Technology team is made up of problem solvers hungry to perfect new products and systems. As SQL Developer, you play a key role. In this role you will focus on SQL reporting. The Technology team cares about results, not activity, and we have fun doing it. If you like challenging problems are analytical, and a great team playerwe want to hear from you!

Responsibilities:
Using SQL Server Integration Services (SSIS), Python, C#
Design, develop, optimize, maintain and support ETL processes using data warehouse design best practices and SQL Server Integration Services (SSIS) to integrate data from multiple source systems into a Data Warehouse, cleansing data, transforming and loading data from multiple formats using SSIS and stored procedures
Create and optimize SQL Statements, Procedures and Functions
Modify databases according to requests and perform tests
Solve database usage issues and malfunctions
Liaise with developers to improve applications and establish best practices
Ensure all database programs meet company and performance requirements
Research and suggest new database products, services and protocols
A successful candidate will have hands-on experience in a multitude of domains; including, but not limited to database design, data warehousing, business intelligence, big data, database tuning, application optimization, security, virtual computing and storage, incident tracking, and general database administration
Documenting procedures for all tools / processes you create.
Qualifications:
Working independently or minimal supervision
Proven work experience as a Database developer
5 years' experience with SSIS
Hands on experience with SQL
Knowledge of software development and user interface web applications
Familiarity working with .Net Framework, JavaScript, HTML and Oracle
Excellent analytical and organization skills
An ability to understand front-end users' requirements and a problem-solving attitude
Excellent verbal and written communication skills
BSc degree in Computer Science or relevant field
Preferred qualifications:
Knowledge of Python, Datomic, Hadoop, Big data
Experience with C# .Net development
Cross River is an Equal Opportunity Employer. Cross River does not discriminate on the basis of race, religion, color, sex, gender identity, sexual orientation, age, non-disqualifying physical or mental disability, national origin, veteran status or any other basis covered by appropriate law. All employment is decided on the basis of qualifications, merit, and business need."
Data Engineer,"Job Description
Data Engineer

OVERVIEW OF THE COMPANY

Under the corporate banner, we produce and distribute content through some of the worlds leading and most valued brands, including: News, Sports, the Network, and the Television Stations. We empower a diverse range of creators to imagine and develop culturally significant content, while building an organization that thrives on creative ideas, operational expertise and strategic thinking.

JOB DESCRIPTION

We are building state of the art software and algorithms to improve the way that our media company transacts, interact with consumers and customers, and make vital business decisions with large revenue impacts. Advertising technology plays a very important role toward these goals, so as a Data Engineer supporting the Data Science team in the Ad Ops department, you will frame, pose and translate business problems to build AI-powered solutions that directly contribute to data products. You will analyze the content and advertising data and work on linking the two together and get the opportunity to work on complex data pipelines and on the latest cloud-based technologies like AWS lambda, SageMaker, and Apache Spark.

A SNAPSHOT OF RESPONSIBILITIES
Create and maintain data pipelines and process data to derive insights form it
Collaborate with product management and engineering departments to understand company needs and devise solutions
Analyze ad-server logs to keep track of thousands of ad campaigns on multiple devices and multiple types of digital content
Implement methods to measure KPIs in presence of noise over long periods of time
Develop and monitor models
Analyze user behavior on Web, linear and digital TV
Build and maintain cloud-based systems on AWS and other cloud providers
Reconcile data arriving at high speed from multiple diverse sources
Optimize models for on-device and multi-modal intelligence
WHAT YOU WILL NEED
Experienced Data Engineer with a BS, MS in a quantitative field (CS, Engineering, Physics, etc.)
Experience with AWS infrastructure
Python ML packages, BOTO, Postgres/Redshift
Experience with Python. Knowledge of one or two other programming languages
Strong knowledge of relational and distributed databases
General knowledge and familiarity with digital advertising ecosystem and stack
Log analysis. Experience with ad server logs a plus
Experience working with third party data APIs
Knowledge of Spark and distributed data systems
Experience with AWS lambda and SageMaker a plus
Knowledge of Alteryx workflows will be a plus
Statistical Analysis/Inference will be a plus
resumes to annie@ingenium.agency"
Data Engineer,"Data Engineer

JOB SUMMARY:

Allied World is seeking an individual to lead integration efforts of
underwriting and claims platforms with our corporate Enterprise Data Warehouse
(EDW). The successful candidate will work within our Agile environment as part
of a Scrum team to develop new, and support pre-existing Extract, Transform
& Load (ETL) processes. We look for
people who can work effectively within a team as well as take the lead on new
initiatives.

JOB DUTIES AND RESPONSIBILITIES:

• Work with EDW &
Architecture groups to define and implement integration of new systems and
processes.

• Actively participate
in sprint planning, design, coding, unit testing, and sprint reviews.

• Participate in peer
code reviews and adhere to best practice development methodologies.

• Participate in an
on-call rotation to provide warehouse support if issues arise.

• Must be a
self-starter with a great attitude, not afraid to take the initiative, and work
with little supervision

• Preference given for
holders of a University Degree or Diploma in Computer Science or related field.

• Preferred experience
working within an Agile - Scrum environment

• Minimum 3 years ETL
experience, real world experience with SSIS

• Minimum 3 years of
experience with SQL server

• Extensive knowledge
and experience using concepts including but not limited to:

o Stored Procedures/TSQL

o Query Performance Tuning

o Data Warehousing & OLAP

o Source control tools & concepts
(CI/CD, Git, Bitbucket, Github)

• Prior experience
working with PowerShell and/or Python would be an asset

• Practical experience
with property & casualty insurance/reinsurance and multi-currency systems
is an asset

• Ability to analyze
and write business and technical specifications and interact with our business
users effectively

• Possess a systematic,
disciplined and analytical; approach to problem solving

About
Fairfax

Fairfax is a
holding company which, through its subsidiaries, is engaged in property and
casualty insurance and reinsurance and investment management.

About
Allied World

Allied World Assurance Company Holdings, Ltd, through its subsidiaries and brand
known as Allied World, is a global provider of innovative property, casualty
and specialty insurance and reinsurance solutions. Allied World offers superior
client service through a global network of offices and branches. All of Allied
World's rated insurance and reinsurance subsidiaries are rated A by A.M. Best
Company, A by Standard & Poor's, and A2 by Moody's, and our Lloyd's
Syndicate 2232 is rated A by Standard & Poor's and AA- by Fitch.

Our generous
benefits package includes: Health and Dental Insurance, 401k plan, and Group
Term Life Insurance. Allied World Insurance Company is an Equal
Opportunity and Affirmative Action Employer. All qualified applicants
will be considered for employment without regard to an individual's race,
color, national origin, religion or belief, sex, age, genetic information,
marital or civil partnership status, family status, sexual orientation, gender
identity, or their protected veteran or disability status. Please visit
www.awac.com for further information on Allied World."
Data Engineer,"Job Description
Data Strategist - Engineer

NYC Hedge Fund

J Harlan Group is currently conducting a search for a Data Strategist - Engineer at a prominent NYC Hedge Fund

An exciting opportunity to join the Quantitative Strategy team designing and developing the infrastructure for evaluating, ingesting, parsing and processing structured and big data sets. The Data Strategist will work closely with data scientists, quantitative researchers, and portfolio managers to design, implement, and scale the organizations overall data architecture.

The firm views Technology / Data Strategies as a distinct commercial advantage - enabling a host of opportunities that differentiate them from their competitors.

The firm is a fast growing and dynamic investment organization that is experiencing exceptional growth and is looking for exceptional people to power its next phase of growth. The Data Strategist will be a significant contributor in enabling their vision and helping the organization to grow and improve their businesses.

The ideal candidate would have a background:
Strong computer science and technical background with experience and an interest in working with data. (prior experience with big data sets preferred)
Experience with designing and implementing data processing pipelines
Experience using distributed computing frameworks
An interest in financial markets (with prior experience a big plus)
Programming skills required - proficiency in memory managed languages required such as; Python, C#, Java, F#, scala, haskel, ada, smalltalk, prologue, etc. Python and .Net is a plus.
Ability to work in an open collaborative and highly charged trading floor environment. You will sit in a trading floor seat in an open environment surrounded by high-intensity colleagues.
Strong desire to contribute and build a business – not just solve novel problems.
Ability to work with a wide range of users – traders, portfolio managers, research analysts, quants and the wider investment management team to develop and implement technology in support of a dynamically growing Hedge fund platform
An individual who loves solving deep and complex technical and business problems and wants to have an outsized impact with the products they build and deliver. An individual with a passion for technology, high level of intellectual curiosity, a commitment to excellence and an unparalleled drive to deliver world-class software across the firm.

About the Client:

The firm is a leading alternative asset manager managing more than $22.0bn of assets under management with an outstanding track record of following a comprehensive, multi-strategy approach to investing and allocating capital dynamically to the most compelling opportunities and harvesting multiple sources of alpha.

They have a relentlessly focuses on innovation and integration: innovation in new products, markets and businesses as well as new tools, models and technology management and performance structures; integration of fundamental research, quantitative strategies and technical analysis, all supported by an intensive focus on operational excellence and comprehensive risk management.

They employ over 240+ talented professionals in New York, London, and Tokyo locations across portfolio management, trading, credit, research, quantitative strategy, trading technology, investment management analysis and business management administration and strategy.

They seek candidates who are high-energy self-starters who want to join an investment management firm on the leading edge of the global markets. The management team needs individuals of the highest professional caliber who are leaders, problem solvers, analytic, detail-oriented, and entrepreneurial. Everyone at the firm works side-by-side with the firm's senior partners in a highly collaborative and charged trading floor environment.

Successful candidates are:
Analytic and relentless in pursuit of the right answer
Strong communicators who excel at rapid synthesis
Able to demonstrate sound business judgment
Able to digest complexity while maintaining an understanding of the “big picture” of business needs
Team players who are energized by a collaborative enterprise
The firm's employees maintain the highest professional and ethical standards. The firm has earned a reputation for honesty, fair dealing, and transparency in a competitive industry. They believe that these standards are the foundation for superior investment performance and are critical to delivering performance to clients"
Data Engineer,"Job Description


Job #: 1076197

Title- Data Engineer

Client- Large Financial Institute

Location- Manhattan, NYC

Duration- 12+ Month Contract with Potential to Extend/Convert

Description Summary

Financial regulation for banks has increased dramatically over the past few years. This role is to work on the Global Banking and Markets (GBAM) Non-Financial Regulatory Reporting (NFRR) Data Delivery program, a new initiative to define and implement consistent and efficient regulatory reporting processes that adhere to enterprise standards, simplify controls and enable re-use.

As part of this initiative, we are developing a Data Processing Framework using Scala and Big Data technologies for authoring domain-specific-language (DSL) components to seamlessly combine data from multiple data sources. The DSL components interpret transformation rules written in a configuration style syntax that can be applied to one or more standard data sets to produce a transformed data set. This framework provides a unified language for describing the data needs of a report.
• Enables users to seamlessly retrieve and combine data from multiple sources
• Enables users to author reports without having to worry about the mechanics of actually retrieving, filtering, projecting, or aggregating the data
• Ensures proper versioning of the report definitions and the running of the reports as they existed at specific points in time
• Ensures that the system is scalable enough to run hundreds of reports in parallel, if require

Role
We are looking for a Scala developer with Apache Hadoop/Spark/Flink experience to assist with the development of the regulatory reporting processing engine using Scala and Big Data technologies to provision regulatory reporting data via domain-specific-language (DSL) scripts. The role requires close partnership with the NFRR Program analysis and development teams.

Responsibilities
The candidate will work directly with the Director of the Data Processing Engine and will participate in all phases of development of the platform and subsequent reports.
• Develop technical specification and component level design for DSL Extensions for input and output
• Design parameterized and configurable modules for DSL Extensions
• Develop DSL for the data outputs needed for NFRR Reports
• Integrate Code Repository and Management

Qualifications
The candidate must be a self-starter, able to work in a fast paced and results driven environment with minimal oversight. The candidate is required to have excellent communications skills and possess a strong sense of accountability and responsibility.

• 5+ years development experience
• Good general Scala programming skills
• Experience with Hadoop Distributed File System (HDFS), HBase and Hive
• Experience with custom aggregations within Spark/Flink, preferred
• Experience with databases, a plus
• Experience on regulatory or reporting projects, preferred
• Ability to perform detailed and complex data analysis
• Attention to detail and ability to work independently
• Ability to handle tight deadlines, and competing demands in a fast paced environment
• Knowledge of Global Banking and Markets’ products/asset classes and associated data including fixed income, equities, derivatives, and foreign exchange securities, preferred
Scala developer with Apache Hadoop/Spark/Flink experience
5-7 years

If you are interested in applying to this position or learning more please reach out to Yasmin Aminyar directly at yaminyar@apexsystems.com .

EEO Employer

Apex Systems is an equal opportunity employer. We do not discriminate or allow discrimination on the basis of race, color, religion, creed, sex (including pregnancy, childbirth, breastfeeding, or related medical conditions), age, sexual orientation, gender identity, national origin, ancestry, citizenship, genetic information, registered domestic partner status, marital status, disability, status as a crime victim, protected veteran status, political affiliation, union membership, or any other characteristic protected by law. Apex will consider qualified applicants with criminal histories in a manner consistent with the requirements of applicable law. If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation in using our website for a search or application, please contact our Employee Services Department at employeeservices@apexsystemsinc.com or 844-463-6178."
Data Engineer,"Work it! Here at Shake Shack, we take care of each other first and foremost so that we can make raves for our guests, community, suppliers, and investors. After all, teamwork makes the dream work. We work our buns off, but we play hard too, with a Team Appreciation Day, unlimited meal discounts, volunteer opportunities, and so much more. If you're looking for a deeply fulfilling, financially rewarding, and really fun career, you're in the right place.Sr Data EngineerThe Sr Data Engineer should be an expert with all of the data warehousing technical components (e.g. ETL, Reporting, Data Model), infrastructure (e.g. hardware and software) and their integration. The ideal candidate will be responsible for developing overall architecture and high level design.Key Responsibilities* Lead architecture design and implementation of next generation cloud BI solution.* Build robust and scalable data integration (ETL) pipelines using SQL, EMR, Python and Spark.* Build and deliver high quality data architecture to support business analysis, data scientists, and customer reporting needs.* Interface with other technology teams to extract, transform, and load data from a wide variety of data sources* Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for business constituents.* Support, maintain, optimize, upgrade, enhance our current and future state Data Infrastructure (SQL server, AWS, Snowflake, Tableau, Alteryx).Skills & Knowledge* Bachelors Degree in related field* Demonstrated strength in data modeling, ETL development, and Data warehousingExperience* 4-6 years of data engineering experience* Experience in working and delivering end-to-end projects independently* Experience with AWS services including S3, Redshift, EMR, Kinesis and RDS preferred* Experience with cloud Data Warehouses such as Snowflake are a plus* Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.)Benefits include:* Medical, Dental, and Vision Insurance* Transit Discount Program* 401K Plan* Paid Time Off Program* Flexible Spending Accounts* Employee Dining Program* Referral Bonus* Online Training Program* Career Development* Corporate Fitness Discount Programs* Choice of Global Cash Card or Direct DepositAbout UsBeginning as a hot dog cart in New York City's Madison Square Park, Shake Shack was created by Danny Meyer, Founder and CEO of Union Square Hospitality Group and best-selling author of Setting the Table. Shack Fans lined up daily, making the cart a resounding success, and donating all proceeds back to the park beautification efforts. A permanent stand was eventually builtand the rest is Shack history! With our roots in fine dining and giving back to the community, we are committed to high quality food served with a high level of hospitality. Our team members enjoy a positive work environment that is deeply committed to the philosophy they we ""Stand for Something Good.""Shake Shack is an Equal Opportunity EmployerAll qualified applicants will receive consideration for employment without regard to race, color, ancestry, national origin, religion, creed, age (over 40), disability (mental and physical), sex, gender identity, sexual orientation, gender expression, medical condition, genetic information, marital, military and veteran status.Our company will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of applicable laws."
Data Engineer,"Join SADA as a Sr. Data Engineer!

Your Mission

As a Sr. Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring batch or streaming data pipelines, data lakes and data warehouses.

You will be expected to run point on whole projects, end-to-end, and to mentor less experienced Data Engineers. You will be recognized as an expert within the team and will build a reputation with Google and our customers. You will demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise. You will also participate in early-stage opportunity qualification calls, as well as lead client-facing technical discussions for established projects.
Pathway to Success

#BeOneStepAhead: At SADA we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our engineers know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.

Your success starts by positively impacting the direction of a fast-growing practice with vision and passion. You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions.

As you continue to execute successfully, we will build a customized development plan together that leads you through the engineering or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:
Google Professional Data Engineer Certified or able to complete within the first 45 days of employment

Required Qualifications:
Mastery in at least one of the following domain areas:
Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or customer-facing role

Useful Qualifications:
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Experience operationalizing machine learning models on large datasets
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem

About SADA

Values: We built our core values on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing

Work with the best: SADA has been the largest partner in North America for GCP since 2016 and has been named the 2019 and 2018 Google Cloud Global Partner of the Year. SADA has also been awarded Best Place to Work by Inc. as well as LA Business Journal!

Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match, professional development reimbursement program as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."
Data Engineer,"Our client is looking Data Engineer for Long term project in NJ,NY,PA below is the detailed requirements. Job Title Data Engineer Location NJ,NY,PA Duration Long term Job description Bachelor's degree in Computer science or equivalent, with minimum 8+ Years of relevant experience. You should have 5+ years of strong experience with Data Engineering in Data Science. Must have strong experience in Python Should have experience in Write advance code in SQL for analysis. Must have experience in Apache Air flow or Google Composer. Strong Experience in Version Control, Git and Bitbucket Demonstrate excellent communication skills including the ability to effectively communicate with internal and external customers. Ability to use strong industry knowledge to relate to customer needs and dissolve customer concerns and high level of focus and attention to detail. Strong work ethic with good time management with ability to work with diverse teams and lead meeting"
Data Engineer,"Job Description
Phyton Talent Advisors is partnering with a hedge fund here in NYC in their search for a Data Engineer. They are looking for talented individuals who have experience developing and maintaining the data ingestion and data quality processes and platform that feed systems to support risk management and trading in Equities, Fixed Income, Commodities, Credit, and FX business. This person would be working closely with quants, risk managers, and other technologies in global offices to develop automated rules and validation frameworks to ensure quality of underling inputs and outputs to and from the risk platform.

Requirements:
Strong analytical skills, and experience using Python to build statistical/analytical libraries/systems
Three to Five years of Experience with larger scale python in a data intensive environment
Experience working with Machine Learning/Data related libraries; Scikit-learn, Tensorflow, Keras and/or PyTorch as well as Pandas/Numpy.
Professional experience with a SQL-based database, such as MS SQL Server
Experience with big data tools Spark, Hadoop, and especially streaming platforms like Kafka
Able to work independently in a fast-paced environment.
BA or Master in computer science or any other scientific/data fields"
Data Engineer,"Who we are
Albert is a new type of financial service that uses powerful technology to automate your finances, with a team of human experts to guide you. Albert saves and invests automatically for you, helps you avoid overdrafts, finds savings you’re missing, identifies bills you’re overpaying, and much more. Text Albert a financial question, and we’ll not only offer guidance; we’ll help you make it happen.
We’re an LA-based startup with a proven business model, backed by top-tier institutional investors, with over 2 million users who have trusted Albert to help them achieve their financial goals. We’re on a mission to improve the financial lives of millions of people with a beautifully-designed, simple product, and we’re looking for thoughtful, talented people to join us on our journey.

About the role
Managing, transforming, and accessing data efficiently is critical to every business process at Albert, from backend and mobile development to growth and business analytics. We are looking for a talented engineer to own our data analytics pipelines and systems as well as help us evolve our data architecture to support our growth as we scale.
Things you're good at
Shipping: Delivering great products that you're proud of on a regular basis.
Architecture: Getting it done is important. Getting it done in way that will scale is equally important.
Diving in: Taking ownership of the data stack.
Collaboration: We bring the best out of each other. We're looking for people who will bring the best out of all of us.
Responsibilities
Take over existing data pipelines, ETL and task running processes, starting with our ETL processes for BI analytics
Partner closely with VP of Analytics to make data accessible to the entire company so we can make timely decisions backed by data
Monitor our analytics data pipelines to ensure data quality and timeliness
Continuously improve our BI tooling, platforms and monitoring to help the team create dynamic tools and reporting
Drive optimization, testing, and tooling to improve data quality
Write clean, maintainable and well-documented code to support our data processes. Help improve and evolve out data architecture over time by planning, developing, and deploying infrastructure using state of the art tools and practices appropriate for our needs
Concisely and effectively communicate the benefits and implications of adding new data technologies and techniques to our infrastructure
Requirements
4+ years of experience in a Data Engineering role, with a focus on building data pipelines
BI tooling and/or data app development
4 year bachelor degree in Computer Science or other technical or science degree
Proficiency in Python
Experience with some or all of the following: Postgres, Redshift, Celery, Elasticsearch, Kafka, and Airflow.
Benefits
Competitive salary and meaningful equity
401k Match
Health, vision and dental insurance
Free lunch"
Data Engineer,"Click ""Apply for this Job"" below to submit your application. After doing so, please take our 4-minute web analytics assessment followed by a 15-minute aptitude test by clicking the following link:

Start Assessment

Who We Are:

2-time winner of Crain's 100 Best Places to Work, Direct Agents is a data-driven digital marketing agency with offices in NY and LA.

Established in 2003, we are an independent company, and our business has been built on adapting and driving change since the very beginning. Fueled by the grit and hustle of our diverse team, we’re inspired by experimentation and seek to push the boundaries on innovative approaches to advertising. Our teams are committed to growing and innovating alongside clients like Colgate, Walker & Co., Belkin, The CW, Wacoal, and more.

Together, we think differently and we continue to shape the digital space into the future…

We are the Shapers of the Digital World. Join us and become a shaper of change!

As a Data Engineer, you will create tangible value by helping to develop innovative data and tech systems for current and future clients. The work you do directly contributes to our ability to improve holistic business and marketing strategies aimed to meet and exceed client expectations.

You will have the opportunity to work on leading-edge AI/ML, data automation, and predictive analytics projects including: bidding algorithms, image recognition, attribution modeling, performance forecasting, propensity scoring, and much more. The position requires fostering close working relationships with internal marketing teams as well as clients.

What You’ll Do:
Create automated data systems using APIs, Selenium, web scrapers, etc.
Set up and maintain database ecosystems
Build advanced data models to provide granular and predictive insights
Develop AI systems to perform advanced operations based upon data inputs
Visualize data in an easily digestible format
Supplying internal teams with datasets to be used for strategic optimizations
Maintain data integrity and work with teams to troubleshoot discrepancies
Stay abreast of latest industry topics, trends, news, methodologies, and tools
What You’ll Need:
Bachelor's or Master’s Degree in Mathematics/Computer Science/Engineering/Finance or related field
Advanced Python skillset
Strong Statistical and/or Machine Learning background
Strong knowledge of database structures and SQL
Excellent quantitative skills and attention to detail
Experience using Power BI data visualization software is a plus
Highly motivated to identify and develop solutions to complex problems
Strong time management skills – ability to prioritize and meet deadlines
Diligent work ethic. Must be self-motivated and able to take the initiative to get the job done
Digital Marketing experience is preferred but definitely not required
What You’ll Get:
Competitive pay & health benefits.
Weekly social, wellness, and team building events
Access to health benefits and many other perks like One Medical
An amazing values based company culture ripe with collaboration, encouragement and camaraderie
Monthly education and training in digital advertising
Access to the best in class marketing technology and methodology
Ability to innovate and make an impact with your ideas in real-time
A fast tracked path to growth based on what you put into the role and your passion for learning in the company
Check out our Culture Videos:

Welcome to Direct Agents: https://www.youtube.com/watch?v=EC-LBxTw0To

DA 2019 Culture in Review: https://www.youtube.com/watch?v=rP12YTMF4Dg"
Data Engineer,"Join SADA as a Data Engineer!

Your Mission

As a Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data issues facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring a combination of batch or streaming data pipelines, data lakes and data warehouses.

You will be recognized as an established contributor by your team. You will contribute design and implementation components for multiple projects. You will work mostly independently with limited oversight. You will also participate in client-facing discussions in areas of expertise.
Pathway to Success

#BeOneStepAhead: At SADA we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage adaptability. This means that not only do our engineers understand that change is inevitable, but they embrace this change to continuously broaden their skills, preparing for future customer needs.

Your success comes from your enthusiasm, insight, and positive impact. You will be given direct feedback quarterly with respect to the scope and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, your collaboration with your peers, and the consultative skill you demonstrate in customer interactions.

As you continue to execute successfully, we will build a personalized development plan together that leads you through the engineering or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with a first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:
Google Professional Data Engineer Certified or able to complete within the first 45 days of employment

Required Qualifications:
Expertise in at least one of the following domain areas:
Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or other customer-facing role

Useful Qualifications:
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Applied experience operationalizing machine learning models on large datasets
Knowledge and understanding of industry trends and new technologies and ability to apply trends to architectural needs
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem

About SADA

Values: We built our core values on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing

Work with the best: SADA has been the largest partner in North America for GCP since 2016 and has been named the 2019 and 2018 Google Cloud Global Partner of the Year. SADA has also been awarded Best Place to Work by Inc. as well as LA Business Journal!

Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, RRSP, professional development reimbursement program as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."
Data Engineer,"Do you believe that people with compassion will support one another to create a better world? Well, we do! GoFundMe is the largest social fundraising community in the world and is just getting started. With over $9 billion raised from more than 120 million donations, GoFundMe is the largest social fundraising community in the world and is just getting started.Data is at the center of all decisions and strategy at GoFundMe. The Data Engineer will be a key part of our growing platform engineering team to help build scalable data platforms that enable business analytics, data science and data products, resulting in driving business growth toward a global culture of peer-to-peer giving. This role requires technical expertise in a wide variety of technologies to develop and own our enterprise data warehouse, sourcing data from various databases/web APIs and integrate data with external systems. If you are interested in working in a fast paced environment and like being challenged with fun data problems to solve, come join us in our Los Angeles or San Diego offices.What you'll be doing day to day...* Develop and maintain enterprise data warehouse (in Amazon Redshift)* Create and manage ETL data pipelines (sourcing data from databases, streaming data, various web APIs, etc.)* Integrate data from data warehouse into 3rd party tools to make data actionable* Develop and maintain REST API endpoints for data science products* Provide ongoing maintenance and enhancements to existing data warehouse solutions* Ensure data quality through automated testing* Collaborate with analysts, engineers and business users to design solutions* Research innovative technologies and make continuous improvementsWhat you bring to the role...* 3+ years as a data engineer designing, developing and maintaining enterprise data warehouse solutions consisting of structured and unstructured data* Proficiency with building data pipelines using ETL/data preparation tools* Experience with web APIs and data integrations across internal and external systems* Expertise in writing and optimizing SQL queries* Knowledge of Python, Java, C++ or other scripting languages* Experience with Spark and Scala* Good understanding of database architecture and best practices* Understanding of data science and machine learning technologies a plus* Experience with event tracking is a plus* Bachelor's degree in Engineering* Ping pong skills, a love for boba tea, and a sense of humorWhy you'll love it here...* Your work has real purpose and will be helping to change lives at a global scale.* Our people consistently vote GoFundMe a Great Place to Work®.* You can nominate your favorite GoFundMes to receive a donation from the company.* Great perks like lunch, snacks, wellness, company/team activities, and full benefits.* The company is strong and growing with incredible opportunities ahead.* We're a fun, close team of people who care about their work and impact.More about GoFundMe...https://www.gofundme.com/2019https://www.gofundme.com/c/heroeshttps://medium.com/gofundme-storieshttps://www.gofundme.com/why-gofundmeGoFundMe is changing the way the world gives. Every day friends, family, and members of the community come together to support one another and the causes they care about most. Our campaigners have raised over $9 billion for medical expenses, education, community projects, sports, emergencies, pets and other personal causes and life events--making us the world's largest crowdfunding platform.GoFundMe has assembled one of the best teams to go build the next leading consumer Internet company - including leaders from LinkedIn, Intuit, Groupon, YouTube, Facebook, Twitter, GoPro, Uber and several others. We are also funded by some of Silicon Valley's best venture capital firms, including Accel, Greylock, TCV, and others.GoFundMe is proud to be an equal opportunity employer that actively pursues candidates of diverse backgrounds and experiences. We are committed to providing diversity, equity, and inclusion training to all employees, and we do not discriminate on the basis of race, color, religion, ethnicity, nationality or national origin, sex, sexual orientation, gender, gender identity or expression, pregnancy status, marital status, age, medical condition, mental or physical disability, or military or veteran status."
Data Engineer,"Company Overview:
Age of Learning is a leading education technology innovator based in Glendale, California, with a talented team of 500+ individuals comprised of nationally-renowned educators, curriculum experts, designers, animators, engineers, and more. We develop engaging, effective digital learning technology and content to help children build a strong academic foundation for lifelong success.
Our flagship product ABCmouse.com Early Learning Academy® is a comprehensive online curriculum and the #1 digital learning product for young children. To-date, more than 30 million children worldwide have completed over 6 billion Learning Activities on ABCmouse. We recently launched Adventure Academy, the first massively multiplayer online (MMO) game designed specifically to help elementary- and middle-school-aged children learn. It features thousands of engaging Learning Activities—including minigames, books, original animated and live action series, and more—in a fun and safe virtual world. Other Age of Learning programs include immersive English language learning products for children in China and Japan; ReadingIQ, a digital library and literacy platform; and a groundbreaking personalized, adaptive digital learning system that individualizes math instruction for every child through AI-driven technology.

We are committed to helping all children succeed. We provide our educational programs at no cost to teachers, Head Start programs, public libraries, and other community organizations, and have served millions of children through these initiatives. We recently established the Age of Learning Foundation to expand this work globally.
As we expand our global reach and increase the educational impact of our programs, we’re looking for passionate, ambitious, and collaborative leaders to become a part of our growing team.


Summary:
We are seeking a full-time, in-house Senior Data Engineer to join our development team. This person will be helping us develop high performance, high-throughput services using modern technologies and techniques.
Responsibilities:
Design, develop, test, implement and support applications using custom ETL (Extract Transform Load) or open source tools such as Talend
Prepare high-level component architecture; design documents, data flow diagrams, detail design documents, data schema and modeling combined with test plan documents
Design, develop and test highly available and scalable data pipelines and relevant data storage systems to enable business success across a multi-product functionality
Proactively identify operational and systemic issues within the data supply value chain (from collection to processing to reporting) and work with production operations (DevOps) team to implement monitoring solutions
Ensure testing and validation best practices are followed across the team so that accuracy of data transformations and data verification are complete and documented
Execute in a fast-paced matrix organization across product and engineering teams to identify best data-driven solutions for the underlying data infrastructure and platform
Ensure high operational efficiency and quality of your solutions to meet SLA (Service Level Agreement) and support commitment to stakeholders (both internal and external)
Be an active participant and advocate of agile/scrum practice to ensure health and process improvements for your team
Required Qualifications:
Experience designing and building large, scalable data systems, preferably across a multi-product portfolio
Strong SQL skills with proven ability to write complex data queries across large data sets.
Exposure to software development, preferably in an Agile/Scrum/Kanban environment across multiple products
Experience analyzing and manipulating data across diverse data sources (Python, Scala)
Experience working across AWS (Amazon Web Services) cloud environment (EC2, S3, RDS, Sagemaker)
Strong exposure to Big Data technology, preferably across a containerized environment (Hadoop, Spark, Hive, Presto)
Experience with sourcing and modeling data from Restful API (Application Programming Interface)
Strong attention to detail with excellent analytical, problem-solving, and communication skills
Bachelor’s degree in Computer Science, Computer Engineering, or Information Technology or a related field, or an equivalent combination of education and experience
Exemplary communication skills (both written and verbal), with experience producing technical and design documentation of complex processes
Good time management and ability to work on concurrent assignments with different priorities
Ability to work in a fast paced, iterative development environment with short turnaround times
Preferred Qualifications:
Experience with A/B Testing and related optimization across desktop and mobile in a digital environment a plus (examples include: Optimizely, Leanplum, deltaDNA)
Experience analyzing and manipulating data across several data formats (JSON, Avro, Parquet, ORC)
Experience building and architecting data warehouse workflows in large cloud-based production environments (Snowflake is an example)
Understanding of columnar data warehouse solutions (Redshift, Vertica)
Experience migrating on-prem data solutions to the cloud with a strong data operational hygiene
Experience developing and maintaining metadata catalogue APIs across a variety of data sources (AWS Glue, Metacat)
Prior experience working in educational technology companies or a related competitive landscape is a plus
We Provide:
Medical, Dental, Vision + 401k
Highly competitive PTO policy
Casual Dress Code
Snacks + Drinks (Coca Cola Freestyle Machine)
Gaming room including an Arcade (2,000+ games)
Frequent team and company outings
Limitless opportunities for professional growth!"
Data Engineer,"We appreciate your interest in employment with The Honest Company! The Honest Company is committed to a policy of equal employment opportunity, and will not discriminate against an applicant or employee on the basis of race, color, religion, creed, national origin, ancestry, sex, gender, age, physical or mental disability, veteran or military status, genetic information, sexual orientation, gender identity, gender expression, marital status, or any other legally-recognized protected basis under federal, state, or local law. Applicants with disabilities who need assistance with the application process may be entitled to a reasonable accommodation in accordance with applicable law. If you need assistance in completing this application or with the application process because of a disability, please contact the Human Resources Department at 1.888.862.8818 or 1.310.857.3020.

About Us

Founded in 2012, The Honest Company® is a mission-driven consumer products company dedicated to empowering people to live happy, healthy lives. Consumers seeking thoughtfully formulated, safe and effective baby, personal care and beauty products, along with education and support can find The Honest Company products across North America via honest.com and honestbeauty.com and at more than 17,000 retail locations. Beginning in spring 2019, consumers in Europe can find Honest Beauty at select Douglas retail locations. A leader in the natural baby category and a trailblazer in clean beauty, The Honest Company is committed to ensuring all families have access to basic necessities and the latest health information for safe growth and development - a commitment reflected in its ongoing partnerships with organizations such as Baby2Baby and Mount Sinai. The Honest Company is privately held and headquartered in Los Angeles, California.

Our Mission

We're on a mission to empower people to live happy, healthy lives. We're a wellness brand with values rooted in consciousness, community, transparency, and design. Every day and in every way, we hold ourselves to an Honest standard. We believe that it is our responsibility to do our part to help create a healthy and sustainable future for all.

The Role

The Honest Company is looking for a Data Engineer to help us in our mission of delivering safe affordable products to current and future customers. We sit at the center of technology and the business, supplying the company with the tools and training to make better use of our vast datasets. You will serve a key function, integrating critical systems, and translating raw data into information that drives the business forward.

What you'll do:
Build data pipelines to integrate systems and move data into our data warehouse. We use Amazon Redshift, Apache Airflow, Fivetran, Mule ESB, Python and Bash, among other tools.
Collaborate with data analysts across the business to build the tables and tools they need to support better decisions.
Integrate data flows between operational systems, build tools for data reconciliation
Intimately understand the company's operational data models, troubleshoot data flow issues, and improve system reliability and fault-tolerance
You'll love this job if you:
Love data and have a passion for turning it into insights
Are a dynamite problem solver. You're motivated to solve customer issues and are always thinking one step ahead.
Create strong partnerships with stakeholders
Love iterating quickly on software
Are a strong advocate of high-quality code
What you'll need:
2+ years software or data engineering experience
Strong SQL skills
Strong analytical and problem-solving skills
The ability to take business requirements and translate them into technical requirements
Effective communication and interpersonal skills
Bonus Points for:
Knowledge of the AWS ecosystem, especially as it relates to data
Experience with Mule ESB, Java, Python, Ruby and/or Apache Airflow
Experience in a retail or ecommerce environment
Benefits & Perks

We offer a competitive benefits package including comprehensive health and wellness coverage, 401k with company match, wellness incentives including a monthly fitness reimbursement and onsite fitness classes, options for education reimbursement, and a discount on all products. We value work-life balance and offer a generous and flexible vacation policy. Thinking about adding little ones to your family? Honest offers generous maternity and paternity leave. We love the furry kids too and offer pet insurance so your companions are well taken care of.

California Privacy Rights Notice for Californian Job Applicants and Prospective Talent

Effective Date: January 1, 2020

Under the California Consumer Privacy Act of 2018 (""CCPA""), The Honest Company, Inc. (""Honest"" or ""us"" or ""we"") is required to inform California residents who are our job applicants or prospective talent (together ""job applicants"" or ""you"") about the categories of personal information we may collect about you and the purposes for which we use this information. Click here if you are a California resident to read disclosures required by the CCPA. Note this notice applies only to personal information that is subject to the CCPA.

Categories of Personal Information We Collect. We may collect the following categories of personal information about our job applicants, who are California residents:
Name
Signature
Social Security Number
Email and mailing address
Telephone number
Education
Employment history
How We Use Job Applicants' Personal Data. We use and disclose the personal information we collect for our business purposes. These business purposes include, without limitation:
Processing evaluating your application to determine your qualifications for the role to which you've applied, and communicating with you about your application, including to check references or your background, and communicate with you about other jobs that may interest you.
Other business purposes as identified in the CCPA, which include:
Auditing related to our interactions with you;
Legal compliance
Detecting and protecting against security incidents, fraud, and illegal activity;
Debugging;
Performing services for us, such as analytics;
Internal research for technological improvement; and
Internal operations.
Other Interactions with The Honest Company. More information about our privacy practices can be found in our Privacy Policy, which is incorporated herein by reference.

Contact Us. For questions or concerns about our Privacy Policy, please contact us at privacy@honest.com."
Data Engineer,"About codeSparkcodeSpark is turning programming into play for kids everywhere. We're building the largest community of young kid coders in the world. Our award-winning app, codeSpark Academy, has a unique word-free interface that allows kids as young as four to become makers. We have a strong focus on making codeSpark Academy exciting for both girls and boys.We believe all kids should have the opportunity to master this new form of literacy and creative expression. Our self-directed subscription service thoughtfully combines structured challenges and open-ended creative play.Our new Data Engineer will have the opportunity to explore all sorts of interesting data about kids' learning through play. You'll also help us improve our marketing efforts through development and reporting on success metrics. The right person joining us will be interested in our mission and motivated to find all sorts of other cool data to analyze to help us grow, too!Data Engineer Job OverviewWe are looking for a Data Engineer who will support our product, marketing, and leadership with insights gained from analyzing company data. The ideal candidate is self-sufficient and adept at using large data sets to find opportunities for product and process optimization, and in building and using models to test the effectiveness of different courses of action. They are experienced in using a variety of data mining/data analysis methods, using a variety of data tools, building and implementing models, using/creating algorithms and creating/running simulations. They must demonstrate a proven ability to drive business results with their data-based insights. Finally, they must be comfortable working with a wide range of stakeholders and functional teams. The right candidate will have a passion for discovering solutions hidden in large data sets and working with stakeholders to improve business outcomes.Responsibilities:? Collaborate with stakeholders throughout the organization to identify opportunities to leverage company data in driving business solutions.? Mine and analyze data from company databases to drive optimization and improvement of product development, marketing techniques and business strategies.? Assess the effectiveness and accuracy of new data sources and data gathering techniques.? Develop custom data models and algorithms to apply to data sets.? Use predictive modeling to increase and optimize customer experiences, onboarding, revenue generation, ad targeting and other business outcomes.? Leverage A/B testing framework and test model quality.? Develop processes and tools to monitor and analyze model performance and data accuracy.? Maintain company KPI dashboard.Essential Qualifications:? 5-7 years of experience manipulating data sets and building statistical models.? Bachelor's or Master's in Statistics, Mathematics, Computer Science or another quantitative field.? Coding knowledge of several languages: C#, Python, JavaScript, Go, or similar.? Querying databases and using statistical computer languages: R, Python, Mongo, JQL, etc.? Using AWS services.? Analyzing data from 3rd party providers: Google Analytics, Mixpanel, Facebook, Appsflyer, Email service providers, etc.? Experience visualizing/presenting data for stakeholders.Highly desired:? Experience working with product development and marketing teams, especially subscription products and kids games or apps.? Experience using statistical computer languages to manipulate data and draw insights from large data sets.? Experience working with and creating data architectures.? Knowledge of advanced statistical techniques and concepts (regression, properties of distributions, statistical tests and proper usage, etc.) and experience with applications.? Excellent written and verbal communication skills for coordinating across teams.? A proactive approach to learning and mastering new technologies and techniques.Perks? Opportunity to shape strategy for a company having a global impact on K-5 education.? The rare opportunity to change the world by helping create the first generation in history that is truly connected and collaborative.? The fun of working with an effective and multi-award winning product that is beloved by customers.? Sharp, motivated co-workers in a creative and supportive office environment.Commitment to Diversity and InclusioncodeSpark believes in diversity and inclusion of all people, of all genders, races, ethnicities, sexual orientations, educational backgrounds, religions, abilities, socioeconomic backgrounds, immigration statuses, and more. Just as we hope to close access gaps in computer science education, we aim to create a culture within codeSpark that is inclusive and accessible to all current and potential employees.codeSpark is an Equal Opportunity Employer."
Data Engineer,"Hello, Hope you are doing Well !!! My name is Ravi from Sumeru Inc., one of our clients has a current opening for a professional to add to their team. Below are some key highlights of the position, If you are available in the Job market and interested in this opportunity, Please feel free to connect with me at (408) 520-9163 or reach me out at ravi.satiwarasumerusolutions.com httpsmail.google.commailu0h7ydjsnpg5mq7?ampcswhampvbamptoravi.satiwarasumerusolutions.com . Skills Required AWS (S3, EMR, Glue), Python, SQL, Shell Scripting, JSON, Postgres."
Data Engineer,"Job Summary


The J. Paul Getty Trust is looking for an enthusiastic Data Engineer, with the experience and passion to carry out the execution of technical projects to enhance the institution's cultural heritage knowledge bases, through the application of machine learning and other data transformation techniques. Our aim is to provide a deeply connected and consistent experience for scholars, researchers, and enthusiasts as they explore the complex information held across the organization, and your participation is crucial for that to be successful.

You will report to the Enterprise Semantic Architect, and interact with software engineers, data engineers and content specialists in the cultural heritage programs. Your work will improve the quality, reliability, connectedness, and consistency of our data by engineering project-specific data pipelines to produce new knowledge from existing internal and external data sources. You will have a hands-on role with content specialists in the programs, and be responsible for working with them to understand data requirements and then implement those requirements in software.

The initial focus of this role is to assess and gather the necessary data to apply computer vision tools to a collection of more than half a million digitized photographs of paintings, drawings and prints. The aim of this work is to enhance our descriptions of these objects with high confidence metadata generated without expert curatorial intervention. The use of computer vision is a high priority in the institution's digital strategy and the success of this work will lead to further exciting projects that put the developed skills, transformation workflows and systems to good use.

The Getty is among the most prestigious cultural heritage organizations in the world, dedicated to furthering the study of the history of art. You will work on an amazing campus amongst fabulous art, architecture, and information systems, collaborating with world-class scientists, curators, librarians, archivists, and academics. We offer paid vacation, personal and sick leave plus every other Friday off, excellent benefits, and a very strong commitment to balancing work and personal life.

Major Job Responsibilities
With the Semantic Architect, work with technical and content stakeholders to understand data-oriented project requirements
With other Data Engineers, ensure high quality data transformation pipelines can migrate and enhance institutional managed collections
Integrate external content services to enrich and reconcile our datasets
Assess the feasibility of applying data enhancement tools and the quality of their results to determine their suitability for project requirements
Work in an agile way, including supporting testing, continuous integration and deployment
Assist software engineering teams by translating stakeholder requirements into feature requests
Qualifications
Bachelor's degree in a related field or a combination of education and relevant experience
2-5 years software development experience
Knowledge, Skills and Abilities
Experience of data-oriented work within cultural heritage organizations
Attention to detail combined with a focus on data usability
Excellent verbal and written communication skills, especially when interacting with non-technical stakeholders
Proficiency in Python, or willingness to translate experience in equivalent language
Familiarity with machine learning techniques and/or tools
Familiarity with cultural heritage data standards, such as Linked Open Data
Familiarity with engineering tools such as git
Familiarity with test driven and agile software development methodologies"
Data Engineer,"Position: Data Engineer

Reason Open: Has a project coming up-very similar to current project and set to launch in September. This person will lead the Data Side. Must be a senior person- very innovative product. Need someone who can think out of the box!

Start: ASAP!

Budget: DOE

Duration: 1+ year contract- ongoing

Work hours: 40 hours per week and must attend 9:30 am PST scrum meeting but can be flexible with hours!

Location: 100% REMOTE

Day to Day:
Lead the data side of a brand new, cutting edge product from the ground up
Product will find new sports insights anonymously (deep insights across multiple dimensions) - will span across multiple data sets (i.e. NFL, MBL, eventually the news)
Data heavy / data driven project with some AI involved
Work closely with Data Scientist
Data engineer will set the foundation for data scientist to build upon
Startup mentality- must want to wear multiple hats and be comfortable in a fast paced environment with high level direction
Must Haves:
Senior level Data Engineer (5+ years ideally)
Experience with Databases (SQL, redshift, etc- client open to any)
Must have experiencing setting up architecture from scratch
Docker
Will be building on Postgres databases into docker
Must have experience with POSTGRES (AKA PostgreSQL)
Need to have worked in a distributed environment
Sports acumen / go getter / out of the box thinker
Python coding is a HUGE nice to have!
Nice to haves:
Python coding
Redshift"
Data Engineer,"Responsibilities:
Responsible for building and maintaining the machine learning data and development platform.
Build, integrate and deploy machine learning solutions into the BlackLine application in collaboration with product management, cloud, engineering and data science teams.
Create and maintain scalable data pipeline in the cloud (AWS and GCP).
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement processes automation and data delivery.
Build infrastructure for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Execute extract, transform and load (ETL) operations on large datasets including data identification, mapping, aggregation, conditioning, cleansing, and analyzing.
Build analytics tools to provide actionable insights into business and product performance.
Keep data separated, isolated and secured.
Assist data scientists in implementing achine learning algorithms and contribute to building and optimizing our product into an innovative industry leader.
Participate in establishing best practices while team is transitioning to new technologies, tools and infrastructure. Maintain specifications and metadata; follow the best practices.
Recommend and implement process improvements.
Maintain specifications and metadata; follow and develop best practices.
Coach and technically train data analysts, if needed.
Qualifications:
5+ years as a data engineer.
Experience with SQL, Python, R languages.
ETL experience using Python.
Experience with Hadoop, Spark, Hive. Presto is a plus.
Practical experience with GIT version control.
Strong familiarity with GCP, AWS, SQL Server.
Comfortable working with open source tools in Unix/Linux environments.
Data warehousing experience, data modeling and database design.
Experience with machine learning packages and various ML algorithms.
Experience with predictive and prescriptive analytics, modeling, and segmentation.
Experience with data analytics, big data, and analytics architectures.
Comfortable handling large amounts of data.
Experience ensuring data and modeling accuracy, cleanliness, reliability.
Works independently without the need for supervision.
Experience translating business requirements into functional, and non-functional requirements.
Strong sense systems and data ownership."
Data Engineer,"ZEFR is hiring! Our Software Engineering team is hiring a Data Engineer to be involved in designing and building large-scale applications and systems to acquire, process, store, and illuminate multi-terabytes of YouTube, Facebook and other social media data. This role is an important part of the rapidly scaling infrastructure and data management demands of being the leader in VideoID technology for enterprise media companies and content owners.

Technology @ ZEFR:
Languages: Python, Scala, Java, and Kotlin
Data stores: PostreSQL, Elasticsearch, Redshift
Data processing: Apache Kafka, Apache Spark
DevOps: Jenkins, Docker, Terraform, Ansible, AWS ECS, AWS EMR

Here's what you'll get to do:
Provide seamless and timely data access for your users
Build reliable and dependable ETL
Build and maintain production machine learning infrastructure
Troubleshoot complex issues in distributed systems
Debate data processing philosophies and methodologies with your team
Here's what we're looking for:
Bachelor's or Master's degree in Computer Science or related field
Fluency with Python, Java, Kotlin, or Scala
Experience with distributed systems
Strong foundation in data structures, algorithms and software design
Experience with digital media, social media, and video APIs such as YouTube's Data API is a big plus
Thorough testing and code review standards/practices
Strong verbal and written communication skills
Openness to new technologies and creative solutions"
Data Engineer,"The Data Engineer will be a member of the Media Supply Chain (MSC) team within
the WarnerMedia Technology (WMTO) organization, the position will be
responsible for the design and implementation of search and data relationship
functionality for the WB content platform. The Data Engineer must have
experience with data modeling, modern database technologies, and API driven
search design and development. The Data Engineer will make technology and
design decisions, and partner across the organization in a collaborative manner
to achieve results for our mission critical content and data management and
distribution applications.

The Media Supply Chain Who We Are
Media Supply Chain is tasked to architect, engineer, and program manage a wide
range of applications, workflows and services for our internal partners who
operationally manage and distribute WarnerMedia content globally. Our
applications and technology solutions are responsible for scheduling, image &
asset metadata management, as well as, content processing and delivery as well
as content mastering, localization and preservation. We are the team pioneering
new ways to process and deliver WarnerMedia content to its global customers

The Media Supply Chain Mission
Our mission is to provide and sustain practical, innovative, agile software and
technology solutions which are highly reliable, automated, measurable,
optimized, modern systems capable of distributing high-quality content and
metadata to our domestic and global platforms & partners.

The Daily
Develop and provide support for core data relationship, data ingest, data
transformation services and search capabilities. Creates functional and
technical specifications. Creates and executes against a plan to launch
and maintain applications.
Review project objectives and determine best technology for
implementation. Implement best practice standards for development, build
and deployment automation.
Evaluate software products and vendors for WarnerMedia (WM) Technology
and other divisions. Recommend action, develop and lead implementation of
selected products/services.
Work with internal and external developers to ensure (WM) Technology code
standards and best practices are performed for development of
applications.

The Essentials
B.S. in Computer Science or equivalent experience.
AWS Developer Certification preferred.
AWS Database or Data Analytics Certification preferred.
3+ years data engineering experience.
Demonstrated proficiency in data modeling and data structures.
Demonstrated experience implementing database technologies such as NoSQL,
and Relational. Experience in graph databases a plus.
Demonstrated expertise and experience in ELK stack (elasticsearch,
logstash, kibana).
Demonstrated expertise and experience in modern databases such as Mongo,
Couchbase, Neptune, Neo4j, or equivalent. Experience in MarkLogic a plus.
Proficient in one or more modern query languages such as elasticsearch
query DSL, cypher, gremlin, or graphql. xQuery preferred but not
required.
Highly proficient in XML, JSON and YAML data exchange formats. Experience
in XSDs and triple stores.
Proficient in API design and development, specifically REST APIs.
Experience with Swagger 2.0 and AWS API gateway is highly preferred.
Demonstrated experience in data analytics tools such as Tableau, Kibana
etc.
Experience in working with data streaming technologies such as Amazon
Kinesis, Apache Kafka etc.
Experience in AWS at scale leveraging services such as elasticsearch,
RDS, Redshift, Neptune and ec2.
Highly proficient in at least one modern programming language such
python, java, or node.js. Bash experience preferred.
Demonstrated expertise and experience in deploying containerized
application using Docker, Kubernetes or equivalent.
Experience with source code and knowledge repositories such as git, jira,
or equivalent systems.
Proficient in a Linux environment.
Proficient in core DevOps principles.
Proficient in the SDLC in an agile environment.
Systems design and architecture.
Ability to work with outside vendors and clients under sometimes adverse
circumstances and under time critical constraints.
Must be able communicate effectively and tactfully with all levels of
personnel (in person, written, telephone).
Must be able to pay close attention to detail.
Must be able to handle multiple tasks in a fast-paced environment.
Must be able to organize and schedule work effectively.
Must be able to work flexible hours, including overtime, if and when
necessary.
Must be able to respond to after-hours pager notifications to provide
support for applications as necessary."
Data Engineer,"Req ID: 95763

NTT DATA Services strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.

We are currently seeking a Data Engineer to join our team in Anaheim, California (US-CA), United States (US).

The individual will be responsible for the following:
Design, develop, and implement total systems solutions to the enterprise-wide technical requirements of customers.
Define new interface layouts for integrating into data warehouse.
Create source to target mappings and ETL design for integration of new or modified data streams into the data warehouse and/or data marts.
Validate the ETL design and ensure that technical specifications are complete, consistent, concise, and achievable.
Create ETL processes and reports using Informatica/MSBI/Stored Procedures
Develop ETL code as per the technical specifications and business requirements according to the established designs.
Create process flows and dimensional models using data modeling tools.
Create and maintain design related artifacts (data models, mapping documents, architecture diagrams, and data flow diagrams).
Provide performance tuning of ETL processes, participate in ETL architecture design reviews, and conduct ETL unit testing and code reviews.
Participate in system and integration testing and identifying and remedying solution defects.
Basic Qualifications:
Minimum of 5 years experience with Informatica Powercenter
Minimum of 3 years experience with SQL Server/Oracle
The position requires a Bachelor’s degree in Computer Engineering, Computer Science, Computer Information Systems, Information Technology or related field of study plus 5 years of experience.
Preferred Skills:
Healthcare domain knowledge- 3+ years( Good to have experience on Meditech or Epic EMR systems)
About NTT DATA Services

NTT DATA Services is a global business and IT services provider specializing in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services. We are part of the NTT family of companies, a partner to 85 % of the Fortune 100.

NTT DATA Services is an equal opportunity employer and will consider all qualified applicants for employment without regard to race, gender, disability, age, veteran-status, sexual orientation, gender identity, or any other class protected by law.

INDHCLSMC"
Data Engineer,"Tapcart makes launching a mobile shopping app easy, fun and attainable for every brand.
The world of shopping will one day be powered entirely by mobile, made possible by our products, team and vision.
We are inspired by the future of mobile. We work to inspire that future.

As the market leader in mobile Ecommerce, we focus on providing great experiences for our customers and for mobile shoppers worldwide. Our platform powers the mobile apps of some of the largest shopping brands, including Fashion Nova, Chubbies, The Hundreds and many more.

Tapcart is trusted by over 10,000 brands to launch and manage their mobile apps. We were recently featured at Google I/O, Shopify UNITE and in The Verge.

This role is perfect for an ambitious Data Engineer looking to grow and work with real-time data. Someone who is engineering-driven, can translate business and data requirements into production-ready stacks, and provide innovative systems with a focus on data resilience and accuracy for the e-commerce space.

We are looking for someone who shows passion and can champion data not only internally but to our customers. This person would help shape data collection, compliance, processes, and work cross-functionally with all Tapcart teams.
What you need to know
Be confident working within a real-time data collection system
Provide data-focused solutions and obtain buy-in from stakeholders
Experience working with stacks such as BigQuery, DataFlow, DataPrep, Data Studio, Pub/Sub, Kafka, Apache Airflow and the like
Experience defining schemas and growing a Data Warehouse
Experience with HBASE derived databases, especially BigTable
What you'll be doing
Creating data pipelines to handle batch and steaming ETLs
Create and implement efficient solutions to enrich existing data workflows
Create software/technology that will be the foundations for future machine learning engines such as personalization and recommendations
Identifying and report data resilience issues to key stakeholders
Bonus if you have
Experience at a B2B and/or SaaS startup
Experience creating data pipelines for SaaS dashboard products like Intercom, Canva, FreshBooks
Worked within highly functional engineering teams


Who we are.
We are a well funded, young and growing startup located in sunny Santa Monica, CA . Our employees and culture are very important to us and as such, we aim to make coming to work fun, challenging and rewarding for our team. We know that doing great work depends on showing up with creative solutions to face our many business challenges. It all starts with having good people, and helping them grow both personally and professionally. We can't wait to hear how your unique skills and personality will add to our company and culture.

Learn more about who we are and what we offer on our careers page, and check out some of our recent features on Google I/O, Shopify UNITE and in The Verge."
Data Engineer,"Search Jobs


Job Description

Are you looking for a high energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform?

Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley?

Your next adventure at VMware is only a click away!

VMware's Data Engineering Team is looking for a Data Engineer to help build on Next generation Near Realtime Data Platform based on Hadoop, Spark & SAP HANA. You will be responsible for building and enhancing the solutions on the existing platform based on the business needs in partnering with fellow Developers and Business groups.

Responsibilities:

· Understand the business capability/requirements and transform them into robust design solutions

· Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed

· Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms.

· Perform hands on work using SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform.

· Integrate data sets from difference sources using Informatica, Python, SAP SDI/SLT

· Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data.

· Help data consumers to correctly understand and use the data.

Qualifications:

· 8+ years of experience in as a Data Engineer handling large volumes of data.

· Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools.

· Expertise in writing advanced SQL queries.

· Experience working with Informatica, SAP SDI/SLT

· Expertise in SAP HANA, Hive/Hadoop/Hawq/Spark

· Working knowledge of BI Reporting tools like BOBJ and Tableau

· Experience in Python Scripting

· Strong analytical and troubleshooting skills

· Excellent verbal and written communication skills

· Bachelor’s degree in Computer science, Statistics, Mathematics, Engineering or relevant field.

VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. VMware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.

VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. VMware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.

Search Jobs"
Data Engineer,"Job Title
Data Engineer

06-Jul-2020

Business Segment
Operations & Technology

Responsibilities
The Analytics Center of Excellence (ACOE) at NBCUniversal is looking for a passionate problem solver who’s looking to build the next generation of data pipelines and applications. Reporting to the Director supporting the research portfolio, the Data Engineer role is right for you if you’re a “hands-on” coder who can build and cleanse large datasets in order to report out actionable insights.

As part of the global Operations & Technology organization, the ACOE is focused on data and analytics strategies for the future. We support NBCU’s vast portfolio of brands - from broadcast, cable, news, and sports networks to film studios, world-renowned theme parks, and a diverse suite of digital properties. We take pride in supplying our business groups with data to advise and shape strategic business decisions related to our content.

In the Data Engineer role, you’ll be working with internal stakeholders, data engineers, visualization experts, data scientists, and other technologists across the business. If you’re someone who loves to take large, disparate data sets and build them into flexible and scalable analytics applications and databases, you’ve come to the right place. Here you can create the extraordinary. Join us.

Responsibilities:
Collaborate with business leaders, engineers, and product managers to understand data needs.
Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL and AWS big data technologies
Design, build, and scale data pipelines across a variety of source systems and streams (internal, third-party, as well as cloud-based), distributed/elastic environments, and downstream applications and/or self-service solutions
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Help continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers
Participate in development sprints, demos, and retrospectives, as well as release and deployment
Build and manage relationships with supporting IT teams in order to effectively deliver work products to production
Qualifications:
1+ years of experience in a data engineering role
Direct experience with data modeling, ETL development, and data warehousing
Knowledge of data management fundamentals and data storage principles
Experience with Python/Javascript or similar programming languages
Hands-on experience with SQL and Tableau
Bachelor's degree in Computer Science, Data Science, Statistics, Informatics, Information Systems or related field.
Desired Characteristics:
Analytical – You have experience in delivering data analytics solutions that promote data discovery
Proficient with ETL processes, application development, and data science principles
Experience with Snowflake, Amazon Web Services, or related cloud platforms a plus
Media-focused – Strong knowledge/passion for media including broadcast TV, digital, and mobile
Experience working with data sources such as Nielsen, Adobe Analytics, comScore, and other industry research sources a plus
Communicator – You have excellent verbal and written skills with the ability to communicate ideas effectively across all levels of the organization, both technical and non-technical
Action-oriented – You're constantly figuring out new problems and are regularly showing results with a positive attitude, always displaying ethical behavior, integrity, and building trust
Strong understanding of Agile principles and best practices
You’ve dealt with ambiguity and can make quality decisions in a dynamic, fast-paced environment
Job Number
56039BR

Posting Category
Technology & Engineering

Country
United States

Sub-Business
Technology

About Us
At NBCUniversal, we believe in the talent of our people. It’s our passion and commitment to excellence that drives NBCU’s vast portfolio of brands to succeed. From broadcast and cable networks, news and sports platforms, to film, world-renowned theme parks and a diverse suite of digital properties, we take pride in all that we do and all that we represent. It’s what makes us uniquely NBCU. Here you can create the extraordinary. Join us.

State/Province
California

Career Level
Experienced

City
Universal City

Notices
NBCUniversal’s policy is to provide equal employment opportunities to all applicants and employees without regard to race, color, religion, creed, gender, gender identity or expression, age, national origin or ancestry, citizenship, disability, sexual orientation, marital status, pregnancy, veteran status, membership in the uniformed services, genetic information, or any other basis protected by applicable law. NBCUniversal will consider for employment qualified applicants with criminal histories in a manner consistent with relevant legal requirements, including the City of Los Angeles Fair Chance Initiative For Hiring Ordinance, where applicable."
Data Engineer,"Who are we?

Sense360 is a Series A startup that is funded by prominent VC funds such as FirstMark Capital (Airbnb, Pinterest, Riot Games) and Upfront Ventures (Maker, TrueCar). Our data insights have been referenced by many publications including Bloomberg and CNBC. Sense360 was founded by successful repeat entrepreneurs.

We are transforming the way businesses make decisions by combining massive, disparate datasets and turning them into accessible, actionable, and accurate insights that drive the strategic decisions that brands make. See our Culture Deck here.



What will you do?

As a data engineer you will working on our data pipeline that processes 1.6+ TB sensor data/day and that is built using Spark, AWS, Python, Airflow and Rails. Below is a list of the major initiatives that you will be helping with.

Scale our data pipeline to handle 5MM+ users
Research, analyze and integrate new 3rd party datasets
Move our system from batch-based to real-time
Work closely with our data science team to build model building platforms
Build new data delivery mechanisms for our clients


What do we look for?

Engineers who thrive at Sense360 have a few key traits:

Care about their work, their team and their company
Heavy bias towards delivering value
Expect their ideas to be challenged because they believe that the best ideas can come from anywhere
View feedback as a gift that they give and receive
Fun and interesting


Qualifications

3+ years experience
experience with a distributed processing technology (e.g. Spark, Storm, Presto, Hadoop, Samza, Flink, etc)
solid CS and testing fundamentals
experience working in a startup or an extremely strong desire to do so


Willing to relocate if already within the US and sponsor visas."
Data Engineer,"Job Description:About the Team:TrueCar is seeking to add Engineers to our Data Engineering team. This team applies subject matter expertise to ingest, analyze, and validate the automotive data required from internal and 3rd party sources. Data engineers are responsible for building and maintaining highly scalable data pipelines to power the website while also providing data for our analytical engine to derive insights in a meaningful fashion.About the Job:* Design and develop efficient and scalable data processing pipelines using big data technologies ( Hadoop, Spark, HBase, Kinesis, MapReduce, etc.) on large scale structured/unstructured data sets for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL/NoSQL.* Build complex workflows and orchestrate data dependencies.* Monitor and support data pipelines to honor internal and external SLA's.* Work within standard engineering practices (i.e. SCRUM, unit/integration testing, design review, code reviews, continuous integration, etc.) to deliver product features with optimal efficiency for TrueCar customers and clients.* Closely work with product owners & analysts to understand business and functional requirements and contribute to the design and prioritization discussions.* Working with a team of engineers where mentorship is valued.* Ability to learn and adapt to continually evolving technologies in the big data ecosystem.What you need:* 3 years of experience programming in Java.* 1+ years of experience in the Big Data technologies.* Experience in any of big data technologies: MapReduce, Spark, HBase,* Proficient in SQL and experience with RDBMS/NoSQL databases.* Experience working with Cloudera/Hortonworks/EMR distribution in AWS.* Ability to self-manage tasks and be proactive in working with other teams to accomplish them while taking pride and ownership in their work.* Team-player with strong collaboration and communication skills, who is able to respond positively to feedback.* Bachelor degree (or Master) in Computer Science or related engineering fieldAbout TrueCar:TrueCar is a leading automotive digital marketplace that enables car buyers to connect to our nationwide network of Certified Dealers. We are building the industry's most personalized and efficient car buying experience as we seek to bring more of the purchasing process online. Consumers who visit our marketplace will find a suite of vehicle discovery tools, price ratings, and market context on new and used cars -- all with a clear view of what's a great deal. When they are ready, TrueCar will enable them to connect with a local Certified Dealer who shares in our belief that truth, transparency, and fairness are the foundation of a great car buying experience. As part of our marketplace, TrueCar powers car-buying programs for over 250 leading brands, including AARP, Sam's Club, and American Express. Nearly half of all new-car buyers engage with TrueCar powered sites, where they buy smarter and drive happier.TrueCar is headquartered in Santa Monica, California, with offices in Austin, Texas, and Boston, Massachusetts.Location(s):Santa Monica, CA"
Data Engineer,"THE COMPANY

Welcome to Aetion! Since our debut in 2013, we have grown into one of the country's leading science-driven technology companies using real-world evidence to provide innovative healthcare solutions.

We achieve this with our Aetion Evidence Platform, a software platform used to evaluate the safety, effectiveness and value of medications, delivering better outcomes to patients, medical professionals and clients. We've partnered with top biopharma companies and are backed by leading venture capital firms to help increase our medical research and expand our product line.

To continue our mission to transform healthcare, we're assembling a team of talented individuals who know how to work collaboratively and authentically, to innovate and think transformation, not status quo.

If that's you, we'd love to hear from you.

DESCRIPTION

Your primary responsibility will be developing transformation logic against disparate datasets in Aetion Evidence Platform. You will work closely with our Product and Science team in developing custom transformation logic for longitudinal data, which is in Python, Scala, and R UDFs and executed over a Spark cluster. In addition, you will be integral in developing and enhancing our platform and its connections to Spark and a combination of big data infrastructure.

RESPONSIBILITIES:

The following duties include, but are not limited to:
Develop transformation logic to convert disparate datasets into Aetion's proprietary format.
Work with the Science team to develop this logic in Spark UDFs executed over a Spark cluster.
Assess, develop, troubleshoot and enhance our measure system, which utilizes a combination of Java, Scala, Python, and R.
Modify JavaScript Object Notation (JSON) files to describe the schemas of the datasets, ensuring system functionality through routine maintenance and testing.
Work on a full-stack rapid-cycle analytic application.
Develop highly effective, performant, and scalable components capable of handling large amounts of data for over 10 million patients.
Work with the Science and Product teams to understand and assess client needs, and to ensure optimal system efficiency.
REQUIRED QUALIFICATIONS:
Bachelor's degree or equivalent in Computer Science, Computer Engineering, Information Systems, or a related field.
4 years of experience in the position offered or related position, including 4 years of experience with: designing, developing, maintaining large-scale data ETL pipelines using Java/Scala in AWS, Hadoop, Spark, and DataBricks to manage Apache Spark infrastructure.
Experience building backend modules, low latency REST API in distributed environment using Java, Docker, SQL, MVN, Spring, Jenkins.
Experience writing complex SQL queries, UDFs to process large amounts of data across relational, non-relational databases, JSON and Spark SQLs.
Experience translating requirements from product, DevOps teams to technology solutions using SDLC.
ABOUT AETION
Lead at all levels – Aetion is a diverse workforce of scientific thought leaders and technological innovators coming together with a vision to dramatically improve the Healthcare industry. Aetion supports and maintains a presence in organizations such as ISPOR, ISPE, ASHP, and HIMSS.
Located in three cities — We have offices located in Midtown Manhattan near Penn Station, Boston's financial district, and Los Angeles. All locations are accessible by various forms of public transportation.
Social and energetic offices – with a modern layout, and a giant kitchen and eating/social area. We have an open floor plan with an abundance of conference rooms, designed for impromptu collaboration, company gatherings, and industry meetups and events.
Great perks – We offer competitive salaries, top-of-the-line benefits, company ownership stock options, 401(k) and unlimited PTO
Aetion is an Equal Opportunity Employer. Aetion is committed to being an employer of choice, not just a good place to work, but a great and inclusive place to work. To that end, we strive to recruit and maintain a workforce that meaningfully represents the diverse and culturally rich communities that we serve. Qualified applicants will receive consideration for employment without regard to their race, color, religion, national origin, sex, sexual orientation, gender identity, protected veteran status or disabled status or, genetic information.

###"
Data Engineer,"Dave is the finance version of David and Goliath taking on the big banks. A financial friend to the millions of Americans who use the app. Dave helps with budgeting, building credit, finding work and accessing money to cover immediate expenses before payday. Instead of mandatory fees, Dave lets users pay what they think is fair through a 'tip' based model. Dave users plant a tree for each % tip they leave when taking an advance, resulting in tens of millions of trees planted.We are choosing to build a quality team vs a large team. You will be a formative part of our company culture moving forward. To that point, we want someone that can take ownership of a project from beginning to end and can do it on their own. We'll be there to support you but not hold your hand.Sure, we've checked all the boxes you'd expect from a fast-moving startup, but the best thing we can offer you is a supportive environment for learning and opportunities for personal/professional growth.Key Responsibilities* We're looking for a Data Engineer with an amazing opportunity to design, develop, and deliver our data platform from the ground up* Design, develop and deliver/implement data solutions to include: architecture design, prototyping of concepts to proof of concept, development of standards, design, and development of test plans, code and module design, development and testing, data solution debugging, and design and implementation* Optimize existing data pipelines and create new ones to manage data sets while learning the platforms from which we extract data; Develop and maintain third-party API processes for data pipelines* Design, implement and manage data warehouse solutions* Support and maintain data and database systems to meet business delivery specifications and needs. Document structure and processes* Work with Data Science to create and launch new models* Work with various business and engineering teams to ensure reliable, scalable, robust architecture for our data platformExperience Required* 5+ Years of Software Engineering Experience with a focus on Data* Experience programming in one or more general purpose programming languages, including but not limited to Python, Java, or Javascript* Experience programming in SQL (No-SQL is a plus)* Experience with ETL tools such as Talend/Stitch, Alooma, Fivetran, or similar tools DBT tools* Experience with Data Warehousing solutions such as BigQuery, Snowflake, Redshift, or similar managed solutions* Expert data modeling skills with experience tuning and optimizing for performance* Experience designing and operating data services & data pipelines* Familiarity with GCP's data tooling (e.g. Dataflow, CloudComposer, BigQuery, PubSub) or similar cloud toolin* Experience with financial datasets and associated reporting* Strong CS fundamentals and problem-solving skills* Excellent communication, analytical, and development skillsBenefits & Perks* Premium Medical, Dental, and Vision Insurance plans* Insurance premiums 100% covered for you and your dependents* Competitive salary and equity compensation packages* Generous and flexible PTO* Voluntary life insurance plan* Voluntary worksite benefit plans for accidents, critical illness and hospital indemnity* Flexible work hours* 401(k) savings plan* Generous paid parental leave* 1UP Wednesdays where you can teach or learn something new from a fellow Dave* A collaborative environment with opportunities for learning and growth* Virtual social eventsDave Inc is proud to be an Equal Employment Opportunity employer. We do not discriminate based upon race, ethnicity, religious creed, color, national origin, ancestry, gender (including pregnancy, childbirth, or related medical conditions), gender identity, gender expression, sexual orientation, marital status, age, status as a protected veteran, status as an individual with a disability, medical condition, or any other category protected by applicable federal and state law, including the City of Los Angeles' Fair Chance Initiative for Hiring Ordinance relating to an applicant's criminal history."
Data Engineer,"Click ""Apply for this Job"" below to submit your application. After doing so, please take our 4-minute web analytics assessment followed by a 15-minute aptitude test by clicking the following link:

Start Assessment

Who We Are:

2-time winner of Crain's 100 Best Places to Work, Direct Agents is a data-driven digital marketing agency with offices in NY and LA.

Established in 2003, we are an independent company, and our business has been built on adapting and driving change since the very beginning. Fueled by the grit and hustle of our diverse team, we’re inspired by experimentation and seek to push the boundaries on innovative approaches to advertising. Our teams are committed to growing and innovating alongside clients like Colgate, Walker & Co., Belkin, The CW, Wacoal, and more.

Together, we think differently and we continue to shape the digital space into the future…

We are the Shapers of the Digital World. Join us and become a shaper of change!

As a Data Engineer, you will create tangible value by helping to develop innovative data and tech systems for current and future clients. The work you do directly contributes to our ability to improve holistic business and marketing strategies aimed to meet and exceed client expectations.

You will have the opportunity to work on leading-edge AI/ML, data automation, and predictive analytics projects including: bidding algorithms, image recognition, attribution modeling, performance forecasting, propensity scoring, and much more. The position requires fostering close working relationships with internal marketing teams as well as clients.

What You’ll Do:
Create automated data systems using APIs, Selenium, web scrapers, etc.
Set up and maintain database ecosystems
Build advanced data models to provide granular and predictive insights
Develop AI systems to perform advanced operations based upon data inputs
Visualize data in an easily digestible format
Supplying internal teams with datasets to be used for strategic optimizations
Maintain data integrity and work with teams to troubleshoot discrepancies
Stay abreast of latest industry topics, trends, news, methodologies, and tools
What You’ll Need:
Bachelor's or Master’s Degree in Mathematics/Computer Science/Engineering/Finance or related field
Advanced Python skillset
Strong Statistical and/or Machine Learning background
Strong knowledge of database structures and SQL
Excellent quantitative skills and attention to detail
Experience using Power BI data visualization software is a plus
Highly motivated to identify and develop solutions to complex problems
Strong time management skills – ability to prioritize and meet deadlines
Diligent work ethic. Must be self-motivated and able to take the initiative to get the job done
Digital Marketing experience is preferred but definitely not required
What You’ll Get:
Competitive pay & health benefits.
Weekly social, wellness, and team building events
Access to health benefits and many other perks like One Medical
An amazing values based company culture ripe with collaboration, encouragement and camaraderie
Monthly education and training in digital advertising
Access to the best in class marketing technology and methodology
Ability to innovate and make an impact with your ideas in real-time
A fast tracked path to growth based on what you put into the role and your passion for learning in the company
Check out our Culture Videos:

Welcome to Direct Agents: https://www.youtube.com/watch?v=EC-LBxTw0To

DA 2019 Culture in Review: https://www.youtube.com/watch?v=rP12YTMF4Dg"
Data Engineer,"Join SADA as a Data Engineer!

Your Mission

As a Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data issues facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring a combination of batch or streaming data pipelines, data lakes and data warehouses.

You will be recognized as an established contributor by your team. You will contribute design and implementation components for multiple projects. You will work mostly independently with limited oversight. You will also participate in client-facing discussions in areas of expertise.
Pathway to Success

#BeOneStepAhead: At SADA we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage adaptability. This means that not only do our engineers understand that change is inevitable, but they embrace this change to continuously broaden their skills, preparing for future customer needs.

Your success comes from your enthusiasm, insight, and positive impact. You will be given direct feedback quarterly with respect to the scope and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, your collaboration with your peers, and the consultative skill you demonstrate in customer interactions.

As you continue to execute successfully, we will build a personalized development plan together that leads you through the engineering or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with a first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:
Google Professional Data Engineer Certified or able to complete within the first 45 days of employment

Required Qualifications:
Expertise in at least one of the following domain areas:
Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or other customer-facing role

Useful Qualifications:
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Applied experience operationalizing machine learning models on large datasets
Knowledge and understanding of industry trends and new technologies and ability to apply trends to architectural needs
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem

About SADA

Values: We built our core values on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing

Work with the best: SADA has been the largest partner in North America for GCP since 2016 and has been named the 2019 and 2018 Google Cloud Global Partner of the Year. SADA has also been awarded Best Place to Work by Inc. as well as LA Business Journal!

Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, RRSP, professional development reimbursement program as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."
Data Engineer,"About codeSpark
codeSpark is turning programming into play for kids everywhere. We’re building the largest community of young kid coders in the world. Our award-winning app, codeSpark Academy, has a unique word-free interface that allows kids as young as four to become makers. We have a strong focus on making codeSpark Academy exciting for both girls and boys.

We believe all kids should have the opportunity to master this new form of literacy and creative expression. Our self-directed subscription service thoughtfully combines structured challenges and open-ended creative play.

Our new Data Engineer will have the opportunity to explore all sorts of interesting data about kids’ learning through play. You’ll also help us improve our marketing efforts through development and reporting on success metrics. The right person joining us will be interested in our mission and motivated to find all sorts of other cool data to analyze to help us grow, too!

Data Engineer Job Overview
We are looking for a Data Engineer who will support our product, marketing, and leadership with insights gained from analyzing company data. The ideal candidate is self-sufficient and adept at using large data sets to find opportunities for product and process optimization, and in building and using models to test the effectiveness of different courses of action. They are experienced in using a variety of data mining/data analysis methods, using a variety of data tools, building and implementing models, using/creating algorithms and creating/running simulations. They must demonstrate a proven ability to drive business results with their data-based insights. Finally, they must be comfortable working with a wide range of stakeholders and functional teams. The right candidate will have a passion for discovering solutions hidden in large data sets and working with stakeholders to improve business outcomes.

Responsibilities:
Collaborate with stakeholders throughout the organization to identify opportunities to leverage company data in driving business solutions.
Mine and analyze data from company databases to drive optimization and improvement of product development, marketing techniques and business strategies.
Assess the effectiveness and accuracy of new data sources and data gathering techniques.
Develop custom data models and algorithms to apply to data sets.
Use predictive modeling to increase and optimize customer experiences, onboarding, revenue generation, ad targeting and other business outcomes.
Leverage A/B testing framework and test model quality.
Develop processes and tools to monitor and analyze model performance and data accuracy.
Maintain company KPI dashboard.

Essential Qualifications:
5-7 years of experience manipulating data sets and building statistical models.
Bachelor’s or Master’s in Statistics, Mathematics, Computer Science or another quantitative field.
Coding knowledge of several languages: C#, Python, JavaScript, Go, or similar.
Querying databases and using statistical computer languages: R, Python, Mongo, JQL, etc.
Using AWS services.
Analyzing data from 3rd party providers: Google Analytics, Mixpanel, Facebook, Appsflyer, Email service providers, etc.
Experience visualizing/presenting data for stakeholders.

Highly desired:
Experience working with product development and marketing teams, especially subscription products and kids games or apps.
Experience using statistical computer languages to manipulate data and draw insights from large data sets.
Experience working with and creating data architectures.
Knowledge of advanced statistical techniques and concepts (regression, properties of distributions, statistical tests and proper usage, etc.) and experience with applications.
Excellent written and verbal communication skills for coordinating across teams.
A proactive approach to learning and mastering new technologies and techniques.

Perks
Opportunity to shape strategy for a company having a global impact on K-5 education.
The rare opportunity to change the world by helping create the first generation in history that is truly connected and collaborative.
The fun of working with an effective and multi-award winning product that is beloved by customers.
Sharp, motivated co-workers in a creative and supportive office environment.

Commitment to Diversity and Inclusion
codeSpark believes in diversity and inclusion of all people, of all genders, races, ethnicities, sexual orientations, educational backgrounds, religions, abilities, socioeconomic backgrounds, immigration statuses, and more. Just as we hope to close access gaps in computer science education, we aim to create a culture within codeSpark that is inclusive and accessible to all current and potential employees.

codeSpark is an Equal Opportunity Employer."
Data Engineer,"The Enterprise Data Intelligence Team of Cedars-Sinai is looking for an experienced Data Engineer/Data Intelligence Analyst. The ideal candidate is passionate about data, coding, technology and will utilize their skills to help solve complex healthcare questions. The candidate should possess a data engineering background, business acumen to think strategically and love working with people. Within this role, the candidate will experience a wide range of problem solving situations requiring extensive use of data collection and analysis. The successful candidate will work with Data Engineers, Data Scientists, Business Analysts, Doctors, Nurses and other stakeholders across organization.

Summary of Essential Job Duties:
Expert Oracle Query and Development Skills (Extensive RedShift skills a plus).
Strong AWS Cloud Platform tools: S3, Kinesis, RedShift.
Familiar with Dashboarding tools like Tableau/Qlik.
Good Linux, Python, Bash skills.
Desire to learn new tools, techniques and solve data challenges.

Educational Requirements:
Four (4) year degree in Computer Science or similar experience.

Experience:
Hospital Based Healthcare experience, especially Epic Clarity data model is a plus."
Data Engineer,"Click ""Apply for this Job"" below to submit your application. After doing so, please take our 4-minute web analytics assessment followed by a 15-minute aptitude test by clicking the following link:

Start Assessment

Who We Are:

2-time winner of Crain's 100 Best Places to Work, Direct Agents is a data-driven digital marketing agency with offices in NY and LA.

Established in 2003, we are an independent company, and our business has been built on adapting and driving change since the very beginning. Fueled by the grit and hustle of our diverse team, we’re inspired by experimentation and seek to push the boundaries on innovative approaches to advertising. Our teams are committed to growing and innovating alongside clients like Colgate, Walker & Co., Belkin, The CW, Wacoal, and more.

Together, we think differently and we continue to shape the digital space into the future…

We are the Shapers of the Digital World. Join us and become a shaper of change!

As a Data Engineer, you will create tangible value by helping to develop innovative data and tech systems for current and future clients. The work you do directly contributes to our ability to improve holistic business and marketing strategies aimed to meet and exceed client expectations.

You will have the opportunity to work on leading-edge AI/ML, data automation, and predictive analytics projects including: bidding algorithms, image recognition, attribution modeling, performance forecasting, propensity scoring, and much more. The position requires fostering close working relationships with internal marketing teams as well as clients.

What You’ll Do:
Create automated data systems using APIs, Selenium, web scrapers, etc.
Set up and maintain database ecosystems
Build advanced data models to provide granular and predictive insights
Develop AI systems to perform advanced operations based upon data inputs
Visualize data in an easily digestible format
Supplying internal teams with datasets to be used for strategic optimizations
Maintain data integrity and work with teams to troubleshoot discrepancies
Stay abreast of latest industry topics, trends, news, methodologies, and tools
What You’ll Need:
Bachelor's or Master’s Degree in Mathematics/Computer Science/Engineering/Finance or related field
Advanced Python skillset
Strong Statistical and/or Machine Learning background
Strong knowledge of database structures and SQL
Excellent quantitative skills and attention to detail
Experience using Power BI data visualization software is a plus
Highly motivated to identify and develop solutions to complex problems
Strong time management skills – ability to prioritize and meet deadlines
Diligent work ethic. Must be self-motivated and able to take the initiative to get the job done
Digital Marketing experience is preferred but definitely not required
What You’ll Get:
Competitive pay & health benefits.
Weekly social, wellness, and team building events
Access to health benefits and many other perks like One Medical
An amazing values based company culture ripe with collaboration, encouragement and camaraderie
Monthly education and training in digital advertising
Access to the best in class marketing technology and methodology
Ability to innovate and make an impact with your ideas in real-time
A fast tracked path to growth based on what you put into the role and your passion for learning in the company
Check out our Culture Videos:

Welcome to Direct Agents: https://www.youtube.com/watch?v=EC-LBxTw0To

DA 2019 Culture in Review: https://www.youtube.com/watch?v=rP12YTMF4Dg"
Data Engineer,"Position Information

Position Information

Job Title: Data Engineer Position Number: 2013141055 Department: Information Technology Job Category: Classified Unit A Time (Percent Time): 100% Term (months/year): 12 months/year Current Work Schedule (days, hours): Monday - Friday: 7:30 a.m. - 4:30 p.m. Salary Range: A-126 Salary: Steps 1 - 6: $6,902- $8,810 per month Shift Differential: Shift differential eligibility based on the current collective bargaining agreement Open Date: 12/18/2019 Initial Screening Date: Open Until Filled: Yes Application Procedure:

Complete application packets will be accepted until the position is filled.

Applicants must submit all of the following materials online unless otherwise noted at Mt.SAC Employment Website to be considered for this position:
A Mt. San Antonio College online application.
A cover letter describing how the applicant meets the required education and experience.
A detailed résumé that summarizes educational preparation and professional experience for the position.
Two letters of recommendation that reflect relevant experience (do not use social media or professional networks as a means to provide letters of recommendation).
If applicable, College and/or university transcripts showing the awarded/conferred degree are required and must be submitted with the online application by all applicants, including current or former employees of the college, to demonstrate that the required educational qualifications are met. Unofficial transcripts are acceptable at the time of application; however, copies of diplomas are not accepted in lieu of transcripts.
Health & Welfare:

The College contributes an annual premium up to the family coverage amount for Kaiser Permanente $15 office visit medical, DeltaCare HMO dental, VSP vision and life insurance plans. Lifetime retirement benefits provided for eligible retirees.

The District participates in the Public Employees' Retirement System (PERS), State Teachers' Retirement System (STRS) retirement programs, and National Benefit Services.
Note Salary and Health & Welfare Benefits are subject to change
Basic Function/Overview:

DEFINITION

Under general direction, leads and coordinates day-to-day operations of the Operational Data Store (ODS), Data Warehouse, and all related technologies; analyzes and transforms data into a format that can be easily used by different departments for reporting; collaborates with programmers, Business Analysts, and functional areas in gathering requirements and clarifying their needs for implementation, generation, optimization, and support of their data; provides complex professional staff assistance to the Director, Enterprise Application Systems in areas of expertise.

SUPERVISION RECEIVED AND EXERCISED

Receives general direction from the assigned managerial personnel. Provides coordination and lead work direction to staff.

CLASS CHARACTERISTICS

This is a highly specialized class in the Information Technology (IT) Department that leads a wide variety of technical duties pivotal to the College's Operational Data Store and Data Warehouse. Responsibilities include performing diverse, specialized, and complex work involving significant accountability and decision-making responsibility providing guidance, suggestions, and coordination to the College as it relates to data infrastructure, modeling, data refresh timelines, and operations. Successful performance of the work requires an extensive professional background, as well as skill in leading and coordinating departmental work with that of other College departments. This class is distinguished from Director, Enterprise Application Systems by the latter's management and supervisory authority in planning, organizing, and directing the full scope of enterprise operations within the department.

Essential Duties/Major Responsibilities:

EXAMPLES OF ESSENTIAL FUNCTIONS (Illustrative Only)
Provides technical support, analysis, and programming to ensure complete and appropriate use of the College's Operational Data Store and Data Warehouse.
Responds and evaluates ad hoc requests for data, statistical analysis, research projects and studies; prepare requests for processing; arrange and maintain project schedules and timelines; design strategies to complete assignments; analyze and compare a variety of data solutions; make team project recommendations to the manager.
Assists manager to evaluate and respond to requests for complex or original support from within and outside the College; works independently with requestors to clarify their needs and optimize the utility of results.
Reviews user needs and requests and develops proposed solution for usable data design or format for the users' reporting and analysis needs; monitors and tunes report queries and views.
Assists in implementing ways to improve data reliability, efficiency and quality.
Develops, constructs, tests, and maintains Operational Data Store and Data Warehouse architecture.
Discovers opportunities for data acquisition.
Expands the existing schema design to handle new data formats.
Develops and documents Operational Data Store and Data Warehouse standards, scripts, guidelines, and usage procedures; enforces standards for use, control, updates, and maintenance for the Operational Data Store and Data Warehouse environments.
Interacts and coordinates with other IT areas and key end users.
Ensures data models, design, and architecture that are in place support the requirements of the programmers, Business Analysts, researchers, and different functional areas.
Provides guidance on reporting, query or data extraction design, development, and maintenance, including monitoring performance tuning and optimization in queries; Recommends selection of query or reporting tools, methodologies, and procedures for development of reports and views.
Develops data set processes for data modeling, mining, and production; prepares data for use in predictive and prescriptive modeling.
Leverages large volumes of data from internal and external sources to answer reporting needs.
Participates on committees, task forces, and special assignments, including, but not limited to Screening and Selection Committees and affiliated trainings. Prepares and delivers oral presentations related to assigned areas if needed.
Performs other related or lower classification duties as assigned.
Other Duties:

Performs other related duties as assigned.

Knowledge Of:
State-of-the-art information systems as applied to large, complex administrative, or educational organizational environments.
Principles and techniques of computer systems and software architectures.
Operating System (OS)-based platforms.
Programming languages, including but not limited to PL/SQL, SQL, Python, and Shell Scripting.
Database front-end programs such as Microsoft (MS) Access, Statistical Package for the Social Sciences (SPSS), and related products.
Data warehousing and techniques.
Principles and concepts of Relational Database Management System (RDBMS), Big Data, and Operational Data Store.
Statistical tools and research methods.
Principles, techniques, and methodologies in project management and leadership.
Business letter writing and record-keeping principles and procedures.
Use, capability, characteristics, and limitations of computer systems and databases
Methods, techniques, and practices of data collection and report writing.
Modern office practices, method, and computer equipment and applications related to the work, including word processing, database, and spreadsheet software.
Techniques for effectively representing the College in contacts with governmental agencies, community groups, and various business, professional, educational, regulatory, and legislative organizations.
Skills and Abilities:
Analyze informational requirements and needs, identify problems, provide technical advice and consultation, and ensure efficient computer system utilization.
Analyze data and develop logical solutions to problems.
Experience with Source code management systems.
Code Debugging and Performance troubleshooting.
Master new technologies quickly; stays abreast of current trends and developments in Operational Data Store, Data Warehouse, and Big Data.
Conduct complex research projects on a wide variety of information technology and database administration topics, evaluate alternatives, make sound recommendations, and prepare effective technical staff reports.
Interpret, explain, and ensure compliance with College policies and procedures.
Establish and maintain a variety of filing, record-keeping, and tracking systems.
Organize and prioritize a variety of projects and multiple tasks in an effective and timely manner; organize own work, set priorities, and meet critical time deadlines.
Use English effectively to communicate in person, over the telephone, and in writing at both technical and functional levels.
Understand scope of authority in making independent decisions.
Review situations accurately and determine appropriate course of action using judgment according to established policies and procedures.
Establish, maintain, and foster positive and effective working relationships with those contacted in the course of work.
Explores and examines data to find hidden patterns.
Tell stories to key stakeholders based on the analysis.
Learns and applies emerging technologies, as necessary, to perform duties in an efficient, organized, and timely manner.
Minimum Qualifications/ Education & Experience:

Equivalent to graduation from a regionally accredited four-year college or university with major coursework in computer science, data science or a related field, and four (4) full time equivalent years of experience in database management including two (2) full time equivalent years of progressive experience as a data analyst, data engineer, or researcher.

Equivalencies: Preferred Qualifications: License(s) & Other Requirements:

The incumbent may periodically be required to travel to a variety of locations. If operating a vehicle, employees must have the ability to secure and maintain a valid California driver's license.

Examination Requirements: Working Environment:

Incumbents work in an office environment with moderate noise levels, controlled temperature conditions, and no direct exposure to hazardous physical substances. Incumbents may interact with staff and/or public and private representatives in interpreting and enforcing departmental policies and procedures.

Physical Demands:

Must possess mobility to work in a standard office setting and use standard office equipment, including a computer; to operate a motor vehicle to visit various College and meeting sites; vision to read printed materials and a computer screen; and hearing and speech to communicate in person and over the telephone. This is primarily a sedentary office classification although standing in and walking between work areas is frequently required. Finger dexterity is needed to access, enter, and retrieve data using a computer keyboard or calculator and to operate standard office equipment. Incumbents in this classification occasionally bend, stoop, kneel, reach, push, and pull drawers open and closed to retrieve and file information. Incumbents must possess the ability to lift, carry, push, and pull materials and objects up to 20 pounds.

Hazards: Conditions of Employment:

Official offers of employment are made by Mt. San Antonio College Human Resources and are made contingent upon Board approval. It is also required that a final offer of employment will only be made after the candidate has successfully been live-scanned and clearance for employment is authorized by Human Resources. Costs for live-scan services shall be borne by the candidate.

Notice to all prospective employees - The person holding this position is considered a 'mandated reporter' under the California Child Abuse and Neglect Reporting Act and is required to comply with the requirements set forth in Administrative Procedure 3518, titled Child Abuse Reporting, as a condition of employment.

As required by the Jeanne Clery Disclosure of Campus Security Policy and Campus Crime Statistics Act, the Mt. San Antonio Community College Annual Security Report is available here: Mt. SAC Annual Security Report 2017

The person holding this position is considered a 'Responsible Employee' under Title IX of the Educational Amendments Act of 1972 and is required to report to the College's Title IX Coordinator all relevant details reported to him or her about an incident of alleged sexual misconduct including sexual harassment, sexual assault, dating and domestic violence and stalking.

Typing Certificate Requirements: Special Notes:

Please note: A confirmation number will be assigned when your application packet indicates the supplemental questions have been answered and a document has been attached to each required link. Assistance with the online application process is available through the Office of Human Resources at 1100 N. Grand Avenue, Walnut, CA 91789-1399. Human Resources: (909) 274-4225. E-mail: employment@mtsac.edu.

DO NOT include photographs or any demographic information (e.g. D.O.B, place of birth, etc.) on your application or supporting documents.

TRAVEL POLICY: Costs associated with travel in excess of 150 miles one way from residence for the purpose of an interview will be reimbursed up to $500 maximum. Relocation costs will be borne by the successful candidate. Travel reimbursement claims (original receipts) must be submitted no later than 30 days following the interview date.

Foreign Transcripts:

Foreign Transcripts: Transcripts issued outside the United States require a course-by-course analysis with an equivalency statement from a certified transcript evaluation service verifying the degree equivalency to that of an accredited institution within the USA. This report must be attached with the application and submitted by the filing deadline.

Inquiries/Contact:

Human Resources at 1100 N. Grand Avenue, Walnut, CA 91789-1399. Human Resources: (909) 274-4225. E-mail: employment@mtsac.edu.

Selection Procedure:

A committee will evaluate applications, taking into account breadth and depth of relevant education, training, experience, skills, knowledge, and abilities. The screening committee reserves the right to limit the number of interviews granted. Meeting the minimum qualifications for a position does not assure the applicant of an interview.

Interviews may include a writing sample, committee presentation, and/or performance test. The start date will be following Board approval and receipt of live scan clearance.

Special Instructions to Applicants:

To be guaranteed consideration, it is the applicant's responsibility to ensure that all required materials are received before the initial screening date and time indicated on the job posting. Incomplete application packets will not be considered. All application materials will become College property, will not be returned, and will not be copied. Please visit our employment website at Mt. SAC Employment Website to complete and submit your application for this position.

Letters of Recommendation

Confidential letters of recommendation are not accepted for this position. All letters of recommendation must be uploaded to the application.

EEO Policy:

The College is an equal opportunity employer. The policy of the College is to encourage applications from ethnic and racial minorities, women, persons with disabilities, and Vietnam-era veterans. No person shall be denied employment because of race, religious creed, color, national origin, ancestry, physical disability, mental disability, medical condition, marital status, sex (gender), age, sexual orientation, or the perception that a person has one or more of these characteristics.

Conflict of Interest

Mt. San Antonio College employees and the Board of Trustees members shall not engage in any employment or activity that is inconsistent with, incompatible with, or in conflict with Mt. San Antonio College's Administrative Procedures (AP 2710 Conflict of Interest, AP 2712 Conflict of Interest Codes).

Cancel RTF Policy:

WE RESERVE THE RIGHT TO RE-OPEN, RE-ADVERTISE, DELAY OR CANCEL FILLING THIS POSITION.

THIS RECRUITMENT MAY BE USED TO FILL FUTURE VACANCIES.

Quick Link http://hrjobs.mtsac.edu/postings/7464"
Data Engineer,"Job Description
Our client is looking for a Sr. ETL Developer with IICS, DW design and implementation experience with Snowflake, as well as Cloud experience in AWS

• Responsibilities include performing data analysis, defining ETL architecture, data modeling and implementing robust data pipelines in the cloud using AWS, IICS and Snowflake. Requirements:

• 7+ years hands-on experience in BI and Data Warehousing with at least 2 full life cycle implementations. Includes architecture, design, data modeling (Kimball methodology), ETL and reporting.

• 2+ years experience implementing a Cloud DW/Data Lake.

• Demonstrably deep understanding of SQL, relational and analytical databases (Snowflake, Redshift, BigQuery, etc.)

• Hands-on, expert level experience of Informatica Intelligent Cloud Services

• Experience with AWS services (S3, Lambdas, etc.)

• Knowledge of Agile (Scrum) practices required

• Knowledge of ERPs (Oracle EBS, etc.) highly desired"
Data Engineer,"Analog Devices (NASDAQ: ADI) designs and manufactures semiconductor products and solutions. We enable our customers to interpret the world around us by intelligently bridging the physical and digital worlds with unmatched technologies that sense, measure and connect.Analog Devices (NASDAQ: ADI) designs and manufactures semiconductor products and solutions. We enable our customers to interpret the world around us by intelligently bridging the physical and digital worlds with unmatched technologies that sense, measure, connect, and analyze. We are world leader in the measurement of real-world signals for challenging applications the and interpretation of these measurements, which is enabling Industrial IoT applications.ADI is a leader in the industrial space, and Industrial IOT has the potential to be a significant future growth driver for the company in both product sales and emerging analytics revenues. It has been recognized as a corporate priority to drive expansion. This role requires an energetic and motivated individual with strong vision and technical acumen to drive this initiative forward.The Analytics Insights & Diagnostics Group (AID) is responsible for developing and delivering full end to end solutions in a variety of applications, where a combination of core differentiating ADI hardware and software technology is brought to market. This postion is responsible for using and growing the OtoSense AI framework powering Sensing Interpretation systems for a large variety of customers. The ideal candidate will be fluent in Python, familiar with the processing of sensing data and familiar with machine learning techniques and implementations methods. There is no expectation of any deep expertise in Machine Learning or Data Science, but more an eargerness to learn from the system in place and its current architects. This person must be exceedingly well organized, detail oriented and flexible, embracing the challenges of working with a variety of situations and circumstances while having the ability to interact with collaborators and external stakeholders at all levels in a complex environment. The AID group is a fast paced and dynamic team where innovation is nurtured. The variety and scope of projects provides an interesting and challenging opportunity for an impactful individual.Responsibilities* Processing datasets using provided data processing, machine learning and visualization components of the OtoSense AI platform.* Communicating with all stakeholders (data science, product engineering, marketing, sales etc.) to present results matching specific evaluation metrics.* Improving platforms components and processes related to data processing pipelines.* Respecting software engineering best practices, from abstracted architecture to variable naming.* Documenting all changes brought to the system following established documentation guidelines.Requirements* Bachelor or Master degree in a data-intensive and software-intensive discipline.* Proficient in Python.* Good understanding of featurization and machine learning techniques.* Excellent analytical and problem-solving skills.* Excellent interpersonal skills and an ability to develop and articulate system requirements as part of a broader strategy.#LI-NS1For positions requiring access to technical data, Analog Devices, Inc. may have to obtain export licensing approval from the U.S. Department of Commerce - Bureau of Industry and Security and/or the U.S. Department of State - Directorate of Defense Trade Controls. As such, applicants for this position - except US Citizens, US Permanent Residents, and protected individuals as defined by 8 U.S.C. 1324b(a)(3) - may have to go through an export licensing review process.Analog Devices, Inc. is an Equal Opportunity Employer Minorities/Females/Vet/DisabilityEEO is the Law: Notice of Applicant Rights Under the LawEducation Level: Bachelor's DegreeTravel Required: No"
Data Engineer,"Data Engineer

Bigtime Entertainment Co. building state of the art software and
algorithms to improve the way that our
media company transacts; interacts with consumers and customers;
and makes
vital business decisions with large revenue impacts. As a Data
Engineer
supporting the Data Science team, you will frame, pose and
translate business
problems to build AI-powered solutions that directly contribute
to data
products. As a member of the Data Science & Engineering team, you
will be
designing and building scalable models & architectures upon while
ML algorithms
can thrive, as well as refining existing model implementations so
that they
automatically build context in order to perform above and beyond
expectations.

From creating
experiments and prototyping implementations to designing new
architectures, we
resolve challenging and meaningful problems with compelling
business use cases.
Our team is committed to continuously leveraging and furthering
the latest
advances in ML research to transform the broader media market
through our data
product successes

In this role you will:

· Collaborate with the
data science team to build future-proof frameworks and
abstractions

· Collaborate with
product management and engineering departments to understand
company needs and
devise AI powered solutions

· Build tools that
will increase the productivity of the Data Analytics team-members
developing AI-based systems

· Implement models
that the data science team develops into working prototypes,
proof of concepts,
self-supporting model ecosystems

· Build data pipelines
that contribute to a self-sustaining data model system

· Build demos and
conduct training in conjunction with data scientist to help the
broader
engineering organization (and/or business partners) effectively
use the product

· Demonstrate and
apply theories through research efforts to develop new and
improved products,
processes, or technologies

· Participate in
cutting-edge research in artificial intelligence and machine
learning
applications

· Optimize models for
on-device and multi-modal intelligence

Qualifications:

• Experienced Data
Engineer with a BS, MS or PhD in a quantitative field (CS,
Engineering,
Physics, etc.)

• 4+ years hands-on
business experience, demonstrated implementations of ML models
and techniques
is a plus

• Advanced in Python

• Experience with AWS
infrastructure (AWS Certifications are a plus)

• Strong knowledge of
relational and distributed databases, extremely strong in SQL

• Multiple
Implementations that feature good memory, disk I/O, and CPU/GPU
management.

• Experience with
Apache infrastructure

• Experience with
streaming data and video manipulation

• Familiarity with
common ML algorithms (i.e. neural networks, tree-based methods,
unsupervised
learning, feature engineering)

• Familiarity with
ML/AI frameworks, e.g,. TensorFlow, Spark, and modular/modern
software design
practices.

• History of research
publications and/or implementations of state of the art
techniques hosted on
open source repositories is a plus

• Passionate about
ML/AI and how it can improve both the media industry and the
world

Technologies we use:

AWS, Python, Python
Data Science packages, Spark, Hadoop, Apache

Resumes sent to: jim@ingenium.agency

Top base salary, excellent benefits and work culture"
Data Engineer,"Job Description
Title: Data Engineer
Job ID: TJ3682741021
Location: Los Angeles, CA

Our client is looking to hire a Data Engineer to join their team. The ideal candidate thrives in a fast paced environment has the ability to communicate clearly. This role will be responsible for automating and maintaining batch pipelines that collect and process data; automating and maintaining batch pipelines that collect and process data; designing and building tools that provide confidence in their data quality; and mentoring and coaching other software engineers.

Qualifications:

Bachelor’s degree in Computer Science or comparable field
5+ years experience in Python and SQL
5+ years experience in Java, Scala, or similar OO experience
5+ years experience with Spark, Hadoop, or Databricks
Experience with data analysis, processing, and validation
Professional experience with open source ETL frameworks such as Airflow, Luigi, or similar
Knowledge within a diverse set of public cloud technologies: AWS RDS, S3, EC2, Lambda, Google Cloud Big Query, Google Cloud Bigtable, etc.
For more information about TEEMA and to consider other career opportunities, please visit our website at www.teemagroup.com"
Data Engineer,"Develops and maintains scalable cloud-based ingress and egress data pipelines
Support continuing increases in data volume and complexity
Collaborates with team’s analytics members to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization
Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it
Writes unit/integration tests, contribute to engineering wiki, and documents work
Performs data integrity tasks required to troubleshoot data-related issues and assist in the resolution of data issues
Design data integrations and data quality framework
Contribute to the continuing technology stacks advancements in a leadership level
Works closely with engineering clients/partners to develop strategies for long term data platform architecture
Qualifications / Skills:
Knowledge of Microsoft Azure-based ETL best practices and IT operations in an always-up, always-available service
Experience with or knowledge of Agile Software Development methodologies
Excellent problem solving and troubleshooting skills
Process-oriented with great documentation skills
Excellent oral and written communication skills with a keen sense of customer service
Experience Requirements:
BS or MS degree in Computer Science or a related technical field
4+ years of Python or Java development experience
4+ years of SQL experience, specifically in sparkSQL and hiveSQL
4+ years of experience with schema design and dimensional data modeling
2+ years of DataBricks or equivalent spark development using scala or python
2+ years of Apache Data Frames in Microsoft Azure data blob, Databricks Delta Lake experience preferred.
2+ years of experience designing, building, and maintaining ETL systems
Experience of Azure Function Apps, Event Hub and TomCat Servers
Experience of supporting Microsoft SQL and MySQL
Experience of Kafka, Oracle, Splunk, Google Analytics, Apple Store data interface
Experience of developing JDBC and ODBC connection
Experience of developing REST API interface
#LI-AS

NortonLifeLock is proud to be an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive and accessible environment for all employees. All employment decisions are based on merit, experience, and business needs, without regard to race, color, national origin, age, religion, sex, pregnancy (including childbirth or related medical conditions), genetic information, disability (physical or mental), medical condition, marital status, sexual orientation, gender identity or gender expression, military or veteran status, or any other consideration made unlawful by federal, state, or local law. NortonLifeLock strictly prohibits unlawful discrimination based on such protected characteristics and seeks to recruit the most talented candidates from diverse cultures and backgrounds.

We also consider for employment qualified individuals with arrest and conviction records. In addition, NortonLifeLock will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. Learn more about pay transparency.

EEO is the law. Applicants and employees of NortonLifeLock Inc. are protected under Federal law from discrimination. See the EEO poster and supplement.

Apply"
Data Engineer,"Role Data Engineer Location Sherman Oaks, CA Duration 6 Months 2+ years of professional experience as a Data Engineer Experience with SQL and NoSQL databases and tools of the trade such as Snowflake, Redshift, Airflow, etc. Experience with Python Strong experience writing and optimizing SQL statements Some knowledge of schema design (logical and physical) is desired Familiarity with distributed processing (Map Reduce, MPP, etc.) An interest in learning new languages, identifying and using best-in-class tools, and applying industry best practices"
Data Engineer,"Job Description
Data Engineer

Bigtime Entertainment Co. building state of the art software and algorithms to improve the way that our media company transacts; interacts with consumers and customers; and makes vital business decisions with large revenue impacts. As a Data Engineer supporting the Data Science team, you will frame, pose and translate business problems to build AI-powered solutions that directly contribute to data products. As a member of the Data Science & Engineering team, you will be designing and building scalable models & architectures upon while ML algorithms can thrive, as well as refining existing model implementations so that they automatically build context in order to perform above and beyond expectations.

From creating experiments and prototyping implementations to designing new architectures, we resolve challenging and meaningful problems with compelling business use cases. Our team is committed to continuously leveraging and furthering the latest advances in ML research to transform the broader media market through our data product successes

In this role you will:

Collaborate with the data science team to build future-proof frameworks and abstractions

Collaborate with product management and engineering departments to understand company needs and devise AI powered solutions

Build tools that will increase the productivity of the Data Analytics team-members developing AI-based systems

Implement models that the data science team develops into working prototypes, proof of concepts, self-supporting model ecosystems

Build data pipelines that contribute to a self-sustaining data model system

Build demos and conduct training in conjunction with data scientist to help the broader engineering organization (and/or business partners) effectively use the product

Demonstrate and apply theories through research efforts to develop new and improved products, processes, or technologies

Participate in cutting-edge research in artificial intelligence and machine learning applications

Optimize models for on-device and multi-modal intelligence

Qualifications:

Experienced Data Engineer with a BS, MS or PhD in a quantitative field (CS, Engineering, Physics, etc.)

4+ years hands-on business experience, demonstrated implementations of ML models and techniques is a plus

Advanced in Python

Experience with AWS infrastructure (AWS Certifications are a plus)

Strong knowledge of relational and distributed databases, extremely strong in SQL

Multiple Implementations that feature good memory, disk I/O, and CPU/GPU management.

Experience with Apache infrastructure

Experience with streaming data and video manipulation

Familiarity with common ML algorithms (i.e. neural networks, tree-based methods, unsupervised learning, feature engineering)

Familiarity with ML/AI frameworks, e.g,. TensorFlow, Spark, and modular/modern software design practices.

History of research publications and/or implementations of state of the art techniques hosted on open source repositories is a plus

Passionate about ML/AI and how it can improve both the media industry and the world

Technologies we use:

AWS, Python, Python Data Science packages, Spark, Hadoop, Apache

Resumes sent to: jim@ingenium.agency

Top base salary, excellent benefits and work culture"
Data Engineer,"FabFitFun is one of the best places to work and its amazing success (over 1 million members) has been achieved due to our incredible employees, dedicated leadership, inclusive corporate culture, and career growth opportunities. Guided by our company values, FFF seeks to maintain a work culture that encourages innovation, rewards creativity, values teamwork, and supports diversity, equity and inclusion. The company endeavors to foster confidence, effectiveness, and success for all employees who work with these values every day.

FabFitFun delivers the season's best beauty, fashion, fitness, and lifestyle products straight to your doorstep. Our mission is to deliver ""happiness and well-being to everyone, everywhere"". The FabFitFun subscription box is a quarterly, custom curated box with a vast scope of member benefits, including an engaged community, FFF TV, exclusive sales, and more.

We are looking for a Senior Data Engineer to develop data pipelines for making real-time decisions and business recommendations. The ideal candidate would have a passion for and knowledge of data engineering, big data, and distributed/cloud computing, and enjoys supporting and fulfilling the strategic objectives of a highly cross-functional organization.

What You'll Do:
Build, test and refine data pipelines for data analytics and data science
Data modeling, process design, and overall data pipeline architecture
Ensure data quality and consistency with monitoring and support, and play an active role in establishing data governance around company KPIs
Work closely with the analytics, data science, product, finance, operations, and marketing teams to design, build, and test end-to-end solutions
What You'll Bring:
5+ years of software engineering experience with a focus on data
Solid experience in SQL development
Solid experience with Python, Spark, shell scripting
Extensive ETL development experience with large-scale DBS or big data systems such as Redshift, Snowflake, Databricks, etc.
Experience working with reporting tools such as Tableau or Looker
Extensive experience with design & development of relational databases and data warehouses
Ability to look at solutions unconventionally and explore opportunities and devise innovative solutions
Excellent communication skills (verbal and written) and interpersonal skills and an ability to communicate with both business and technical teams
Experience gathering business requirements and identifying data needs
Experience analyzing data to identify deliverables, gaps, and inconsistencies
Bachelor's degree in computer science or related field
Bonus Points:
Data lake experience
Experience on subscription service products
Knowledge of accounting, FP&A, and marketing functions
What You'll Get:
Amazing benefits including medical, dental, vision, FSA, and 401k
Generous PTO
Free FabFitFun subscription and quarterly credit in the Add-Ons store"
Data Engineer,"Data Engineer
If you are a Data Engineer looking for a new opportunity, read on!

We are a technology company based in Pasadena and we are growing rapidly. Currently, we are looking for a Data Engineer to join our team!

As a Data Engineer, you will work within our engineering team to maintain, expand and optimize the data warehouse and data pipeline processes. You will also work with our data scientists, data analysts and product stakeholders to implement processes and infrastructure in order to support our data driven reports and
analytics. These systems process billions of location data points per day.

Apply now!
What You Need for this Position
- SPARK
- Redshift
- Linux
- UNIX
- Shell
- AWS
- SQL
So, if you are a Data Engineer with experience, please apply today!
-
Applicants must be authorized to work in the U.S.


CyberCoders, Inc is proud to be an Equal Opportunity Employer

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.

Your Right to Work In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.

CyberCoders will consider for Employment in the City of Los Angeles qualified Applicants with Criminal Histories in a manner consistent with the requirements of the Los Angeles Fair Chance Initiative for Hiring (Ban the Box) Ordinance."
Data Engineer,"Job Description
Our entertainment company is seeking an innovative Senior Data Engineer to join our fast-paced team! We utilize and develop cutting-edge technology to deliver high-end and creative content on a global scale. We work across multiple platforms to transform how we see media.

The ideal candidate will possess all of the required skills listed below and more! They will demonstrate excellent interpersonal and communication skills and adhere to the standards of quality and excellence at our company's core.

Please, only apply if you are able to work directly for a U.S. company for the next three years. We are not currently able to work with C2C, H1, or OPT for this position.

Duties & Responsibilities:
Build data pipelines orchestration.
Create the design and architecture for data-lake, data-marts, data-models, and data-warehouse.
Ensure efficiency of data science workflows and advanced machine learning algorithms.
Build and optimize performance of Hadoop and Spark batch jobs (Spark, Kafka, Cassandra, etc.).
Construct and improve ElasticSearch performance.
Contribute to open source solutions and communities.
Stay current on emerging tools and technologies.
Collaborate cross-functionally with other software engineers and their teams.
Establish and demonstrate technologies, solutions, and leading practices.
Balance resources, requirements, and complexity.
Qualifications:
5+ years of experience in full software development lifecycle.
5+ years of experience developing bis data apps.
Bachelor’s or Master’s degree in Computer Science, Engineering, Mathematics or Physical Sciences.
Minimum 3 years’ experience with Apache Spark and Spark Streaming.
Minimum 3 years’ experience with Hadoop batch processing framework and map reduce design patterns.
Minimum 3 years’ experience with Java/Scala/Python in support of data applications.
Expertise in Java 1.8+, Scala 2.11, and Linux.
Demonstrated proficiency in programming and analysis (design patterns, hardware, software requirements, systems requirements, deployment protocols, etc.).
Previous experience in an Agile environment using Scrum.
Previous experience with open source technologies, widely used RDBMS’ and SQL, and at least one columnar NoSQL solution.
Preferred Qualifications:
Prior experience developing and maintaining large scale, consumer facing web apps.
Prior experience working for a large data enterprise organization, creating robust and reliable data pipelines, and using multiple NoSQL solutions (HBase, MongoDB, Neo4j).
Previous operational experience with large software systems.
Previous Schema or Cube design experience.
Familiarity with statistics, machine learning, and natural language processing apps.
Knowledge of security and PII and PCI compliance issues.
Previous experience with the following preferred:
Apache Airflow
Amazon AWS
AWS EMR
docker and Kubernetes
Restful APIs
Apache Kafka
Apache HBase
Apache Hive and/or Apache Crunch
Apache Avro
Powered by JazzHR

h23E4mKY4q"
Data Engineer,"Pluto TV is the leading free streaming television service in America, delivering 200+ live and original channels and thousands of on-demand movies in partnership with major TV networks, movie studios, publishers, and digital media companies. Pluto TV is available on all mobile, web and connected TV streaming devices and millions of viewers tune in each month to watch premium news, TV shows, movies, sports, lifestyle, and trending digital series. Headquartered in Los Angeles, Pluto TV has offices in New York, Silicon Valley, Chicago and Berlin. Pluto is a subsidiary of ViacomCBS (NASDAQ: VIAB, VIA), a global content company with premier television, film and digital entertainment brands.
As a Data Engineer, you have a solid understanding of both the business and the technical aspects of BI in relation to digital media business. You will drive the completion of projects within the established scope, while simultaneously planning for and leading unknown future BI requirements in a dynamic environment.
Design, model and develop data sets to support reporting and analytics in a cloud environment.
ETL development: cover all aspects of programming assignments and assist with systems design.
Develop and maintain a technical metadata framework and repository of data events and ETL operations.
Lead and administer Analytics tools and tag management systems.
Help plan and maintain the technical infrastructure, its configuration, performance, and storage requirements, with consideration of tiered data and data archiving.
Generate ad-hoc queries and reports based on business requirements.
Provide ongoing evaluations of technology solutions and capabilities to ensure alignment with business objectives, identify areas of risk, while monitoring the current environment and potential improvement areas.
Work with business partners to gather, analyze, and translate requirements in BI reporting area – either recommending an existing solution, developing a solution, or synthesizing delivery requirements to engineering teams for development.
Actively question and challenge customers to understand their requirements and reach the best solutions, near and long term.
Understand and adhere to development and documentation standards, database design and storage.
Successfully execute process improvements impacting own work and work of others.
On-call application support is required.

3+ years of premier Data engineering experience; at least 2 years on cloud Infrastructure.
Working knowledge of digital media ecosystem, including how digital video streaming, ad servers, DSPs, SSPs work.
Experience with a mix of Cloud and Enterprise data environments with real world implementation of data collection and processing on AWS environment.
Knowledge of web technologies and online advertising systems.
Experience with real-time Big Data analytics.
Experience with Hadoop, MapReduce, Spark, Flink and/or other Big Data processing platforms.
Excellent knowledge of OLAP concepts.
Familiarity with columnar databases like Redshift, Vertica etc.
Programming language such as Java, and scripting languages like python, ruby and Unix shell scripts.
Experienced working in a fast-paced, high-tech environment (preferably software development) and comfortable navigating conflicting priorities and ambiguous problems.
Experience with data visualization tools such as Looker, Tableau.

Great communication and collaboration skills across technical and non-technical partners.
A Bachelor’s degree in Computer Science or equivalent preferred."
Data Engineer,"Job Description
Qualifications
Bachelor’s Degree in Computer Science, Data Analytics or similar discipline including Mathematics, Statistics, Physics, or Engineering is preferred
Advanced degree in Life or Physical Science, Bioengineering, Biomedical Engineering or closely related discipline is preferred
Minimum of 4 years work related experience with degree or sufficient transferable experience to demonstrate functional equivalence to a degree
Advanced Experience with programming scripts such as Python, Java, Scala, C++ in Linux/Unix, and R
Experience in applying data analysis techniques to a large set of data using big data systems such as Hadoop, Spark, MongoDB, or similar software
Advanced analytics knowledge and application in the field of
Statistics
Mathematical programming
Business acumen and experience with operational or strategic systems

Company Description
We are an equal opportunity employer and make hiring decisions based on merit. Recruitment, hiring, training, and job assignments are made without regard to race, color, national origin, age, ancestry, religion, sex, sexual orientation, gender identity, gender expression, marital status, disability, or any other protected classification."
Data Engineer,"Req ID: 177552

The Job

Warner Bros. has been entertaining audiences for more than 90 years through the worlds most-loved characters and franchises. Warner Bros. employs people all over the world in a wide variety of disciplines. We're always on the lookout for energetic, creative people to join our team.

WB Technology combines Warner Bros. industry-leading technologists and disciplines to ensure global alignment with business strategy and accelerated delivery of innovative technology solutions studio- and industry-wide. From pre-production through archiving, the WBT organization will provide critical business and technology intelligence and services to all Studio business units. WBT manages the Studios enterprise systems and solutions, emerging platforms, information security, consumer intelligence, content mastering and delivery, and more.

The WBT Data Intelligence group is seeking an enthusiastic Data Engineer to join our growing team. As a Data Engineer at WB, you will be responsible for the creating data engineering components using Snowflake in an AWS ecosystem. The Engineer will also be responsible for hands on development of high-quality scalable enterprise data solutions and data services, managing product launches and maintenance from inception to launch and through continuous delivery.

The ideal candidate must collaboratively partner across the organization to understand Warner Bros. business functions and data requirements. The engineer will translate those into data products, software features and services to allow Warner Bros. to continue to innovate. The candidate must be able to implement seamless data services across cross-functional teams and continually improve upon key performance indicators. You must also be an experienced problem-solver and be comfortable navigating ambiguity and influencing. You will work closely with product managers, project managers, data architects and data engineers to implement and maintain data pipelines leveraging emerging and established data technologies. You will need to be able to work in a constantly changing, fast-paced environment and must be able to adapt to new technologies and new ideas. You must be detail oriented, with a strong affinity for data and analytics to support business decisions.

The Daily
Work closely with the business and technical project manager to understand the business requirement and translate into technical specs.
Provide analysis reports and estimations.
Design, develop, install, test and maintain data integrations from a variety of formats including files, database extracts and external APIs into data stores (including Snowflake, Elastic, S3, etc) using ETL tools, techniques and programming languages like Python, Spark, SQL, etc.
Build high-performance data engineering algorithms and prototypes.
Create flexible data models, tune queries and ETL components.
Manage job orchestration using tools like Airflow.
Research possible customization for tuning, cost optimization, performance enhancements, data reliability and quality.
Ensure that all solutions meet the business/company requirements for solution data reliability, quality and disaster recovery.
Own the application/data end-to-end from requirements to post production working closely with other teams. Provide engineering leadership by actively advocating best practices and standards for software engineering.
Collaborate with other team members such as data architects, data scientists etc.
Consistently contribute into the project management practices using Agile method.
Present the prototype to the stakeholders and leadership.
The Essentials
Bachelors degree in Computer Science or related field.
Minimum of 5 years of data analytics/data engineering, complex ETL/ELT experience in database systems like Snowflake, Teradata, etc. using Python.
Minimum 5 years of experience in using SQL/NoSQL, JSON and XML data structures.
Minimum 5 years of big data technologies including Hadoop, Apache Spark, Snowflake and AWS Suite of technologies (S3, EMR, Lambda).
Minimum 3 years of Experience using Restful APIs for ETL purposes.
Expert problem solver with strong analytical skills.
Expert in SQL (Snowflake, Teradata) and Python.
Expert in ETL/ELT tools and techniques.
Experience using big data tools (Hadoop, Map-reduce, Elastic search, Kinesis, Kafka, Solr).
Experience using AWS technologies (EMR, S3, Kinesis, Lambda).
Experience using Restful APIs for ETL purposes.
Experience using Git or SVN and Jira.
Experience using Spark (Scala/Java), Spark SQL and Spark Streaming is a plus.
Experience in Segments, MParticle, Adobe Site Catalyst or any other similar digital analytics product products is a plus.
Experience in Tableau is a plus.
Strong communication skills and proficient in Excel, Word, PowerPoint, MS Project and MS Visio Snowflake.
Ability to work independently or collaboratively.
Detail oriented with strong organization and prioritization skills.
Entertainment and/or Social Media experience a plus
Demonstrated ability to work well under time constraints.
Must be able to work flexible hours, including possible overtime, when necessary.
Must be able to maintain confidentiality.
Management has the right to add or change duties and job requirements at any time.
177552"
Data Engineer,"Greetings from Trovetechs!!!

Â

We have an immediate need for Data Engineer Role @ Universal City, CA. Please find the below Job Description for your kind reference.

Â

Duration: Long term Contract

Experience: 8+ Years

Rate: DOE$/hr.

Â

Deploy and maintain data pipelines
Assemble large, complex data sets
Build optimal ETL infrastructure using AWS offerings
Build high performance, fault-tolerant, and scalable systems
Collaborate and coordinate with other teams and parts of the business
Communicate technical concepts to non-technical stakeholders
Strong Python programming skills
Strong hands-on Spark programming experience
Strong SQL coding abilityÂ
Redshift experience
Strong experience with AWS Services, well versed with various AWS ETL Services (EMR, Glue, etc.), RDS, AWS Lambda, etc. (Preferred)
Work experience with Databricks is highly desirable
Â

If you are interested, please send us your updated resume along with best Time & Number to reach you ASAP.

Â

Thanks and look forward to working with you,

Â"
Data Engineer,"The Enterprise Data Intelligence Team of Cedars-Sinai is looking for an experienced Data Engineer/Data Intelligence Analyst. The ideal candidate is passionate about data, coding, technology and will utilize their skills to help solve complex healthcare questions. The candidate should possess a data engineering background, business acumen to think strategically and love working with people. Within this role, the candidate will experience a wide range of problem solving situations requiring extensive use of data collection and analysis. The successful candidate will work with Data Engineers, Data Scientists, Business Analysts, Doctors, Nurses and other stakeholders across organization.
Summary of Essential Job Duties:
Expert Oracle Query and Development Skills (Extensive RedShift skills a plus).
Strong AWS Cloud Platform tools: S3, Kinesis, RedShift.
Familiar with Dashboarding tools like Tableau/Qlik.
Good Linux, Python, Bash skills.
Desire to learn new tools, techniques and solve data challenges.
Educational Requirements:
Four (4) year degree in Computer Science or similar experience.

Experience:
Hospital Based Healthcare experience, especially Epic Clarity data model is a plus.
Working Title: Data Engineer
Department: EIS Data Analytics Team
Business Entity: Corporate Services
City: Los Angeles
Job Category: Information Technology
Job Specialty: Business Intelligence/Reporting
Position Type: Full-time
Shift Length: 8 hour shift
Shift Type: Day"
Data Engineer,"The Enterprise Data Intelligence Team of Cedars-Sinai is looking for an experienced Data Engineer/Data Intelligence Analyst. The ideal candidate is passionate about data, coding, technology and will utilize their skills to help solve complex healthcare questions. The candidate should possess a data engineering background, business acumen to think strategically and love working with people. Within this role, the candidate will experience a wide range of problem solving situations requiring extensive use of data collection and analysis. The successful candidate will work with Data Engineers, Data Scientists, Business Analysts, Doctors, Nurses and other stakeholders across organization.
Summary of Essential Job Duties:
Expert Oracle Query and Development Skills (Extensive RedShift skills a plus).
Strong AWS Cloud Platform tools: S3, Kinesis, RedShift.
Familiar with Dashboarding tools like Tableau/Qlik.
Good Linux, Python, Bash skills.
Desire to learn new tools, techniques and solve data challenges.
Educational Requirements:
Four (4) year degree in Computer Science or similar experience.

Experience:
Hospital Based Healthcare experience, especially Epic Clarity data model is a plus.
Working Title: Data Engineer
Department: EIS Data Analytics Team
Business Entity: Corporate Services
City: Los Angeles
Job Category: Information Technology
Job Specialty: Business Intelligence/Reporting
Position Type: Full-time
Shift Length: 8 hour shift
Shift Type: Day"
Data Engineer,"Company Overview:
Age of Learning is a leading education technology innovator based in Glendale, California, with a talented team of 500+ individuals comprised of nationally-renowned educators, curriculum experts, designers, animators, engineers, and more. We develop engaging, effective digital learning technology and content to help children build a strong academic foundation for lifelong success.
Our flagship product ABCmouse.com Early Learning Academy® is a comprehensive online curriculum and the #1 digital learning product for young children. To-date, more than 30 million children worldwide have completed over 6 billion Learning Activities on ABCmouse. We recently launched Adventure Academy, the first massively multiplayer online (MMO) game designed specifically to help elementary- and middle-school-aged children learn. It features thousands of engaging Learning Activities—including minigames, books, original animated and live action series, and more—in a fun and safe virtual world. Other Age of Learning programs include immersive English language learning products for children in China and Japan; ReadingIQ, a digital library and literacy platform; and a groundbreaking personalized, adaptive digital learning system that individualizes math instruction for every child through AI-driven technology.

We are committed to helping all children succeed. We provide our educational programs at no cost to teachers, Head Start programs, public libraries, and other community organizations, and have served millions of children through these initiatives. We recently established the Age of Learning Foundation to expand this work globally.
As we expand our global reach and increase the educational impact of our programs, we’re looking for passionate, ambitious, and collaborative leaders to become a part of our growing team.


Summary:
We are seeking a full-time, in-house Senior Data Engineer to join our development team. This person will be helping us develop high performance, high-throughput services using modern technologies and techniques.
Responsibilities:
Design, develop, test, implement and support applications using custom ETL (Extract Transform Load) or open source tools such as Talend
Prepare high-level component architecture; design documents, data flow diagrams, detail design documents, data schema and modeling combined with test plan documents
Design, develop and test highly available and scalable data pipelines and relevant data storage systems to enable business success across a multi-product functionality
Proactively identify operational and systemic issues within the data supply value chain (from collection to processing to reporting) and work with production operations (DevOps) team to implement monitoring solutions
Ensure testing and validation best practices are followed across the team so that accuracy of data transformations and data verification are complete and documented
Execute in a fast-paced matrix organization across product and engineering teams to identify best data-driven solutions for the underlying data infrastructure and platform
Ensure high operational efficiency and quality of your solutions to meet SLA (Service Level Agreement) and support commitment to stakeholders (both internal and external)
Be an active participant and advocate of agile/scrum practice to ensure health and process improvements for your team
Required Qualifications:
Experience designing and building large, scalable data systems, preferably across a multi-product portfolio
Strong SQL skills with proven ability to write complex data queries across large data sets.
Exposure to software development, preferably in an Agile/Scrum/Kanban environment across multiple products
Experience analyzing and manipulating data across diverse data sources (Python, Scala)
Experience working across AWS (Amazon Web Services) cloud environment (EC2, S3, RDS, Sagemaker)
Strong exposure to Big Data technology, preferably across a containerized environment (Hadoop, Spark, Hive, Presto)
Experience with sourcing and modeling data from Restful API (Application Programming Interface)
Strong attention to detail with excellent analytical, problem-solving, and communication skills
Bachelor’s degree in Computer Science, Computer Engineering, or Information Technology or a related field, or an equivalent combination of education and experience
Exemplary communication skills (both written and verbal), with experience producing technical and design documentation of complex processes
Good time management and ability to work on concurrent assignments with different priorities
Ability to work in a fast paced, iterative development environment with short turnaround times
Preferred Qualifications:
Experience with A/B Testing and related optimization across desktop and mobile in a digital environment a plus (examples include: Optimizely, Leanplum, deltaDNA)
Experience analyzing and manipulating data across several data formats (JSON, Avro, Parquet, ORC)
Experience building and architecting data warehouse workflows in large cloud-based production environments (Snowflake is an example)
Understanding of columnar data warehouse solutions (Redshift, Vertica)
Experience migrating on-prem data solutions to the cloud with a strong data operational hygiene
Experience developing and maintaining metadata catalogue APIs across a variety of data sources (AWS Glue, Metacat)
Prior experience working in educational technology companies or a related competitive landscape is a plus
We Provide:
Medical, Dental, Vision + 401k
Highly competitive PTO policy
Casual Dress Code
Snacks + Drinks (Coca Cola Freestyle Machine)
Gaming room including an Arcade (2,000+ games)
Frequent team and company outings
Limitless opportunities for professional growth!"
Data Engineer,"Job DescriptionTechStyle Fashion Group is looking for a Data Engineer.How Do You Fit In?As the Data Engineer, you will take ownership of Development oversight, guidance, and direction setting for the data pipeline as well as the Data Management platforms. The right candidate would be a self-motivated, highly detail-oriented team-player with a positive drive to strategize and implement BI Solutions that enable the business to derive valuable insights. You will join a tight knit group of key contributors who are actively working together to achieve aggressive goals and meet timelines to drive the business forward. This role is critical in laying the foundation for the key Decision support systems that will form the backbone of our Big Data ecosystem.This position will report to the Director of Data AnalyticsResponsibilities:* Design, Build and Maintain hundreds of data pipelines using Airflow (Python)+ SQL(Snowflake) to extract, transform, clean, audit and move data from internal or external systems into a Cloud Based Data Warehouse (Snowflake)* Work with Data / Data Warehouse Architect to develop data warehouse models, design specifications, metadata process and documentation. Develop detailed ETL specifications based on business requirements* Interface with other technology teams to develop/maintain the ETL footprint and quality from a wide variety of in-house and 3rd party data sources* Implement and monitor machine learning algorithms and solutions in production.* Constantly Monitor, refine and maintain system performance and provide statistical reporting* Participate in cross-functional meetings to review business requirements/use stories, assist in fit/gap analysis and provide detailed technical design documentation* Partner with business users, senior architects, product managers, engineering teams, and other teams to deliver a robust data services platform* Diagnose ETL and database related issues, perform root cause analysis(RCA), and recommend corrective actions to management* Recommend ways to improve data reliability, efficiency and quality* Profile and understand the large amounts of source data available, including structured and semi-structured/web/mobile activity data* Mentor the team on the industry standards and best practices for effective use of data integration and data quality technologies and exception handling* Provide cross organizational business stakeholders operational support on existing and newly developed data pipelineRequired Skills:* 3+ years of data engineering experience with high performance Big Data platforms including cloud-based Data Warehousing on large scale development efforts leveraging industry standard ETL tools* 3+ years of experience creating and managing data pipelines using Python (experience with Airflow preferred)* 3+ years of Data Warehouse development for 100's of gigabytes of data and billions of records (Snowflake or Redshift is preferred)* Experience developing ELT pipelines using Snowflake, Terradata, Vertica, Redshift or similar data warehousing technologies* Experience implementing streaming pipelines (Kinesis, Kafka, Storm, Spark, Flink)* Experience working in an environment that ingests large amounts of raw data (web logs, Click stream, data feeds)* Experience with source code management using Git or Subversion and release processes.* Experience with scalable systems in a load balanced environment and experience conducting load tests* Ability to extend your scope into the Analytics domain and partner with that team to optimize the output of the Analytics function* Ability to create and interact with very large data processing pipelines, distributed data stores, and distributed file systems* Experience with Spark or Databricks in a production setting is a plus* Ability to learn quickly and multi-task in a fast-paced, dynamic environment* Understanding of data warehouse architectures (Kimball a plus) & Strong metadata modeling experience* Experience with EDA (Exploratory Data Analysis) and Data Visualization a plus* E-commerce or retail or internet experience a plusTechStyle is an Equal Opportunity Employer: M/F/PV/D (minority, female, protected veteran, disability)"
Data Engineer,"The Business

GradTests.com is a leading provider of practice psychometric tests to graduates, students and young professionals.

The Role

The core purpose of the role is to make high quality, high availability, accurate data available for our data analysts and data scientists to do their analysis, derive their insights and build their models. You are the Scotty Pippin to the Michael Jordans. You are the Xavi to the Messis.

You'll do things like:
Ensure our data warehouse is well structured, running smoothly and efficiently for all business intelligence
Set up and maintain various data pipelines used for customer analytics, marketing analytics and product analytics
Skills and experience

Non negotiables:
SQL
Python
Strong knowledge of traditional relational databases - we don't mind which
Some experience with cloud technologies - again we don't mind if it's AWS, GCP or Azure
Experience in any streaming technology
Great experience in using third party APIs at scale
Some web scraping experience
An obsession with data quality
Strong communication skills
Nice to haves:
Experience in working with analysts
Any basic knowledge of advanced analytics techniques
Experience in a visualisation tool like Tableau
Job Types: Full-time, Contract

Salary: $100,000.00 /year

Work Remotely:
Yes"
Data Engineer,"We are looking to bring on a Senior Data Engineer to join their team. You will be responsible for designing, training and delivering machine learning solutions to a myriad of datasets. You will also formulate learning algorithms and implement new ones as well as deliver ML solutions to some of the most challenging IoT problems. If you’re looking to work in a collaborative work environment with some of the brightest minds in the industry, this would be a great opportunity.

Required Skills & Experience

3+ years in application of ML algorithms and deep learning
2+ years using either CNN, LSTM or RNN
Experience using TensorFlow, Keras, PyTorch
Experience with Python, Java and Matlab
Experience using AWS (EC2, ECS, Lambda, EKS)

Desired Skills & Experience

Familiarty with Graph-based Machine Learning
Familiarity with high impact publications

What You Will Be Doing

Tech Breakdown
75% Data analysis
25% AWS cloud services
Daily Responsibilities
100% Analysis
The Offer
Competitive Salary: Open

Applicants must be currently authorized to work in the United States on a full-time basis now and in the future."
Data Engineer,"Company Mission and Highlights:

mPulse Mobile, the leader in mobile health engagement, drives improved health outcomes and business efficiencies by engaging individuals with tailored and meaningful dialogue. mPulse Mobile combines technology, analytics and industry expertise that helps healthcare organizations activate their consumers to adopt healthy behaviors. With 9 years, 60+ healthcare customers, and more than a hundred million messages sent annually, mPulse Mobile has the data, the experience and the technology to drive healthy behavior change.

Our Core Values:
Model Integrity and Collaboration
Drive Innovation and Thought Leadership
Support Decision Making at All Levels
Create Value for Clients by Empowering Consumers
Improve Customer Experience Through Simple Design
Celebrate Success Often
Purpose of the Role:

The mission of the Data Science and Analytics (DSA) team at mPulse is to uncover insights from data in order to help drive better patient engagement and health outcomes. We are looking at everything from tactical optimizations to broad level strategic direction that is grounded in data evidence and heavy analytical rigor.

This requires a multidisciplinary blend of data science, behavioral science, and business strategy, all applied in tandem to discover key insights that lie hidden in our data sets.The Data Engineer will help build a research-based and data-driven approach to optimize mobile customer engagement. This role will focus on deep diving into a broad variety of exploratory initiatives to improve segmentation, tailoring and personalization of mobile engagement.

Duties and Responsibilities:
Working with the data science and data analytics team to refine and develop data science and analytics (DSA) product roadmap
Support Redshift cluster management including monitoring, performance tuning, and optimization
Responsible for data loads and data extracts via Airflow DAG and python code.
Engaging in exploratory A/B studies to extract data features and determine the relative value of multiple data types
Building rich and interactive data visualizations to display findings from A/B studies, to guide and inform exploratory data analysis, and to deepen customer engagement
Understanding and applying data mining techniques, including NLP, clustering algorithms and regression analysis to generate deep insight and discover effective solutions to challenging problems
Skills, Abilities, and Experience:
2-5 years of experience in a corporate, start-up, or research environment
2-5 years of experience mentoring data analysts (corporate) or graduate students (academia)
2-5 years of experience in Airflow DAG creation, debugging and maintenance
2-5 years of experience in PostgreSQL and Elasticsearch
Strong background and solid skills in interactive data visualization (Tableau, Django, Shiny, D3.js)
Experience in research methods, exploratory data analysis, and machine learning
Intense intellectual curiosity strong desire to always be learning
Analytical, creative, and innovative approach to solving difficult problems
Minimum Qualifications:
4 year BS/ BA Degree in Computer Science, Computer Engineering or other related field
2 years of direct experience as a data engineer or working directing in data engineering / data science.
1-2 years of experience with Python (Pandas, NumPy, sciKit-learn), SQL and R
*Please note, due to the requirements of this position, responses may automatically disqualify you from moving forward in the application process. Please review minimum qualifications thoroughly before applying.

Behavioral Competencies:
Customer Focused
Attention to Detail
Independent Self-Starter
Highly Organized
Critical Thinker
Problem Solver
Excellent Communicator
Ability to Prioritize
Team Work & Collaboration
Multi-Tasker with Strong Sense of Urgency
The Perks:
Enjoy Flexible PTO and flexible work hours
Full Vision, Dental and Healthcare - all individual premiums paid by mPulse!
401K Program with a 4% match
3 Weeks Paid Maternity/Paternity Leave
Weekly team lunches to celebrate victories
Paid Parking as well as Car Pooling incentives
Laptop fitness stations
Ping pong conference table and Foosball
Free snacks and drinks
Powered by JazzHR"
Data Engineer,"Altruist is an LA fintech company on a mission to make financial advice fair for everyone. We’re hiring a talented front end developer with a passion for startup culture. If you have a background in fintech with an understanding of the complexities around security, high availability, and accuracy of building systems, we’re especially excited to hear from you.

Our team is solving a huge problem that requires big-picture thinkers who are motivated to make a real difference through their work.

What you’ll do:
Work with product and technology teams to understand different entities and objects
Define a model to store source data that feeds into a transactional system and analytics platform
Work with content acquisition team in building partnerships with different data providers to enable integration
Create and maintain multiple pipeline architectures
What you’ll need:
10+ years of experience building scalable, modular data lakes/data warehouse or 8+ years of experience building large scale data warehouse systems
Degree in Computer Science with specialization in Analytics, Information System etc.
Strong data modeling skills
Strong analytical skills working with unstructured data
Advanced knowledge in SQL and scripting
Experience with big data tools: EMR, Hadoop, Spark etc.
Experience with Relational and No SQL: Cassandra & Postgres etc.
Experience with workflow management tool: Airflow or something similar
Experience with AWS cloud service: EMR, EC2, ECS etc.
Experience with object-oriented programming: Java, Python, Scala, R
Bonus Points
Experience with BI tools like Tableau/Domo or Looker
Worked in a fast-paced startup environment and has a track record of building scalable infrastructure and analytics platform
What you’ll get

At Altruist, you’ll work with a talented group of hungry creators looking to disrupt our industry. You’ll be given the freedom to do your best work alongside down-to-earth developers, designers, and thinkers who are at the leading edge of their discipline.

We offer top-of-the-line health benefits, 401(k) with employer match, a competitive salary, and a flexible schedule including unlimited vacation time. You’ll join on the ground floor and have the opportunity to make your mark and have a real say in how we build products. Our office is located in the heart of Venice on Abbot Kinney.

You’ll join a proven founding team that is backed by one of the most storied VCs in the US: Venrock. Together, we’re going to try to fix the broken parts of finance and investing."
Data Engineer,"Who are we?Sense360 is a Series A startup that is funded by prominent VC funds such as FirstMark Capital (Airbnb, Pinterest, Riot Games) and Upfront Ventures (Maker, TrueCar). Our data insights have been referenced by many publications including Bloomberg and CNBC. Sense360 was founded by successful repeat entrepreneurs.We are transforming the way businesses make decisions by combining massive, disparate datasets and turning them into accessible, actionable, and accurate insights that drive the strategic decisions that brands make. See our Culture Deck here.What will you do?As a data engineer you will working on our data pipeline that processes 1.6+ TB sensor data/day and that is built using Spark, AWS, Python, Airflow and Rails. Below is a list of the major initiatives that you will be helping with.Scale our data pipeline to handle 5MM+ usersResearch, analyze and integrate new 3rd party datasetsMove our system from batch-based to real-timeWork closely with our data science team to build model building platformsBuild new data delivery mechanisms for our clientsWhat do we look for?Engineers who thrive at Sense360 have a few key traits:Care about their work, their team and their companyHeavy bias towards delivering valueExpect their ideas to be challenged because they believe that the best ideas can come from anywhereView feedback as a gift that they give and receiveFun and interestingQualifications3+ years experienceexperience with a distributed processing technology (e.g. Spark, Storm, Presto, Hadoop, Samza, Flink, etc)solid CS and testing fundamentalsexperience working in a startup or an extremely strong desire to do soWilling to relocate if already within the US and sponsor visas."
Data Engineer,"Team and Role Overview

Relativity is looking for a talented data engineer to join the multifaceted Additive Manufacturing team comprised of software, automation and computer vision engineers who work closely with welding engineers. Analysis using this data will be at the heart of what enables us to disrupt both the Aerospace and Additive Manufacturing industries.

The Mission/Outcomes and Objectives

To support our mission to reimagine the way rockets are built and flown you will be working with our Additive Manufacturing team in order to create data pipelines that feed how we develop, debug, analyze, and predict the behavior of our 3D printers. This means that you will be using and building software that runs in real-time to manage data from robots, sensors, cameras, and software systems.

Candidate Profile

You have ideally designed and built large-scale data processing architectures, where your software engineering skills have been used to marry systems together in an efficient manner. You have also worked in multi-disciplinary environments requiring collaboration with different teams who have different types of data. At Relativity, you will be responsible for building out and maintaining software that is critical to our autonomous 3D printing process. This role requires that you are able to follow agile development practices and understand how to implement quality software that is continuously integrated and deployable to our production cells. You are excited to solve complex problems to which you can offer elegant solutions.

Minimum Required Skills and Competencies
Bachelor's in Computer Science or related technical field and 5+ years of experience in software development
Expert in algorithms and data structures
Expert in one or more object-oriented languages like C++, C#, Python, and Java
Experience designing and building large scale, high performance data processing pipelines and data warehousing
Experience with time-series, SQL, and NoSQL databases, and handling of multimedia data types
Experience using streaming engines and message queuing systems like Apache Kafka, Spark
Experience with database administration/configurations
Experience with Continuous Integration and Agile Development
Preferred Skills and Competencies
Master's in Computer Science or related technical field and 7+ years of experience in software development
Experience with big data systems like , Hive, MapReduce, Cassandra, BigTable or similar
Experience visualizing time series data and building user interfaces
This position must meet Export Control compliance requirements, therefore a United States Person as defined by 22 C.F.R. § 120.15 is required.

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
Data Engineer,"Senior Data Engineer

Location/City: Playa Vista, CA or Any Where in the Unites States

Responsibilities

Steady is seeking sharp, highly-motivated data engineers who are excited to play a major role in our product and engineering evolution. We are looking for product focused, results-oriented engineers who thrive in a collaborative, team-focused culture. You will work closely with data scientists, analysts, product managers, business stakeholders, and your team to help define, implement, and ship significant increments to the data infrastructure that powers Steady's mission-driven product offering.

As a Senior Data Engineer, you will:
Work across all phases of the software development lifecycle in a cross-functional, agile development team setting
Collaborate with data scientists and analysts to prepare complex data sets that can be used to solve difficult problems
Administer, maintain, and improve Steady's data infrastructure and data processing pipeline, including ETL jobs, events processing, and job monitoring and alerting.
Deliver high-quality, well-tested technical solutions that make sense for the problem at hand
Fearlessly work across components, services, and concerns to deliver business value
Partner with engineers, data scientists, and the CDO to define and refine our data architecture and technology choices
Help define, implement, and reinforce data engineering best practices and processes
Contribute to Steady's technical vision
Skills & Requirements
Significant data engineering and/or software development experience (5-7 years minimum)
Experience with ingesting, processing, and transforming data at scale
Demonstrated proficiency with SQL, relational database, and data warehousing concepts
Demonstrated aptitude with ETL concepts and tools such as Airflow or AWS Glue
Experience of the AWS data ecosystem (Glue, Kinesis, S3, Lambda, EMR, Redshift, etc.)
Understanding of event-driven and/or streaming workflows with tools like Kafka and Spark
Experience administering cloud-based analytics databases (like Snowflake or Redshift)
Knowledge of Python, R or other common languages used in data science
Ability to thrive in a fast-paced and dynamic environment
Ability to work well in teams of all sizes with representatives from a diverse set of technical backgrounds.
Preferred: Experience with infrastructure automation through tools like Terraform or CloudFormation
Preferred: Experience with indexing and search technologies like elasticsearch, SOLR, etc.
Preferred: Experience building or maintaining a data science modeling environment such as Sagemaker or Databricks, including deployment and monitoring using tools like MLFlow
Bachelor's or Master's degree in Computer Science or equivalent experience
Benefits

We value our Steady Team Members and are committed to their success. We are located in downtown Atlanta near MARTA, and in the heart of Playa Vista, CA. We offer competitive compensation packages and a 100% company-paid benefits package including medical, dental, and vision. Steady is a relaxed, casual work environment with flexible hours in addition to a weekly meal stipend. We also offer a generous PTO plan, 401K, and future growth opportunities within the company. We strive to maintain a positive and fun environment for our employees where people can learn and grow with the company.

About Steady

Steady is a mission-driven business that helps people earn more. Through the free Steady app users can build & track income, networks, and buying power allowing them to augment retirement savings, work around childcare responsibilities, pay down debt, save for purchases, and supplement their income from primary employers, while limiting their income volatility and better positioning themselves for access to a sustainable financial services ecosystem.

Every Steady member will build something different. We celebrate those who take the initiative to create a path that works for them. Whatever that looks like, Steady is there to support it, providing a stable foundation that allows people to build financial success over a lifetime.

Mission: Steady turns the tables by putting workers back in charge of their financial future with an income-building platform that creates financial stability and helps its users to take charge of their future.

For more information, visit us at www.steadyapp.com or on Twitter @TheSteadyApp.

Steady, Platform Inc. (Steady) is an Equal Employment Opportunity employer All qualified applicants/employees will receive consideration for employment without regard to that individual's age, race, color, religion or creed, national origin or ancestry, sex (including pregnancy), sexual orientation, gender, gender identity, physical or mental disability, veteran status, genetic information, ethnicity, citizenship, or any other characteristic protected by law.

Statement to ALL Third-Party Agencies and Similar Organizations: Steady accepts resumes only from agencies with which we formally engage their services. Please do not forward resumes to our applicant tracking system, Steady employees, Steady hiring manager, or send to any Steady facility. Steady is not responsible for any fees or charges associated with unsolicited resumes."
Data Engineer,"A growing, well backed startup in El Segundo is currently looking to bring on a Data Engineer with a background in software development to their small team of 6. Working closely with product manager, engineers, and business stakeholders, you will work to build and optimize ETL processes using Python, AWS, and SQL.Required Skills & Experience* At least 3 years of experience with ETL development* Python or Java backend development experience* SQL, RDBMS, and data modeling experience* Preferably experience working in an AWS environmentThe Offer* Competitive Salary: Up to $160,000/year, DOEYou will receive the following benefits:* Comprehensive health, dental, and vision with lots of options* Lots of PTO* 401k and equity packages* Flexible schedule and work from home* Summer hours* Growth opportunity!Applicants must be currently authorized to work in the United States on a full-time basis now and in the future.Jobspring Partners, part of the Motion Recruitment network, provides IT Staffing Solutions (Contract, Contract-to-Hire, and Direct Hire) across 10 major North American markets. Our unique expertise in today's highest demand tech skill sets, paired with our deep networks and knowledge of our local technology markets, results in an exemplary track record with candidates and clients."
Data Engineer,"Job Description
Our client is searching for a Data Engineer to join their team. The Data Engineer will be responsible for creating solutions, comprehensive analytics and will be integral in implementing company-wide data strategy.

Responsibilities:
Collaborate with other departments to understand data needs
Develop and optimize ETL processes
Optimize data warehouse by defining technical requirements
Improve and maintain data access for our BI tools
Auditing and automating data quality
Work closely with development teams to create long term plans for problem resolution
Requirements:
Bachelors Degree or greater (technical or science degree preferred).
2 years of experience in the following:
scalable architectures and large data processing
AWS, Linux and Shell Scripting
ETL and data mining
open source programming language
data warehousing concepts
SQL, and System-level DBA functions ex: configuring replication, automating backups, performance tuning and RDBMS"
Data Engineer,"Responsibilities:* Responsible for building and maintaining the machine learning data and development platform.* Build, integrate and deploy machine learning solutions into the BlackLine application in collaboration with product management, cloud, engineering and data science teams.* Create and maintain scalable data pipeline in the cloud (AWS and GCP).* Assemble large, complex data sets that meet functional / non-functional business requirements.* Identify, design, and implement processes automation and data delivery.* Build infrastructure for optimal extraction, transformation, and loading of data from a wide variety of data sources.* Execute extract, transform and load (ETL) operations on large datasets including data identification, mapping, aggregation, conditioning, cleansing, and analyzing.* Build analytics tools to provide actionable insights into business and product performance.* Keep data separated, isolated and secured.* Assist data scientists in implementing achine learning algorithms and contribute to building and optimizing our product into an innovative industry leader.* Participate in establishing best practices while team is transitioning to new technologies, tools and infrastructure. Maintain specifications and metadata; follow the best practices.* Recommend and implement process improvements.* Maintain specifications and metadata; follow and develop best practices.* Coach and technically train data analysts, if needed.Qualifications:* 5+ years as a data engineer.* Experience with SQL, Python, R languages.* ETL experience using Python.* Experience with Hadoop, Spark, Hive. Presto is a plus.* Practical experience with GIT version control.* Strong familiarity with GCP, AWS, SQL Server.* Comfortable working with open source tools in Unix/Linux environments.* Data warehousing experience, data modeling and database design.* Experience with machine learning packages and various ML algorithms.* Experience with predictive and prescriptive analytics, modeling, and segmentation.* Experience with data analytics, big data, and analytics architectures.* Comfortable handling large amounts of data.* Experience ensuring data and modeling accuracy, cleanliness, reliability.* Works independently without the need for supervision.* Experience translating business requirements into functional, and non-functional requirements.* Strong sense systems and data ownership."
Data Engineer,"Minimum 5 years of data engineering, ETL or software engineering experience with a focus on data extraction, transformation and publishing.
Experience creating and maintaining automated data pipelines, data standards, and best practices to maintain integrity and security of the data; ensure adherence to developed standards.
Experience in relevant technical languages and tools/technologies such as SQL, Python, REST APIs, Airflow, Spark, AWS
Work closely with data architect to design and implement data solutions.
Develop automated pipelines for ingesting, cleansing and publishing data.
Develop extracts from REST API sources in a recoverable manner.
Support existing solutions for Digital Application analytics."
Data Engineer,"Job_Description

TechStyle Fashion Group is looking for a Data Engineer.

How Do You Fit In?

As the Data Engineer, you will take ownership of Development oversight,
guidance, and direction setting for the data pipeline as well as the Data
Management platforms. The right candidate would be a self-motivated, highly
detail-oriented team-player with a positive drive to strategize and implement
BI Solutions that enable the business to derive valuable insights. You will
join a tight knit group of key contributors who are actively working together
to achieve aggressive goals and meet timelines to drive the business forward.
This role is critical in laying the foundation for the key Decision support
systems that will form the backbone of our Big Data ecosystem.

This position will report to the Director of Data Analytics

Responsibilities
Design, Build and Maintain hundreds of data pipelines using Airflow
(Python)+ SQL(Snowflake) to extract, transform, clean, audit and move
data from internal or external systems into a Cloud Based Data Warehouse
(Snowflake)
Work with Data / Data Warehouse Architect to develop data warehouse
models, design specifications, metadata process and documentation.
Develop detailed ETL specifications based on business requirements
Interface with other technology teams to develop/maintain the ETL
footprint and quality from a wide variety of in-house and 3rd party data
sources
Implement and monitor machine learning algorithms and solutions in
production.
Constantly Monitor, refine and maintain system performance and provide
statistical reporting
Participate in cross-functional meetings to review business requirements/
use stories, assist in fit/gap analysis and provide detailed technical
design documentation
Partner with business users, senior architects, product managers,
engineering teams, and other teams to deliver a robust data services
platform
Diagnose ETL and database related issues, perform root cause analysis
(RCA), and recommend corrective actions to management
Recommend ways to improve data reliability, efficiency and quality
Profile and understand the large amounts of source data available,
including structured and semi-structured/web/mobile activity data
Mentor the team on the industry standards and best practices for
effective use of data integration and data quality technologies and
exception handling
Provide cross organizational business stakeholders operational support on
existing and newly developed data pipeline

Required_Skills
3+ years of data engineering experience with high performance Big Data
platforms including cloud-based Data Warehousing on large scale
development efforts leveraging industry standard ETL tools
3+ years of experience creating and managing data pipelines using Python
(experience with Airflow preferred)
3+ years of Data Warehouse development for 100 s of gigabytes of data
and billions of records (Snowflake or Redshift is preferred)
Experience developing ELT pipelines using Snowflake, Terradata, Vertica,
Redshift or similar data warehousing technologies
Experience implementing streaming pipelines (Kinesis, Kafka, Storm,
Spark, Flink)
Experience working in an environment that ingests large amounts of raw
data (web logs, Click stream, data feeds)
Experience with source code management using Git or Subversion and
release processes.
Experience with scalable systems in a load balanced environment and
experience conducting load tests
Ability to extend your scope into the Analytics domain and partner with
that team to optimize the output of the Analytics function
Ability to create and interact with very large data processing pipelines,
distributed data stores, and distributed file systems
Experience with Spark or Databricks in a production setting is a plus
Ability to learn quickly and multi-task in a fast-paced, dynamic
environment
Understanding of data warehouse architectures (Kimball a plus) & Strong
metadata modeling experience
Experience with EDA (Exploratory Data Analysis) and Data Visualization a
plus
E-commerce or retail or internet experience a plus
TechStyle is an Equal Opportunity Employer: M/F/PV/D (minority, female,
protected veteran, disability)
Show moreShow less"
Data Engineer,"As a Senior Data Engineer at The RealReal, you'll drive automation, personalization and data pipeline initiatives. You will own critical systems throughout the platform and be responsible for their development, adding new, valuable features and ensuring that these systems perform correctly.What You Get To Do Every Day* Collaborate with senior management, product management, and other engineers in the development of data products* Develop reliable, near real time and batch data pipelines that make data easily consumable by end users and other systems* Develop tools to monitor, debug, and analyze data pipelines* Design and implement data schemas and models that can scale* Mentor team members to build the company's overall expertise* Work to make The RealReal an innovator in the space by bringing passion and new ideas to work every dayWhat You Bring To The Role* Ability to communicate effectively with stakeholders to define requirements* Strong knowledge of Python & SQL* Experience with Airflow* Experience with cloud platforms (GCP, AWS, AZURE) with strong preference to GCP* Experience with BigQuery or similar (Redshift, Snowflake, other MPP databases)* Experience building data pipelines & ETL* Experience with command line, version control software (Git)* E-commerce experience* Startup experience* Experience with data streaming such as Apache Kafka, AWS kinesis, Spark Streaming or similar tools* Experience with many other big data technologies at scale* Experience with ETL tools such as segment* Experience with BI tools such as LookerThe RealReal is the world's largest online marketplace for authenticated, consigned luxury goods. With a rigorous authentication process overseen by experts, The RealReal provides a safe and reliable platform for consumers to buy and sell their luxury items. We have 150+ in-house gemologists, horologists and brand authenticators who inspect thousands of items each day. As a sustainable company, we give new life to pieces by hundreds of brands, from Gucci to Cartier, supporting the circular economy. We make consigning effortless with free in-home pickup, drop-off service and direct shipping for individual consignors and estates. At our stores in LA, NYC and San Francisco, customers can shop, consign, and meet with our experts. At our 10 Luxury Consignment Offices, four of which are in our retail stores, our expert staff provides free valuations. Founded in 2011 and listed publicly in 2019 (Nasdaq: REAL), we're growing fast and fundamentally changing the way people buy and sell luxury - a multi-billion dollar industry. Build your career with us and enjoy 401K matching, health, dental and vision insurance, commuter flex spending, healthcare flex spending, generous PTO, a mother's room, and flexible work hours!The RealReal is committed to providing an equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or expression, or Veteran status. We will consider qualified applicants for a position regardless of arrest or conviction records, consistent with legal requirements.#LI-DANGERSIND001"
Data Engineer,"Our company is looking for a Senior Data Engineer to join our team! Our team utilizes and develops cutting-edge technology, in order to deliver unparalleled and creative content around the world. We work across multiple platforms to change the face of media.

Please, only apply if you are able to work directly for a U.S. company for the next three years. We are not currently able to work with C2C, H1, or OPT for this position.

Duties & Responsibilities:
Construct data pipelines orchestration.
Build and optimize performance of Hadoop/Spark batch jobs, Spark, Kafka, Cassandra, ELK, etc.
Build and optimize performance of ElasticSearch cluster and relevance.
Work with cross-functional teams and other software engineers.
Design and architect high quality data-lake, data-warehouse, and data-marts data models.
Facilitate Data Science workflows and advanced machine learning algorithms.
Contribute to Open Source solutions and communities.
Keep updated on current and emerging technologies and tools.
Provide innovative ideas to a larger professional community.
Demonstrate technologies, solutions, and leading practices.
Balance resources, requirements, and complexity.
Qualifications:
BS in Computer Science or related field.
5+ years experience as a Data Engineer.
Passionate about coding. (You will be asked to code before or during the interview.)
Understanding of distributed systems and computation.
Working knowledge of Scala, Java, Python, and Go-Lang.
Working knowledge of data Apache Hadoop/Spark ecosystem (Spark, Hive, Presto, Oozie, Pig, Hue, Zeppelin).
Demonstrated working knowledge of data modeling.
Excellent communication and interpersonal skills.
Knowledge of the following, required:
Unit, Integration, and Load testing.
Developing REST APIs.
Git.
Ant, Maven, SBT, and/or Gradle.
Unix/Linux.
Docker containers building and deployment.
Knowledge of the following, preferred:
GraphQL knowledge
Kubernetes knowledge
Apache Spark MLlib
Apache Spark GraphX
Amazon AWS or other cloud Services
Jenkins
Powered by JazzHR"
Data Engineer,"Job Description
This is a 100% Remote position. Candidate may reside anywhere within the United States

We are currently seeking a Senior Data Engineer to help elevate our communications platform which is being used by the largest companies in the world in some of the most technically complex environments you can find. You will architect and design “big data” systems which require queries returning within sub-second response times. Ready for a challenge?

We are a distributed team. We build solutions for distributed workforces so we model our workforce the same way. In this role you really can work where you want, but for this role we are only considering candidates based in the United States.

Responsibilities
Design systems that reliably and efficiently provide interactive query performance on large amounts of multi-modal data
Build systems that handle scale
Build the infrastructure required for optimal extraction, transformation, and loading of data from a variety of data sources using SQL and AWS ‘big data’ technologies
Collect, parse, analyze, and visualize large sets of data
Turn data into insights
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product
Qualifications
Experience with large-scale data and query optimization techniques
Experience with ETL to data warehouse systems
Experience with AWS cloud services: EC2, RDS, Redshift, Aurora
Expert in SQL, NoSQL, and RDBMS
Knowledge in multiple scripting languages (e.g. Python)
Knowledge of cloud, distributed systems, and stream-processing systems
Passionate about learning new technologies and solving hard problems in a fast-paced environment
The ideal fit…
Has a Computer Science degree
Is a ""student of the game"" and thrives on new challenges
Enjoys learning from teammates, and isn't afraid to teach others at the same time
Sees the glass half-full. This is a new industry space...your vision could make all the difference!
Wants to make a lasting impact and lifelong connections, this is not just another paycheck
Must Haves:
Expert level SQL
Expert level Python
Deep experience with AWS
Experience building SaaS products through data insights
Why Us?

Because you care about people, the work you do, and the connections you make. Work is such a large part of life, it only makes sense to make it awesome.

If you want to engage brilliant minds in a true start-up environment where ideas are rewarded regardless of who they come from, join us. This is a rapidly changing space so if you thrive on ambiguity, are hungry for a challenge, and have the guts to speak your mind it could be a perfect fit.

Come for the challenges, come for engaging people in a casual and friendly environment. Come for the unlimited PTO, the health benefits, the 401k plan, the annual retreats (including family), the twice-a-year hackathons, the 10% exploratory time, the ability to contribute to open source, and the potential to work from anywhere.

Whatever the reason, your new co-workers along with a leadership team who truly believes in your growth both professionally and personally will keep you here.

For immediate consideration, please submit a recent resume that is closely aligned to the description in addition to a detailed cover letter with salary expectations.
Company Description
Together, we can find effective solutions that further your interests. Speak with us about high-volume recruitment for your business! We work with small and large companies, and we maintain up-to-date databases full of qualified candidates. Come to us as a professional or an employer, and be connected with a top-flight company! We specialize in the aviation, manufacturing, health care, and financial services industries.

Work with us on your business or career
Turn to the Southern California Company with a national reach. We're a Woman / Minority Owned Business Enterprise!

We're the experts in high-volume recruitment! Turn to us when you have a need for a large new addition to your workforce. You'll get fast results and superior candidates.

Get scalable solutions for your business

With our stringent examinations, we can ensure that you'll connect only with the most qualified candidates for your positions. Work with us, and let us comb through our constantly up-to-date database to uncover the right fits for any job in any industry. Our professionals are experienced, intelligent, driven, qualified, and ready to help your business from day 1. We have more than 30 years of combined experience!"
Data Engineer,"Job Summary


The J. Paul Getty Trust is looking for an enthusiastic Data Engineer, with the experience and passion to carry out the execution of technical projects to enhance the institution's cultural heritage knowledge bases, through the application of machine learning and other data transformation techniques. Our aim is to provide a deeply connected and consistent experience for scholars, researchers, and enthusiasts as they explore the complex information held across the organization, and your participation is crucial for that to be successful.

You will report to the Enterprise Semantic Architect, and interact with software engineers, data engineers and content specialists in the cultural heritage programs. Your work will improve the quality, reliability, connectedness, and consistency of our data by engineering project-specific data pipelines to produce new knowledge from existing internal and external data sources. You will have a hands-on role with content specialists in the programs, and be responsible for working with them to understand data requirements and then implement those requirements in software.

The initial focus of this role is to assess and gather the necessary data to apply computer vision tools to a collection of more than half a million digitized photographs of paintings, drawings and prints. The aim of this work is to enhance our descriptions of these objects with high confidence metadata generated without expert curatorial intervention. The use of computer vision is a high priority in the institution's digital strategy and the success of this work will lead to further exciting projects that put the developed skills, transformation workflows and systems to good use.

The Getty is among the most prestigious cultural heritage organizations in the world, dedicated to furthering the study of the history of art. You will work on an amazing campus amongst fabulous art, architecture, and information systems, collaborating with world-class scientists, curators, librarians, archivists, and academics. We offer paid vacation, personal and sick leave plus every other Friday off, excellent benefits, and a very strong commitment to balancing work and personal life.

Major Job Responsibilities
With the Semantic Architect, work with technical and content stakeholders to understand data-oriented project requirements
With other Data Engineers, ensure high quality data transformation pipelines can migrate and enhance institutional managed collections
Integrate external content services to enrich and reconcile our datasets
Assess the feasibility of applying data enhancement tools and the quality of their results to determine their suitability for project requirements
Work in an agile way, including supporting testing, continuous integration and deployment
Assist software engineering teams by translating stakeholder requirements into feature requests
Qualifications
Bachelor's degree in a related field or a combination of education and relevant experience
2-5 years software development experience
Knowledge, Skills and Abilities
Experience of data-oriented work within cultural heritage organizations
Attention to detail combined with a focus on data usability
Excellent verbal and written communication skills, especially when interacting with non-technical stakeholders
Proficiency in Python, or willingness to translate experience in equivalent language
Familiarity with machine learning techniques and/or tools
Familiarity with cultural heritage data standards, such as Linked Open Data
Familiarity with engineering tools such as git
Familiarity with test driven and agile software development methodologies"
Data Engineer,"Responsibilities:
Responsible for building and maintaining the machine learning data and development platform.
Build, integrate and deploy machine learning solutions into the BlackLine application in collaboration with product management, cloud, engineering and data science teams.
Create and maintain scalable data pipeline in the cloud (AWS and GCP).
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement processes automation and data delivery.
Build infrastructure for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Execute extract, transform and load (ETL) operations on large datasets including data identification, mapping, aggregation, conditioning, cleansing, and analyzing.
Build analytics tools to provide actionable insights into business and product performance.
Keep data separated, isolated and secured.
Assist data scientists in implementing achine learning algorithms and contribute to building and optimizing our product into an innovative industry leader.
Participate in establishing best practices while team is transitioning to new technologies, tools and infrastructure. Maintain specifications and metadata; follow the best practices.
Recommend and implement process improvements.
Maintain specifications and metadata; follow and develop best practices.
Coach and technically train data analysts, if needed.
Qualifications:
5+ years as a data engineer.
Experience with SQL, Python, R languages.
ETL experience using Python.
Experience with Hadoop, Spark, Hive. Presto is a plus.
Practical experience with GIT version control.
Strong familiarity with GCP, AWS, SQL Server.
Comfortable working with open source tools in Unix/Linux environments.
Data warehousing experience, data modeling and database design.
Experience with machine learning packages and various ML algorithms.
Experience with predictive and prescriptive analytics, modeling, and segmentation.
Experience with data analytics, big data, and analytics architectures.
Comfortable handling large amounts of data.
Experience ensuring data and modeling accuracy, cleanliness, reliability.
Works independently without the need for supervision.
Experience translating business requirements into functional, and non-functional requirements.
Strong sense systems and data ownership."
Data Engineer,"Job Title
Senior Data Engineer

01/15/2020

Job Description
POSITION PURPOSE:
Design, develop and implement data integration components and backend solutions. Support project and enterprise level data strategies by taking lead responsibility for processes in all areas of the data warehouse.

ESSENTIAL FUNCTIONS:
Work closely with Data Analysts and Data Modelers to develop enterprise data integration solutions that promote re-usability and standardization. Prepare and present data collection and analyses for business.
Modify and create backend processes using shell scripts/java/perl/python. Execute all phases of programming activities including program design, coding, debugging, testing, documentation,validation and implementation.
Develop automated data quality checks to audit collected data for completeness, accuracy, and errors. Re-Develop and tune existing Data Warehouse applications to ensure optimum performance. Monitor existing Data Warehouse production processes and resolve issues as needed. Encourage and support less experienced coworkers to promote a team environment.

EDUCATION: Bachelor's Degree

YEARS OF EXPERIENCE: 4-8 Years

Req #
22452BR

Location
LA – World Headquarters

Location - City, Region or Area
Los Angeles, CA"
Data Engineer,"Job Description:

We are looking for a Senior Data Engineer/Architect to join our new Data Center and Networking AI Team to solve some of the hardest and most interesting technological challenges facing the industry today. This is a high visibility, hands-on data engineering lead role.

The Team

You will be joining a new team of world-class high-performing and low ego engineers and scientists that have an entrepreneurial and hacker spirit. The team will operate in a very self-driven, agile and fast-paced environment.

The team operates by self-organizing, around shared values, vision and objectives.

Keys to Hiring

You are the best candidate for this position if you stay up to date with the latest and greatest data engineering innovations and off-the-shelf solutions. You are both an architect and a hands-on data engineer. You enjoy working with and supporting data scientists. You generate new ideas and initiatives to solve problems and identify trends and opportunities. You are comfortable working independently and figuring things out, and also enjoy working collaboratively in a small, fast-paced entrepreneurial environment. You enjoy being challenged and respectfully challenging others.

You write code almost every day and are comfortable working with your team to produce tools, pipelines, packages, modules, features, dashboards, or whatever is needed to move the team forward. You have a hacker’s spirit. You’re a quick study and technically flexible.

The Role

Primary
Build a scalable architecture to support real-time analytics and offline ML model training pipelines.
Build an auto-scalable (Kubernetes) containerized ML microservices for anomaly detection, prediction, forecasting, correlation analysis, etc.
Ensure a modular platform architecture for deployment in public and private clouds.
Build a scalable AI platform to build, train and deploy custom ML models as microservices for real-time predictive intelligence, deep actionable insights for faster RCA.
Use off-the-shelf tools as needed to prevent reinventing the wheel.
Attract and foster a world-class data team
Qualifications
15 years of hands-on experience in data engineering
4+ years of experience leading data engineering teams for AI/ML platforms that serve real-time products
4+ years of experience leading data engineering recruitment, mentoring and conducted hands-on training sessions to data scientists, analysts and BI engineers.
2+ years of experience collaborating to design standards, championing data engineering culture and developing world-class technical talent.
2+ years of experience as a Data Architect for AI/ML platforms and driving engineering roadmap.
Track record of being a hands-on contributor for various data science & engineering projects
Track record of bringing 2 enterprise products to market from conception to deployment
Experience building and delivering a cloud-based analytics solution
Proven track record of meeting aggressive release schedules; demonstrated ability to balance multiple priorities in a complex environment and manage teams to successful project completion.
See what it's like working at our Crimson House West campus in San Mateo, CA!
Sponsorship tag select one!


Employment eligibility to work with Rakuten and it's brands in the U.S. is required as the company will not pursue visa sponsorship for these positions.

Rakuten may choose to consider applicants that require some form of company sponsorship to work in the United States at its sole discretion, based on business needs.

*******************************************************************************

Rakuten is an equal opportunity employer. We do not discriminate based on race, color, ethnicity, ancestry, national origin, religion, sex, gender, gender identity, gender expression, sexual orientation, age, disability, veteran status, genetic information, marital status or any legally protected status.

Women, minorities, individuals with disabilities and protected veterans are encouraged to apply.

Languages:"
Data Engineer,"The Data Team at ReSci is in a new phase of its evolution. We've scaled our predictive models to 350M+ users and we are looking for a Senior Machine Learning Engineer to help us innovate new models and improve our existing algorithms, to take our predictions to the next level! On our Data Team, you will work closely with other Data Scientists and Data Engineers to create the Machine learning backbone that powers our marketing automation platform ""Cortex"" for commerce brands. Your expertise at algorithms will be paired closely with the software expertise of Data Engineers, and you will jointly own these ML systems.

We are looking for an experienced data engineer who is passionate about writing clean, well-tested code. You should want to make a huge impact in a fast-paced and cutting-edge start-up environment. You should have great experience with scala/spark or python/serverless and working closely with ML algorithms and data scientist. You’ll be building robust real-time data pipelines that process billions of events per day and sitting on the team that builds end-to-end ML algorithms. You'll help our data scientists productionize our ML models.

In addition to having meaningful responsibilities and gaining experience in product development, you will also receive comprehensive exposure to all aspects of our business. The code and ideas that you contribute will have a tangible impact on the business as a whole. Your code will touch millions of end-users. You will have full responsibility for your projects and have a real sense of product ownership. You will also have the opportunity to learn tremendously from our awesome team of humble yet super engineers. You'll work with and learn the latest technologies and apply them across our distributed systems.

About You:

5+ years experience working on production data products in Python, Scala, or similar
Proficient in at least one statically typed language
Experience designing and implementing large, scalable services
Passionate about enforcing software engineering principles, production code quality, and regular use of design patterns
Experience interfacing with APIs - SOAP, REST, etc.
Comfortable using Git, Bitbucket/Github
Strong belief that tests and code go hand-in-hand
Deep understanding of SQL, query optimizations, joins etc.
Excellent CS foundation: data structures, time complexities, algorithms, etc.
Startup work experience a major plus!
About Us:

ReSci's mission is to make artificial intelligence accessible and usable for brands.

Our values:

Inspire with passion.
Persevere with determination.
Collaborate with unity.
Grow without bounds.
Create with impact.
Lead with character.

Based out of Santa Monica, CA, our team consists of serial entrepreneurs who have all made Retention Science a leader in AI marketing. Our SaaS platform, “Cortex” helps online businesses target, engage, and retain customers. The Cortex marketing platform uses machine-learning algorithms to predict customer behavior by analyzing massive sets of demographic, social, and behavioral data to generate 1-to-1 retention campaigns personalized to each customer. Cortex makes 3.5+ billion predictions per day and processes 5k+ events per second.

Our founders have been recognized as the Ernst & Young Entrepreneurs of the Year, and our company was awarded Top 10 Big Data Startup of the Year by CRN, one of Fast Company's €œInnovation Agents€, Top 10 Software Company in Southern California from SocalTech, and identified by Inc. Magazine as one of the most innovative startups. Retention Science has also been featured in Forbes, the Wall Street Journal, TechCrunch, Bloomberg, and Reuters, among other notable publications. In addition, the most prestigious startup accelerator in LA (part of the TechStar Network), as well as many reputable angel investors and Venture Capital firms have provided their support and backing for our business.

We're passionate about what we do and we put our people first! We are a close-knit family whose members drink too much coffee, work hard, and never cease to brainstorm creative new ways to improve our solutions. We foster a dynamic and exciting start-up environment that is conducive for innovative thought; join us if you are interested in working with our world-class team!"
Data Engineer,"Senior Data Engineer
If you are a Senior Data Engineer with experience, please read on!
Top Reasons to Work with Us
Located in Los Angeles, we are a subscription platform focused on financial empowerment. We've been around for decades and pride ourselves on evolving as the needs of our client base grows and changes. Our Loyal customer base, and consistent stream of new business makes us a sure bet for an Engineer looking for stability without loosing a fresh agile perspective on their projects.

If this sounds like a match for you, apply today or send your resume directly to savannah.diaz@cybercoders.com! We are actively interviewing this week and next week.
What You Will Be Doing
Our next Data Engineering hire will lead the architecture ETL pipelines to expedite our Data Ingestion; and contribute to the BI, business analytics, and data science needs of the entire company. You'll be able to automate existing processes, optimizing data delivery, and even re-designing infrastructure to better overcome the challenges of scalability. Your day to day tech-stack will include SQL, AWS S3, Redshift and Big Data tools like Spark.

Collaboration is also a huge part of our day-to-day; you will interface with a diverse team to develop and build SQL code and logical and physical data models. You'll have the opportunity to collaborate and continually learn, while improving reporting and analysis processes for the growing business need, and evolving technical challenges.
What You Need for this Position
1) 5+ years of Data Engineering experience
2) 3+ years using Python
3) Data Warehousing experience
4) Understanding of relational databases; including mySQL, Postress, and DynamoDB
5) Knowledge of ETL architecture

Nice to have, but Not required:
1) Spark streaming experience
2) Prior exposure to Docker containerization
3) Leadership or Mentoring experience
4) A BS on MS in Mathematics, computer science, or related field
What's In It for You
- Competitive base salary - up to $150K (DOE)
- 10% annual bonus
- Excellent benefits package
- The cool perks of an agile growing company, with all the benefits of stability
So, if you are a Senior Data Engineer with experience, please apply today!
-
Applicants must be authorized to work in the U.S.


CyberCoders, Inc is proud to be an Equal Opportunity Employer

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.

Your Right to Work In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.

CyberCoders will consider for Employment in the City of Los Angeles qualified Applicants with Criminal Histories in a manner consistent with the requirements of the Los Angeles Fair Chance Initiative for Hiring (Ban the Box) Ordinance."
Data Engineer,"This is a 100% Remote position. Candidate may reside anywhere within the United States

We are currently seeking a Senior Data Engineer to help elevate our communications platform which is being used by the largest companies in the world in some of the most technically complex environments you can find. You will architect and design big data systems which require queries returning within sub-second response times. Ready for a challenge?

We are a distributed team. We build solutions for distributed workforces so we model our workforce the same way. In this role you really can work where you want, but for this role we are only considering candidates based in the United States.
Responsibilities
Design systems that reliably and efficiently provide interactive query performance on large amounts of multi-modal data
Build systems that handle scale
Build the infrastructure required for optimal extraction, transformation, and loading of data from a variety of data sources using SQL and AWS big data technologies
Collect, parse, analyze, and visualize large sets of data
Turn data into insights
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product
Qualifications
Experience with large-scale data and query optimization techniques
Experience with ETL to data warehouse systems
Experience with AWS cloud services: EC2, RDS, Redshift, Aurora
Expert in SQL, NoSQL, and RDBMS
Knowledge in multiple scripting languages (e.g. Python)
Knowledge of cloud, distributed systems, and stream-processing systems
Passionate about learning new technologies and solving hard problems in a fast-paced environment
The ideal fit
Has a Computer Science degree
Is a ""student of the game"" and thrives on new challenges
Enjoys learning from teammates, and isn't afraid to teach others at the same time
Sees the glass half-full. This is a new industry space...your vision could make all the difference!
Wants to make a lasting impact and lifelong connections, this is not just another paycheck
Must Haves:
Expert level SQL
Expert level Python
Deep experience with AWS
Experience building SaaS products through data insights
Why Us?
Because you care about people, the work you do, and the connections you make. Work is such a large part of life, it only makes sense to make it awesome.

If you want to engage brilliant minds in a true start-up environment where ideas are rewarded regardless of who they come from, join us. This is a rapidly changing space so if you thrive on ambiguity, are hungry for a challenge, and have the guts to speak your mind it could be a perfect fit.

Come for the challenges, come for engaging people in a casual and friendly environment. Come for the unlimited PTO, the health benefits, the 401k plan, the annual retreats (including family), the twice-a-year hackathons, the 10% exploratory time, the ability to contribute to open source, and the potential to work from anywhere.

Whatever the reason, your new co-workers along with a leadership team who truly believes in your growth both professionally and personally will keep you here.

For immediate consideration, please submit a recent resume that is closely aligned to the description in addition to a detailed cover letter with salary expectations."
Data Engineer,"The Job

The Data Engineer will be a member of the Media Supply Chain (MSC) team within the WarnerMedia Technology (WMTO) organization, the position will be responsible for the design and implementation of search and data relationship functionality for the WB content platform. The Data Engineer must have experience with data modeling, modern database technologies, and API driven search design and development. The Data Engineer will make technology and design decisions, and partner across the organization in a collaborative manner to achieve results for our mission critical content and data management and distribution applications.

The Media Supply Chain – Who We Are
Media Supply Chain is tasked to architect, engineer, and program manage a wide range of applications, workflows and services for our internal partners who operationally manage and distribute WarnerMedia content globally. Our applications and technology solutions are responsible for scheduling, image & asset metadata management, as well as, content processing and delivery as well as content mastering, localization and preservation. We are the team pioneering new ways to process and deliver WarnerMedia content to its global customers

The Media Supply Chain – Mission
Our mission is to provide and sustain practical, innovative, agile software and technology solutions which are highly reliable, automated, measurable, optimized, modern systems capable of distributing high-quality content and metadata to our domestic and global platforms & partners.

The Daily
Develop and provide support for core data relationship, data ingest, data transformation services and search capabilities. Creates functional and technical specifications. Creates and executes against a plan to launch and maintain applications.
Review project objectives and determine best technology for implementation. Implement best practice standards for development, build and deployment automation.
Evaluate software products and vendors for WarnerMedia (WM) Technology and other divisions. Recommend action, develop and lead implementation of selected products/services.
Work with internal and external developers to ensure (WM) Technology code standards and best practices are performed for development of applications.
The Essentials
B.S. in Computer Science or equivalent experience.
AWS Developer Certification preferred.
AWS Database or Data Analytics Certification preferred.
3+ years data engineering experience.
Demonstrated proficiency in data modeling and data structures.
Demonstrated experience implementing database technologies such as NoSQL, and Relational. Experience in graph databases a plus.
Demonstrated expertise and experience in ELK stack (elasticsearch, logstash, kibana).
Demonstrated expertise and experience in modern databases such as Mongo, Couchbase, Neptune, Neo4j, or equivalent. Experience in MarkLogic a plus.
Proficient in one or more modern query languages such as elasticsearch query DSL, cypher, gremlin, or graphql. xQuery preferred but not required.
Highly proficient in XML, JSON and YAML data exchange formats. Experience in XSDs and triple stores.
Proficient in API design and development, specifically REST APIs. Experience with Swagger 2.0 and AWS API gateway is highly preferred.
Demonstrated experience in data analytics tools such as Tableau, Kibana etc.
Experience in working with data streaming technologies such as Amazon Kinesis, Apache Kafka etc.
Experience in AWS at scale leveraging services such as elasticsearch, RDS, Redshift, Neptune and ec2.
Highly proficient in at least one modern programming language such python, java, or node.js. Bash experience preferred.
Demonstrated expertise and experience in deploying containerized application using Docker, Kubernetes or equivalent.
Experience with source code and knowledge repositories such as git, jira, or equivalent systems.
Proficient in a Linux environment.
Proficient in core DevOps principles.
Proficient in the SDLC in an agile environment.
Systems design and architecture.
Ability to work with outside vendors and clients under sometimes adverse circumstances and under time critical constraints.
Must be able communicate effectively and tactfully with all levels of personnel (in person, written, telephone).
Must be able to pay close attention to detail.
Must be able to handle multiple tasks in a fast-paced environment.
Must be able to organize and schedule work effectively.
Must be able to work flexible hours, including overtime, if and when necessary.
Must be able to respond to after-hours pager notifications to provide support for applications as necessary.
Requisition #
177937BR

Area of Interest
Technology/Information Technology

Industry
Film Production and Distribution

Location
United States - California - Burbank

Position Type
Full Time

Business Unit
WB Technology

Business Unit Overview
WB Technology combines Warner Bros.’ industry-leading technologists and disciplines to ensure global alignment with business strategy and accelerated delivery of innovative technology solutions studio- and industry-wide. From pre-production through archiving, the WBT organization will provide critical business and technology intelligence and services to all Studio business units. WBT manages the Studio’s enterprise systems and solutions, emerging platforms, information security, consumer intelligence, content mastering and delivery, and more.

Company Overview
WarnerMedia is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including: HBO, HBO Now, HBO Max, Warner Bros., TNT, TBS, truTV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others.

Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law."
Data Engineer,"Our client is searching for a Data Engineer to join their team.
The Data Engineer will be responsible for creating solutions,
comprehensive analytics and will be integral in implementing
company-wide data strategy.

Responsibilities:
Collaborate with other departments to understand data needs
Develop and optimize ETL processes
Optimize data warehouse by defining technical requirements
Improve and maintain data access for our BI tools
Auditing and automating data quality
Work closely with development teams to create long term plans
for problem resolution

Requirements:
Bachelors Degree or greater (technical or science degree
preferred).
2 years of experience in the following:
scalable architectures and large data processing
AWS, Linux and Shell Scripting
ETL and data mining
open source programming language
data warehousing concepts
SQL, and System-level DBA functions ex: configuring replication,
automating backups, performance tuning and RDBMS"
Data Engineer,"ÂGreetings from Trovetechs!!!

Â

Â

We have an immediate need for Big Data EngineerÂ@ÂLos Angeles, CA. Please find the below Job description for your kind reference.

Â

Duration: 6+ Months

Rate: $DOE/hr.

Â

Â

Key: Python, Spark, SQL, Redshift, AWS (with certification)

Â

Job Description:
Deploy and maintain data pipelines.
Assemble large, complex data sets.
Build optimal ETL infrastructure using AWS offerings.
Build high performance, fault tolerant, and scalable systems.
Collaborate and coordinate with other teams and parts of the business.
Communicate technical concepts to non-technical stakeholders.
Â

Â

Skills & Experience We're Looking For:
Strong Python programming skills.
Spark programming knowledge.
Redshift experience.
Strong experience with AWS Services, well versed with various AWS ETL Services (EMR, Glue, etc.), RDS, AWS Lambda, etc. (Preferred).
Should have hands on experience with effort estimations, setting up RDS, IAM roles / policies management, etc.
Work experience with Databricks is highly desirable.
Strong written and verbal communication skills in English.
Â

Thanks & Regards,

Â

Vishnuwar S

Talent Acquisition â US

Direct: +1 (908) 533 â 7359

Mobile: +1 (732) 588 â 7828

Email: vishnus@trovetechs.com

Â

LinkedIn: https://www.linkedin.com/in/vishnu-mba-us-it-recruiter-trovetechs-a2810a131"
Data Engineer,"Job Description
At 12traits we believe the path to a better future lies in unlocking the true potential of humankind. We do this through harmonizing human behavior data with psychological trait sets, leveraging a multitude of scientifically valid data perspectives that allow us to understand human beings more deeply and sustainably than ever before: ushering in a new era of personalized experiences, starting in gaming, optimized for human potential and health.

Elevating the human experience is at the core of everything we do. That core is realized through our human-facing machine learning algorithms, our unparalleled understanding of a user within any given environment, and our ability to personalize experiences in real time.

As a data engineer, you’ll work closely with data scientists, engineers, psychometricians, as well as UX designers and researchers to actualize the potential derived from combining some of the richest behavioral data sets available with cognitive data. You'll not only get to push your own boundaries, but the boundaries of artificial intelligence.

WHAT YOU WILL OWN AND DRIVE
Implementation, improvement and maintenance of our data pipeline.
Job orchestration and deployment.
Management of our data stores (Bigquery, Cloud Storage, MySQL etc.)
Understand existing database architecture and study the needs of the business team to propose new solutions and design
Data ingestion jobs to help our data science team be at their best.
Proactively seek new value-add opportunities for customers and convert those new opportunities to realized value.
Advising and implementing best practices in multiple technical domains
Python codebase with pandas DataFrame.

WHAT YOU NEED TO BRING TO THE TABLE
Experienced in IT with at least 2-3 years in a direct Data Engineer position.
Experience with real time data streaming tools (e.g. Apache Kafka, AWS Kinesis)
Hands-on experience with leading commercial Cloud platforms, preferably GCP
A good understanding of Software Development Life Cycle, Disaster Recovery and Data Resiliency.
Experience working in Agile/Scrum environments
A portfolio of activities / a side project that showcases your intellectual curiosity
Software engineering skills: Python

WHAT MAKES YOU STAND OUT
Familiarity with Lambda Architecture, GCS, Bigquery.
Detail-oriented, with excellent analytical, technical and problem-solving skills
Good ability to derive and design technical specifications from general product requirements
Experience building personalization or recommendation engines
Experience with Machine Learning/Data Science
Familiarity with and interest in psychometric data"
Data Engineer,"OverviewUnder the direction of Director - Data Engineer, the Senior Data Engineer works closely with business leaders, managers, staff and vendor to accurately gather and interpret requirements, specifications. Develop technical specifications and recommend, design, develop, test, implement, and support innovative and optimal data solutions. Serves as a coach and mentor to Data engineering teamResponsibilitiesLead on all aspects of a modern and flexible SDLC approach, applied to data engineering, analytics and data science efforts Provide business guidance and steer efforts toward desired outcomes; partner with internal stakeholders and external vendors to meet strategic data warehouse goals Research, evaluate and formally recommend third party software and technology package Responsible for new & existing integrated system & data flow enterprise architecture Set standards for data engineering functions; design templates for the data management which are scalable, repeatable, and simple Defines and develops appropriate controls to monitor quality of the enterprise data warehouse process Preparation of technical specification Perform all other related duties as assignedQualificationsMinimum 7 years' experience in a complex data warehouse role for a mid to large size organization; Health Care industry experience highly desired Minimum 4 years hands on experience with ETL/Data Integration, BI, data mining and modeling, SSIS strongly preferred Proficiency in all facets of DW Architecture, data flow strategy, data modeling, metadata and master data management Mastery expertise in SQL and RDBMS systems such as Microsoft SQL Server Knowledge of and experience working with reporting tools like SSRS and Tableau Stay abreast of established and industry emerging data technologies Demonstrated ability to produce high quality technical documentation. Experience in implementing a data architecture with separation between storage and compute preferred. Deep understanding of data architecture and ability to coordinate with the implementation team is highly desired.#LI-JP1"
Data Engineer,"Job Description
Summary:

We are looking for a seasoned Senior Data Engineer to become a core member of ZipRecruiters Data Services team and make an impact taking our data platform to the next level! You will be taking on exciting new development projects and not just maintenance work, working with rapidly growing scale that presents exciting challenges...

What you'll be doing:
Understand our business domain as it relates to our data needs and guide our infrastructure towards supporting those needs
Analyze Big Data technologies and their innovative applications, bring these insights and best practices to ZipRecruiter
Work with various teams to build an infrastructure/pipeline to collect and analyze all necessary data
Translate high level requirements to actionable tasks/deliverables and creative problem solving a must
What we're looking for:
Bachelor's Degree in Computer Science/related field or equivalent work experience
5+ years of experience in development of large scale data processing systems and ETL pipelines
Must have good command of Scala/Python/Java, Hadoop/Spark and Kinesis/Kafka
Experience with tuning Hadoop clusters, Amazon EMR, AirFlow, and other big data ETL technologies
Experience with Docker, Jenkins, or similar build tools
Experience with monitoring tools
Experience with Hive, Presto, Storm, Flink, Druid, Elastic Search, DynamoDB/Cassandra/HBase/Riak is a plus
Experience with AWS for data applications is a plus
Benefits & Perks:
A fun environment where work-life balance is valued
Opportunities for advancement as our young startup grows
Very competitive salary
Generous bonus plan
Employer-matched 401(k) plan
Competitive benefits package
Healthy snacks
Local gym discount
Attractive paid time off policy - Open/Flexible vacation policy
ZipRecruiter is an equal opportunity employer (M/F/D/V). All applicants must be authorized to work in the U.S. This organization uses E-Verify.

Category: Computer/Software
Company Description
ZipRecruiter is the fastest growing employment marketplace. We have helped over 1 million businesses and 100 million job seekers find their next perfect match through partnerships with the best job boards on the web, curated email alerts, award-winning mobile apps, and the world’s best search algorithm for jobs."
Data Engineer,"Our client in Pasadena is specialized in ML solutions for the Aerospace Industry. They are actively looking for a Senior Data Engineer to join their team. They have an awesome collaborative culture and learning environment. It's a relatively flat hierarchy, and most of the candidates we have placed come from a highly technical hands-on engineering manager or architect roles.Required Skills & Experience* Team-player or experience in a startup environment* Proficient in at least one of the following: Java, Python, or Golang* Experience building streaming data systems from scratch that are highly scalable and fault tolerant* Use latest streaming data frameworks to solve complex problems related to performance and scalability* Involvement in design of data processing systems with small team* Ability to work autonomously* Experience with any number of the following technologies - Kafka, BigTable, BigQuery, Elasticsearch, Hadoop, Cassandra, Amazon Web Services, Google Cloud Platform (GCP), SQS, S3, PostgreSQL, and similar big data technologies* Required - an understanding of large-scale big data systems, both architecture and designDesired Skills & Experience* Expertise in Java and Python* Experience in a startup environment* Exposure to networking protocols and understanding of cyber-security principles* Exposure to machine learning techniques and cross-function rolesWhat You Will Be DoingTech Breakdown* 50% Java* 50% PythonDaily Responsibilities* 100% hands-onThe Offer* Competitive Salary: Up to $180K/year, DOEYou will receive the following benefits:* Medical Insurance & Health Savings Account (HSA)* 401(k)* Paid Sick Time Leave* Pre-tax Commuter BenefitApplicants must be currently authorized to work in the United States on a full-time basis now and in the future.Jobspring Partners, part of the Motion Recruitment network, provides IT Staffing Solutions (Contract, Contract-to-Hire, and Direct Hire) in major North American markets. Our unique expertise in today's highest demand tech skill sets, paired with our deep networks and knowledge of our local technology markets, results in an exemplary track record with candidates and clients."
Data Engineer,"Data Engineer
Do you have Streaming Platform experience and want to build the next Netflix? If yes and you are a Data Engineer with experience, please read on!
Top Reasons to Work with Us
As the Data Engineer on the team, you will be a part of the team that is responsible for developing, constructing, test existing and new architectures. You will play an integral role in identifying ways to improve data reliability, efficiency, and quality. This position will work closely with the Content Intelligence team along with all of the Data Engineering and cross-functional teams.
What You Will Be Doing
"" Provide thought leadership and lead best practice solutions for the entire Data Engineering team.
"" Take lead in driving a culture of high quality, innovation, and incremental experimentation.
"" Design, implement and fully own critical portions of analytical data models and pipelines used to populate them.
"" Lead the collaboration with stakeholders to understand requirements, lead and design data structures using best practices, and develop batch or real-time data pipelines to ensure the timely delivery of high-quality data.
"" Own, maintain and support existing data pipelines and platforms storing petabytes of data quickly and reliably.
"" Creatively explore and demonstrate the value of data for the team. Be part of a driving force that enables and empowers better decision making through data.
"" Has a knack of using flexible and scalable methodologies that can be applied to a broad set of problems across the Data Engineering organization.
"" Always be thirsty for optimizations and always be eager to find new, improved ways to address an old problem.
What You Need for this Position
"" 2-3 years in a lead capacity.
"" SQL Rockstar: can code complex SQL logic, lead best practices and drive the development of other data engineers.
"" Proven Computer Science fundamentals in Algorithms and Data Structures.
"" 7-8 years of experience in Python or other programming languages such as Scala or Java.
"" Significant experience with Big Data technologies such as Apache Spark.
"" 2-3 years of experience with MPP/Cloud Data Warehouses such as Snowflake.
"" Five years of experience with core AWS services related to data engineering.
"" Education: Bachelor's or Master's degree in computer science or related field.
The Nice to Haves
"" Experience with using Apache Kafka or AWS Kinesis as a message bus for storing and processing high-velocity real-time data.
"" Experience with building data notebooks using Jupyter, AWS Sagemaker, or Databricks.
"" Experience with building and productionizing ML Pipelines and other solutions.
"" Experience with Graph-based workflow orchestration engine such as Apache Airflow.
"" Familiarity with marketing and media platforms/data sets.
What's In It for You
Great Benefits Package!
So, if you are a Data Engineer with experience, please apply today!
-
Applicants must be authorized to work in the U.S.


CyberCoders, Inc is proud to be an Equal Opportunity Employer

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.

Your Right to Work In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.

CyberCoders will consider for Employment in the City of Los Angeles qualified Applicants with Criminal Histories in a manner consistent with the requirements of the Los Angeles Fair Chance Initiative for Hiring (Ban the Box) Ordinance."
Data Engineer,"The Enterprise Data Intelligence Team of Cedars-Sinai is looking for an experienced Data Engineer/Data Intelligence Analyst. The ideal candidate is passionate about data, coding, technology and will utilize their skills to help solve complex healthcare questions. The candidate should possess a data engineering background, business acumen to think strategically and love working with people. Within this role, the candidate will experience a wide range of problem solving situations requiring extensive use of data collection and analysis. The successful candidate will work with Data Engineers, Data Scientists, Business Analysts, Doctors, Nurses and other stakeholders across organization.

Summary of Essential Job Duties:
Expert Oracle Query and Development Skills (Extensive RedShift skills a plus).
Strong AWS Cloud Platform tools: S3, Kinesis, RedShift.
Familiar with Dashboarding tools like Tableau/Qlik.
Good Linux, Python, Bash skills.
Desire to learn new tools, techniques and solve data challenges.

Educational Requirements:
Four (4) year degree in Computer Science or similar experience.

Experience:
Hospital Based Healthcare experience, especially Epic Clarity data model is a plus."
Data Engineer,"Description:

PROLIM Global Corporation (www.prolim.com) is currently seeking Big Data Engineer in Burbank, CA with one of our top
Clients.

Job Description

Big
Data Engineer
The purpose of the opportunity is to
identify, propose, develop, validate, and deploy innovative data insight
systems to business partners within the company.
Responsibilities
Leads and supports efforts to provide
timely, relevant, and clean data to various groups within the Company.
Identify, propose, develop, validate,
and deploy innovative data insight systems to business partners.
Provide business insights by integrating
data from disparate systems and services in maintainable ways and developing
validated solutions to provide those insights.
Provide support to other engineers in
the form of solution identification, mentoring, automation and data or effort
validation.
Identify opportunities for data sharing
and develop federated systems to supply that data.
Basic
Qualifications.
8+ years of software engineering 5+
years of large scale systems
A strong knowledge of the Java
programming language
Working experience with Hadoop batch
processing system and the map-reduce horizontally scalable paradigm
Experience with the Linux operating
system
Strong grounding in object oriented
programming, aspect oriented programming, design patterns, concurrency,
algorithms & data structures
Development experience using service
oriented architecture, JAX-RS and JAXB . Strong ability to research solutions,
processes, industry trends and best practices.
Experience with alternative data
processing platforms including Storm, Spark, Shark, Apache Mesos, Hive, PIG,
and Apache Crunch
Experience with an analytical approach
to tuning models
Experience with machine learning and
machine learning libraries like Apache Mahout
Experience with deployment environments
and systems including Amazon Web Services & Chef
Experience with diverse storage systems,
platforms and methods including hBase, MongoDB, Apache Avro, PostgreSQL,
Greenplum, Teradata
Experience with data integration
including Apache Camel, Spring Batch and Talend
Experience with pilot application
frameworks including Spring Roo, GWT 2.0, Portlets and Spring MVC
Experience with disciplined development
practices including using tools like Maven, TestNG, Findbugs, CheckStyle,
Checker, Sonar, wikis, coding standards and Git
Experience working in an evolved
technical organization including contributing to and following coding
guidelines, best practices, documenting, and presenting at brown bags,
professional groups and conferences.
Experienced in participating in scrum or
other agile development environments.
Apply online for immediate
consideration, please send your updated resume and contact info via email
rohit.sheelvant@prolim.com or Contact (248) 8760896 Ext 225.

About PROLIM Corporation

PROLIM
is a global company focused on product innovation by leveraging IT and PLM
Technologies. PROLIM has more than a decade of PLM Service history – serving
Automotive, Aerospace, High Tech and Industrial Machinery companies throughout
the world.

Our
Product Lifecycle Management (PLM) practice is powered by a team of consultants
who provide comprehensive end-to-end service offerings. We have a PLM
Center-of-Excellence (CoE) that enables our customers to build innovative
solutions to the most-pressing PLM challenges. The CoE is supported by the
right skills, resources, technologies and methodologies and helps develop
customized solutions to better manage the entire product lifecycle."
Data Engineer,"Interested in joining an organization that is disrupting the food delivery space? They are growing their suite of products and have ample opportunities for growth into leadership roles. In this role you will be building data pipelines, intelligent monitoring, ETL, & working with databases such as PostgreSQL, DynamoDB, & Snowflake.You'd also be interacting with API's, Docker / Kubernetes, & CI / CD pipelines. Think millions of queries per second, petabytes of data, with solutions that have a a massive scale impact (used by millions of people).Ready to be a database pioneer?Required Skills & Experience* Python, Java development* 5+ years experience with relational SQL databases & distributed NoSQL data stores (ETL, MongoDB, Hadoop, databases/stores, APIs, MySQL, Postgres, DynamoDB, etc)* Docker ( Kubernetes is a plus)* Skilled at data validation, quality, ETL, and data streams.* ElasticSearch, ZooKeeper, HBase, Memcache, Kafka, Openshift* CI / CD pipelines with Jenkins is a plus* Adept knowledge in AWS + AWS tools (ElasticSearch, RDS, EC2, S3, ELB, etc.)The Offer* Competitive Salary: Up to $170K/yearYou will receive the following benefits:* Award winning culture* 100% employer paid benefits (medical, dental, vision)* Unlimited Snacks & drinks* Kombucha on tap* 401(k) with match and immediate vesting* Equity* Unlimited and encouraged Paid Time Off* Wellness Programs* Team EventsApplicants must be currently authorized to work in the United States on a full-time basis now and in the future. This position does not offer sponsorship.Jobspring Partners, part of the Motion Recruitment network, provides IT Staffing Solutions (Contract, Contract-to-Hire, and Direct Hire) in major North American markets. Our unique expertise in today's highest demand tech skill sets, paired with our deep networks and knowledge of our local technology markets, results in an exemplary track record with candidates and clients."
Data Engineer,"Job Information

Industry IT Services Work Experience 5+ years Salary $155,000- 180,000 City Anaheim State/Province California Zip/Postal Code 90005

Job Description

Technical members of our solutions teams require little guidance, but love to learn, collaborate, and problem solve. This position requires a mid to senior level of experience, a passion for mission support, and a strong desire to solve our customers hardest technical and data challenges.

The Data Engineer will be responsible for the following:
Assess, transform, organize, and optimize data for use by machine learning algorithms
Generating representative data sets for systems development and data science initiatives
Build data pipelines that enables data scientists and engineers and other stakeholders
Requirements
8+ years of experience designing data models and data warehouses supporting analytics, using both relational and non-relational distributed data storage systems
Demonstrated experience building and maintaining ETL pipelines. Bonus points for experience with Apache Beam
Demonstrated experience with large-scale distributed processing
Experience building data pipelines in ML frameworks. Kubeflow experience is desired
Experience working in a fast-paced agile environment
Experience in a high-level programming language, such as Java, Python... etc.
Demonstrated proficiency with Git version control systems
Experience working in Linux environments
Benefits

- 6% 401k match
Health insurance
Dental insurance.
Pet insurance.
Life insurance."
Data Engineer,"Senior Data Engineer
If you are a Senior Data Engineer with experience, please read on!
Top Reasons to Work with Us
Located in Los Angeles, we are a subscription platform focused on financial empowerment. We've been around for decades and pride ourselves on evolving as the needs of our client base grows and changes. Our Loyal customer base, and consistent stream of new business makes us a sure bet for an Engineer looking for stability without loosing a fresh agile perspective on their projects.

If this sounds like a match for you, apply today or send your resume directly to savannah.diaz@cybercoders.com! We are actively interviewing this week and next week.
What You Will Be Doing
Our next Data Engineering hire will lead the architecture ETL pipelines to expedite our Data Ingestion; and contribute to the BI, business analytics, and data science needs of the entire company. You'll be able to automate existing processes, optimizing data delivery, and even re-designing infrastructure to better overcome the challenges of scalability. Your day to day tech-stack will include SQL, AWS S3, Redshift and Big Data tools like Spark.

Collaboration is also a huge part of our day-to-day; you will interface with a diverse team to develop and build SQL code and logical and physical data models. You'll have the opportunity to collaborate and continually learn, while improving reporting and analysis processes for the growing business need, and evolving technical challenges.
What You Need for this Position
1) 5+ years of Data Engineering experience
2) 3+ years using Python
3) Data Warehousing experience
4) Understanding of relational databases; including mySQL, Postress, and DynamoDB
5) Knowledge of ETL architecture

Nice to have, but Not required:
1) Spark streaming experience
2) Prior exposure to Docker containerization
3) Leadership or Mentoring experience
4) A BS on MS in Mathematics, computer science, or related field
What's In It for You
- Competitive base salary - up to $150K (DOE)
- 10% annual bonus
- Excellent benefits package
- The cool perks of an agile growing company, with all the benefits of stability
So, if you are a Senior Data Engineer with experience, please apply today!
-
Applicants must be authorized to work in the U.S.


CyberCoders, Inc is proud to be an Equal Opportunity Employer

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.

Your Right to Work In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.

CyberCoders will consider for Employment in the City of Los Angeles qualified Applicants with Criminal Histories in a manner consistent with the requirements of the Los Angeles Fair Chance Initiative for Hiring (Ban the Box) Ordinance."
Data Engineer,"Company Description
Sagence is a management advisory firm dedicated to helping our clients optimize the value of their data assets. From thinking to doing, Sagence works with leading institutions in the acquisition, evaluation, development and management of their critical data assets and in the application of analytics to discover new insights, shorten time-to-value, and drive competitive advantage.
**All candidates must be currently authorized to work in the US on a full-time basis. Sagence does not provide sponsorship for work visas upon hire or once on board.**
Job Overview
Sagence is looking for experienced, client-facing Data Engineers to help us build and sustain our client’s data capabilities and our competitive advantage. Along with the requirements below, candidates must possess deep, practical experience in one or more of the following Data Management competencies; data warehousing, data lakes, reference data or master data management, data architecture, data modeling, data governance, data analysis, business intelligence.
Skills & Requirements:
MUST BE HANDS ON. Candidates should have system development experience and proficiency with system development methodologies and possess the following skills and knowledge:
Must have significant hands-on experience with various IT concepts of data management/engineering (ETL, data modeling, data warehousing, etc.)
Must have significant hands-on experience with SQL, data profiling, and data discovery
Experience building business intelligence, analytics, or reporting solutions - either front-end consumption mechanisms (e.g., Microsoft, Tableau & Qlik) or supply of data for these purposes
Familiarity with data architecture principles/approaches, data environment infrastructure considerations, and data modeling principles/approaches
Ability to drive out technical requirements with business and IT stakeholders for implementations of data solutions
Hands-on experience with Agile delivery methodology
Prior professional experience in an IT management, management consulting, or client facing role is preferred
Knowledge of the Financial Services, Insurance, Healthcare or Retail industries is preferred
Demonstrated ability in engaging, communicating, and presenting to stakeholders across both business and technology functions
Tools and Technology:
Proficient at leveraging tools and technology to drive value for clients. Examples include the following;
Database Management Tools:
Relational – e.g. Oracle, MySQL, Microsoft SQL Server, PostgreSQL, DB, or similar
NoSQL – e.g. MongoDB, Couchbase, DataStax, Redix, MarkLogic, or similar
Cloud – e.g. AWS, Azure, xxx, xxx, xxx
ETL Tools - e.g. Informatica, Talend, Microsoft SSIS, or similar
Data Modeling tools - e.g., Toad, ERWin, ER/Studio, PowerDesigner, IBM Data Architect, or similar
Industry Leading BI tools - e.g., Business Objects, Microsoft, Cognos, Tableau, OBIEE, Qlickview, or similar
General:
Must be currently authorized to work in the US on a full-time basis. Sagence does not provide sponsorship for work visas
3+ years of professional experience working in a related role
Must be collaborative, innovative, curious, and resourceful, and exhibit a positive attitude
Strong desire to work on interesting projects with smart and creative people
Willingness to travel up to 80% of the week (M-Th)
Chicago or New York area candidates preferred, but will consider candidates in other parts of U.S.
Our Culture
Passionate, diverse, creative, genuine, flexible, hands-on…these are just a few of the words that describe our culture. Our Partners are deeply involved in the client work on a daily basis. We have a high-energy workplace with a focus on producing high-quality, impactful results. We are committed to equality of opportunity, fairness, work and lifestyle balance, and mutual respect. We promote an entrepreneurial spirit by encouraging individual initiative and foster a collaborative culture and work environment which includes open communication and on-going learning. We build teamwork through small, dedicated teams who continuously teach each other and learn from one another. We strongly believe these characteristics enable our employees to develop to their fullest potential. To learn more, please visit us at www.sagenceconsulting.com"
Data Engineer,"The Job Details are as follows:

OVERVIEW

We are looking for creative and enthusiastic Data Engineers to join our team in building the best Data Platform on the street and enabling our investment teams to monetize data assets. We treat our data systems as software systems and engineer them accordingly. In your role, you will:
Develop cloud-first data ingestion processes using Python, SQL, and Spark
Engineer data models and infrastructure for a wide variety of market and alternative datasets
Design and build services and plugins to enhance our Data Acquisition Platform
Maintain alerting systems to ensure smooth day-to-day operations for hundreds of datasets
Author tests to validate data quality and the stability of the platform
Investigate and defuse time-sensitive data incidents
Communicate with data providers to onboard new datasets and troubleshoot technical issues
Evangelize best practices to our partners throughout the firm
Work directly with Analysts, Quants, and Portfolio Managers to understand requirements and provide end-to-end data solutions
WHAT YOU’LL BRING
Bachelors/Masters degree in Computer Science or a related field
3+ years experience with at least one of Spark, Airflow, data warehousing
Strong analytical, data and programming skills (Python/SQL/NoSQL/Spark)
Ability to understand and contribute to our existing data system software
Experience containerizing workloads with Docker (Kubernetes a plus)
Aptitude for designing infrastructure, data products, and tools for Data Scientists a plus
Strong oral and written communication skills, most importantly, must be a team player"
Data Engineer,"77 West Wacker Dr (35012), United States of America, Chicago, Illinois

At Capital One, were building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Data Engineer

Do you want to work for a tech company that writes its own code, develops its own software, and builds its own products? We experiment and innovate leveraging the latest technologies, engineer breakthrough customer experiences, and bring simplicity and humanity to banking. We make a difference for 65 million customers. We're changing banking for good. At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who love to solve real problems and meet real customer needs. We want you to be curious and ask what if? Capital One started as an information strategy company that specialized in credit cards, and we have become one of the most impactful and disruptive players in the industry. We have grown to see ourselves as a technology company in consumer finance, with great opportunities for software engineers who want to build innovative applications to give users smarter ways to save, transact, borrow and invest their money, as we seek to disrupt the industry again. As a Capital One Software Engineer, you'll work on everything from customer-facing web and mobile applications using cutting-edge open source frameworks, to highly-available RESTful services, to back-end Java based systems using the hottest techniques in Big Data.

You'll bring solid experience in emerging and traditional technologies such as: node.js, Java, AngularJS, React, Python, REST, JSON, XML, Ruby, HTML / HTML5, CSS, NoSQL databases, relational databases, Hadoop, Chef, Maven, iOS, Android, and AWS/Cloud Infrastructure to name a few.

You will:

- Work with product owners to understand desired application capabilities and testing scenarios

- Continuously improve software engineering practices

- Work within and across Agile teams to design, develop, test, implement, and support technical solutions across a full-stack of development tools and technologies

- Lead the craftsmanship, availability, resilience, and scalability of your solutions

- Bring a passion to stay on top of tech trends, experiment with and learn new technologies, participate in internal & external technology communities, and mentor other members of the engineering community

- Encourage innovation, implementation of cutting-edge technologies, inclusion, outside-of-the-box thinking, teamwork, self-organization, and diversity

Basic Qualifications:

- Bachelors Degree

- At least 3 years of SDLC experience using Java technologies

- At least 3 years experience with leading big data technologies like Cassandra, Accumulo, Python, HBase, Scala, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper

- At least 1 years experience in one of the following Cloud technologies: AWS, Azure, OpenStack, Docker, Ansible, Chef or Terraform

Preferred Qualifications:

- Master's Degree

- 2+ year experience with Spark

- 3+ years experience developing software solutions to solve complex business problems

At this time, Capital One will NOT sponsor a new applicant for employment authorization for this position."
Data Engineer,"Why VillageMD?

VillageMD is changing the trajectory of healthcare by empowering primary care physicians to make informed decisions and engage patients in meaningful ways. We work with thousands of clinicians and healthcare disruptors across the country to build and contribute to our platform to improve patient health while driving down the cost to deliver it.

We are a mission-oriented organization and are thrilled about the work that we do every day. We're transparent, collaborative, and relentless in pursuit of our mission, all while doing so with humility and a low ego. We believe that diverse backgrounds and experiences create the best opportunity for innovation and the community that we are creating is greater than any individual.

We've built our technology using the best of cloud and open-source technologies to create an open, data-first platform that is enriched with analytical models and modernly connected to internal and external apps. These apps drive clinical decision support, patient engagement, and other facilitators of innovative, information-enriched health experiences.

Data Engineers at VillageMD build distributed components, pipelines, and tools that enable our organization to make analytical, data-driven decisions. We're in a unique position to impact everyone in primary care from independent, family-owned practices to world-class health systems. We aggregate, process, and deliver rich datasets to improve the effectiveness of primary care for our doctors and patients.

What are examples of work that Data Engineers have done at VillageMD?
Built and implemented a data profiling tool to reverse engineer data schemas from new data sources facilitating normalization of the data into our data model
Created a summary data platform supporting our presentation layer that allows clinicians and operators in our practices to pinpoint interventions on-demand to patients most in need
Analyzed and designed the best ways to expand our data model to incorporate more data that's mission critical
What will make you successful here?
Strong analytical and technical skills
A real passion for problem solving and learning new technology
Vision to balance speed and maintainability in solution design
The ability to handle multiple, concurrent projects
Crafting and implementing requirements, keeping projects on track, and engaging partners
Challenging the status quo to improve our processes and tools
Communicating complex technical details in meaningful business context
A low ego and humility; an ability to gain trust by doing what you say you will do
What you might do in your first year:
Own ten projects to design and implement best-in-class data processing enabling clean data flow directly to our data model and on to our presentation layer
Work with analytics, engineering and operations to design and implement a new analytics product that supports improving patient health
Design a new concept within our data model to meet a new operational or analytical need
The following experience is relevant to us:
5+ years of full-time experience including extensive experience with healthcare data
Ability to understand and design relational data structures required
Very strong capabilities manipulating data using SQL
Knowledge of, and/or willingness to learn, non-relational data structures and other technologies (e.g. Postgres, Redshift, Cassandra, MongoDB, Neo4j, S3, etc.)
Experience or willingness to learn building information pipelines utilizing Python or Java a plus
BS/MS in computer science, math, engineering, or other related fields is required.
Track record of successfully executing projects with multiple partners
What can we offer you?
Competitive salary, bonus, and health benefits
Paid gym membership
Fun, fast-paced, startup environment (with snacks)
Pre-tax savings on commute expenses
Remote flexibility
A highly-collaborative, conscientious, forward-thinking environment that welcomes the impact you can make from Day 1.
A clear link between our daily work on products and services and the improved quality of healthcare that this work facilitates for patients.
At VillageMD, we see diversity and inclusion as a source of strength in transforming healthcare. We believe building trust and innovation are best achieved through diverse perspectives. To us, acceptance and respect are rooted in an understanding that people do not experience things in the same way, including our healthcare system. Individuals seeking employment at VillageMD are considered without regard to race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
Data Engineer,"Join SADA as a Data Engineer!

Your Mission

As a Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data issues facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring a combination of batch or streaming data pipelines, data lakes and data warehouses.

You will be recognized as an established contributor by your team. You will contribute design and implementation components for multiple projects. You will work mostly independently with limited oversight. You will also participate in client-facing discussions in areas of expertise.
Pathway to Success

#BeOneStepAhead: At SADA we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage adaptability. This means that not only do our engineers understand that change is inevitable, but they embrace this change to continuously broaden their skills, preparing for future customer needs.

Your success comes from your enthusiasm, insight, and positive impact. You will be given direct feedback quarterly with respect to the scope and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, your collaboration with your peers, and the consultative skill you demonstrate in customer interactions.

As you continue to execute successfully, we will build a personalized development plan together that leads you through the engineering or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with a first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:
Google Professional Data Engineer Certified or able to complete within the first 45 days of employment

Required Qualifications:
Expertise in at least one of the following domain areas:
Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or other customer-facing role

Useful Qualifications:
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Applied experience operationalizing machine learning models on large datasets
Knowledge and understanding of industry trends and new technologies and ability to apply trends to architectural needs
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem

About SADA

Values: We built our core values on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing


Work with the best: SADA has been the largest partner in North America for GCP since 2016 and has been named the 2019 and 2018 Google Cloud Global Partner of the Year. SADA has also been awarded Best Place to Work by Inc. as well as LA Business Journal!

Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, RRSP, professional development reimbursement program as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."
Data Engineer,"Riskonnect is the leading integrated risk management software solution provider that empowers organizations to anticipate, manage and respond in real-time to strategic and operational risks across the extended enterprise.Riskonnect is the only provider ranked in the leadership and visionary quadrants by world renowned industry analysts - Gartner, Forrester and Advisen RMIS Review.We employ more than 500 risk professionals in the Americas, EMEA and Asia Pacific and serve over 900 customers across 6 continents.The combination of innovative risk technology, a customer success mindset, and employee-first belief makes Riskonnect a sought after place to work.

Responsibilities:
Develop strategy for new multi-platform data integration and analytics.
Develop strategy for new multi-platform-sourced data lake.
Contribute to API strategy to facilitate application connectivity and analytics.
Contribute to the maintenance and evolution of best practices.
Contribute to process documentation.
Perform multiple proofs of concept (POCs).
Contribute to implementation plan for decided-upon solution(s).
Required Qualifications:
Experience with JavaScript/Java/ Python or Jitterbit and other developer languages.
Experience with Data Analytics.
Experience with Web Services and APIs.
Experience in the development of batch and real-time data integration and data consolidation processes.
Experience with machine learning, AI, and data lakes.
Proficiency in TSQL/PLSQL query-writing, stored procedure development, and views.
Strong analytical skills with ability for problem-solving.
Understands the importance of data provenance and the ability to demonstrate it to clients.
Detail oriented, organized, self-motivated.
Preferred Qualifications:
Experience with Salesforce.
Experience in the Risk Management, Healthcare, Financial, and/or Insurance industries is recommended.
Experience with Financial data sets, involving financial validation."
Data Engineer,"Job Description

Data Engineer

At Citadel, data is the core of the investment process. Data Engineers architect and build our data platforms which drive how we source, enrich, and store data that integrates into the investment process. These Data Engineers own the entire data pipeline starting with how we ingest data from the outside world, transforming that information into actionable insights, and ultimately designing the interfaces and APIs that our investment professionals and quantitative researchers use to monetize ideas. Throughout the process, our Data Engineers partner with top investment professionals and data scientists to design systems that solve our most critical problems and answer the most challenging questions in finance.

YOUR OPPORTUNITY:

Develop solutions that enable investment professionals to efficiently extract insights from data. This includes owning the ingestion (web scrapes, S3/FTP sync, sensor collection), transformations (Spark, SQL, Kafka, Python/C++/Java), and interface (API, schema design, events)
Partner with the industry’s top investment professionals, quantitative researchers, and data scientists to design, develop, and deploy solutions that answer fundamental questions about financial markets
Build tools and automation capabilities for data pipelines that improve the efficiency, quality and resiliency of our data platform
Drive the evolution of our data strategy by challenging the status quo and identifying opportunities to enhance our platform
YOUR SKILLS & TALENTS:

Passion for working with data in order to accurately model and analyze complex systems such as a publicly traded company, commodity market, economy, or financial instruments
Strong interest in financial markets and a desire to work directly with investment professionals
Proficiency with one or more programming languages such as Java or C++ or Python
Proficiency with RDBMS, NoSQL, distributed compute platforms such as Spark, Dask or Hadoop
Experience with any of the following systems: Apache Airflow, AWS/GCE/Azure, Jupyter, Kafka, Docker, Kubernetes, or Snowflake
Strong written and verbal communications skills
Bachelor’s, Master’s or PhD degree in Computer Science or equivalent experience
About Citadel


Citadel is a global investment firm built around world-class talent, sound risk management, and innovative leading-edge technology. For a quarter of a century, Citadel’s hedge funds have delivered meaningful and measurable results to top-tier investors around the world, including sovereign wealth funds, public institutions, corporate pensions, endowments and foundations.

With an unparalleled ability to identify and execute on great ideas, Citadel’s team of more than 675 investment professionals, operating from offices including Chicago, New York, San Francisco, London, Hong Kong and Shanghai, deploy capital across all major asset classes, in all major financial markets."
Data Engineer,"At American Family Insurance, we’re driven by our customers and employees. That’s why we provide more than just a job – we provide opportunity. Whether you’re already part of our team in search of a new challenge or new to our company and ready for what’s next, you’re in the right place. Every dream is a journey that starts with a single step. Start your journey right here. Join our team. Bring your dreams.

Job ID:
R15759 Senior Data Engineer (Open)
Summary:
Job Family Summary

Determines and builds the technical solution(s) to allow unstructured data to be structured and used by Data Scientists. Seeks to understand the data being worked with as its often unstructured data sets. Often are data gurus who prepare data for all stages of the modeling process including exploration, training, testing, and deployment.

Job Summary Wording

As a Data Engineer III, you’ll work on collecting, storing, processing and building Business Intelligence and Analytics applications within our big data platform. Presently, our team is constructing an enterprise data lake to enable analysts and scientists to self-service data at scale across American Family’s operating companies. We’re leveraging open source technologies like Spark, Python, Hadoop, and cloud native tools to curate high-quality data sets. You’ll also be responsible for integrating these applications with the architecture used across the organization. Adjacent responsibilities include establishing best practices with respect to data integration, data visualization, schema design, performance and reliability of data processing systems, supporting data quality, and enabling convenient access to data for our scientists and business users.

Job Description:


Job Level Summary
Requires specialized depth and/or breadth of expertise in own job discipline or field
Leads others to solve complex problems
Works independently, with guidance in only the most complex situations
May lead functional teams or projects
Primary Accountabilities
Perform exploratory data analysis to determine which questions can be answered effectively with a given dataset. Ability to analyze new (possibly unstructured) data sources to determine what additional value they may bring and how to effectively make use of them.
Design and develop highly scalable and extensible data pipelines from internal and external sources.
Work on cross-functional teams to design, develop, and deploy data-driven applications and products, particularly within the space of data science.
Lead in prototyping emerging technologies involving data ingestion and transformation, distributed file systems, databases and frameworks.
Work with our data science team on applying improvements to their machine learning algorithms and platforms.
Design, build, and maintain tools to increase the productivity of application development and client facing teams.
Partner with business analyst to define, develop, and automate data quality checks.
Review developed solutions to solve specific business problems. Optimize queries, data models, and storage formats to support common usage patterns.
Design and develop big data applications and data visualization tools.
Code structure moves beyond procedural ad-hoc workflows and exhibits modularity and comprehensive test cases. Develops code at a high-level of abstraction – writes, maintains, and reuses utilities/libraries across projects.
Influences strategy related to processes and workflows across their division.
Participates in mentoring junior colleagues.
Specialized Knowledge & Skills Requirements
Demonstrated experience providing customer-driven solutions, support or service.
In-depth knowledge of SQL or NoSQL and experience using a variety of data stores (e.g. RDBMS, analytic database, scalable document stores)
Extensive hands-on Python programming experience, with an emphasis towards building ETL workflows and data-driven solutions. Able to employ design patterns and generalize code to address common use cases. Capable of authoring robust, high quality, reusable code and contributing to the division’s inventory of libraries.
Expertise in big data batch computing tools (e.g. Hadoop or Spark), with demonstrated experience developing distributed data processing solutions.
Applied knowledge of cloud computing (AWS, GCP, Azure).
Knowledge of open source machine learning toolkits, such as sklearn, SparkML, or H2O.
Solid data understanding and business acumen in the data rich industries like insurance or financial
Applied knowledge of data modeling principles (e.g. dimensional modeling and star schemas).
Strong understanding of database internals, such as indexes, binary logging, and transactions.
Experience using tools for infrastructure-as-code (e.g. Docker, CloudFormation, Terraform, etc.)
Experience with software engineering tools and workflows (i.e. Jenkins, CI/CD, git).
Practical experience authoring and consuming web services.
Education and Licenses
Bachelor’s degree in computer science or related field, or equivalent combination of education and experience.
Travel Requirements
This position requires travel up to 10% of the time.
Additional Job Information:
Top candidates will have 5-10 years of post-academic data engineering experience. This is not a Business Intelligence role nor an ETL Developer.Depending on qualifications, candidates may be hired at a different levels.We are seeking candidates with demonstrated experience using Python and SQL--a coding challenge will be incorporate into the selection process.Knowledge and experience of cloud platforms a plus, specifically AWS.Candidates experience with Python, AWS, and/or Hadoop preferred.

Offer to selected candidate will be made contingent on the results of applicable background checks.

Offer to selected candidate is contingent on signing a non-disclosure agreement for proprietary information, trade secrets, and inventions.

Relocation assistance is available.

Stay connected: Join our Talent Community!

LI:DB1

At American Family Insurance, we’re driven by our customers and employees. That’s why we provide more than just a job – we provide opportunity. Whether you’re already part of our team in search of a new challenge or new to our company and ready for what’s next, you’re in the right place. Every dream is a journey that starts with a single step. Start your journey right here. Join our team. Bring your dreams.

Job ID:
R15759 Senior Data Engineer (Open)
Summary:
Job Family Summary

Determines and builds the technical solution(s) to allow unstructured data to be structured and used by Data Scientists. Seeks to understand the data being worked with as its often unstructured data sets. Often are data gurus who prepare data for all stages of the modeling process including exploration, training, testing, and deployment.

Job Summary Wording

As a Data Engineer III, you’ll work on collecting, storing, processing and building Business Intelligence and Analytics applications within our big data platform. Presently, our team is constructing an enterprise data lake to enable analysts and scientists to self-service data at scale across American Family’s operating companies. We’re leveraging open source technologies like Spark, Python, Hadoop, and cloud native tools to curate high-quality data sets. You’ll also be responsible for integrating these applications with the architecture used across the organization. Adjacent responsibilities include establishing best practices with respect to data integration, data visualization, schema design, performance and reliability of data processing systems, supporting data quality, and enabling convenient access to data for our scientists and business users.

Job Description:


Job Level Summary
Requires specialized depth and/or breadth of expertise in own job discipline or field
Leads others to solve complex problems
Works independently, with guidance in only the most complex situations
May lead functional teams or projects
Primary Accountabilities
Perform exploratory data analysis to determine which questions can be answered effectively with a given dataset. Ability to analyze new (possibly unstructured) data sources to determine what additional value they may bring and how to effectively make use of them.
Design and develop highly scalable and extensible data pipelines from internal and external sources.
Work on cross-functional teams to design, develop, and deploy data-driven applications and products, particularly within the space of data science.
Lead in prototyping emerging technologies involving data ingestion and transformation, distributed file systems, databases and frameworks.
Work with our data science team on applying improvements to their machine learning algorithms and platforms.
Design, build, and maintain tools to increase the productivity of application development and client facing teams.
Partner with business analyst to define, develop, and automate data quality checks.
Review developed solutions to solve specific business problems. Optimize queries, data models, and storage formats to support common usage patterns.
Design and develop big data applications and data visualization tools.
Code structure moves beyond procedural ad-hoc workflows and exhibits modularity and comprehensive test cases. Develops code at a high-level of abstraction – writes, maintains, and reuses utilities/libraries across projects.
Influences strategy related to processes and workflows across their division.
Participates in mentoring junior colleagues.
Specialized Knowledge & Skills Requirements
Demonstrated experience providing customer-driven solutions, support or service.
In-depth knowledge of SQL or NoSQL and experience using a variety of data stores (e.g. RDBMS, analytic database, scalable document stores)
Extensive hands-on Python programming experience, with an emphasis towards building ETL workflows and data-driven solutions. Able to employ design patterns and generalize code to address common use cases. Capable of authoring robust, high quality, reusable code and contributing to the division’s inventory of libraries.
Expertise in big data batch computing tools (e.g. Hadoop or Spark), with demonstrated experience developing distributed data processing solutions.
Applied knowledge of cloud computing (AWS, GCP, Azure).
Knowledge of open source machine learning toolkits, such as sklearn, SparkML, or H2O.
Solid data understanding and business acumen in the data rich industries like insurance or financial
Applied knowledge of data modeling principles (e.g. dimensional modeling and star schemas).
Strong understanding of database internals, such as indexes, binary logging, and transactions.
Experience using tools for infrastructure-as-code (e.g. Docker, CloudFormation, Terraform, etc.)
Experience with software engineering tools and workflows (i.e. Jenkins, CI/CD, git).
Practical experience authoring and consuming web services.
Education and Licenses
Bachelor’s degree in computer science or related field, or equivalent combination of education and experience.
Travel Requirements
This position requires travel up to 10% of the time.
Additional Job Information:
Top candidates will have 5-10 years of post-academic data engineering experience. This is not a Business Intelligence role nor an ETL Developer.Depending on qualifications, candidates may be hired at a different levels.We are seeking candidates with demonstrated experience using Python and SQL--a coding challenge will be incorporate into the selection process.Knowledge and experience of cloud platforms a plus, specifically AWS.Candidates experience with Python, AWS, and/or Hadoop preferred.

Offer to selected candidate will be made contingent on the results of applicable background checks.

Offer to selected candidate is contingent on signing a non-disclosure agreement for proprietary information, trade secrets, and inventions.

Relocation assistance is available.

Stay connected: Join our Talent Community!

LI:DB1"
Data Engineer,"Senior Data Engineer
Work Location


Chicago

Job Code

3299

Apply Now

Senior Data Engineer

Job Location: Chicago, IL

Join our team

Centric Consulting is not the typical consulting company. We live our core values and bring them with us in all we do for our clients and community. Our core purpose is simple: every day we strive to provide unmatched customer and employee experiences and foster a culture of respect, diversity, and transparency. It’s this culture, our people, and delivery that come together to make Centric a trusted advisor to local businesses.

Our Chicago business unit is part of a larger group of business and technology experts at the company, but we add a lot of individual expertise and personality to our work. We care about positively influencing the people around us. To help accomplish this, we connect local leaders and encourage knowledge sharing. We’re looking for ­­­­­­­­­­­a Senior Data Engineer to join our growing team.

A successful candidate will have the following characteristics:

· 7-10 years of experience in data transformation and extraction: some combination of ETL/ELT including auditing and quality control, table and database design, query design, performance analysis and optimization

· Senior-level design and architecture experience in data transformation. Personal responsibility for data engineering design

· Significant experience with Informatica Integration strongly preferred; extensive experience with related tech (SSIS/ADF, Talend, Pentaho, etc.)

· Significant experience with one of more data-preparation tools (Alteryx, Paxata, Catalytic) and/or capabilities (DataBricks, Python, Azure Functions) strongly preferred

· Experience managing and mentoring data engineering teams preferred

· Experience working with offshore engineering and QA teams preferred

What makes Centric different?

· Special Culture – Our people make us different. We have highly talented, intelligent individuals across a broad variety of disciplines – who are eager to learn from you and share their own expertise. We embrace fresh perspectives and each other. Don’t take our word for it – check us out on Glassdoor, Facebook, Twitter or Instagram to get a glimpse inside what makes us different.

· Growth opportunities – As a mid-size firm, there is more flexibility when it comes to career paths. Figure out what you want to do and we'll help you figure out how to get there.

· Impact – We think of ourselves as a big company with a small company feel – a local player with global reach that combines business, technology and industry expertise.

· Unmatched Experiences – We are allowed to be ourselves here. We are encouraged to be human. It’s at the root of who we are as a firm and why we’re here.

· Innovation – We value passion, determination, perseverance, and innovation. We are inspired because we believe in what we are doing and where we are going.

· Passion for the greater good – We are steadfast in our devotion to the communities we serve and in actively promoting employee involvement in community improvement projects.

Centric believes the best solutions come from diverse teams. We strive to have an environment where everyone has an opportunity to be successful regardless of their race, color, religion, gender, national origin, ancestry, age, disability, military or veteran status, sexual orientation or gender identity."
Data Engineer,"The Job Details are as follows:

OVERVIEW

We are looking for creative and enthusiastic Data Engineers to join our team in building the best Data Platform on the street and enabling our investment teams to monetize data assets. We treat our data systems as software systems and engineer them accordingly. In your role, you will:
Develop cloud-first data ingestion processes using Python, SQL, and Spark
Engineer data models and infrastructure for a wide variety of market and alternative datasets
Design and build services and plugins to enhance our Data Acquisition Platform
Maintain alerting systems to ensure smooth day-to-day operations for hundreds of datasets
Author tests to validate data quality and the stability of the platform
Investigate and defuse time-sensitive data incidents
Communicate with data providers to onboard new datasets and troubleshoot technical issues
Evangelize best practices to our partners throughout the firm
Work directly with Analysts, Quants, and Portfolio Managers to understand requirements and provide end-to-end data solutions
WHAT YOU’LL BRING
Bachelors/Masters degree in Computer Science or a related field
3+ years experience with at least one of Spark, Airflow, data warehousing
Strong analytical, data and programming skills (Python/SQL/NoSQL/Spark)
Ability to understand and contribute to our existing data system software
Experience containerizing workloads with Docker (Kubernetes a plus)
Aptitude for designing infrastructure, data products, and tools for Data Scientists a plus
Strong oral and written communication skills, most importantly, must be a team player"
Data Engineer,"77 West Wacker Dr (35012), United States of America, Chicago, Illinois

At Capital One, were building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Data Engineer

Do you want to work for a tech company that writes its own code, develops its own software, and builds its own products? We experiment and innovate leveraging the latest technologies, engineer breakthrough customer experiences, and bring simplicity and humanity to banking. We make a difference for 65 million customers. We're changing banking for good. At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who love to solve real problems and meet real customer needs. We want you to be curious and ask what if? Capital One started as an information strategy company that specialized in credit cards, and we have become one of the most impactful and disruptive players in the industry. We have grown to see ourselves as a technology company in consumer finance, with great opportunities for software engineers who want to build innovative applications to give users smarter ways to save, transact, borrow and invest their money, as we seek to disrupt the industry again. As a Capital One Software Engineer, you'll work on everything from customer-facing web and mobile applications using cutting-edge open source frameworks, to highly-available RESTful services, to back-end Java based systems using the hottest techniques in Big Data.

You'll bring solid experience in emerging and traditional technologies such as: node.js, Java, AngularJS, React, Python, REST, JSON, XML, Ruby, HTML / HTML5, CSS, NoSQL databases, relational databases, Hadoop, Chef, Maven, iOS, Android, and AWS/Cloud Infrastructure to name a few.

You will:

- Work with product owners to understand desired application capabilities and testing scenarios

- Continuously improve software engineering practices

- Work within and across Agile teams to design, develop, test, implement, and support technical solutions across a full-stack of development tools and technologies

- Lead the craftsmanship, availability, resilience, and scalability of your solutions

- Bring a passion to stay on top of tech trends, experiment with and learn new technologies, participate in internal & external technology communities, and mentor other members of the engineering community

- Encourage innovation, implementation of cutting-edge technologies, inclusion, outside-of-the-box thinking, teamwork, self-organization, and diversity

Basic Qualifications:

- Bachelors Degree

- At least 3 years of SDLC experience using Java technologies

- At least 3 years experience with leading big data technologies like Cassandra, Accumulo, Python, HBase, Scala, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper

- At least 1 years experience in one of the following Cloud technologies: AWS, Azure, OpenStack, Docker, Ansible, Chef or Terraform

Preferred Qualifications:

- Master's Degree

- 2+ year experience with Spark

- 3+ years experience developing software solutions to solve complex business problems

At this time, Capital One will NOT sponsor a new applicant for employment authorization for this position."
Data Engineer,"Job Description
What We Do
Kalderos delivers technology that solves the challenges facing the US healthcare system. At Kalderos, we develop technology solutions with a focus on simplifying the complex coordination of drug discount programs from exhaustive data services to intelligent reporting to issue resolution.

To learn more: https://www.kalderos.com/company/about

What Data Engineers Do

The Data team’s mission at Kalderos is to provide data modeling expertise to internal teams, and build data storage and processing platforms that can scale as Kalderos scales. They can work within the Ledger and Master Data team, the Data Science team, or others as needs arise. Below are some possible day to day tasks for a data engineer at Kalderos.
Work with product teams to understand and develop data models that can meet requirements and operationalize well
Build out automated ETL jobs that reliably process large amounts of data, and ensure these jobs runs consistently and well
Build tools that enable other data engineers to work more efficiently
Try out new data storage and processing technologies in proof of concepts and make recommendations to the broader team
Tune existing implementations to run more efficiently as they become bottlenecks, or migrate existing implementations to new paradigms as needed
Learn and apply knowledge about the drug discount space, and become a subject matter expert for internal teams to draw upon
What We Are Looking For
4+ years work experience as a Data Engineer in a full-time role
Candidates must be authorized to work in the US and be able to work in Chicago/Milwaukee
Excellent project managements skills, and familiarity working in an agile environment
Some ways you may demonstrate this are:
Describing a time in which you had a large project you needed to manage
Relevant work experience on a Agile team
Proven data engineering experience that involved creating, and maintaining a database and implementing data processing pipelines
Some ways you may demonstrate this are:
Professional experience described on your resume
Open source implementations of data intensive applications
A personal side project that involved maintaining a database like a website
Online course completion in relevant areas
Vendor certification in a relevant field
Programming experience with a modern computing language that supports data engineering work (Python, C#, JVM-languages like Scala)
Some ways you may demonstrate this are:
Professional project descriptions on your resume
GitHub repositories that have working code that you created
Sending a code snippet that exemplifies your work
Advanced SQL skills and understanding performance trade-offs of various SQL implementations
Some ways you may demonstrate this are:
Professional project descriptions on your resume
SQL Certifications you have earned
Answering SQL questions in a phone interview
Advanced SQL queries you wrote in your Github repository
What May Set You Apart
Experience with SQL databases like MS SQL Server
Experience with Azure cloud computing such as hosted databases
Experience with streaming technologies such as Kafka or Event Hubs
Experience with orchestration frameworks like Azure Data Factory or Airflow
Experience with NoSQL data stores such as MongoDB and CosmosDB
Experience in the healthcare or pharmaceutical industries
Experience with large scale migrations of databases
Experience creating RESTful APIs to service data needs for web applications
Experience with implementing Software Development Lifecycle approaches to database work, such as Testing, CI/CD and other automation

If you think you meet some of the list of the above, but not everything, that’s perfectly fine! We encourage you to apply regardless. At Kalderos, we know talent comes in many forms, and we’re willing to look beyond a set of rigid criteria to find the right teammate.

Company Perks
401K plan with matching
Healthcare benefits
Flexible schedule and a fair PTO system that allows for a healthy work-life balance
Opportunity to work on new technologies and learn new skills
Celebration, and education stipends

To learn more: https://www.kalderos.com/company/culture

Kalderos is an equal opportunity workplace. We are committed to equal opportunity regardless of race, color, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, or veteran status.

Due to the circumstances of the COVID-19 pandemic, Kalderos has decided to protect our current and future employees by shifting to an entirely remote workforce. We will continue to operate, interview, onboard, and work remotely. Please be aware that some of our roles will not be remote long-term and will return to an office setting once we're safe to do so following the guidance of local health authorities and the CDC."
Data Engineer,"Position: Data Engineer
Location: Bristol, CT
Type: Permanent/Direct-hire*
Start: ASAP

*Sorry, this client is unable to sponsor or work with 3rd party vendors*

Requirements:
Experience on Apache Airflow tool.
Must have hands on experience implementing AWS Big data lake using EMR and Spark.
Working experience on AWS platform.
Experience in using CI/CD pipeline (Gitlab)
Experience in Code Quality implementation (Used Pep8/Pylint) tools or any other code quality tool.
Experience of Python Plugins /operators like FTP Sensor, Oracle Operator etc.
Implement Industry Standards /Best Practices.
Excellent analytical and problem-solving skills
Business acumen to work directly with clients. Excellent verbal and written communication skills"
Data Engineer,"Description


At our heart, we are technologists with a keen understanding of business environments, challenges, and initiatives. Our team of experts bring decades of technical and consulting knowledge, project leadership, and an understanding of complex problem-solving. We have an established track record in all things Microsoft Azure and Microsoft 365 including Application Services, Azure Infrastructure, Data Platform Services, Machine Learning, User Experience, and IoT.

We are seeking Data Engineers as a critical part of our Azure Cloud Enablement (ACE) team.
Location: Chicago IL
Travel: Up to 30%
ACE in a nutshell


The ACE team sits at the intersection of technology, innovation, creativity, and blended learning. At our core, we are obsessed with providing customers with the best solutions to their biggest challenges.

We build B2B enterprise software: smart vehicles, custom web apps that hit hundreds of thousands of people, and big data warehouses are just some of what you will have the opportunity to develop. We are Microsoft Gold Certified in Application Development, Cloud Platform, Data Analytics, Data Platform and many more.

You’ll work out of our bustling Chicago office, side-by-side with experts in all things Data Platform, Data Science, Application Development, User Experience, and Cloud Infrastructure. Expect rapid career growth—we preach recognition and reward those with the aptitude and attitude to get it right.

Once you’re here…


You'll have full access to our reference architecture. Our services methodology is built on reusable technology components, patterns, and processes that have been refined and proven on thousands of production solutions. You’ll learn to develop complex data architectures, solve for real-time data needs of large businesses, and navigate the changing landscape of the Azure stack.

You will collaborate regularly with our experts, work across numerous industries, clients, and technologies. You will learn something new every day and begin adding value to our team almost immediately. You’ll join our flagship Launch Program to get you up to speed as quickly as possible.

You will partner with clients across several key industries including Healthcare, Manufacturing and Industrial Products, Financial Services, Professional Services, Engineering and High Technology, Retail and more.

Beyond the tech, you'll benefit from dollar for dollar 401(k) matching (up to 6% of your salary), ongoing structured learning opportunities, subsidized healthcare, vision, and dental plans (amongst others), a fully stocked snack, coffee, and drink bar, and a laid-back team looking to disrupt the status quo.

Relevant experience


Our basic requirement is that you must have completed a relevant job, program, degree, or internship at some point in the recent past. You should have experience working with others collaboratively, thinking critically about solving problems and technically executing. You should be hungry to learn new technology, humble with your willingness to help others, and socially capable of working collaboratively.
You must be able to work with customer stakeholders to understand business and technical requirements.
You should have skill in SQL at an advanced level, including complex queries, data definition, constraint specification, coding with SQL or similar, and database modeling.
You are inherently iterative, with knowledge of Agile methodologies, estimation techniques, and (optionally) workshop and prototype techniques.
You have a strong understanding of one or more of the following:
Power BI or similar data visualization tool
Azure or similar cloud platform
Python or Spark or similar Data Engineering language
PowerShell or similar scripting language
Azure Data Warehouse (ADW) or similar MPP database platform
Azure Data Lake (ADL)
Nice to haves:

Experience with Azure Data Platform (Azure Data Factory v.2, Azure Data Warehouse, Databricks, etc.)
Knowledge of the overall internal structure of a database system including indexing, query processing, transaction management, and fault tolerance.
Ability to design a database, implement a design in a real database system, and construct user interfaces to the database via an API or 3rd party tool.
2+ internships in related roles."
Data Engineer,"Job Description
Job Overview

We are looking for a Data Engineer to join our growing team of analytics experts, where you will have the opportunity to author and manage data pipelines from ingest to insights and all the plumbing in between. The Data Engineer will support our data scientists with both existing and net new projects, ensuring that data delivery is consistent and optimized. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.

Responsibilities for Data Engineer
Create and maintain optimal data pipeline architecture.
Assemble large, complex data sets to meet functional and non-functional business requirements.
Author the pipeline code required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Work with Data Scientists and Systems Engineers to design data delivery architecture.
Work with stakeholders including the Executive and Product teams to assist with data-related technical issues and support their data insight needs.
Create data tools for team members that assist them in building and optimizing our products.
Qualifications for Data Engineer
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. PostgreSQL administration familiarity a plus.
Strong Python scripting skills. Ruby a plus.
Familiarity with API endpoint interactions and techniques for handling query complications.
Understanding of Containerization, Micro-Service, and Server-less a strong plus.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Working knowledge of message queuing, stream processing, and highly scalable data stores.
Experience supporting and working with cross-functional teams in a dynamic environment.
The ideal candidate would have 3+ years of experience in a Data Engineer role, or 5+ years in any Software Engineering role with demonstrable familiarity of the Data Engineering/Science space. Work experience, academic instruction, and/or code portfolio will all considered.
Powered by JazzHR

sEaphUQBLW"
Data Engineer,"Chicago, Illinois
Skills : AWS (s3, redshift, EMR, EC2, lambda, SNS), unix shell scripting,python spark,scala
Description : MUST HAVE

AWS (s3, redshift, EMR, EC2, lambda, SNS),
unix shell scripting,python

spark,scala


PREFER TO HAVE

snowflake,presto,
arrow,Airflow,Hadoop,Hive

AWS (s3, redshift, EMR, EC2, lambda, SNS), unix shell scripting,python spark,scala"
Data Engineer,"We are looking for a talented, passionate Data Engineer with the skill and desire to contribute to our team. In simplest terms, you'll be building the foundation upon which we grow our business.

If you have experience working to extract knowledge and actionable information from multiple data sources, then we would love to talk with you. If you are the type of person who comes to work every day expecting to learn, contribute, teach, and have fun, then we think you will fit right in.

About our Team

We aim to derive meaning from our data, enabling us to run our business better, and also equip our clients to do the same. We believe in agile software development (lowercase 'a') and use elements from Scrum and Lean as a base for how we manage our work. 'Inspect & Adapt' is more than just a catchphrase to us. Most of our development work is done in Python and we use Airflow for orchestration of our data pipelines. We pair as-needed, but not as a rule. We are willing to have every problem under the sun exactly once, in exchange for never having the same problem twice.

Core Responsibilities & Qualifications
In this role, you will focus on the design, implementation, operation, and refactoring of data management systems to meet the Brads Deal's business needs. This includes designing how the data will be stored, consumed, and integrated into our systems.
You will take business requirements, transform them into data models, and develop ETL processes to populate those models. We are a small (and growing) team so it's important that you enjoy working a project from end-to-end.
You will identify, design, and implement internal improvements to how we process data. This could include automating manual processes, optimizing data delivery, and re-designing infrastructure for greater scalability.
You should have hands-on experience with a variety of the data warehousing concepts and practices, covering both technical development as well as 'not-necessarily technical practices' such as data governance. This list of experiences includes data manipulation, database partitioning, data structures, data management, and best engineering practices.
You should have advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of database platforms.
You have a successful history of manipulating, processing and extracting value from large disconnected datasets.
You have experience with scripting languages (We use Python).
In addition to developing and implementing ETL processes, you have experience dealing with performance and scaling issues.
You have at least 3+ years of end-to-end experience with data warehousing and BI systems. This includes data modeling, ETL architectures, OLAP, and Big Data tools (such as Hive).
You have hands-on experience with AWS and the different data tools that are offered on that platform (Redshift, EMR, Kinesis, S3, etc).
About Us

Shop Smart, one of Forbes' 100 Most Promising Companies and a 101 Chicago Best and Brightest Organization, publishes deals and coupons from national retailers across a network of sites. The flagship, Brad's Deals, is the largest editorial-driven deal site with over 5m monthly unique visitors.

Launched in 2002, the rapidly growing company has changed the way millions of consumers shop and is a leader in our space and in the Chicago technology community. We have relationships with over 2,500 top retailers (Amazon, Target, Dell, etc.), exposure to over 50M consumers a year and a consistent national media presence (USA Today, Today Show, WSJ, MSNBC).
Please note: Due to the potential personal and business impacts of Covid-19, Brad's Deals is taking a proactive approach by allowing our employees to work from home. As such, our hiring teams will be conducting virtual interviews with potential candidates as we continue to monitor the effects of the pandemic across our local community."
Data Engineer,"Job Description
Our Mission

With wasteful healthcare spending in the U.S. estimated at $935 billion, we take our role as a healthcare innovator very seriously. Simply put, Apervita is on a mission to change the face of healthcare. We intend to do this by continuously improving the value of healthcare delivery through performance, measurements and insights.
Recognized as the trusted source for performance measurement through our proprietary cloud-based rules engine, we address the complexities of an industry transitioning to accountable-based care arrangements. Serving one of the nation’s largest regulatory organizations, more than 1-in-2 U.S. hospitals and several nationally recognized health plans, Apervita conducts more than 10 billion value-based computations annually, providing vital insights that enable collaboration, build trust and advance value-centric care.

As we move into our next phase of growth, we’re looking for smart, passionate and fearless professionals to help us pursue our mission. We’ve earned the trust of our investors, including Optum Ventures, Baird Capital, Pritzker Group Venture Capital and Math Venture Partners, to further grow and scale our business in 2020. We’ve also been recognized as an industry leader by Gartner, CIOReview, Healthcare Tech Outlook, and Healthcare Informatics. Always in search of the next “first,” Apervita was recently named the first company to certify electronic clinical quality measures from the National Committee for Quality Assurance (NCQA) using our proprietary clinical quality language (CQL) ingestion engine. This is a huge step forward in healthcare quality innovation and an indication of where we’re headed. We’re clearly on the cusp of something big and bold and can’t wait to see what happens next.

Are you up to the challenge?

Data Engineer

About the role
The Data Engineer is responsible for deploying and managing ETL/ELT pipelines, jobs, orchestration frameworks, and ensuring data quality. The charter of the data engineering team is to optimize how our customers data turn into insights as fast as possible meeting customer SLA’s. The data engineer can expect to work closely with business analysts, data scientists, analytics experts and developers to build the ETL/RLT pipelines, and data models. The data engineer will also have the opportunity to inform the design, implementation, and best practices or this system including deployment of modern tools.

Responsibilities
Build cloud-based data warehousing environments, data processing pipelines, and data models that support a variety of business needs
Support a variety of data processing pipelines, integrate new data sources into our data warehouse, and create jobs to load, transform, and QA vital datasets
Work with analysts and developers in the product development process to ensure that newly designed data models meet analytics requirements and follow best practices
Share your expertise on scalable data processing with analysts and data scientists to further our goal of being a truly data driven organization
About you
5+ years of experience as a data engineer using data warehousing technologies like, Amazon Redshift, RDS, S3, Athena, EMR, and Hadoop/Hive/Spark
Proficient in SQL including one or more relational databases like MySQL, Oracle, Postgres, or similar
5+ years experience with ETL and job scheduling or orchestration using tools like Airflow, Luigi, Oozie, or similar
5+ years experience programming in python and familiarity with AWS and git
Excellent communication skills and ability to work on a growing team
Bonus points if you have
Experience with web-scale data or working with healthcare data in a HIPAA-compliant environment
Experience with Healthcare Payer data as well as Optum Impact Intelligence tool.
Experience with data modeling, data visualization, and/or BI tools like Looker, Metabase, or Tableau
Experience with AB Testing
Benefits
Stock Options
PTO
Unlimited sick and sanity days
Commuter benefits
Medical, Dental, Vision
401K with matching
Unlimited snacks in office
Powered by JazzHR

17RCjfuDup"
Data Engineer,"Role Data Engineer Location Richmond, VA Chicago, IL Duration 7+Months (will extended to 1 year) Job description Required Technologies Strong Programming experience with object-orientedobject function scripting languages Python. Experience with big data tools Hadoop, Apache Spark Experience with AWS cloud services S3, EC2, EMR, RDS, Redshift Experience with stream-processing systems Storm, Spark-Streaming, etc. (Nice to have) Experience with relational SQL, Snowflake and NoSQL databases Detailed overview of functional and technical role expectations Candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have working experience using the following softwaretools 3+ years of experience (Mid-level) Strong Programming experience with object-orientedobject function scripting languages Python 3+ years of experience (Mid-level) Experience with big data tools Hadoop, Apache Spark, Kafka, etc 1+ years of experience with AWS cloud services S3, EC2, EMR, RDS, Redshift. Experience with stream-processing systems Storm, Spark-Streaming, etc. (Nice to have). 1+ Years of experience with relational SQL, Snowflake and NoSQL databases, including Postgres and Cassandra. Responsibilities for Data Engineer Create and maintain optimal data pipeline architecture, Assemble large, complex data sets that meet functional non-functional business requirements. Identify, design, and implement internal process improvements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability etc. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS 'Big data' technologies. Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader. Required Skills Python, Hadoop, Apache Spark, AWS, SnowflakeSQL knowledge. -- Thanks Regards, G. Naga Tarun US IT Recruiter Email tarunkeylent.com Direct 407-801-8682 linkedin.cominnaga-tarun-b741b7140 Keylent Inc 1000 N West Street, Suite 1200 Wilmington,DE,19801 www.keylent.com httpwww.keylent.com"
Data Engineer,"Data Engineering is the foundational layer of our Data & Analytics stack at Arrive. Data Engineers not only build and optimize data pipelines that transform and store data in a way that allows the rest of the organization (analysts, data scientists, other stakeholders) analyze and consume data. They are also in charge of building the systems and establishing the processes to enable the rest of the data team to develop, test, and deploy analyses and code in an efficient and scalable way.

We’re looking for an experienced Data Engineer to help us grow our data infrastructure and platform. If you're seeking a role that is high impact and full of ownership....please read on.

Please note...we are fully open to remote candidates for this position! Being in Chicago would be great, but not necessary.
What you'll tackle
Design new enterprise data models and ETL processes to populate them
Extract and transform data from production databases and 3rd party services to provide consumable data and support functions across the organization
Detect quality issues, track them to their root source, and implement fixes and preventative audits
Manage and optimize Redshift clusters/data lake to ensure current health and performance and future scaling needs
Help maintain the process we use to develop, test, and deploy good code
Become the “go to” expert of our data. Work closely with staff to understand all data from our core systems, partner services, and any other platforms we rely on
What you bring to the table
Experience with AWS; expertise in Redshift, Postgres or other RDBSs (preferably column-oriented)
Expertise in SQL and ability to write and optimize complex queries
Experience with Docker, Elastic Container Service, Lambda a plus
Ability to write customized software in Python, Bash, Go or other common open source languages. Experience with Airflow or similar scheduling service a plus
Experience with CI/CD tools like Jenkins or Drone
Creativity in approaching data organization challenges with an understanding of the end goal
A collaborative nature and entrepreneurial spirit. Prior startup experience a huge plus"
Data Engineer,"About XSELL

Ready to write the best chapter of your career? XSELL Technologies is an artificial intelligence company focused on increasing sales. Our cloud-based machine learning engine uses predictive analytics and natural language processing to equip sales professionals with the best real-time responses, driving improved conversion rates and customer experiences. We pride ourselves on our high performing, collaborative culture. We are passionate about our product, our clients, and our industry leading results.

XSELL is currently seeking a Data Engineer to serve as a data expert to help structure, manage and optimize our applications data layer. This role will partner with the Engineering team and ensure the data layer powers the model development and reporting for our customers. The XSELL platform using a combination of data science and machine learning to derive the best actions, tactics, and strategies from top customer representatives and make those available to all representatives driven by sales and service outcome data.

The ideal candidate is an experienced data pipeline builder and wrangler who enjoys optimizing data systems. The Data Engineer will support our software developers, data scientists and customer success managers on data initiatives and will be on the ground floor as we scale our data architecture.

Job Description
Mapping heterogeneous data sources, including descriptions of the business meaning of the data, its uses, its quality, the applications that maintain it and the database technology/schema in which it is stored.
Managing a task-queue/message broker architected system for data pipelines and interfacing with the infrastructure team
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, data indexing, re-designing infrastructure for greater scalability, etc.
Document data relationships and translate business rules to data rules.
Build a scalable infrastructure and tiered persistence that will enable our data scientists to derive insights and build models faster and more efficiently
Work with stakeholders including the Executive, Product, Data Science, and Client Success teams to assist with data-related technical issues and support their data infrastructure needs.
Be able to perform manual data quality testing as requested in business requirements.
Deploy these data flows in modern cloud environments: AWS and Azure.
Background & Qualifications

In order to be successful, you will need the following:
2+ years of experience in a Data Engineer/Data Architect role and knowledge of building data pipelines from multiple data sources including third party APIs, flat files or DB queries.
Advanced working knowledge of SQL and Relational Databases.
Python with scientific python libraries (numpy, pandas, scikit-learn, etc)
Be a consummate collaborator, able to establish good relationships with technical, product, and business owners also to work with vendor technical teams
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large distributed datasets.
Preferred:
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience in developing and maintaining ETL/ELT data pipelines.
Having experience in the following software/tools/languages:
PostGres SQL, Amazon RDS, EC2 and EMR
Python, Git
Flask, Redis, Kubernetes
Sisense Self Hosted instances or other Reporting platforms.
We provide competitive compensation, generous benefits, and a professional atmosphere. XSELL fosters an entrepreneurial, results driven work environment where you will have the opportunity to be part of a collaborative, inclusive team and be able to grow and develop your professional career.

XSELL Technologies is an Equal Employment Opportunity Employer and all employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Powered by JazzHR"
Data Engineer,"5+ years of experience in a Data Engineer role Experience with big data tools Hadoop, Spark, Kafka and relational SQL and NoSQL databases,"
Data Engineer,"RAPP Chicago is looking for a Data Engineer

to join our rapidly growing Technology Team.


ABOUT RAPP

Our purpose

We are the agency absolutely, utterly, fiercely focused on the individual. We use our data, technology and creative smarts to make meaningful, connections with every single person a brand knows.

Our family

We are part of the OPMG group, which is in turn a part of Omnicom. This group also includes Critical Mass, Targetbase, Proximity, Credera, and sparks&honey and other well-known agencies.

Our clients

From national defense, to buying a sweet new ride, to biting into a Big Mac®, we provide smart solutions for companies like Army, Toyota, McDonald's, and more.

We are looking for people who want to strategically and functionally lead a business that is based in database marketing principles, anchored in data and insights at the core, and understand the need to create a connected experience through traditional and emerging channels, across all disciplines-data-driven creativity at its finest.

ABOUT YOU

As a data engineer, you define solutions for the use, extraction and manipulation of data - driven by the balanced combination of business needs and consumer preferences. You work in close collaboration with multidisciplinary teams to provide the data needed, in the optimal format for making critical, real-time decision.

You can communicate through a project with ease and understanding and you know your tools like the back of your hand.

You are experienced with writing complex SQL statements and mapping relational database structure. You strive in designing & continuously improving data workflows using enterprise campaign management solutions such as Adobe Campaign, Unica, Eloqua, Marketo and/or Salesforce Marketing Cloud.

You may have experimented in the past with BI tools such as Tableau, Domo or QlikView, and have R/Python in your radar

Most importantly not only can you do the job, you can explain it to a relative and they actually understand. You are self-directed, highly motivated and have fun while working hard in a fast-paced environment."
Data Engineer,"GoHealth is looking for Data Engineers who will be responsible for the design, development, and delivery of its various batch and streaming data pipelines. We are seeking candidates who have experience in building large batch pipelines, as well as experiencing building stable, high throughput streaming systems. In this role, you will work with other members of Engineering, Product and Project Management, and various business groups to ensure timely availability of usable data to all parts of the business that need it.

Due to the unprecedented situation of COVID-19, GoHealth has decided to protect our current and future employees by managing our business remotely. This is inclusive of interviewing, onboarding and each role day-to-day. Please consider that our roles will not be remote long-term and will return to an office setting once we're safe to do so following the guidance of local health authorities' and the CDC.

Responsibilities:
Design, develop and deploy batch and streaming data pipelines.
Monitor and ensure operational stability of data pipelines.
Create and maintain documentation of the technical detail design, operational support and maintenance procedures for all data pipelines.
Ensure data quality and compliance with development, architecture, reporting, and regulatory standards throughout entire data pipeline.
Collaborate with the rest of the Engineering Team, subject matter experts and department leaders to understand, analyze, build and deliver new data-related processes and/or reports.
Skills and Experience:
Bachelor's Degree in computer science or equivalent experience required.
2+ years of experience in the design and development of data pipelines and tasks.
Strong analytical and problem solving ability with strong attention to detail and accuracy.
Understanding of data warehousing concepts and dimensional data modeling.
Hands-on experience with troubleshooting performance issues and fine tuning queries.
Experience extracting data from relational and document databases.
Experience consuming data over HTTP and in formats such as HTML, XML, and JSON.
Knowledge of and experience with a version control system (such as Git, Mercurial, SVN, etc).
Proficiency in Java or Python programming languages.
Familiarity with data warehousing platforms, such as Redshift, Snowflake, SQL Server, etc.
Benefits and Perks:
Open vacation policy
401k program with company match
Medical, dental, vision, and life insurance benefits
Flexible spending accounts
Commuter and transit benefits
Professional growth opportunities
Casual dress code
Generous employee referral bonuses
Happy hours, ping-pong tournaments, and more company-sponsored events
Subsidized gym memberships
GoHealth is an Equal Opportunity Employer
#LI-JC1"
Data Engineer,"Concurrency, Inc. is seeking a Data Engineer which requires both broad and deep technology knowledge and the ability to develop data solutions by mapping customer business requirements to end-to-end technology solutions. Demonstrated ability to engage and deliver in technical projects related to agility, business value, data warehousing, and cloud-oriented data solutions is also a must. Data & AI Engineers are key enablers for other consultants and partner staff by sharing knowledge in large enterprise implementations, best practices, and frameworks or patterns.
Responsibilities
Assist and author functional requirements and technical design documentation
Participate in milestone meetings and planning discussions for aligned client projects
Work with BA and PM teams to plan project sprints, scope and resource allocation
Manage at project milestones to ensure successful solution delivery and client satisfaction
Continuous learning and research on modern data solutions, and relevant technologies
Promote service offerings through blogs posts, industry groups, and speaking events
Qualifications
Bachelor's Degree in Computer Science or related fields, or the equivalent in work experience
Three or more years of experience in enterprise systems and warehouse architecture design, development, testing, deployment, and operational support
Experience with advanced integration scenarios such as hybrid cloud architectures
Experience with enterprise architecture, server topologies, and distributed systems
Knowledge of Agile methodologies
Demonstrated verbal and written communication skills
Demonstrated technical documentation skills
Prior consulting experience is a plus
Skill Requirements
Azure ETL tools such as Synapse, Databricks or Data Factory
Azure Data Platform experience required
Data Modeling (Tabular)
Data security monitoring and data governance
Power BI
SQL Server and T-SQL Development
Up to 15% travel required
Machine Learning languages such as R or Python a plus
IoT experience a plus
Spark experience a plus

Encouraging a healthy work/life balance and providing our colleagues great benefits are just part of what makes Concurrency a great place to work. Concurrency full-time employees receive complete and competitive benefits. We offer a collaborative work environment, competitive compensation, generous work/life opportunities and a comprehensive benefits package that includes paid time off plus holidays. In addition, all colleagues are eligible for a number of rewards and recognition programs, excellent training program and bonus opportunities."
Data Engineer,"Are you interested in building the future of healthcare and transforming the patient experience? Are you hopeful about what data and medical research can do to improve medicine? We’re looking for a Data Engineer to ensure PatientIQ remains on the forefront of using data to drive positive healthcare outcomes.

As a core member of the Analytics department, you will be in a very dynamic environment that works cross-functionally with all other departments, such Engineering, Sales, and Product. You will work on a broad array of problems that rely heavily on solid data engineering principles being in place: business intelligence, data science, and software engineering. We heavily value diligence, curiosity, and initiative, as those are key to unlocking the value of PatientIQ's data for our users and our decision-making. Your work will be impactful across the entire organization.

Role Responsibilities
Develop and maintain ETL infrastructure to support the ingestion of external data sources
Migrate client data into PatientIQ's platform per established service level agreements (SLA)
Define and implement key metrics for PatientIQ's data warehouse
Perform data quality assurance checks
Help scale PatientIQ's data strategy as the platform and business grows
Requirements

Ideal Qualifications
BS/MS in Computer Science, Engineering, Mathematics, or related field
Deep knowledge of SQL and at least one database technology
Proficient in Python
Experience with version control systems (e.g. git) and writing reusable and extensible code
Highly self-motivated with strong analytical problem-solving skills and attention to detail
Nice to Haves
Experience with workflow management systems such as luigi or airflow
Experience designing, building, and maintaining ETL infrastructure in a production setting
Experience in machine learning and/or business intelligence
Experience with ETL tools like Apache Kafka, Logstash, Segment, Informatica
Experience with cloud technologies such as AWS, Google Cloud Platform, or Azure
Experience working in Healthcare, Finance or another regulated industry
Benefits
Great Benefits - top-notch health, dental and vision insurance. Additional perks available including 401K.
We are Mission Driven - our team is motivated to solve complex problems, drive medicine forward, and ultimately improve patient outcomes.
True Idea Meritocracy - great ideas win out. We encourage all team members to challenge the status quo because our mission demands this.
Flexible Time Off - we trust you to take the time you need when you feel it is appropriate, given your workload and responsibilities. No need to track it or save up.
World-Class Team - we’re at the top of our industry because of our employees. They’re the best investment we can make, and we never forget that.
Fast Growing - we are building the largest platform for healthcare providers, industry partners, researchers, and others to collaborate on the mission to improve patient outcomes."
Data Engineer,"Founded in 2016, Leaf Trade is a technology company whose platform facilitates ordering and fulfillment in highly regulated cannabis markets. Using the features on Leaf Trade’s platform, vendors can streamline all order fulfillment processes, increase sales through custom storefronts, and generate powerful reports and analytics. Dispensaries can leverage Leaf Trade’s tools to discover trending products through its data-driven shopping process, easily request samples and marketing materials, and manage orders from their customers from start to finish. Headquartered in Chicago, Leaf Trade is partnered with five of the largest multi-state cannabis operators and itself operates in 16 states.
About the Role
Leaf Trade is a fast-paced, high-growth startup in a marketplace that is multiplying in size every day. To continue to lead the marketplace and provide additional value to our clients, we are expanding our capabilities in data science and predictive analytics.
We are looking for a self-managing, organized and intelligent candidate to take on the responsibility of ingesting, staging, and analyzing large transactional datasets. You will manage and prioritize all quantitative analysis projects, and deliver business-quality results partnering with our development team to build products based on your work.
You will provide interesting and actionable insights into the business at a macro - and client level, and build robust predictive analysis tools to help guide internal and client decision making. You will expose your data products via API endpoints that can be used to deliver customer-facing features.
Most of all, you will be a part of the heart of our team, touching all aspects of the business and working with a top-quality team that is driven by data, excellence, and customer experience.
Key Responsibilities:
Architect and design a data warehouse from the ground up.
Create and manage ETL operations.
Determine the most effective way to store, report, and leverage data for data science.
Ability to deliver ad-hoc reporting and identify trends in ad-hoc requests to determine and then build out reporting features that benefit all clients.
Basic Qualifications:
5+ years of SQL experience
5+ years of experience setting up database environments, database schema, and managing deploys/ updates to the schema.
3+ years of experience with ETL tooling.
3+ years of architecting data warehouses.
BONUS POINTS FOR CANDIDATES WITH…
A bachelor’s degree in Actuarial Science, Math, Statistics, Engineering, Data Science, Computer Science or other related majors, or equivalent experience.
Solid experience with Python or Ruby on Rails.
Data visualization knowledge including the usage of Tableau or Power BI.
Experience configuring infrastructure components via code, ideally using Terraform, is a plus.
Experience with AWS, Azure, or Google Cloud Platform (GCP)
Familiarity with scientific tools such as R or SPSS is a plus.
Experience in data visualization.
Published apps, websites, or contributed to open source libraries (feel free to include with application).
Strong intellectual curiosity and passion to learn.
Excellent organizational, communication, and time management skills.
Ability to take complex/ technical ideas and distill them into simple and intuitive code.
Comfortable performing in a fast-paced and ever-evolving business.
Ability to analyze and apply creative problem solving both independently and collaboratively within a team.
Benefits
Competitive salary and meaningful equity
Health insurance
Vacation days, sick days, and corporate holidays
Complimentary gym membership
Fast growth environment with potential for quick upward mobility
Vibrant company culture within a casual environment
Leaf Trade is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans status or any other characteristic protected by law."
Data Engineer,"Requisition Number: 75536

We arewww.Insight.com (NSIT).

As a Fortune 500-ranked global provider of digital innovation, cloud/data center transformation, connected workforce, and supply chain optimization solutions and services, we help clients successfully manage their IT today while transforming for tomorrow.

From IT strategy and design to implementation and management, our employees help clients innovate and optimize their operations to run smarter.
Microsoft Worldwide AI Partner of the Year, 2018
Microsoft Worldwide Modern Desktop Partner of the Year, 2018
Microsoft US Partner Award for Intelligent Cloud - Application Innovation, 2019
Microsoft US Azure Team Partner Choice Award - Data and Artificial Intelligence, 2019
Our Insight Digital Innovation team is searching for an experienced, passionate and professional Data Engineer to join our team.

You’ll utilize the most cutting edge technology such as machine learning, artificial intelligence, big data, IoT, and azure. Here you will have practical, hands-on knowledge of modern data architectures and tools such as data warehousing, ETL/ELT, analytics, and the Azure cloud platform. You should be driven to provide quality solutions to challenging problems. You will work closely with client stakeholders and an award-winning team of engineers, architects and thought leaders to design, build and implement next-generation solutions in advanced analytics, Big Data, BI and the cloud.

What you’ll do at Insight:
You will build enterprise-grade data solutions for a variety of external clients.
Design and code solutions to tough data challenges and provide feedback on others’ work.
Work directly with client stakeholders to develop technical solutions for business cases.
Aggressively grow your skillset and expertise to meet the emerging needs of our clients.
What you’ll need to join Insight:
4+ years of experience working with data and data analytics development within the Microsoft data platform and an excellent grasp of some of following technologies:
SQL Server, Azure SQL Database, and Azure SQL Data Warehouse
Power BI
Tableau
Analysis Services and DAX
Reporting Services
Integration Services
Azure Data Factory
PowerShell scripting
Azure Automation
2 year of experience in some of the following:
Big Data technologies such as Hadoop or HDInsight, Hive, Pig, Python, Spark, Oozie, or any of the other tools with the Hadoop ecosystem
Azure Data Lake and Azure Data Lake Analytics
Predictive analytics: R, Azure Machine Learning
Strong analytical and reasoning skills that result in clear technical execution.
Skill at translating requirements into clean, efficient, quality code
Proven ability to prioritize, self-direct and execute at velocity
Passion to deliver craftsman-quality work both individually and as part of a team
Solid communication skills with both technical and non-technical stakeholders
Desire to learn new skills and grow competencies
Bachelor’s degree in Computer Science or related discipline
Requires travel to Chicago Area client sites (local only).
In the News

•https://searchitchannel.techtarget.com/feature/Data-center-infrastructure-spending-gets-AI-boost

•https://www.insight.com/en_US/about/newsroom/press-releases/2019/07012019-insight-recognized-at-no14-on-crns-2019-solution-provider-500-list.html

•https://investor.insight.com/press-release/insight-enterprises-acquire-pcm-inc

•https://insight.hqprod.businesswire.com/press-release/insight-hosts-global-ai-competition-healthcare-innovation-cincinnati

•https://investor.insight.com/press-release/houston-schools-deploying-iot-enabled-building-safety-platform-improve-emergency
https://investor.insight.com/press-release/insight-recognized-magic-quadrant-managed-workplace-services-north-america
Today's talent leads tomorrow's success. Learn about careers at Insight: jobs.insight.com.

Insight is an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, sexual orientation or any other characteristic protected by law.



The position described above provides a summary of some the job duties required and what it would be like to work at Insight. For a comprehensive list of physical demands and work environment for this position, click here.

Today, every business is a technology business. Insight Enterprises, Inc. empowers organizations of all sizes with Insight Intelligent Technology Solutions™ and services to maximize the business value of IT. As a Fortune 500-ranked global provider of digital innovation, cloud/data center transformation, connected workforce, and supply chain optimization solutions and services, we help clients successfully manage their IT today while transforming for tomorrow. From IT strategy and design to implementation and management, our 6,800 employees help clients innovate and optimize their operations to run smarter. Discover more at insight.com.
Founded in 1988 in Tempe, Arizona
7,400+ teammates in 19 countries providing Intelligent Technology Solutions for organizations across the globe
$7.1 billion in revenue in 2018
Ranked #417 on the 2018 Fortune 500, #12 on the 2018 CRN Solution Provider 500
2018 Dell EMC Server Partner of the Year, 2018 Intel Retail Solution Partner of the Year, 2018 Microsoft Worldwide Artificial Intelligence Partner of the Year
Ranked #23 on the 2019 Fortune 50 Best Workplaces in Technology and #5 on the Phoenix Business Journal 2018 list of Best Places to Work (Extra Large Business)
Signatory of the United Nations (UN) Global Compact and Affiliate Member of the Responsible Business Alliance
Today's talent leads tomorrow's success. Learn about careers at Insight: jobs.insight.com.

Insight is an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, sexual orientation or any other characteristic protected by law.

Posting Notes: Chicago || Illinois (US-IL) || United States (US) || MSOP-SC; None || None || US - Chicago, IL ||"
Data Engineer,"Why VillageMD?

VillageMD is changing the trajectory of healthcare by empowering primary care physicians to make informed decisions and engage patients in meaningful ways. We work with thousands of clinicians and healthcare disruptors across the country to build and contribute to our platform to improve patient health while driving down the cost to deliver it.

We are a mission-oriented organization and are thrilled about the work that we do every day. We're transparent, collaborative, and relentless in pursuit of our mission, all while doing so with humility and a low ego. We believe that diverse backgrounds and experiences create the best opportunity for innovation and the community that we are creating is greater than any individual.

We've built our technology using the best of cloud and open-source technologies to create an open, data-first platform that is enriched with analytical models and modernly connected to internal and external apps. These apps drive clinical decision support, patient engagement, and other facilitators of innovative, information-enriched health experiences.

Data Engineers at VillageMD build distributed components, pipelines, and tools that enable our organization to make analytical, data-driven decisions. We're in a unique position to impact everyone in primary care from independent, family-owned practices to world-class health systems. We aggregate, process, and deliver rich datasets to improve the effectiveness of primary care for our doctors and patients.

What are examples of work that Data Engineers have done at VillageMD?
Built and implemented a data profiling tool to reverse engineer data schemas from new data sources facilitating normalization of the data into our data model
Created a summary data platform supporting our presentation layer that allows clinicians and operators in our practices to pinpoint interventions on-demand to patients most in need
Analyzed and designed the best ways to expand our data model to incorporate more data that's mission critical
What will make you successful here?
Strong analytical and technical skills
A real passion for problem solving and learning new technology
Vision to balance speed and maintainability in solution design
The ability to handle multiple, concurrent projects
Crafting and implementing requirements, keeping projects on track, and engaging partners
Challenging the status quo to improve our processes and tools
Communicating complex technical details in meaningful business context
A low ego and humility; an ability to gain trust by doing what you say you will do
What you might do in your first year:
Own ten projects to design and implement best-in-class data processing enabling clean data flow directly to our data model and on to our presentation layer
Work with analytics, engineering and operations to design and implement a new analytics product that supports improving patient health
Design a new concept within our data model to meet a new operational or analytical need
The following experience is relevant to us:
5+ years of full-time experience including extensive experience with healthcare data
Ability to understand and design relational data structures required
Very strong capabilities manipulating data using SQL
Knowledge of, and/or willingness to learn, non-relational data structures and other technologies (e.g. Postgres, Redshift, Cassandra, MongoDB, Neo4j, S3, etc.)
Experience or willingness to learn building information pipelines utilizing Python or Java a plus
BS/MS in computer science, math, engineering, or other related fields is required.
Track record of successfully executing projects with multiple partners
What can we offer you?
Competitive salary, bonus, and health benefits
Paid gym membership
Fun, fast-paced, startup environment (with snacks)
Pre-tax savings on commute expenses
Remote flexibility
A highly-collaborative, conscientious, forward-thinking environment that welcomes the impact you can make from Day 1.
A clear link between our daily work on products and services and the improved quality of healthcare that this work facilitates for patients.
At VillageMD, we see diversity and inclusion as a source of strength in transforming healthcare. We believe building trust and innovation are best achieved through diverse perspectives. To us, acceptance and respect are rooted in an understanding that people do not experience things in the same way, including our healthcare system. Individuals seeking employment at VillageMD are considered without regard to race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
Data Engineer,"Job Description
We have partnered with a large Healthcare provider the Chicago, IL area to provide them with a Data Engineer.

Please review the below description and if you are interested please contact:
Vahidin Topcagic at vtopcagic@relevante.biz, 314-853-6670
Responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.

Responsibilities of the Data Engineer:
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader
Work with data and analytics experts to strive for greater functionality in our data systems.
Requirements of the Data Engineer:
Bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field (Masters Preferred).
5+ years of experience in a Data Engineer role
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience with health care datasets, clinical data, payer/claims data, SDOH data, etc.
Experience with big data tools: Hadoop, Spark, Kafka, etc. (Preferred)
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Skilled in problem-solving with strong attention to detail.
Excellent customer service skills and the ability to react diplomatically and patiently to internal and external customers.
Excellent follow-up skills paired with the ability to multi-task and determine root causes.
Strong written and verbal communication skills coupled with the ability to read, analyze and interpret technical procedures.
MCSE or equivalent is strongly desired but not required
Benefits of the Data Engineer:
Health Insurance
Dental Insurance
Life Insurance
Long Term Disability"
Data Engineer,"Data Engineer Signature Consultants has an opportunity for a Data Engineer. For this position the candidate needs to have experience with Spark, Scala, and AWSCloud Infrastructure. The candidate will need to demonstrate strong attention to details, proactive thinking, the ability to adapt to change quickly, and teamwork. Responsibilities Demonstrate disciplined software development leadership Execution and deliver solutions on-time with high bar of quality Work with Product Owners to understand the desired capability, to define and prioritize work, determine deliverables, and manage workloads Lead technical discussions and be able to work and influence a large number of cross-functional teams Build reusable solutions that can be shared across teams Continuously improve software engineering practices Elevate team members to sharpen their skills and grow their careers Qualifications Bachelor's degree or military experience 2+ years of experience with SQL 2+ years of experience with Java or Python, scala, spark 2+ years of experience building applications in a cloud environment (AWS, Azure, or Google Cloud) About Signature Consultants, LLC Headquartered in Fort Lauderdale, Florida, Signature Consultants was established in 1997 with a singular focus to provide clients and consultants with superior staffing solutions. For the ninth consecutive year, Signature was voted as one of the ""Best Staffing Firms to Work For"" and is now the 14th largest IT staffing firm in the United States (source Staffing Industry Analysts). With 28 locations throughout North America, Signature annually deploys thousands of consultants to support, run, and manage their clients' technology needs. Signature offers IT staffing, consulting, managed solutions, and direct placement services. For more information on the company, please visit www.sigconsult.com httpwww.sigconsult.com . Signature Consultants is the parent company to Hunter Hollis httpswww.hunterhollis.com and Madison Gunn httpswww.madisongunn.com ."
Data Engineer,"The Business

GradTests.com is a leading provider of practice psychometric tests to graduates, students and young professionals.

The Role

The core purpose of the role is to make high quality, high availability, accurate data available for our data analysts and data scientists to do their analysis, derive their insights and build their models. You are the Scotty Pippin to the Michael Jordans. You are the Xavi to the Messis.

You'll do things like:
Ensure our data warehouse is well structured, running smoothly and efficiently for all business intelligence
Set up and maintain various data pipelines used for customer analytics, marketing analytics and product analytics
Skills and experience

Non negotiables:
SQL
Python
Strong knowledge of traditional relational databases - we don't mind which
Some experience with cloud technologies - again we don't mind if it's AWS, GCP or Azure
Experience in any streaming technology
Great experience in using third party APIs at scale
Some web scraping experience
An obsession with data quality
Strong communication skills
Nice to haves:
Experience in working with analysts
Any basic knowledge of advanced analytics techniques
Experience in a visualisation tool like Tableau
Job Types: Full-time, Contract

Salary: $100,000.00 /year

Work Remotely:
Yes"
Data Engineer,"PDI is seeking a Data Engineer to join our team working in an exciting, high volume retail data ecosystem with over $100 billion of transaction log data. The candidate will benefit from a rich knowledge of software development practices and will need to apply those practices daily. Mastering the relationship between client data and our ecosystem will be an important element for this position.

This position plays a critical role at PDI and for both the retailer and their vendor partners to realize the most value from their data. The overarching responsibilities for this role are (i) on-boarding successfully and efficiently new clients, (ii) maintaining the data and ecosystem on an ongoing basis and introducing & implementing improvements as needed. The successful applicant will be joining an open and diverse team with a ""Can Do"" attitude, and a strong desire to make an impact in a start-up organization.

RESPONSIBILITIES & TASKS
Onboard new customers' data into the PDI data warehouse and platform, writing and installing the required ETLâs
Optimize and manage existing clientsâ data pipelines
Monitor, maintain, and, if needed rectify, various clientsâ data integrity
Develop and/or enhance automated processes to proactively identify any data related issues and/or simplify process
Gather technical requirements from multiple sources for new data-oriented features, integrating new solutions within a developed Java solution base
Participate as a key member of our agile development team
Collaborate with team members to automate queries as needed
REQUIRED QUALIFICATIONS
Bachelor's degree in Math, Statistics, Computer Science or equivalent technical field.
Working and practical knowledge of programming principles, techniques, standards and analytical ability
Strong proficiency in Java
Proven experience with complex SQL query design and optimization
Experience with Bash scripting
Experience in data cleansing, curation, parsing, integration, semantic mapping, or editing
Experience with analytics systems (data warehouses, dimensional models, etc.)
Organizational skills and ability to balance multiple priorities in a dynamic and fast-paced environment
Strong team player with a passion for data and problem solving
Excellent oral and written communication skills
PREFERRED QUALIFICATIONS
Degree in Computer Science, Computer Engineering, Management Information Systems or related field
1-3 years of applied data engineering-related experience
Strong competence in Python
Experience with Google Cloud
Experience with Linux/Unix
PDIâs employee-oriented culture provides a supportive and dynamic work environment for high achievers. PDI seeks individuals who value continuous learning, hold high ethical standards, and are top performers in their respective fields. We offer competitive wages, professional development, great culture, and a competitive benefits package. For more information about PDI, please visit our website at www.pdisoftware.com. PDI is an Equal Opportunity Employer. We verify employment eligibility for all new hires using e-Verify.

Powered by JazzHR"
Data Engineer,"Trident Consulting is seeking a "" Data Engineer with one of our clients inMcLean,VA ""A global leader in business and technology services
Exact Job Location/Work Address Richmond, VA/Chicago, IL
Project Duration (relevant for CWR) 7+ Months
Required Technologies
Strong Programming experience with object-oriented/object function scripting languages: Python/Scala, Spark.
Experience with big data tools: Hadoop, Apache Spark etc.
Experience with AWS cloud services: S3, EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc. (Nice to have)
Experience with relational SQL, Snowflake and NoSQL databases

Job Description: Detailed overview of functional and technical role expectations Candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have working experience using the following software/tools:
3+ years of experience (Mid-level) Strong Programming experience with object-oriented/object function scripting languages: Python/Scala, Spark.
3+ years of experience (Mid-level) Experience with big data tools: Hadoop, Apache Spark, Kafka, etc
1+ years of experience Experience with AWS cloud services: S3, EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc. (Nice to have)
1+ Years of experience Experience with relational SQL, Snowflake and NoSQL databases, including Postgres and Cassandra.
Responsibilities for Data Engineer:
Create and maintain optimal data pipeline architecture, Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS 'Big data' technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.

Preferred Skills Python/Scala, Hadoop, Apache Spark, AWS, Snowflake/SQL knowledge
Years of Experience Required 5+

Trident Consulting handles the staffing and management of part all of the recruitment process for our customers wishing to outsource their staffing requirements. From job profiling, providing new staff, technology, to onboarding a new hire we support our customers in their future business needs.

Our Client is one of the world's leading professional services companies, transforming clients' business, operating and technology models for the digital era. Our unique industry-based, the consultative approach helps clients envision, build and run more innovative and efficient businesses. Headquartered in the U.S., a member of the NASDAQ-100 is ranked 205 on the Fortune 500 and is consistently listed among the most admired companies in the world."
Data Engineer,"Job Description
Level Ex is transforming the way medical professionals hone their skills by practicing high-risk procedures and earning training credit with the latest medical devices and diagnostic treatments in our industry-leading 3D mobile games. In less than four years, Level Ex has exponentially grown, hiring top talent from the video games industry, the digital health ecosystem, and leading medical institutions. Our clients include top 20 pharmaceutical, biotech, and medical device companies including Baxter, Pfizer, Merck, and Medtronic, as well as leading medical associations.

We're now looking for an experienced Data Engineer who can build and refine a centralized data warehouse that is efficient, flexible, and scalable in a pioneering domain that combines mobile games data with healthcare provider data. We need a self-starter that will marshall the company’s data and surface it to business users and data scientists, and that will develop data pipelines to fuel data applications and business intelligence.

What You’ll Be Doing with Us
Architect, build and refine data warehouse that is scalable, efficient, and flexible, as new games and features are introduced.
Develop and maintain ETL pipelines from multiple data sources -- telemetry, sales, product cost, marketing, etc -- to fuel data applications and business intelligence.
Implement best-practices with metadata files and data models. Documenting pipeline details, changes to telemetry, and other important information.
Load new data sources into a centralized data warehouse.
Discovering and fixing data quality, structure and integrity issues.
Defining telemetry details to add or change existing event data. Identifying data and structure needed in order to analyze mobile games with different design patterns.
Leading large projects on a data & analytics roadmap.
Deploying and supporting predictive models to optimize marketing or operations.
Writing and monitoring JIRA/development tickets.

Who We Want to Meet
3+ years of experience working as a data architect or engineer.
3+ years of experience in building data pipelines.
3+ years of writing SQL.
B.S. or M.S. in Computer Science, Mathematics, Statistics, Economics, Analytics, Engineering or equivalent combination of education and experience.
Strong working knowledge of one or more of the following tools -- Python, Scala, R, Spark, AWS.
Ability to collaborate effectively and work as part of a team at a growing start-up.

Bonus Points Awarded For
M.S. or Ph.D. in Computer Science, Mathematics, Statistics, Economics, Analytics, Engineering or equivalent combination of education and experience.
Experience working on mobile game analytics and/or on medical analytics.
Experience in AWS and Snowflake.
Experience in architecting data structures for business intelligence.

How We Make You Happy
Multiple health insurance plans with 100% company-paid premiums
401(k) with company-paid match
Dental and vision insurance
Pre-tax flex spending and commuter accounts
Paid vacation, sick days and holidays
Downtown Loop location convenient to public transportation, and plenty of nearby food options
Unlimited cold brew and gourmet coffee, kombucha, Bevi sparkling water, and craft beer
Full kitchen and food options including breakfast, and both healthy and comfort snacks
Team after-hours events, like Board Game Night
Interested?
Just upload your resume or LinkedIn profile PDF below to apply. We look forward to hearing from you and exploring the possibilities.

Level Ex is an Equal Opportunity Employer (EOE).

No Agencies or Recruiters, please.
Legal authorization to work in the U.S. is required. We will not sponsor individuals for employment visas, now or in the future, for this job."
Data Engineer,"Data Engineer

Chicago, IL, US

Requisition Number: 75536

We are (NSIT).

As a Fortune 500-ranked global provider of digital innovation, cloud/data center transformation, connected workforce, and supply chain optimization solutions and services, we help clients successfully manage their IT today while transforming for tomorrow.

From IT strategy and design to implementation and management, our employees help clients innovate and optimize their operations to run smarter.
Microsoft Worldwide AI Partner of the Year, 2018
Microsoft Worldwide Modern Desktop Partner of the Year, 2018
Microsoft US Partner Award for Intelligent Cloud - Application Innovation, 2019
Microsoft US Azure Team Partner Choice Award - Data and Artificial Intelligence, 2019
Our Insight Digital Innovation team is searching for an experienced, passionate and professional Data Engineer to join our team.

You'll utilize the most cutting edge technology such as machine learning, artificial intelligence, big data, IoT, and azure. Here you will have practical, hands-on knowledge of modern data architectures and tools such as data warehousing, ETL/ELT, analytics, and the Azure cloud platform. You should be driven to provide quality solutions to challenging problems. You will work closely with client stakeholders and an award-winning team of engineers, architects and thought leaders to design, build and implement next-generation solutions in advanced analytics, Big Data, BI and the cloud.

What you'll do at Insight:
You will build enterprise-grade data solutions for a variety of external clients.
Design and code solutions to tough data challenges and provide feedback on others' work.
Work directly with client stakeholders to develop technical solutions for business cases.
Aggressively grow your skillset and expertise to meet the emerging needs of our clients.
What you'll need to join Insight:
4+ years of experience working with data and data analytics development within the Microsoft data platform and an excellent grasp of some of following technologies:
SQL Server, Azure SQL Database, and Azure SQL Data Warehouse
Power BI
Tableau
Analysis Services and DAX
Reporting Services
Integration Services
Azure Data Factory
PowerShell scripting
Azure Automation
2 year of experience in some of the following:
Big Data technologies such as Hadoop or HDInsight, Hive, Pig, Python, Spark, Oozie, or any of the other tools with the Hadoop ecosystem
Azure Data Lake and Azure Data Lake Analytics
Predictive analytics: R, Azure Machine Learning
Strong analytical and reasoning skills that result in clear technical execution.
Skill at translating requirements into clean, efficient, quality code
Proven ability to prioritize, self-direct and execute at velocity
Passion to deliver craftsman-quality work both individually and as part of a team
Solid communication skills with both technical and non-technical stakeholders
Desire to learn new skills and grow competencies
Bachelor's degree in Computer Science or related discipline
Requires travel to Chicago Area client sites (local only).
In the News
-
-
*
Today's talent leads tomorrow's success. Learn about careers at Insight: .

Insight is an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, sexual orientation or any other characteristic protected by law.

The position described above provides a summary of some the job duties required and what it would be like to work at Insight. For a comprehensive list of physical demands and work environment for this position, click here.
Today, every business is a technology business. Insight Enterprises, Inc. empowers organizations of all sizes with Insight Intelligent Technology Solutions and services to maximize the business value of IT. As a Fortune 500-ranked global provider of digital innovation, cloud/data center transformation, connected workforce, and supply chain optimization solutions and services, we help clients successfully manage their IT today while transforming for tomorrow. From IT strategy and design to implementation and management, our 6,800 employees help clients innovate and optimize their operations to run smarter. Discover more at .
Founded in 1988 in Tempe, Arizona
7,400+ teammates in 19 countries providing Intelligent Technology Solutions for organizations across the globe
$7.1 billion in revenue in 2018
Ranked #417 on the 2018 Fortune 500, #12 on the 2018 CRN Solution Provider 500
2018 Dell EMC Server Partner of the Year, 2018 Intel Retail Solution Partner of the Year, 2018 Microsoft Worldwide Artificial Intelligence Partner of the Year
Ranked #23 on the 2019 Fortune 50 Best Workplaces in Technology and #5 on the Phoenix Business Journal 2018 list of Best Places to Work (Extra Large Business)
Signatory of the United Nations (UN) Global Compact and Affiliate Member of the Responsible Business Alliance
Today's talent leads tomorrow's success. Learn about careers at Insight: .

Insight is an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, sexual orientation or any other characteristic protected by law.

Posting Notes: Chicago || Illinois (US-IL) || United States (US) || MSOP-SC; None || None || US - Chicago, IL ||

Nearest Major Market: Chicago
Job Segment: Database, Developer, Engineer, Supply, Computer Science, Technology, Engineering, Operations"
Data Engineer,"What do incubators and Spectrum have in common? Well, they’re great for growth, and even better for stability. As an innovative software development and digital marketing company pioneering the field of artificial intelligence, we can offer our newest Data Engineer the best of both words – a high energy, forward-thinking start-up culture, inside of a well-established, profitable, and stable structure of a 3 decade old organization. So, if you’re interested in getting your hands dirty, and leading a small team, all while inciting change into a larger organization with a vision to change the world uses data today, then please read on.

Responsibilities

Beyond working with state of the art technology you will have many different fantastic projects to work on as a Data Engineer at Spectrum. Here are just a few different responsibilities you can expect off the bat:
Planning query efficient data store schemas with respect to the data requirements of our internal data warehouse
Communicate with data scientists and other software engineers to build proper data pipelines for efficient and cost-effective querying
Develop, construct, test, and maintain architectures with the help and feedback of software engineers
Determine and implement proper data schemas that align with Spectrum’s and Spectrum’s client’s business objectives
Recommend and implement ways to optimize data reliability, efficiency, and quality
Consult and suggest different technologies for our team to test and utilize together
Incorporate the business objectives of key stakeholders in the architecture of our data warehouse
Research new and interesting data acquisition opportunities
Some Characteristics That Define You

We understand that as a Data Engineer for Spectrum, you have many different professional goals and personal interests. As such here are just a few different things that typically define our team members on the Data Science team:
Self-Starter. Building a data warehouse is no simple task. You will need to come in with a self-starter attitude to not only make this data engineering role what you want it to be, but make a stellar and efficient data warehouse while you’re at it.
Analytical. In order to solve problems and build innovative new digital marketing campaigns, it is essential that you know how to take an idea and analyze it from all of its angles.
Developer. In the ever changing world of artificial intelligence, it’s not enough just to build models. You bring to the table an eye for data patterns along with a knack for relational database engineering.
Patient. As a data engineer, you know that you work with extremely large data sets on a daily basis. As such we are looking for someone who is not only meticulous, but patient enough to sit and sift through that data in a thorough way.
Creative. Beyond just analyzing data sets, you are an explorer and a puzzle solver. Pulling insights out of your data and understanding how those insights can better shape our tools is something that you live to do.
Student. More so than most industries, the field of data science is always changing and evolving. As such, you are always looking to learn new things and gain new skills.
Required Skills and Experience

On top of the many intangible skills you bring to the table, there are many skills that can help improve the efficiencies and success of your work at Spectrum. Here are a few of those required skills and experience that you will come in with as a Data Engineer on our team:
A bachelor’s degree/pursuing a bachelor’s degree in computer science, mathematics, statistics, information systems, or a related field
Experience with statistical modeling
1-3 years working experience with Python and/or SAS languages
1-3 years working experience with SQL databases and database querying languages
1-3 years working experience with C#/.NET
Familiarity with Microsoft Azure
Familiarity with Graph database structures
Experience with both RDBMS and TDMS
Experience with data mining and data cleaning
Experience with data visualization and reporting techniques
Written and verbal expression
Benefits

As a Data Engineer at Spectrum there are a ton of fantastic perks and benefits that come along with your work. Here are just a few of the benefits you can expect when joining the Spectrum family:
Comprehensive medical & dental insurance
Retirement planning & company matching
Generous PTO, including sick days & holidays
A state-of-the-art office environment
Nintendo Switch in-office gaming such as FIFA, Arms, Mario Kart, and Rocket League
Year-round gym memberships
Paid continuing education
Casual dress code
Flexible scheduling
Free-Lunch-Friday
Company sponsored parties and group activities outside of the office
About Us

For the past 25+ years, our mission at Spectrum has always been the same— growth. Whether that be helping our customers find quality new business, developing and challenging our team members, or evolving our products and services with advancements in technology and best practices, we have always been looking towards bettering ourselves for the future. Now as we continue to grow, we find ourselves as not only the nation’s leading digital marketing and software provider for the home services industry, but an innovator and ground-shaker for the world of artificial intelligence as well. From marketing automation software powered by AI, to top notch digital marketing services via those same AI insights, we love what we do and are excited to continue to innovate for the future together.

But don’t just take my word for it. Check out what our team is saying about in real time, and learn why we are a certified Great Place to Work today!"
Data Engineer,"GoHealth is looking for Data Engineers who will be responsible for the design, development, and delivery of its various batch and streaming data pipelines. We are seeking candidates who have experience in building large batch pipelines, as well as experiencing building stable, high throughput streaming systems. In this role, you will work with other members of Engineering, Product and Project Management, and various business groups to ensure timely availability of usable data to all parts of the business that need it.

Due to the unprecedented situation of COVID-19, GoHealth has decided to protect our current and future employees by managing our business remotely. This is inclusive of interviewing, onboarding and each role day-to-day. Please consider that our roles will not be remote long-term and will return to an office setting once we're safe to do so following the guidance of local health authorities' and the CDC.

Responsibilities:
Design, develop and deploy batch and streaming data pipelines.
Monitor and ensure operational stability of data pipelines.
Create and maintain documentation of the technical detail design, operational support and maintenance procedures for all data pipelines.
Ensure data quality and compliance with development, architecture, reporting, and regulatory standards throughout entire data pipeline.
Collaborate with the rest of the Engineering Team, subject matter experts and department leaders to understand, analyze, build and deliver new data-related processes and/or reports.
Skills and Experience:
Bachelor's Degree in computer science or equivalent experience required.
4+ years of experience in the design and development of data pipelines and tasks.
Strong analytical and problem solving ability with strong attention to detail and accuracy.
Understanding of data warehousing concepts and dimensional data modeling.
Hands-on experience with troubleshooting performance issues and fine tuning queries.
Experience extracting data from relational and document databases.
Experience consuming data over HTTP and in formats such as HTML, XML, and JSON.
Knowledge of and experience with a version control system (such as Git, Mercurial, SVN, etc).
Proficiency in Java or Python programming languages.
Familiarity with data warehousing platforms, such as Redshift, Snowflake, SQL Server, etc.
Benefits and Perks:
Open vacation policy
401k program with company match
Medical, dental, vision, and life insurance benefits
Flexible spending accounts
Commuter and transit benefits
Professional growth opportunities
Casual dress code
Generous employee referral bonuses
Happy hours, ping-pong tournaments, and more company-sponsored events
Subsidized gym memberships
GoHealth is an Equal Opportunity Employer
*LI-JC1"
Data Engineer,"Data Engineer

US-IL-Chicago

Job Description

Chicago, IL

Full Time Perm

Do you love working in a collaborative environment with free breakfast, coffee/tea/soda, social events and much more? Then read on because Omnitracs is looking for the best and brightest to help us disrupt the freight and logistics industry! What sets us apart from other logistics technology companies is our rich history in data! In 1988, Omnitracs (then Qualcomm) fundamentally changed the way fleets operate and we’re doing it again today. With over a million assets in over 70 countries, Omnitracs has a lot of data. Omnitracs’ newly formed Innovation Lab is innovating on this data to create new products - helping our customers not just survive, but thrive, in today’s complex transportation ecosystem. We are looking for you, Data Engineer, to join our fast-paced Agile team in Chicago. Who You Are As a Data Engineer, you will be responsible for data management tasks including design, development, and technical administration in an AWS environment. You will also provide technical leadership to the team and be responsible for maintaining technical specifications. As a Data Engineer, you will have a heavy focus on designing the solutions to deliver data products. To be successful in this role you will need a solid understanding of data management concepts and how cloud technology can solve data issues.

Responsibilities and Duties As the Cloud Data Architect, your responsibilities will include, but are not limited to:
• Oversight of the design and standards of AWS (S3, Redshift/Snowflake/SQL Server, Glue,) data applications • Train and coach data team developers • Oversee and implement security features, access and standards around data management • Assist in capacity and budgeting for data systems • Provide estimates and oversight within a Scrum environment • Strong SQL skills in data warehouse environment • Spark, Python and/or Scala experience • Lead code reviews Qualifications and Skills • At least 3 years IT experience in AWS data services • Hands on experience working with complex Data Warehouses and or customer linking systems • Data Lake experience using Spark, Scala, EMR and/or Glue • Data Modeling experience • Proficient with SQL • Solid S3 understanding • Experience with AWS Aurora, Oracle and/or SQL Server developer experience • Hands on experience using AWS RDS Nice to Have • Experience with Redshift or Snowflake using Matillion or other ETL tools • Identity & Access Management (Security Provisioning) understanding and knowledge"
Data Engineer,"The Data Engineer will collaborate with various other IT groups, business partners and external service providers and play a key role in the design, development and operations of our new analytics platform.
Responsibilities Include:

Participate in Requirements Gathering: work with key business partner groups (e.g. Product Mgt) and other Data Engineering personnel to understand department-level data requirements.
Design Data Pipelines: work with other Data Engineering personnel on an overall design for flowing data from various internal and external sources into the platform.
Build Data Pipelines: leverage standard toolset and develop ETL/ELT code to move data from various internal and external sources into the platform.
Support Data Quality Program: work with Data QA Engineer to identify automated QA checks and associated monitoring & alerting to ensure consistently high-quality data.
Support Operations: triage alerts channeled to you and remediate as necessary.
Technical Documentation: leverage templates provided and create clear, simple and comprehensive documentation for your development.
Key contributor to defining, implementing and supporting:
Data Services
Data Dictionary
Tool Standards
Best Practices
Data Lineage
User Training"
Data Engineer,"ob Title Data Engineer Location Chicago, IL Duration 12+ Months Required Skills Responsibilities At least 3+ years experience as a Data Engineer Experience and expertise in using Spark in combination with pySpark, Scala or Java, either on-premise or Cloud Experience of software engineering in pythonScalaJavaC++ Experience with relational database and SQL (Postgres, Redshift, Snowflake, SQL Server) Expert knowledge of distributed computing, RDD and optimization techniques Experience with NoSQL and streaming platforms, e.g. Kafka, MongoDB, Neo4j is a plus Experience with developing software products for cloud platform, e.g. AWS. Experience in creating and customizing Reports and Dashboards Experience working on Large enterprise level data sets and ETL Experience in Data analysis, data consolidation and normalization Comfortable on Linux for both development and operations Experience with advanced analytics and modern machine learning techniques is a plus Experience with cloud native services such as AWS EMR"
Data Engineer,"Senior Data Engineer**job details:**+ location:Chicago, IL+ salary:$90,000 - $120,000 per year+ date posted:Tuesday, July 7, 2020+ job type:Permanent+ industry:Construction+ reference:788827**job description**Senior Data Engineerjob summary:**ESSENTIAL DUTIES AND RESPONSIBILITIES**+ Work closely with business and technical teams to deliver enterprise grade datasets that are reliable, flexible, scalable, and provide low cost of ownership.+ Analyzes and estimates feasibility, costs, time, and resources needed to develop, and implement enterprise datasets as needed.+ Create and advocate the use of standards around data documentation.+ Advocate for the use of enterprise datasets on advanced analytical applications at ABC. Work with Data Science teams to educate and promote good stewardship of our data lake.+ Ensure systems meet business requirements and industry practices+ Research opportunities for data acquisition and new uses for existing data+ Employ a variety of languages and tools (e.g. scripting languages) to marry systems together+ Recommend ways to improve data reliability, efficiency and quality+ Collaborate with Enterprise Architecture to publish and contribute to architecture standards and roadmaps.+ Achieves and maintains relevant technical competencies and helps to foster an environment of continued growth and learning among colleagues on existing and emerging technologies.+ Mentor team members and acts as a technical lead for less senior team members.location: Chicago, Illinoisjob type: Permanentsalary: $90,000 - 120,000 per yearwork hours: 8am to 5pmeducation: Bachelorsresponsibilities:**ESSENTIAL DUTIES AND RESPONSIBILITIES**+ Work closely with business and technical teams to deliver enterprise grade datasets that are reliable, flexible, scalable, and provide low cost of ownership.+ Analyzes and estimates feasibility, costs, time, and resources needed to develop, and implement enterprise datasets as needed.+ Create and advocate the use of standards around data documentation.+ Advocate for the use of enterprise datasets on advanced analytical applications at ABC. Work with Data Science teams to educate and promote good stewardship of our data lake.+ Ensure systems meet business requirements and industry practices+ Research opportunities for data acquisition and new uses for existing data+ Employ a variety of languages and tools (e.g. scripting languages) to marry systems together+ Recommend ways to improve data reliability, efficiency and quality+ Collaborate with Enterprise Architecture to publish and contribute to architecture standards and roadmaps.+ Achieves and maintains relevant technical competencies and helps to foster an environment of continued growth and learning among colleagues on existing and emerging technologies.+ Mentor team members and acts as a technical lead for less senior team members.qualifications:+ Experience level: Experienced+ Minimum 6 years of experience+ Education: Bachelorsskills:+ Data Warehouse+ data engineering (5 years of experience is required)+ Tableau+ Azure+ PythonEqual Opportunity Employer: Race, Color, Religion, Sex, Sexual Orientation, Gender Identity, National Origin, Age, Genetic Information, Disability, Protected Veteran Status, or any other legally protected group status."
Data Engineer,"A financial company located downtown is looking for a Data Engineer to join its marketing department. You would be part of their Marketing Analytics team, specifically on a customer - related project. It's the foundation to understand customer experience on their website by gathering analytics to make informed decisions on a variety of marketing initiatives, spending, strategy, etc. The role would need you to extract marketing datasets to be transferred to EDH, set up infrastructure, streaming data to AWS, and so on. Above all, the team needs a data engineer with strong DevOps and AWS experience.Required Skills & Experience* 3+ years of professional experience* Python* Programming and DevOps experience* Kubernetes and Docker* AWS (S3, Lambda, etc.)* Hadoop, Hive, Apache Spark, and other Big Data technologies* B.S. in Computer Science or related fieldDesired Skills & Experience* Data warehousing experience* Certification in cloud computing or big data* Visualization tools like Tableau* M.S. in Computer Science or related fieldWhat You Will Be DoingTech Breakdown* 80% Data Engineering* 20% DevOpsDaily Responsibilities* 85% Hands On* 15% Team CollaborationThe Offer* Competitive Pay: Up to $50/hour, DOE* Contract Duration: 3 MonthsYou will receive the following benefits:* Medical & Dental Insurance* Health Savings Account (HSA)* 401(k)* Paid Sick Time Leave* Pre-tax Commuter Benefit* Add additional perks specific to the work environmentApplicants must be currently authorized to work in the United States on a full-time basis now and in the future.Jobspring Partners, part of the Motion Recruitment network, provides IT Staffing Solutions (Contract, Contract-to-Hire, and Direct Hire) in major North American markets. Our unique expertise in today's highest demand tech skill sets, paired with our deep networks and knowledge of our local technology markets, results in an exemplary track record with candidates and clients.#LI-RB1"
Data Engineer,"Location: Chicago, IL

Job Description:

Diversified Services Network, Inc. (DSN) is seeking a Data Engineer in Chicago, IL

RESPONSIBILITIES
Develop implementation patterns leveraging AWS technologies to support.
Understand business and technical requirements.
Develop Conceptual and Logical Data Solution for data acquisition, data models and pipelines.
Review Solution Data Designs, Models, Pipelines Prototype New Solutions Technical guidance to data designers and developers Solution Implementation Reviews.
TYPICAL DAY
Understand business and technical requirements.
Develop Conceptual and Logical Data Solution for data acquisition, data models and pipelines.
Review Solutions with Platform, Business, and Application teams.
Position Requirements:
S. in Computer Science, Information Systems, or related major or equivalent IS / business experience.
10+ years of experience designing, implementing data persistence and processing solutions.
AWS Certifications
TECHNICAL SKILLS

Required Skills
AWS Cloud Based Technologies for Data Processing and Persistance (DynamoDB, S3, Aurora, Kinesis, SQS, SNS, Lambda, Fargate, Glue).
Ability to design and communicate solutions to meet business and technical requirements.
Demonstrated Data Architecture and Design for Big Data, Analytics, and applications.
SOFT SKILLS
Strong written and verbal communication.
Able to produce architecture and design artifacts in Visio and Office 365.
Additional Information:

An Equal Opportunity Employer"
Data Engineer,"As a Data Engineer, you'll join our growing team of data scientists and engineers, reporting into Operations organization but working across multiple teams throughout the company. In this role, you(TM)ll be responsible for handling the design and construction of scalable data management systems "" ensuring that all data systems meet our company requirements "" and will also research and recommend new uses for data acquisition. As a Data Engineer, you will implement the data models and data structures needed for each use case, in the most convenient format to be used by the Data Science and Business Intelligence teams. Through regular interactions with stakeholders and functional business unit leaders, you will build high-performance algorithms, predictive models, and prototypes that influence data storage, piping, and usage. Additionally, you will participate in data requirements, modeling and testing activities. Each day will be unique, requiring an ability to think strategically and on your feet, be creative, take initiative, and employ a diverse set of skills.

WHO YOU ARE

Knowledgeable, Analytical, and Solution-Oriented. Without a doubt, you(TM)ve got strong quantitative skills and are comfortable analyzing large data set, spotting trends and patterns, and synthesizing relevant observations. You use a hypothesis-driven approach to engage in analysis that will deliver on your client questions. You like thinking outside the box to come up with innovative points of view on new challenges, relying on your previous analytic work and experience to help guide you along the way.

Results-Oriented. You demonstrate an inherent sense of urgency to drive great results, while being precise in executing your work. You are facile with creating and communicating a clear project plan, tracking progress, and keeping your business partners in the loop along the way.

Intellectually Curious. You're inherently interested in the ""why"" so that you can identify opportunities that represent unconventional solutions to the problems you are trying to solve.

Strong Communicator. Your writing and speaking skills are concise, articulate, and effective, providing an ability to interact with all levels/various teams across the organization, be understood, and develop trust and rapport within the organization.

Technologically Savvy. Microsoft Excel is a basic tool to you that you know like the back of your hand. You also have a strong skill set in R, Python, ArcGIS, machine learning, neural networks and/or other advanced analytics tools and techniques.

A Trusted Team Player. You enjoy partnering with others and build constructive working relationships that foster the collaboration necessary to deliver great results. You are accountable to your teammates and follow through on commitments.

Organized and Confident. You are flexible, composed, and able to prioritize multiple tasks and deadlines simultaneously, while confidently interacting with a variety of individuals, across all levels of the organization. You handle pressure well and do so with confidence.

WHAT YOU(TM)LL DO

Create data models and data processes, providing the right format and structure for use case solutions.

Participate in early data modeling and testing for use case development, providing input on how to improve proposed solutions and implement necessary changes.

Help to build, document, and maintain best practices, including but not limited to codebase management, work and issue tracking, testing and quality control/assurance measures, data dictionaries, and a documentation hub for both production level code and ad hoc analyses.

Interact with stakeholders and functional subject matter experts to understand all data requirements in order to develop effective business insights and translate them into actionable data structures and data models.

Assemble large, complex data sets that meet both functional and non-functional business requirements.

Extract relevant data to solve analytical challenges the organization and/or functional business units may face.

Work closely with IT teams on internal data acquisition (e.g., CRM, ERP, etc.).

Partner with stakeholders to provide technical support related to data structures, data models, data management and data infrastructure needs.

Work with data and analytics experts to strive for greater functionality in our data systems. Recommend different ways to constantly improve data reliability and quality.

Research new uses for existing data.

Create data tools for Business Intelligence, Analytics and Data Scientist team members that assist them in building and optimizing our Company use of data.

Collaborate regularly with key stakeholders to support and enhance the day-to-day operations of our business.

Produce various reports for stakeholders, as requested, to highlight areas of opportunity; works with teams to develop and implement changes, as needed.

Develop and maintain formal documentation that describes data and data structures, including data modeling.

PREVIOUS EXPERIENCE & REQUIREMENTS

Bachelor's Degree required, preferably in computer science, software/computer engineering, applied mathematics, or physics statistics.

Minimum 2 years data modeling experience and working with data management systems; deep expertise in data modeling and structuring required.

2+ years experience in high volume data environments and core data engineering activities (i.e. familiarity with cloud database set up, automation scheduling using directed acyclic graph (such as Airflow) and database optimization, including but not limited to partitioning, group and sort keys, and indexes).

Familiarity with a broad base of analytical methods e.g. data modeling (variable transformation and summarization) and processing (i.e. Spark, SQL Server, Hadoop/Hive, neo4j, etc).

Strong attention to detail and ability to think critically/conceptually.

Team oriented and flexible with proven track record in collaborating with multiple stakeholders.

Effective written and verbal communication skills required. Demonstrated ability to quickly learn new technologies a must.

Ability to think creatively when problem solving for new solutions and to work on numerous projects concurrently while effectively prioritizing workload. Tolerance for ambiguity required.

Tools/software:

Familiarity with data loading and management tools (i.e. Azure Storage""BlockBlob and relational and NoSQL databases and tools such as SQL Server, MongoDB, Data Stax, etc) required.

Must have programming and/or scripting experience (Python, Java) as well as experience with version control systems (Git/GitHub), continuous integration (circleCI) and other programming frameworks/approaches.

Proficiency in MS and Google application suites.

Must be available for overnight travel (approximately 10%)

Authorization to work in the US (without need for Visa sponsorship from employer) is required."
Data Engineer,"ob Title Data Engineer Location Chicago, IL Duration 12+ Months Required Skills Responsibilities At least 3+ years experience as a Data Engineer Experience and expertise in using Spark in combination with pySpark, Scala or Java, either on-premise or Cloud Experience of software engineering in pythonScalaJavaC++ Experience with relational database and SQL (Postgres, Redshift, Snowflake, SQL Server) Expert knowledge of distributed computing, RDD and optimization techniques Experience with NoSQL and streaming platforms, e.g. Kafka, MongoDB, Neo4j is a plus Experience with developing software products for cloud platform, e.g. AWS. Experience in creating and customizing Reports and Dashboards Experience working on Large enterprise level data sets and ETL Experience in Data analysis, data consolidation and normalization Comfortable on Linux for both development and operations Experience with advanced analytics and modern machine learning techniques is a plus Experience with cloud native services such as AWS EMR"
Data Engineer,"Locations Richmond VA, McLean, VA, Wilmington, DE, Chicago, IL Required Technologies Strong Programming experience with object-orientedobject function scripting languages Python, PySpark, Scala, etc. Experience with big data tools Hadoop, Apache Spark, Kafka, etc. Experience with AWS cloud services S3, EC2, EMR, RDS, Redshift Experience with stream-processing systems Storm, Spark-Streaming, etc. Experience with relational SQL, Snowflake and NoSQL databases, including Postgres and Cassandra. Job Description Detailed overview of functional and technical role expectations Candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. Should also have working experience using the following softwaretools Strong Programming experience with object-orientedobject function scripting languages Python, PySpark, Scala, etc. Experience with big data tools Hadoop, Apache Spark, Kafka, etc. Experience with AWS cloud services S3, EC2, EMR, RDS, Redshift Experience with stream-processing systems Storm, Spark-Streaming, etc. Experience with relational SQL, Snowflake and NoSQL databases, including Postgres and Cassandra. Responsibilities for Data Engineer Create and maintain optimal data pipeline architecture, Assemble large, complex data sets that meet functional non-functional business requirements. Identify, design, and implement internal process improvements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability etc. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS Big data technologies. Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader."
Data Engineer,"Data Engineer

Our goal at Elkay is to inspire everyday – customers, employees…and the employees of tomorrow. We focus on doing the right thing so we can be in business forever. Our values-driven culture emphasizes investing in people and treating them like part of the family. We’re financially-stable and privately-owned with a solid reputation for ethics, integrity, giving back, and providing an engaging, inclusive environment where careers flourish and grow. Our people are proud to work for Elkay.

And the feeling is mutual – because it’s Elkay’s people who really give us our edge. We empower our employees to take the lead in delivering Elkay’s exceptional customer experience. Our commitment to our people and their professional development is a recipe for success that has fueled our growth from a three-person shop in 1920 to one of today’s leading international suppliers of plumbing, water delivery and branded commercial interiors. If you’re ready for a new career challenge where everything you do will make a difference, talk to us about joining the Elkay family.

The Data Engineer we hire will perform a variety of project and technical tasks supporting Elkay’s analytics infrastructure, including data integration and ETL, database architecture and design, data prep, and reporting. Contribute to all aspects of the project lifecycle, including requirements gathering, design, development, testing, and deployment. Become familiar with Elkay’s business model and the various business functions that utilize Elkay’s analytics infrastructure, including sales, marketing, supply chain, operations, pricing, finance, and customer care. Keep abreast of trends, architectures, and tools in the analytics landscape and evaluate them for incorporation into Elkay’s analytics landscape. Learn quickly and juggle multiple tasks for various customers. Communicate complex technical problems effectively to management and to the business. Collaborate with other technical team members on solutions. Effectively prioritize support tasks with project work.

Specific duties include:
Design, develop and maintain data models, database architectures, and associated database objects in Snowflake, Oracle, and other database solutions such as Azure.
Design, develop, and maintain data integrations using Informatica Power Center, Informatica Integrated Cloud Services, and data prep tools.
Participate in or drive project activities such as requirements gathering, design, develop, test, and deploy.
Assist in the set-up of, and administer, on premise and cloud tools used in the Elkay analytics infrastructure.
Create and maintain necessary technical documentation, including requirements, design, and test documents.
Identify emerging trends, processes, and techniques impacting Elkay’s analytics infrastructure and make suggestions for incorporation of these into the analytics infrastructure.
Must have's:
A Master’s or Bachelor’s degree in Computer Science, MIS, engineering, or a related technical discipline is required.
5+ years of experience in data engineering, data warehousing, business intelligence, ETL on databases such as Oracle or SQL Server, and/or big data is required.
3+ years of experience in ETL/ data integration is required with 2+ years of experience in Informatica PowerCenter, job scheduling tools is required.
Working experience in Python/R/Scala, Snowflake is required.
Hands on experience in writing and understanding complex SQL (e.g. CTE’s others).
Thorough understanding of relational database design and best practices, including dimensional (star, snowflake) models is required.
A collaborative working style and ability to work well within the team and with business consumers is required.
Ability to clearly communicate to technical and non-technical audience by written and verbal is required.
Independent analytical, critical thinking, and problem-solving ability in complex technical environments is required.
Nice to have's:
Production experience in OBIEE, Oracle Analytics Cloud (OAC) and Tableau is nice to have.
Familiarity with big data technologies such as Microsoft Azure Data or AWS is nice to have.
EOE/M/F/D/V/SO"
Data Engineer,"Chicago, Illinois
Skills : Hadoop,big data,AWS,spark,Python,snowflake,presto,hive
Description :

MUST HAVE

AWS (s3, redshift, EMR, EC2, lambda, SNS), Unix shell scripting, Python, Spark, Scala

PREFER TO HAVE

Snowflake, Presto, Arrow, Airflow, Hadoop, Hive

Additonal Notes:
Between 3-5 years exp (can exceed 5 years), looking for mid-senior level experience
2 types of work they are doing – one is they have batch jobs which process the files from their partners and other files from their data processer. These files currently processed in ab initio. Trying to go from ab initio to AWS (EMR)
They have other jobs which are currently running in production, some are ab initio
Hadoop,big data,AWS,spark,Python,snowflake,presto,hive"
Data Engineer,"Net2Source is a Global Workforce Solutions Company headquartered at NJ, USA with its branch offices in Asia Pacific Region. We are one of the fastest growing IT Consulting company across the USA and we are hiring ""Job Title"" for one of our clients. We offer a wide gamut of consulting solutions customized to our 450+ clients ranging from Fortune 5001000 to Start-ups across various verticals like Technology, Financial Services, Healthcare, Life Sciences, Oil Gas, Energy, Retail, Telecom, Utilities, Technology, Manufacturing, the Internet, and Engineering. Role Data Engineer Exact Job LocationWork Address Chicago, IL Project Duration (relevant for CWR) 7+ Months Required Technologies Strong Programming experience with object-orientedobject function scripting languages Python. Experience with big data tools Hadoop, Apache Spark etc. Experience with AWS cloud services S3, EC2, EMR, RDS, Redshift Experience with stream-processing systems Storm, Spark-Streaming, etc. (Nice to have) Experience with relational SQL, Snowflake and NoSQL databases Job Description Detailed overview of functional and technical role expectations Candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have working experience using the following softwaretools 3+ years of experience (Mid-level) Strong Programming experience with object-orientedobject function scripting languages Python 3+ years of experience (Mid-level) Experience with big data tools Hadoop, Apache Spark, Kafka, etc 1+ years of experience with AWS cloud services S3, EC2, EMR, RDS, Redshift Experience with stream-processing systems Storm, Spark-Streaming, etc. (Nice to have) 1+ Years of experience with relational SQL, Snowflake and NoSQL databases, including Postgres and Cassandra. Responsibilities for Data Engineer Create and maintain optimal data pipeline architecture, Assemble large, complex data sets that meet functional non-functional business requirements. Identify, design, and implement internal process improvements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability etc. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS lsquoBig datarsquo technologies. Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader. Preferred Skills Python, Hadoop, Apache Spark, AWS, SnowflakeSQL knowledge Years of Experience Required 5+ About Net2Source, Inc. Net2Source is an employer-of-choice for over 2200+ consultants across the globe. We recruit top-notch talent for over 40 Fortune and Government clients coast-to-coast across the U.S. We are one of the fastest-growing companies in the U.S. and this may be your opportunity to join us! Want to read more about Net2Source?, Visit us at www.net2source.com httpwww.net2source.com Equal Employment Opportunity Commission The United States Government does not discriminate in employment on the basis of race, color, religion, sex (including pregnancy and gender identity), national origin, political affiliation, sexual orientation, marital status, disability, genetic information, age, membership in an employee organization, retaliation, parental status, military service, or other non-merit factor. Net2Source Inc. is one of the fastest growing Global Workforce Solutions company with a growth of 100 YoY for last consecutive 3 years with over 2200+ employees globally and 30 locations in US and operations in 20 countries. With an experience of over a decade we offer unmatched workforce solutions to our clients by developing an in-depth understanding of their business needs. We specialize in Contingent hiring, Direct Hires, Statement of Work, Payroll Management, IC Compliance, VMS, RPO and Managed IT Services. Fast Facts about Net2Source Inception in 2007, privately held, Debt free 2200+ employees globally 375+ In- house Team of Sales, Account Management and Recruitment with coast to coast COE. 30 offices in US and 50+ Offices globally Operations in 20 countries (US, Canada, Mexico, APAC, UK, UAE, Europe, , Europe, Latin America, Japan, Australia) Awards and Accolades 2018 ndash Fastest Growing IT Staffing Firm in North America by Staffing Industry Analysts 2018 ndash Fastest-Growing Private Companies in America as a 5 times consecutive honoree ndash Inc. 5000 2018 ndash Fastest 50 by NJBiz 2018 ndash Techserve Excellence Award (IT and Engineering Staffing) 2018 ndash Best of the Best Platinum Award by Agile1 2018 ndash 40 Under 40 Award Winner by Staffing Industry Analysts 2018 ndash CEO World Gold Award by SVUS 2017 ndash Best of the Best Gold Award by Agile1 Thanks Regards, Ajeet Yadav Sr. Account Manager ndash Enterprise Business (201) 340-8700 Ext 598 (201) 620-3288 (Cell) Email ajeetynet2source.com mailtoajeetynet2source.com Web www.net2source.com httpwww.net2source.com Social Facebook httpswww.facebook.comnet2source Twitter httpstwitter.comnet2source LinkedIn httpswww.linkedin.comcompanynet2source-inc-"
Data Engineer,"Join SADA as a Sr. Data Engineer!

Your Mission

As a Sr. Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring batch or streaming data pipelines, data lakes and data warehouses.

You will be expected to run point on whole projects, end-to-end, and to mentor less experienced Data Engineers. You will be recognized as an expert within the team and will build a reputation with Google and our customers. You will demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise. You will also participate in early-stage opportunity qualification calls, as well as lead client-facing technical discussions for established projects.
Pathway to Success

#BeOneStepAhead: At SADA we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our engineers know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.

Your success starts by positively impacting the direction of a fast-growing practice with vision and passion. You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions.

As you continue to execute successfully, we will build a customized development plan together that leads you through the engineering or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:
Google Professional Data Engineer Certified or able to complete within the first 45 days of employment

Required Qualifications:
Mastery in at least one of the following domain areas:
Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or customer-facing role

Useful Qualifications:
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Experience operationalizing machine learning models on large datasets
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem

About SADA

Values: We built our core values on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing

Work with the best: SADA has been the largest partner in North America for GCP since 2016 and has been named the 2019 and 2018 Google Cloud Global Partner of the Year. SADA has also been awarded Best Place to Work by Inc. as well as LA Business Journal!

Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match, professional development reimbursement program as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."
Data Engineer,"Position Data Engineer Location Elmhurst, IL Duration Contract to Hire middot 5+ years of experience working with enterprise data platforms, building and managing data lakes and using big data technologies middot 2+ years of experience with Spark using PythonScala. Experience with Spark streaming, building real time data pipelines is preferred middot 2+ years of experience working with AWS platform. Experience with solutioning on AWS infrastructure using services like AWS S3, Lambda, EMR, Redshift (or Snowflake) middot Experience with automating and orchestrating jobs on a big data platform using Oozie, Airflow, Jenkins or something similar middot Good understanding and experience working with various products in the Big data ecosystem like Hive, HDFS, Presto, NoSQL databases like Cassandra, DynamoDB middot Experience with setting up and using Kafka for real time streaming is a big plus middot Has to be a team player and open to working with newer technologies as well as supporting legacy systems middot Prior experience with working in a SQL server based environment and using SSIS, SSRS, TSQL is a plus. middot Prior experience with traditional ETL tools like Talend Open Studio, Pentaho or something similar is a plus"
Data Engineer,"PDI is seeking a Data Engineer to join our team working in an exciting, high volume retail data ecosystem with over $100 billion of transaction log data. The candidate will benefit from a rich knowledge of software development practices and will need to apply those practices daily. Mastering the relationship between client data and our ecosystem will be an important element for this position.

This position plays a critical role at PDI and for both the retailer and their vendor partners to realize the most value from their data. The overarching responsibilities for this role are (i) on-boarding successfully and efficiently new clients, (ii) maintaining the data and ecosystem on an ongoing basis and introducing & implementing improvements as needed. The successful applicant will be joining an open and diverse team with a ""Can Do"" attitude, and a strong desire to make an impact in a start-up organization.

RESPONSIBILITIES & TASKS
Onboard new customers' data into the PDI data warehouse and platform, writing and installing the required ETLs
Optimize and manage existing clients data pipelines
Monitor, maintain, and, if needed rectify, various clients data integrity
Develop and/or enhance automated processes to proactively identify any data related issues and/or simplify process
Gather technical requirements from multiple sources for new data-oriented features, integrating new solutions within a developed Java solution base
Participate as a key member of our agile development team
Collaborate with team members to automate queries as needed
REQUIRED QUALIFICATIONS
Bachelor's degree in Math, Statistics, Computer Science or equivalent technical field.
Working and practical knowledge of programming principles, techniques, standards and analytical ability
Strong proficiency in Java
Proven experience with complex SQL query design and optimization
Experience with Bash scripting
Experience in data cleansing, curation, parsing, integration, semantic mapping, or editing
Experience with analytics systems (data warehouses, dimensional models, etc.)
Organizational skills and ability to balance multiple priorities in a dynamic and fast-paced environment
Strong team player with a passion for data and problem solving
Excellent oral and written communication skills
PREFERRED QUALIFICATIONS
Degree in Computer Science, Computer Engineering, Management Information Systems or related field
1-3 years of applied data engineering-related experience
Strong competence in Python
Experience with Google Cloud
Experience with Linux/Unix
PDIs employee-oriented culture provides a supportive and dynamic work environment for high achievers. PDI seeks individuals who value continuous learning, hold high ethical standards, and are top performers in their respective fields. We offer competitive wages, professional development, great culture, and a competitive benefits package. For more information about PDI, please visit our website at www.pdisoftware.com. PDI is an Equal Opportunity Employer. We verify employment eligibility for all new hires using e-Verify.

Powered by JazzHR"
Data Engineer,"By clicking continue you agree to Built In's Privacy Policy and Terms of Use.

ActiveCampaign's category-defining Customer Experience Automation Platform helps over 100,000 businesses in 170 countries meaningfully engage with their customers. The platform gives businesses of all sizes access to hundreds of pre-built automations that combine email marketing, marketing automation, CRM, and machine learning for powerful orchestration, segmentation and personalization across social, email, messaging, chat, and text. Over 70% of ActiveCampaign's customers use its 300+ integrations including Shopify, Square, Facebook, Eventbrite, and Salesforce. ActiveCampaign scores higher in customer satisfaction than any other solution in both Marketing Automation and CRM All-In-One on .

As the fastest-growing SaaS company in Chicago, we are scaling rapidly to keep up with market demand. We are growing all of our teams and looking for people who share our values, deliver innovation frequently and join us in our mission to grow our customer base from 100,000 today to millions.

We are currently seeking a Senior Data Engineer to join our Data Science team. At ActiveCampaign, Senior Data Engineers deal with terabytes of platform transactional data, such as UI interaction time series, marketing content, user e-commerce trails, etc.

Our team is passionate about delivering data-driven features to our end users. We are a closely-knit team with both data scientists and engineers, who seamlessly collaborate and support each other, and hold a high bar of quality work.

If this sounds like you and your next career move, then we would love to hear from you!

What your day could consist of:
Manage and improve existing ETLs and create ETLs to ingest new sources of data
Maintain and scale our data pipeline, enhance and expand our data infrastructure
Programmatically unify data from various sources
Promote and execute the best practices of data management, at both team-wide and company-wide
Communicate and collaborate with product and operations teams, to create more value out of our data
Besides the strong technical skills, the ideal candidate should be a continuous learner, comfortable at working with various teams, have a natural curiosity of how things work, and always committed to delivering top quality work
What is needed:
Advanced SQL knowledge
Proficiency with Python (3+ years of experience)
Hands-on Linux experience
Working experience processing 500GB to 5TB of data
Experience with both batch and streaming data processing
Experience working with structured, semi-structured, and unstructured data
Experience in interacting with and optimizing usage of cloud data warehouses (Snowflake)
Optional: AWS, Terraform
ActiveCampaign is an employee-first culture. We take care of our employees at work and outside of work. We'll share all the details later on but in summary: comprehensive health and wellness benefits including no premiums for employees on our HSA plan, open time off plan, generous 401(k) matching with no vesting, lunch and endless snacks/ beverages, reimbursed commuting, education budgets, ongoing learning and development, a proactive approach to diversity and inclusion, career pathing and lots of swag.

Read Full Job Description

& lever-source%5B%5D=BuiltInChicago

First Name

Last Name

Email Address"
Data Engineer,"Riskonnect is the leading integrated risk management software solution provider that empowers organizations to anticipate, manage and respond in real-time to strategic and operational risks across the extended enterprise.Riskonnect is the only provider ranked in the leadership and visionary quadrants by world renowned industry analysts - Gartner, Forrester and Advisen RMIS Review.We employ more than 500 risk professionals in the Americas, EMEA and Asia Pacific and serve over 900 customers across 6 continents.The combination of innovative risk technology, a customer success mindset, and employee-first belief makes Riskonnect a sought after place to work.

Responsibilities:
Develop strategy for new multi-platform data integration and analytics.
Develop strategy for new multi-platform-sourced data lake.
Contribute to API strategy to facilitate application connectivity and analytics.
Contribute to the maintenance and evolution of best practices.
Contribute to process documentation.
Perform multiple proofs of concept (POCs).
Contribute to implementation plan for decided-upon solution(s).
Required Qualifications:
Experience with JavaScript/Java/ Python or Jitterbit and other developer languages.
Experience with Data Analytics.
Experience with Web Services and APIs.
Experience in the development of batch and real-time data integration and data consolidation processes.
Experience with machine learning, AI, and data lakes.
Proficiency in TSQL/PLSQL query-writing, stored procedure development, and views.
Strong analytical skills with ability for problem-solving.
Understands the importance of data provenance and the ability to demonstrate it to clients.
Detail oriented, organized, self-motivated.
Preferred Qualifications:
Experience with Salesforce.
Experience in the Risk Management, Healthcare, Financial, and/or Insurance industries is recommended.
Experience with Financial data sets, involving financial validation."
Data Engineer,"Join SADA as a Data Engineer!

Your Mission

As a Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data issues facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring a combination of batch or streaming data pipelines, data lakes and data warehouses.

You will be recognized as an established contributor by your team. You will contribute design and implementation components for multiple projects. You will work mostly independently with limited oversight. You will also participate in client-facing discussions in areas of expertise.
Pathway to Success

#BeOneStepAhead: At SADA we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage adaptability. This means that not only do our engineers understand that change is inevitable, but they embrace this change to continuously broaden their skills, preparing for future customer needs.

Your success comes from your enthusiasm, insight, and positive impact. You will be given direct feedback quarterly with respect to the scope and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, your collaboration with your peers, and the consultative skill you demonstrate in customer interactions.

As you continue to execute successfully, we will build a personalized development plan together that leads you through the engineering or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with a first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:
Google Professional Data Engineer Certified or able to complete within the first 45 days of employment

Required Qualifications:
Expertise in at least one of the following domain areas:
Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or other customer-facing role

Useful Qualifications:
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Applied experience operationalizing machine learning models on large datasets
Knowledge and understanding of industry trends and new technologies and ability to apply trends to architectural needs
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem

About SADA

Values: We built our core values on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing


Work with the best: SADA has been the largest partner in North America for GCP since 2016 and has been named the 2019 and 2018 Google Cloud Global Partner of the Year. SADA has also been awarded Best Place to Work by Inc. as well as LA Business Journal!

Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, RRSP, professional development reimbursement program as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."
Data Engineer,"Job Description
Company Description

NOCD is an award-winning digital health company focused on identifying people with obsessive-compulsive disorder (OCD) and managing them using evidence-based treatment. Although OCD affects over 181 million people globally, is ranked by the W.H.O. as a top 10 most disabling condition, and costs 3X more to treat than depression, over 90% of the OCD population isn’t able to access effective care due to stigma and misdiagnosis. As a result, OCD patients often go untreated and suffer in silence, which is why it’s nicknamed ""the secret illness.”

NOCD is solving this global problem by identifying OCD patients online and managing them using our evidence-based OCD treatment platform. Inside NOCD, members can access a licensed mental health professional with specialty training in OCD who can offer online diagnostic interviews, video-based tele-therapy sessions, and message-based support. In between each session, members can access moderated support communities and tech-enabled therapeutic tools to ensure they have constant support whenever their provider isn’t around. In an integrated model using our app, researchers at Columbia Medical Center found it to be effective in reducing symptoms of OCD.

Today, NOCD manages the largest community of OCD patients in the world. We’re financially supported by 7wire Ventures, Chicago Ventures, Meridian Street Capital, and Hyde Park Angels. We’re changing the world and need other like-minded individuals to accelerate and expand our efforts.

Job Description
Lead data analysis projects to generate actionable insights and implement those insights into the company's product.
Collaborate with the product development cycle to measure effectiveness of features and contribute to the direction of the product.
Coordinate with our clinical team to understand what can be done to accelerate positive outcomes.
Help sales teams understand what is important to share and show how data can be used to shed light on the misunderstood nature of OCD.
Work with marketing team to identify what channels resonate with our users and what types of content to make
Present findings to team
Qualifications
Expert in SQL and high-level languages such as Python
Expert in maintaining data warehouses and ETL pipelines (our data warehouse today is MySQL and ETL is done through periodic python scripts)
Experience with Business Intelligence Solutions for consuming the final datasets (we currently use metabase)
Strong computer science fundamentals, algorithms, data structures
Excellent communication skills
Authorized to work in the U.S
Willing to work full time out of our Chicago office
Additional Information
Casual, challenging, and engaging startup environment with an outstanding mission driven team atmosphere
Competitive compensation
Comprehensive benefits package, including medical, dental, vision coverage, and 401(k).
Flexible PTO policy
Awesome office on Michigan Avenue, ability to work remotely
Onsite amenities including a fitness center
NOCD is an equal opportunity employer committed to fostering an inclusive, innovative environment with the best employees. Therefore, we provide employment opportunities without regard to age, race, ethnicity, national origin, religion, disability, sex, gender identity or sexual orientation, or any other protected status in accordance with applicable law"
Data Engineer,"Job DescriptionVertical Trail LLC is a rapidly growing, Chicago-based solutions provider with a mission to help our clients accelerate innovation by deploying advanced analytics solutions in the cloud.We are looking for bright and passionate individuals to become an integral part of our advanced analytics team. Our Big Data Engineer will focus on using modern technologies to transform mountains of data into meaningful insights.As a key part of our specialized delivery team, the Big Data Engineer will work onsite with our clients directly - as well as remotely - to deliver full lifecycle solutions.Are you our next Big Data Engineer? Let's find out!Our Big Data Engineer will* Assist with development and data management tasks as needed to support Big Data and analytics projects* Work closely with developers, consultants, and business analysts to understand needs* Design, build, and launch new data extraction, transformation, and loading processes in production* Assist with the design, build, and launch of new production data models* Support existing processes running in production* Perform Hadoop cluster maintenance* Monitor cluster connectivity and security* Develop scripts and tools to automate common administration tasks* Troubleshoot data load, transformation scripts, and performance issuesOur Big Data Engineer must have* A 4-year Computer Science, Engineering, Math, or Analytics degree* 2+ years of data engineering or database support experience* 1-2 years of Scala / Java development experience* 1+ years of experience with Hadoop-based system supportOther helpful knowledge / experience includes* Cloudera Suite (including Cloudera Manager and/or Cloudera Director)* Cloud computing infrastructure (e.g., Amazon Web Services, Azure)* Exposure to (or experience with) Linux Administration* Exposure to (or experience with) AWS or Azure cloudCandidates must be local to the Chicago area and must be able to work for Vertical Trail as a W2 employee (Vertical Trail is not considering 3rd party or contract candidates for this role).Employment with Vertical Trail is an exciting opportunity that offers ample avenues for professional growth and development, as well as features including:Flexible Work EnvironmentInformed by client needs and optimal work / life balance, work locations for this role may include:* Remote work from your home office* A dedicated Vertical Trail office in Schaumburg, IL* Collaborative office space in downtown Chicago* Client sites (as needed)Attractive Benefits* Comprehensive Health, Dental, and Vision insurance* Employer-paid Life and Disability insurance* 401(k) / Retirement Plan* Professional Development Plan"
Data Engineer,"OverviewBaker Tilly Virchow Krause, LLP (Baker Tilly) is a leading advisory, tax and assurance firm whose specialized professionals guide clients through an ever-changing business world, helping them win now and anticipate tomorrow. Headquartered in Chicago, Baker Tilly, and its affiliated entities, have operations in North America, South America, Europe, Asia and Australia. Baker Tilly is an independent member of Baker Tilly International, a worldwide network of independent accounting and business advisory firms in 145 territories, with 34,700 professionals. The combined worldwide revenue of independent member firms is $3.6 billion. Visit bakertilly.com or join the conversation on LinkedIn, Facebook and Twitter.It's an exciting time to join Baker Tilly!Baker Tilly's principles of integrity, passion and stewardship define us as an organization and an employer. With offices consistently earning 'best place to work' honors and as a top-ranked firm, Baker Tilly recognizes that our approach, strategy and culture are driven by our people. Their focus and commitment have been fundamental in getting us to where we are today and where we will go in the future. Based on the growth we are planning, Baker Tilly is actively recruiting bright, talented individuals who have a passion to succeed.Due to the continued growth of our consulting practice, we are currently recruiting for a Senior Data Engineer to join our team. As a part of Baker Tilly Consulting, you will find that our global brand and entrepreneurial environment will give you the support you need to apply your industry and technical experience to build your career across a wide range of services to meet our client's most important needs. As a member of our team, you will also contribute to some of the most important activities in our firm which include operating and growing the business, serving the client, developing the best people, and shaping our culture.Baker Tilly Annual Report 2018ResponsibilitiesAs we continue to navigate the impact of COVID-19, you may encounter a longer job application, interview experience and/or a deferred offer of employment for this position.* Create and maintain optimal data pipeline architecture* Assemble large, complex data sets that meet functional / non-functional business requirements* Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.* Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS 'big data' technologies* Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics* Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs* Keep our data separated and secure across national boundaries through multiple data centers and AWS regions* Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader* Work with data and analytics experts to strive for greater functionality in our data systemsQualifications* Bachelor's degree in Computer Science, Statistics, Informatics, Information Systems or another related field* Minimum of five (5) years of experience in a Data Engineering* Advanced working SQL knowledge and experience working with relational databases* Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.* Experience with big data tools: Hadoop, Spark, Kafka, etc.* Query authoring (SQL) as well as working familiarity with a variety of databases* Experience building and optimizing 'big data' data pipelines, architectures and data sets* Deep experience with AWS cloud services: EC2, EMR, RDS, Redshift* Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement* Strong analytic skills related to working with unstructured datasets* Build processes supporting data transformation, data structures, metadata, dependency and workload management* Experience with relational SQL and NoSQL databases, including PostgreSQL and Cassandra* A successful history of manipulating, processing and extracting value from large disconnected datasets* Working knowledge of message queuing, stream processing and highly scalable 'big data' data stores* Experience with stream-processing systems: Storm, Spark-Streaming, etc.* Strong project management and organizational skills required* Experience supporting and working with cross-functional teams in a dynamic environment* Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow preferred"
Data Engineer,"Data Engineer

Our goal at Elkay is to inspire everyday – customers, employees…and the employees of tomorrow. We focus on doing the right thing so we can be in business forever. Our values-driven culture emphasizes investing in people and treating them like part of the family. We’re financially-stable and privately-owned with a solid reputation for ethics, integrity, giving back, and providing an engaging, inclusive environment where careers flourish and grow. Our people are proud to work for Elkay.

And the feeling is mutual – because it’s Elkay’s people who really give us our edge. We empower our employees to take the lead in delivering Elkay’s exceptional customer experience. Our commitment to our people and their professional development is a recipe for success that has fueled our growth from a three-person shop in 1920 to one of today’s leading international suppliers of plumbing, water delivery and branded commercial interiors. If you’re ready for a new career challenge where everything you do will make a difference, talk to us about joining the Elkay family.

The Data Engineer we hire will perform a variety of project and technical tasks supporting Elkay’s analytics infrastructure, including data integration and ETL, database architecture and design, data prep, and reporting. Contribute to all aspects of the project lifecycle, including requirements gathering, design, development, testing, and deployment. Become familiar with Elkay’s business model and the various business functions that utilize Elkay’s analytics infrastructure, including sales, marketing, supply chain, operations, pricing, finance, and customer care. Keep abreast of trends, architectures, and tools in the analytics landscape and evaluate them for incorporation into Elkay’s analytics landscape. Learn quickly and juggle multiple tasks for various customers. Communicate complex technical problems effectively to management and to the business. Collaborate with other technical team members on solutions. Effectively prioritize support tasks with project work.

Specific duties include:
Design, develop and maintain data models, database architectures, and associated database objects in Snowflake, Oracle, and other database solutions such as Azure.
Design, develop, and maintain data integrations using Informatica Power Center, Informatica Integrated Cloud Services, and data prep tools.
Participate in or drive project activities such as requirements gathering, design, develop, test, and deploy.
Assist in the set-up of, and administer, on premise and cloud tools used in the Elkay analytics infrastructure.
Create and maintain necessary technical documentation, including requirements, design, and test documents.
Identify emerging trends, processes, and techniques impacting Elkay’s analytics infrastructure and make suggestions for incorporation of these into the analytics infrastructure.
Must have's:
A Master’s or Bachelor’s degree in Computer Science, MIS, engineering, or a related technical discipline is required.
5+ years of experience in data engineering, data warehousing, business intelligence, ETL on databases such as Oracle or SQL Server, and/or big data is required.
3+ years of experience in ETL/ data integration is required with 2+ years of experience in Informatica PowerCenter, job scheduling tools is required.
Working experience in Python/R/Scala, Snowflake is required.
Hands on experience in writing and understanding complex SQL (e.g. CTE’s others).
Thorough understanding of relational database design and best practices, including dimensional (star, snowflake) models is required.
A collaborative working style and ability to work well within the team and with business consumers is required.
Ability to clearly communicate to technical and non-technical audience by written and verbal is required.
Independent analytical, critical thinking, and problem-solving ability in complex technical environments is required.
Nice to have's:
Production experience in OBIEE, Oracle Analytics Cloud (OAC) and Tableau is nice to have.
Familiarity with big data technologies such as Microsoft Azure Data or AWS is nice to have.
EOE/M/F/D/V/SO"
Data Engineer,"VDeploy Consulting is seeking a ""Big Data Engineerrdquo for one of our clients in "" Chicago,IL Position Big Data Engineer Location Richmond,Mclean,VA,Chicago,IL Mode of Hire Contract Job Description Leads with excellent technical abilities, leadership, strong communication skills, and adaptability to new technology Design and build solutions for our business partners using Big Data technologies Strong knowledge of Spark, Scala, Python with deep programming skills (source code management, debugging, testing, deployment etc.) Strong experience in BigData Hadoop architecture especially Spark concepts Strong experience in overall data pipeline building and building aggregates Good in unix shell scripting and good understanding of python scripting Data Engineers with extensive experience in SparkScala with knowledge in Java Should have experience in developing interfaces with Big data batch and streaming tools within the Spark ecosystem Good Experience on Pyspark and open source technologies like Kafka Storm Flume HDFS Must develop spark program using Spark core Good understanding of Hadoop Big data concepts is a must Work closely with various business stakeholders to get requirements converted into Technical design Able to work on multiple projects and initiatives with competing timelines and demands Please reach me at rajvdeployconsulting.com mailtorajvdeployconsulting.com or call me 410-673- 4001 About VDeploy VDeploy is specialized in Executive Search, IT and Professional Services focused on delivering an unparalleled experience. We focus on relationship building and getting to know our clientrsquos specific needs, built through trust. Our unique culture and approach deliver enduring results, true to each client s specific situation. About Client Our Client is a global leader in technology services and consulting. They enable clients in more than 50+ countries to create and execute strategies for their digital transformation. Their team of 200,000+ innovators, across the globe, is differentiated by the imagination, knowledge, and experience, across industries and technologies that they bring to every project they undertake."
Data Engineer,"Are you interested in building the future of healthcare and transforming the patient experience? Are you hopeful about what data and medical research can do to improve medicine? We’re looking for a Senior Data Engineer to ensure PatientIQ remains on the forefront of using data to drive positive healthcare outcomes.

As a core member of the Analytics department, you will be in a dynamic environment that is actively building the future versions of our automated data science platform - Analytics Autopilot. In addition, our team often works cross-functionally with the Engineering, Product, and Sales departments as PatientIQ scales its business. Your work will involve coming up with new software features, defining metrics, streamlining existing data processes, and mentoring other team members. We heavily value diligence, curiosity, and initiative, as those are key to unlocking the value of PatientIQ's data for our users and our decision-making. Your work will be impactful across the entire organization.

Role Responsibilities
Design, develop, and maintain ETL infrastructure to support the ingestion of external data sources
Work on a cross-functional team to design, develop, and maintain PatientIQ's internal reporting infrastructure
Understand data requirements and implement solutions for data science applications
Perform unit and integration testing
Mentor junior team members
Help scale PatientIQ's data strategy as the platform and business grows
Requirements

Ideal Qualifications
Experience designing, building, and maintaining ETL infrastructure in a production setting
BS/MS in Computer Science, Engineering, Mathematics, or related field
Proficient in Python or another object oriented programming language
Deep knowledge of SQL and at least one database technology
Experience with software development lifecycle processes and using version control systems (git), either from prior data engineering work or in a more traditional software engineering setting
Highly self-motivated with strong analytical problem-solving skills and attention to detail
Nice to Haves
Experience with workflow management systems such as luigi or airflow
Experience in machine learning and/or business intelligence
Experience with cloud technologies such as AWS, Google Cloud Platform, or Azure
Experience with ETL tools like Apache Kafka, Logstash, Segment, Informatica
Experience with automated machine learning technologies such as Amazon SageMaker or Google Cloud AutoML
Experience with cloud data warehouse platforms such as Snowflake, Qubole, etc.
Experience working in Healthcare, Finance or another regulated industry
Benefits
Great Benefits - top-notch health, dental and vision insurance. Additional perks available including 401K.
We are Mission Driven - our team is motivated to solve complex problems, drive medicine forward, and ultimately improve patient outcomes.
True Idea Meritocracy - great ideas win out. We encourage all team members to challenge the status quo because our mission demands this.
Flexible Time Off - we trust you to take the time you need when you feel it is appropriate, given your workload and responsibilities. No need to track it or save up.
World-Class Team - we’re at the top of our industry because of our employees. They’re the best investment we can make, and we never forget that.
Fast Growing - we are building the largest platform for healthcare providers, industry partners, researchers, and others to collaborate on the mission to improve patient outcomes."
Data Engineer,"At American Family Insurance, we’re driven by our customers and employees. That’s why we provide more than just a job – we provide opportunity. Whether you’re already part of our team in search of a new challenge or new to our company and ready for what’s next, you’re in the right place. Every dream is a journey that starts with a single step. Start your journey right here. Join our team. Bring your dreams.

Job ID:
R15759 Senior Data Engineer (Open)
Summary:
Job Family Summary

Determines and builds the technical solution(s) to allow unstructured data to be structured and used by Data Scientists. Seeks to understand the data being worked with as its often unstructured data sets. Often are data gurus who prepare data for all stages of the modeling process including exploration, training, testing, and deployment.

Job Summary Wording

As a Data Engineer III, you’ll work on collecting, storing, processing and building Business Intelligence and Analytics applications within our big data platform. Presently, our team is constructing an enterprise data lake to enable analysts and scientists to self-service data at scale across American Family’s operating companies. We’re leveraging open source technologies like Spark, Python, Hadoop, and cloud native tools to curate high-quality data sets. You’ll also be responsible for integrating these applications with the architecture used across the organization. Adjacent responsibilities include establishing best practices with respect to data integration, data visualization, schema design, performance and reliability of data processing systems, supporting data quality, and enabling convenient access to data for our scientists and business users.

Job Description:


Job Level Summary
Requires specialized depth and/or breadth of expertise in own job discipline or field
Leads others to solve complex problems
Works independently, with guidance in only the most complex situations
May lead functional teams or projects
Primary Accountabilities
Perform exploratory data analysis to determine which questions can be answered effectively with a given dataset. Ability to analyze new (possibly unstructured) data sources to determine what additional value they may bring and how to effectively make use of them.
Design and develop highly scalable and extensible data pipelines from internal and external sources.
Work on cross-functional teams to design, develop, and deploy data-driven applications and products, particularly within the space of data science.
Lead in prototyping emerging technologies involving data ingestion and transformation, distributed file systems, databases and frameworks.
Work with our data science team on applying improvements to their machine learning algorithms and platforms.
Design, build, and maintain tools to increase the productivity of application development and client facing teams.
Partner with business analyst to define, develop, and automate data quality checks.
Review developed solutions to solve specific business problems. Optimize queries, data models, and storage formats to support common usage patterns.
Design and develop big data applications and data visualization tools.
Code structure moves beyond procedural ad-hoc workflows and exhibits modularity and comprehensive test cases. Develops code at a high-level of abstraction – writes, maintains, and reuses utilities/libraries across projects.
Influences strategy related to processes and workflows across their division.
Participates in mentoring junior colleagues.
Specialized Knowledge & Skills Requirements
Demonstrated experience providing customer-driven solutions, support or service.
In-depth knowledge of SQL or NoSQL and experience using a variety of data stores (e.g. RDBMS, analytic database, scalable document stores)
Extensive hands-on Python programming experience, with an emphasis towards building ETL workflows and data-driven solutions. Able to employ design patterns and generalize code to address common use cases. Capable of authoring robust, high quality, reusable code and contributing to the division’s inventory of libraries.
Expertise in big data batch computing tools (e.g. Hadoop or Spark), with demonstrated experience developing distributed data processing solutions.
Applied knowledge of cloud computing (AWS, GCP, Azure).
Knowledge of open source machine learning toolkits, such as sklearn, SparkML, or H2O.
Solid data understanding and business acumen in the data rich industries like insurance or financial
Applied knowledge of data modeling principles (e.g. dimensional modeling and star schemas).
Strong understanding of database internals, such as indexes, binary logging, and transactions.
Experience using tools for infrastructure-as-code (e.g. Docker, CloudFormation, Terraform, etc.)
Experience with software engineering tools and workflows (i.e. Jenkins, CI/CD, git).
Practical experience authoring and consuming web services.
Education and Licenses
Bachelor’s degree in computer science or related field, or equivalent combination of education and experience.
Travel Requirements
This position requires travel up to 10% of the time.
Additional Job Information:
Top candidates will have 5-10 years of post-academic data engineering experience. This is not a Business Intelligence role nor an ETL Developer.Depending on qualifications, candidates may be hired at a different levels.We are seeking candidates with demonstrated experience using Python and SQL--a coding challenge will be incorporate into the selection process.Knowledge and experience of cloud platforms a plus, specifically AWS.Candidates experience with Python, AWS, and/or Hadoop preferred.

Offer to selected candidate will be made contingent on the results of applicable background checks.

Offer to selected candidate is contingent on signing a non-disclosure agreement for proprietary information, trade secrets, and inventions.

Relocation assistance is available.

Stay connected: Join our Talent Community!

LI:DB1

At American Family Insurance, we’re driven by our customers and employees. That’s why we provide more than just a job – we provide opportunity. Whether you’re already part of our team in search of a new challenge or new to our company and ready for what’s next, you’re in the right place. Every dream is a journey that starts with a single step. Start your journey right here. Join our team. Bring your dreams.

Job ID:
R15759 Senior Data Engineer (Open)
Summary:
Job Family Summary

Determines and builds the technical solution(s) to allow unstructured data to be structured and used by Data Scientists. Seeks to understand the data being worked with as its often unstructured data sets. Often are data gurus who prepare data for all stages of the modeling process including exploration, training, testing, and deployment.

Job Summary Wording

As a Data Engineer III, you’ll work on collecting, storing, processing and building Business Intelligence and Analytics applications within our big data platform. Presently, our team is constructing an enterprise data lake to enable analysts and scientists to self-service data at scale across American Family’s operating companies. We’re leveraging open source technologies like Spark, Python, Hadoop, and cloud native tools to curate high-quality data sets. You’ll also be responsible for integrating these applications with the architecture used across the organization. Adjacent responsibilities include establishing best practices with respect to data integration, data visualization, schema design, performance and reliability of data processing systems, supporting data quality, and enabling convenient access to data for our scientists and business users.

Job Description:


Job Level Summary
Requires specialized depth and/or breadth of expertise in own job discipline or field
Leads others to solve complex problems
Works independently, with guidance in only the most complex situations
May lead functional teams or projects
Primary Accountabilities
Perform exploratory data analysis to determine which questions can be answered effectively with a given dataset. Ability to analyze new (possibly unstructured) data sources to determine what additional value they may bring and how to effectively make use of them.
Design and develop highly scalable and extensible data pipelines from internal and external sources.
Work on cross-functional teams to design, develop, and deploy data-driven applications and products, particularly within the space of data science.
Lead in prototyping emerging technologies involving data ingestion and transformation, distributed file systems, databases and frameworks.
Work with our data science team on applying improvements to their machine learning algorithms and platforms.
Design, build, and maintain tools to increase the productivity of application development and client facing teams.
Partner with business analyst to define, develop, and automate data quality checks.
Review developed solutions to solve specific business problems. Optimize queries, data models, and storage formats to support common usage patterns.
Design and develop big data applications and data visualization tools.
Code structure moves beyond procedural ad-hoc workflows and exhibits modularity and comprehensive test cases. Develops code at a high-level of abstraction – writes, maintains, and reuses utilities/libraries across projects.
Influences strategy related to processes and workflows across their division.
Participates in mentoring junior colleagues.
Specialized Knowledge & Skills Requirements
Demonstrated experience providing customer-driven solutions, support or service.
In-depth knowledge of SQL or NoSQL and experience using a variety of data stores (e.g. RDBMS, analytic database, scalable document stores)
Extensive hands-on Python programming experience, with an emphasis towards building ETL workflows and data-driven solutions. Able to employ design patterns and generalize code to address common use cases. Capable of authoring robust, high quality, reusable code and contributing to the division’s inventory of libraries.
Expertise in big data batch computing tools (e.g. Hadoop or Spark), with demonstrated experience developing distributed data processing solutions.
Applied knowledge of cloud computing (AWS, GCP, Azure).
Knowledge of open source machine learning toolkits, such as sklearn, SparkML, or H2O.
Solid data understanding and business acumen in the data rich industries like insurance or financial
Applied knowledge of data modeling principles (e.g. dimensional modeling and star schemas).
Strong understanding of database internals, such as indexes, binary logging, and transactions.
Experience using tools for infrastructure-as-code (e.g. Docker, CloudFormation, Terraform, etc.)
Experience with software engineering tools and workflows (i.e. Jenkins, CI/CD, git).
Practical experience authoring and consuming web services.
Education and Licenses
Bachelor’s degree in computer science or related field, or equivalent combination of education and experience.
Travel Requirements
This position requires travel up to 10% of the time.
Additional Job Information:
Top candidates will have 5-10 years of post-academic data engineering experience. This is not a Business Intelligence role nor an ETL Developer.Depending on qualifications, candidates may be hired at a different levels.We are seeking candidates with demonstrated experience using Python and SQL--a coding challenge will be incorporate into the selection process.Knowledge and experience of cloud platforms a plus, specifically AWS.Candidates experience with Python, AWS, and/or Hadoop preferred.

Offer to selected candidate will be made contingent on the results of applicable background checks.

Offer to selected candidate is contingent on signing a non-disclosure agreement for proprietary information, trade secrets, and inventions.

Relocation assistance is available.

Stay connected: Join our Talent Community!

LI:DB1"
Data Engineer,"Bachelor's degree or higher in Computer Science or closely related field 4 years of experience using ""big data"" technologies 2+ years of experience in design and implementing AWS Cloud Solutions at enterprise level Experience with functional or object-oriented languages such as Python, C++, REST, Scala Handson experience on Complex SQl queries Experience with Airflow, Argo, Luigi, or similar orchestration tool Experience performing root cause analysis on Spark jobs to identify areas for improvement Experience with No-SQL databases such as HBase, Cassandra, or Redis. Experience with streaming technologies such as Kafka, Flink, or Spark Streaming Experience with DevOps principals and CICD and Containers and Kubernetes"
Data Engineer,"The Senior Data Engineer will work within the data and the analytics team and partner with multiple businesses and various engineering teams (Platform, Security) to build high quality data pipelines. This individual will be responsible to integrate data from a variety of data sources utilizing cloud-based data structures (AWS) and determine/enhance existing data sources between internal and external stakeholders.
The things that you will tackle:
Solve complex data problems to deliver insights to help our businesses achieve their goals
Advise, consult and coach other data and analytic professionals on data standards and practices
Develop, implement and optimize streaming, data lake and analytics big data solutions
Create and execute testing including unit, integration and end-to-end tests of data pipelines
Foster a culture of sharing, re-use, design for scale stability and operational efficiency of data and analytical solutions
Adapt and learn new technologies in a quickly changing field
Create data products for analytics and engineering team members to improve their productivity
Recommend and implement best tools to ensure optimized data performance
Design, develop, optimize and maintain data architecture and pipelines that adhere to HIMSS principles and business goals
Lead evaluation, implementation and deployment of emerging tools & process for analytic data engineering to improve our productivity as a team

Bachelor’s degree in computer science or related field.
Minimum of five (5) years of application development and implementation experience.
Experience with SQL, Big Data, ETL programming and development within

Our talent anywhere philosophy allows employees the flexibility to work virtually on an as-needed or permanent basis.
Competitive, comprehensive healthcare coverage.
Generous paid time off, including time off to volunteer!
401K with employer match and profit sharing.
Be Well! Lifestyle reimbursement program.
Tuition reimbursement and professional development programs.
Are you a Changemaker?
We’ll do amazing things for healthcare.
HIMSS is an Equal Opportunity Employer: M/F/Vets/Disabled"
Data Engineer,"Our client, a premier investment bank, is looking for a Big Data Engineer to join their growing team in Chicago, IL.

Currently our client spends a lot of time trying to answer client's questions by logging in to several disparate data sources. It is estimated each support analyst spends about an hour every day answering client's questions. The goal of the project is to provide clients access to the data so they can access the data themselves.

Role to be played by the consultant/contractor:
Work with the business, understand and review technical requirements
Design and develop end to end solution
Ability to pick right technology for the execution of project
Profile
Hands on experience working with tools in Big Data ecosystem
Hands on experience in building Kafka streaming application in Java
Hands on experience in building Spark applications in Java or Scala
Hands on experience in working with NoSQL databases
Full stack Java developer
Hands on experience in building production applications using Node.js and React
Environment History of Project:
Cross Asset Query Tool will bring low latency query capabilities to our Cross Asset data"
Data Engineer,"Job Description

Data Engineer

At Citadel, data is the core of the investment process. Data Engineers architect and build our data platforms which drive how we source, enrich, and store data that integrates into the investment process. These Data Engineers own the entire data pipeline starting with how we ingest data from the outside world, transforming that information into actionable insights, and ultimately designing the interfaces and APIs that our investment professionals and quantitative researchers use to monetize ideas. Throughout the process, our Data Engineers partner with top investment professionals and data scientists to design systems that solve our most critical problems and answer the most challenging questions in finance.

YOUR OPPORTUNITY:

Develop solutions that enable investment professionals to efficiently extract insights from data. This includes owning the ingestion (web scrapes, S3/FTP sync, sensor collection), transformations (Spark, SQL, Kafka, Python/C++/Java), and interface (API, schema design, events)
Partner with the industry’s top investment professionals, quantitative researchers, and data scientists to design, develop, and deploy solutions that answer fundamental questions about financial markets
Build tools and automation capabilities for data pipelines that improve the efficiency, quality and resiliency of our data platform
Drive the evolution of our data strategy by challenging the status quo and identifying opportunities to enhance our platform
YOUR SKILLS & TALENTS:

Passion for working with data in order to accurately model and analyze complex systems such as a publicly traded company, commodity market, economy, or financial instruments
Strong interest in financial markets and a desire to work directly with investment professionals
Proficiency with one or more programming languages such as Java or C++ or Python
Proficiency with RDBMS, NoSQL, distributed compute platforms such as Spark, Dask or Hadoop
Experience with any of the following systems: Apache Airflow, AWS/GCE/Azure, Jupyter, Kafka, Docker, Kubernetes, or Snowflake
Strong written and verbal communications skills
Bachelor’s, Master’s or PhD degree in Computer Science or equivalent experience
About Citadel


Citadel is a global investment firm built around world-class talent, sound risk management, and innovative leading-edge technology. For a quarter of a century, Citadel’s hedge funds have delivered meaningful and measurable results to top-tier investors around the world, including sovereign wealth funds, public institutions, corporate pensions, endowments and foundations.

With an unparalleled ability to identify and execute on great ideas, Citadel’s team of more than 675 investment professionals, operating from offices including Chicago, New York, San Francisco, London, Hong Kong and Shanghai, deploy capital across all major asset classes, in all major financial markets."
Data Engineer,"Who is Trace3?

Trace3 is a leading Transformative IT Authority, providing unique technology solutions and consulting services to our clients. Equipped with elite engineering and dynamic innovation, we empower IT executives and their organizations to achieve competitive advantage through a process of Integrate, Automate, Innovate.

Our culture at Trace3 embodies the spirit of a startup with the advantage of a scalable business. Employees can grow their career and have fun while doing it!

Trace3 is headquartered in Irvine, California. We employ more than 850 people all over the United States. Our major field office locations include Atlanta, Denver, Detroit, Indianapolis, Grand Rapids, Lexington, Los Angeles, Louisville, San Diego, San Francisco, and Scottsdale.

Ready to discover the possibilities that live in technology?

Come Join Us!

Street-Smart - Thriving in Dynamic Times

We are flexible and resilient in a fast-changing environment. We continuously innovate and drive constructive change while keeping a focus on the ""big picture."" We exercise sound business judgment in making high-quality decisions in a timely and cost-effective manner. We are highly creative and can dig deep within ourselves to find positive solutions to different problems.

Juice - The ""Stuff"" it takes to be a Needle Mover

We get things done and drive results. We lead without a title, empowering others through a can-do attitude. We look forward to the goal, mentally mapping out every checkpoint on the pathway to success, and visualizing what the final destination looks and feels like.

Teamwork - Humble, Hungry and Smart

We are humble individuals who understand how our job impacts the company's mission. We treat others with respect, admit mistakes, give credit where it's due and demonstrate transparency. We ""bring the weather"" by exhibiting positive leadership and solution-focused thinking. We hug people in their trials, struggles, and failures – not just their success. We appreciate the individuality of the people around us.

About the Role:

What You'll Do:

The Data Engineer will be responsible for leveraging new and emerging technologies to solve key technical challenges for our clients. The Data Engineer will act as an expert and trusted advisor who develops, implements, troubleshoots, and optimizes data solutions across many platforms. This role will work closely with clients, partners and other business units to ensure consulting engagements are successful.

SUMMARY OF ESSENTIAL JOB FUNCTIONS:
Understand customers' overall data estate, IT and business priorities and success measures to design implementation architectures and solutions using advanced analytics and artificial intelligence
Develop deep relationships with key customer IT decision makers, who drive long-term cloud adoption within their company to enable them to be cloud advocates
Maintain and advance deep technical skills and knowledge, keeping up to date with market trends and competitive insights, and share within the technical community
Be a Voice of Customer to share insights and best practices, connect with Engineering team to remove key blockers
Assess the Customers' knowledge of Azure platform and overall cloud readiness to support customers through a structured learning plan and ensure its delivery through partners
Responsible for design, development, and hands-on implementation of data intelligence solutions including data platform build-up, proof of concepts or pilot implementation, software development, software integration, and documentation
Perform hands on development of apache, big data technologies, and framework
Serve as a data intelligence technical resource in team's efforts to determine the needs of our client's businesses that will simplify and automate the applications as well as make them more efficient
Align solutions with standards and best practices working with cross-functional engineering and consulting teams
Collaborate and communicate with Sales and Account Management team to ensure smooth and successful delivery and assist with the identification of additional Advanced Services and Sales opportunities within the customer's environment
Establish strong and lasting relationships with key stakeholders and decision makers in client organizations
Contribute to the development of internal best practices as well as new innovative consulting services offerings that we can take to market
Build a community and following around our company solutions and brand awareness
REQUIRED SKILLS AND EXPERIENCE:
Bachelor's degree from an accredited university required
Understanding and hands on experience with modern distributed data systems(Hadoop ecosystem, public cloud, etc).
Experience building applications in c# or java.
Understanding of BI technologies from traditional data warehousing to SaaS solutions in the cloud.
Experience in designing data and analytics architectures in Microsoft Azure cloud.
Well informed on cloud native technologies that enable batch and streaming data ingestion into cloud (For example: Azure Data Factory, Azure Event Hubs, Azure IoT Hubs etc)
Experienced in designing data lakes in Azure cloud for serving big data analytical workloads.
Proven track record of driving decisions collaboratively, resolving conflicts and ensuring follow through with exceptional verbal and written communication skills
Microsoft Certified Azure Solutions Architect Expert certification a plus.
Previous experience working for a consulting or services organization strongly preferred
5+ years of software development experience in distributed systems and building large-scale applications
5+ years of experience in building large scale, high performance, high availability systems and Strong Computer Science fundamentals (algorithms, data structures)
Hadoop, NoSQL or other Big Data certifications are a huge plus
Experience with Big Data technologies (SPARK, HDFS, HBase, Cloudera, MAPR, Hadoop and other frameworks in Hadoop ecosystem
Deep knowledge of Hadoop tools (MapReduce, SPARK, Oozie, ELK, KAFKA, HUE, HBase)
Fluency in several programming languages such as Python, Scala, or Java, with the ability to pick up new languages and technologies quickly
Intermediate knowledge with software engineering best practices
Must be able to quickly understand technical and business requirements and be able to translate them into technical implementations
Ability to mix deep technical expertise with simple, everyday language to deliver a story that is memorable, educational and useful
Highly organized, detail-oriented, excellent time management skills and able to effectively prioritize tasks in a fast-paced, high-volume, and evolving work environment
Ability to approach customer and sales requests with a proactive and consultative manner; listen and understand user requests and needs and effectively deliver
Comfortable managing multiple and changing priorities, and meeting deadlines in an entrepreneurial environment
Motivated self-starter who loves to troubleshoot and solve challenging problems and feels comfortable working directly with customers
The Perks:
Comprehensive medical, dental and vision plans for you and your dependents
401(k) Retirement Plan with Employer Match, 529 College Savings Plan, Health Savings Account, Life Insurance, and Long-Term Disability
Competitive Compensation
Training and development programs
Stocked kitchen with snacks and beverages
Collaborative and cool office culture
Work-life balance and generous paid time off
***To all recruitment agencies: Trace3 does not accept unsolicited agency resumes/CVs. Please do not forward resumes/CVs to our careers email addresses, Trace3 employees or any other company location. Trace3 is not responsible for any fees related to unsolicited resumes/CVs."
Data Engineer,"This position is for April Health, Alphascript's parent company.

Innovative, adaptable, results-driven. If these are words that you would use to describe yourself, our Data Engineer role might be right for you. We’re looking for someone who can resolve routine problems promptly and efficiently, and who can come up with creative solutions for problems that aren’t so routine.
The role of the Data Engineer is to work with Data Architect to build Enterprise Data Management system from the ground up. The Data Engineer also works as a liaison between the subject matter experts in other departments and the Information Systems Department to understand the business requirements, needs and gaps in order to identify the appropriate datasets to perform analysis and develop insightful reports and dashboards.
You should be open to new and different ways to accomplish your work and be comfortable with new processes, initiatives and changes in priorities. This role requires a lot of collaboration with the Alphascript team, so you should be able to convey facts and information clearly (both verbally and written) and be comfortable sharing your ideas and proactively contributing to group objectives.
Responsibilities
Work with Data Architect to develop a data lake, data warehouse in local and/or Azure cloud environment
Integrate disparate data models into coherent enterprise data models
Develop ETL data pipelines to populate data lake and warehouse
Actively participate in Data Governance Program to maintain metadata and data definitions
Work in a team environment with other departments to develop reports, KPIs and dashboards (very strong communication skills)
Interpret business requirements to identify proper tools and methods to analyze, identify and report data trends and variances
Education and Experience
Bachelor’s Degree in Computer Science, Computer Engineering or Information Systems with university level programming courses
3+ years of data engineer experience
3+ years of recent experience in ETL and data warehouse development or maintenance
3+ years of experience in KPI, reports and dashboard development
Experience in healthcare company is a plus
Experience in Azure and SharePoint is a plus
Experience in agile software development environment is a plus
Skills and Abilities
Proficiency in Power BI, Azure Cloud, C#, ETL (SSIS preferred), T-SQL, Excel
Ability to develop data dictionaries of an existing database
Ability to write, analyze and debug SQL queries
Ability to develop dashboards and data models in Power BI; must be familiar with DAX and Power Pivot, and be willing to get proficient at them
Ability to develop business models and perform analysis in MS Excel
Proficiency in R and Python preferred
Working at April Health
We’re a growing team driven by a culture of passion and a commitment to excellence. You’ll work in a modern office alongside dedicated colleagues who work to consistently provide a superior patient experience. We offer a variety of competitive benefits like fully paid health insurance premiums, 401k matching and financial planning assistance, ongoing training and more."
Data Engineer,"Data Engineer

ETL automation tools such as Informatica, Mulesoft, SSIS, Alooma, or Apache Airflow

Experience with traditional RDBMS solutions such as Microsoft SQL Server and Oracle

Experience with cloud-based platforms and tools

Familiarity with DevOps tools and practices such as Git, Jenkins, JIRA, Azure DevOps

Experience with integrating to both database systems and APIs

Experience with documenting technical requirements, designs and systems

Extensive experience building scalable and resilient data pipelines

Extensive experience writing SQL

Experience with a procedural, functional or object-oriented programming language such as Python, Java, Scala, R

Additionally, candidates should be able to perform at a high level in at least two of the following technology categories:
Big Data tools such as Apache Hadoop, Spark, and Hive including managed solutions such as Databricks, Amazon EMR, Azure HD Insight, and Google Cloud Dataproc
Cloud data storage solutions such as Amazon S3, Azure Blob Storage, or Google Cloud Storage
Data warehousing solutions such as Redshift, BigQuery, and Snowflake
Message broker solutions such as Kafka, Google Pub/Sub, Amazon Kinesis
Stream processing solutions such as Flume, Storm, Spark Streaming
NoSQL Databases such as HBase, Cassandra, Redis
Requirements

3-5+ years of experience in technology and/or consulting

Bachelor’s Degree in CS, MIS, CIS, or a comparable technical degree

US Citizen or GC Holder

Benefits

Sense Corp powers insight-driven organizations.

We turn data into actionable insights and transform organizations for the digital era.

Our people, culture, and how we engage with our clients are differentiators. Brilliant, Creative, Human, and Fun exemplify who we are. We are regularly recognized as a Best Place to Work by Austin, Houston, Dallas, and St. Louis Business Journals. With operations in Austin, Atlanta, Columbus, Dallas, Houston, San Antonio, and St. Louis we serve mid-market to Fortune 50 companies.

The Sense Corp Compass

We may be the only management consulting firm in the country where being brilliant isn’t enough to land you a job. Sense Corp people must be brilliant, creative, human, and fun all at once. In other words, we hire terrific, well-rounded people. It’s one reason clients love working with us. And it’s why we enjoy working with each other. We may not sound like typical consultants but that’s OK. We don’t think like them either.

Visit us at www.sensecorp.com."
Data Engineer,"Data EngineerSlalom is a modern consulting firm focused on strategy, technology, and business transformation. We believe in what's possible and shape what's next.At Slalom, personal connection meets global scale. We build deep relationships with our clients in cities across the U.S., U.K., and Canada, while sharing insights across markets to bring the full breadth of Slalom's expertise to every engagement. Our seven regional Build Centers are hubs for innovation, attracting top talent to rapidly co-create the technology products of tomorrow. We also nurture strong partnerships with over 200 leading technology providers, including Amazon Web Services, Google Cloud, Microsoft, Salesforce, and Tableau.Founded in 2001 and headquartered in Seattle, Slalom has organically grown to over 6,500 employees. We were named one of Fortune's 100 Best Companies to Work For in 2019 and are regularly recognized by our employees as a best place to work. Learn more at slalom.com.About the RoleAs a Data Engineer, you'll work in small teams to deliver innovative solutions on Amazon Web Services, Azure, and Google Cloud using core cloud data warehouse tools, Hadoop, Spark, Event Stream platforms, and other Big Data related technologies. In addition to building the next generation of data platforms, you'll be working with some of the most forward-thinking organizations in data and analytics.Responsibilities* Work as part of a team to develop Cloud Data and Analytics solutions* Participate in development of cloud data warehouses and business intelligence solutions* Data wrangling of heterogeneous data and explore and discover new insights* Gain hands-on experience with new data platforms and programming languages (e.g. Python, Hive, Spark)Qualifications* 3+ years of related work experience in Data Engineering or Data Warehousing* Hands-on experience with leading commercial Cloud platforms, including AWS, Azure, and Google* Proven experience with data warehousing, data ingestion, and data profiling* Proficient in SQL* Strong aptitude for learning new technologies and analytics techniques* Highly self-motivated and able to work independently as well as in a team environment* Understanding of agile project approaches and methodologies* Proficient in a source code control system, such as Git* Proficient in the Linux shell, including utilities such as SSHPreferred Experience* Familiarity with implementing analytics solutions with one or more Hadoop distributions (Cloudera, Hortonworks, MapR, HDInsight, EMR)* Familiarity with streaming data ingestion* Proficient in Python and/or Java* Consulting experience* Familiarity or strong desire to learn quantitative analysis techniques (e.g., predictive modeling, machine learning, segmentation, optimization, clustering, regression)Slalom is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law."
Data Engineer,"Senior Data EngineerIMMEDIATE NEED for a Senior Data Engineer to join an amazing company at their Houston, TX location regarding Direct Hire employment.

Responsibilities:

• Collaborate with data scientists, product management, and web engineers to deliver value and project outcomes
• Convert prototype models and data pipelines built by data scientists for use in production.
• Evaluation and debugging of model performance to ensure parity with prototype (Spark/Java/Python).
• Develop low-latency, real-time predictive models in a microservice environment (Java).
• Balance long-term code health and maintainability with business needs.
• Profiling and performance tuning of production code.
• ML Ops: support of Dataproc, Zeppelin, Gitlab, continuous integration systems, monitoring, alerting, etc.

Qualifications:

• Experience and interest in Big Data technologies (Hadoop/ Spark/ NoSQL DB)
• Experience working on projects within the cloud ideally Azure/AWS
• Strong development background with experience in at least two scripting, object oriented, or functional programming language, etc. SQL Python, Java, Scala, R
• Experience in at least one ETL tool
• Ability to work across structured, semi-structured, and unstructured data, extracting information and identifying linkages across disparate data sets

Please send your resume to Andrew Butler, Senior Technical Recruiter for immediate consideration. Let us help you secure an interview!

ABOUT US

Irvine Technology Corporation (ITC) is a leading provider of technology and staffing solutions for IT, Security, Engineering, and Interactive Design disciplines servicing startups to enterprise clients, nationally. We pride ourselves in the ability to introduce you to our intimate network of business and technology leaders bringing you opportunity coupled with personal growth, and professional development! Join us. Let us catapult your career!

Irvine Technology Corporation provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, Irvine Technology Corporation complies with applicable state and local laws governing non-discrimination in employment in every location in which the company has facilities.

Job Requirements:"
Data Engineer,"Home » New Job from Competentia
Data Engineer

Vacancy Number: 26110

Location: Houston, TX

Date Posted: May 18, 2020

Data Engineer

About the role

Our client’s Petroleum Innovation Team enables the discovery, prioritization, de-risking and delivery of step change and disruptive innovation opportunities in support of Petroleum growth and value. We are seeking unique and diverse skills to inject new thoughts and perspectives into our Innovation efforts. Our client is excited to bring together passionate and creative individuals with diverse Petroleum, Technology and Innovation backgrounds to join our team.

The Innovation Data Engineer role will be part of an agile team responsible for de-risking and reducing uncertainty around how new technologies can be used to solve our most challenging problems. This role is responsible for the extraction, collection warehousing and preparation of diverse data sets to support building data science pipelines. In this role, you will be helping our client make a better use of its vast amounts of data and develop insights that have business value. This role requires the ability to quickly build and deploy data collection, ingestion and delivery pipelines to facilitate data use by analytics and advanced machine learning/AI engines. We need out of the box thinkers, unafraid to try new things and learn from how other industries work with data. This is individual contributor role in which you will deliver data engineering solutions to support a multi-disciplinary team in their quest to build scalable, deployable innovative solutions with maximum benefits to the company.

In this role you will:
Work under the supervision of data science lead, geoscientists and subject matter experts.
Collect, aggregate and wrangle large volumes of data from multiple sources using advanced methodologies in data engineering.
Develop data ingestion and preparation workflows for data science pipelines for high and low velocity data.
Perform descriptive analytics on large heterogeneous data sets and create metrics to measure data quality and readiness for analytics.
Collaborate with other team members and the business to improve data models that feed BI tools and data science pipelines.
Assist with the build out and maintenance of an AWS development environment
Research and identify potential data engineering solutions from external partners
Attend relevant industry and technology conferences/seminars and bring back learnings for sharing to the broader Innovation Team and Petroleum Business
Help to build and personally model capabilities and behaviours that value and promote innovation
About you
Bachelors (Master is preferred) degree in STEM major from accredited institution.
5+ years’ experience in data engineering and managing large, complex, disparate data sets.
Good understanding of descriptive analytics and data engineering techniques for data science.
Demonstrated skills in data collection, cleansing, visualization, data quality assessment, and the use of analytics to build minimum viable data (MVD)
Demonstrated expertise in the development of data engineering pipelines for machine learning and artificial intelligence applications using structured, unstructured and semi-structured data sets.
Must have experience building data models to integrate diverse and high dimensional data sets.
Excellent problem solving and critical thinking skills with a thirst to learn new areas.
Experience with real-time data ingestion.
Good understanding of analytics and machine learning project lifecycle.
Interdisciplinary mind, i.e. demonstrated ability to map experiences across different domains.
Demonstrated skills in the use of one or more analytics software tools and languages (e.g. Python, R, Matlab, Java, Scala)
Experience in working with and analysing complex geospatial data sets, and knowledgeable in Geospatial analytic tools such as ArcGIS, ArcPRO, ArcPy, etc.
Good understanding of distributed computing, virtualization, and cloud technologies.
Excellent oral and written communication skills, able to effectively explain technical information to various audiences
Experience in Oil & Gas is a plus

Competentia is an Equal Opportunity / Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status, or other status protected by law or regulation.

Competentia, participates in E-Verify as required by law."
Data Engineer,"As a member of our Software Engineering Group, we look first and foremost for people who are passionate around solving business problems through innovation and engineering practices. You'll be required to apply your depth of knowledge and expertise to all aspects of the software development lifecycle, as well as partner continuously with your many stakeholders on a daily basis to stay focused on common goals. We embrace a culture of experimentation and constantly strive for improvement and learning. You'll work in a collaborative, trusting, thought-provoking environment-one that encourages diversity of thought and creative solutions that are in the best interests of our customers globally.This Data Engineer position will be primarily responsible for Data & Analytics development within JP Morgan's Legal Technology. This position will be an integral part of building and maintaining a net new data streaming platform to unify and govern reporting and data access across various products. Additional responsibilities may include design, construction, testing, optimization, and deployment of Data Engineering & Analytics technologies including but not limited to Relational Databases (Oracle), Kafka, Python as well as AI/ML & advanced analytics skills like Regression & Classification modeling in R or Python.The Data Engineer will need to work effectively with a globally distributed team of developers & product owners/stakeholders to implement reporting & analytics solutions leveraging the JPMC Legal generated data. Overall, the ideal candidate for this position will be highly skilled in data warehouse, streaming data, data manipulation & advanced analytics tools and have knowledge of visualization and presentation of enterprise data.This role requires a wide variety of strengths and capabilities, including:* BS/BA degree or equivalent experience* Advanced knowledge of application, data, and infrastructure architecture disciplines* Understanding of architecture and design across all systems* Working proficiency in developmental toolsets* Knowledge of industry-wide technology trends and best practices* Ability to work in large, collaborative teams to achieve organizational goals* Passionate about building an innovative culture* Proficiency in one or more modern programming languages* Understanding of software skills such as business analysis, development, maintenance, and software improvement* 6+ years of experience in a Data Engineering role, with a focus on data & analytics technologies* Experience delivering product with Agile / Scrum methodologies* Ability to communicate cross-functionally, derive requirements and architect shared datasets; ability to synthesize, simplify and explain complex problems to different types of audiences* Applied experience with Kafka streaming, JMS and other messaging technologies* Familiar with Docker, Kubernetes and other container technologies* Knowledge and applied experience in Java with Spring Framework, Spring Boot and other Spring technologies* Proficient in data analytics skills for more than one data engineering & analytics technologies including data warehouse (Oracle) and advanced analytics tools & methodologies (NLP, Classification models) Comprehensive analysis & design experience with demonstrated knowledge of Oracle based data warehouse / database structures.* Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.* Previous working experience in SQL, PL/SQL a must.* Proficiency in one or more modern programming languages- experience in Python, R etc.* Experience with Tableau & similar visualization tools a plus.* Strong data analysis skills and problem solving ability* Working proficiency in a selection of software engineering disciplines and demonstrates understanding of overall software skills including business analysis, development, testing, deployment, maintenance and improvement of software.* Knowledge of industry wide technology strategies and best practices* Ability to work in large, collaborative teams to achieve organizational goals, and passionate about building an innovative cultureOur Corporate Technology team relies on smart, driven people like you to develop applications and provide tech support for all our corporate functions across our network. Your efforts will touch lives all over the financial spectrum and across all our divisions: Global Finance, Corporate Treasury, Risk Management, Human Resources, Compliance, Legal, and within the Corporate Administrative Office. You'll be part of a team specifically built to meet and exceed our evolving technology needs, as well as our technology controls agenda.When you work at JPMorgan Chase & Co., you're not just working at a global financial institution. You're an integral part of one of the world's biggest tech companies. In 14 technology hubs worldwide, our team of 40,000+ technologists design, build and deploy everything from enterprise technology initiatives to big data and mobile solutions, as well as innovations in electronic payments, cybersecurity, machine learning, and cloud development. Our $9.5B+ annual investment in technology enables us to hire people to create innovative solutions that will not only transform the financial services industry, but also change the world.At JPMorgan Chase & Co. we value the unique skills of every employee, and we're building a technology organization that thrives on diversity. We encourage professional growth and career development, and offer competitive benefits and compensation. If you're looking to build your career as part of a global technology team tackling big challenges that impact the lives of people and companies all around the world, we want to meet you"
Data Engineer,"Job Description
Walker Elliott has partnered with an exciting Energy Technology firm! Our client is in a great strategic position to not only grow but thrive in this changing market. They are looking for a talented Data Engineer to be a key player in building their next generation, artificial intelligence platform, serving as their core product!

In this role you will immediately have the opportunity to work on key product features for this energy related SaaS offering.

Other responsibilities will include:
Working with big data and artificial intelligence
Architecting and implementing data solutions across the entire platform
Working in a cloud-centric environment
Required Skills:
Experience working with the Azure stack
Database experience using MongoDB and SQL
Python scripting is a PLUS
Do not apply unless you are authorized to work in the United States for any employer as client company cannot sponsor or transfer visas at this time.

Walker Elliott is an Equal Opportunity Employer.

For additional information, please email your resume to resumes@walker-elliott.com or apply online.

http://www.walkerelliott.com/candidates/jobs/jobDetail/default.aspx?GUID=11321&Apply=true"
Data Engineer,"Search Jobs


Job Description

Are you looking for a high energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform?

Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley?

Your next adventure at VMware is only a click away!

VMware's Data Engineering Team is looking for a Data Engineer to help build on Next generation Near Realtime Data Platform based on Hadoop, Spark & SAP HANA. You will be responsible for building and enhancing the solutions on the existing platform based on the business needs in partnering with fellow Developers and Business groups.

Responsibilities:

· Understand the business capability/requirements and transform them into robust design solutions

· Perform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as needed

· Perform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms.

· Perform hands on work using SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform.

· Integrate data sets from difference sources using Informatica, Python, SAP SDI/SLT

· Protect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data.

· Help data consumers to correctly understand and use the data.

Qualifications:

· 8+ years of experience in as a Data Engineer handling large volumes of data.

· Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools.

· Expertise in writing advanced SQL queries.

· Experience working with Informatica, SAP SDI/SLT

· Expertise in SAP HANA, Hive/Hadoop/Hawq/Spark

· Working knowledge of BI Reporting tools like BOBJ and Tableau

· Experience in Python Scripting

· Strong analytical and troubleshooting skills

· Excellent verbal and written communication skills

· Bachelor’s degree in Computer science, Statistics, Mathematics, Engineering or relevant field.

VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. VMware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.

VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. VMware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.

Search Jobs"
Data Engineer,"Position: Data EngineerLocation: Houston, TX 77002

Duration: 6+ months contract

Job Description:

We are currently seeking a Data Platform Engineer with 5+ years experience to join the Big Data and Advanced Analytics department. As part of the Data Engineering team, the Data Platform Engineer will work closely with Data Engineers and IT Infrastructure team to ensure the data platform is highly available, reliable, and stable. This individual will provide technical and thought leadership to the team to streamline the delivery of analytics to the business.

Must Have:

Kubernetes

Linux

MapR

Responsibilities

Implementation and ongoing administration of the platform
Perform incident investigation, diagnosis and provide resolution
Manage cluster provisioning, performance tuning, and security configuration
System monitoring and remediation of any production issues
Identify recurring problems and perform root cause analysis
Provide and implement sustainable solutions
Collaborate with the application teams to install updates, fixes, and patches
Coordinate and perform version upgrades
File system management and monitoring
Act as a primary contact for the platform
Point of contact for hardware and vendor escalation
Document all service levels
Perform application deployments acting as a gatekeeper to the production environment.

Regards,

Hari Haran.S

Wise Men Consultants

O: (281) 957-5888 Ext: 192

C: (754) 205-1604

Email: hari.haran@wisemen.com

www.wisemen.com"
Data Engineer,"Location Houston, TX W2 role, Looking for USC, GC and H1B Transfer. Must Have Skills Python Spark SQL Responsibilities Architect end to end data solutions including data collection and storage, data modeling, and data consumption Work independently on data projects for multiple business functions Implement data flows connecting operational systems, BI systems, and the big data platform Design and implement an Enterprise Data Warehouse Automate manual data flows for repeated use and scalability Develop data-intensive applications with API and streaming data pipelines Prepare and transform data into a usable state for analytics Document and maintain source-to-target mappings and data lineage Productionize mathematical models and machine learning models Assists data analysts and data scientists with query optimization, performance tuning, and data processing Identify opportunities for data improvements and presents recommendations to management"
Data Engineer,"Innovative, adaptable, results-driven. If these are words that you would use to describe yourself, our Data Engineer role might be right for you. We’re looking for someone who can resolve routine problems promptly and efficiently, and who can come up with creative solutions for problems that aren’t so routine.

The role of the Data Engineer is to work with Data Architect to build Enterprise Data Management system from the ground up. The Data Engineer also works as a liaison between the subject matter experts in other departments and the Information Systems Department to understand the business requirements, needs and gaps in order to identify the appropriate datasets to perform analysis and develop insightful reports and dashboards.

You should be open to new and different ways to accomplish your work and be comfortable with new processes, initiatives and changes in priorities. This role requires a lot of collaboration with the Alphascript team, so you should be able to convey facts and information clearly (both verbally and written) and be comfortable sharing your ideas and proactively contributing to group objectives.

Responsibilities
Work with Data Architect to develop a data lake, data warehouse in local and/or Azure cloud environment
Integrate disparate data models into coherent enterprise data models
Develop ETL data pipelines to populate data lake and warehouse
Actively participate in Data Governance Program to maintain metadata and data definitions
Work in a team environment with other departments to develop reports, KPIs and dashboards (very strong communication skills)
Interpret business requirements to identify proper tools and methods to analyze, identify and report data trends and variances

Education and Experience
Bachelor’s Degree in Computer Science, Computer Engineering or Information Systems with university level programming courses
3+ years of data engineer experience
3+ years of recent experience in ETL and data warehouse development or maintenance
3+ years of experience in KPI, reports and dashboard development
Experience in healthcare company a plus
Experience in Azure and Sharepoint a plus
Experience in agile software development is a plus

Skills and Abilities
Proficiency in Power BI, Azure Cloud, C#, ETL (SSIS preferred), T-SQL, Excel
Ability to develop data dictionaries of an existing database
Ability to write, analyze and debug SQL queries
Ability to develop dashboards and data models in Power BI; must be familiar with DAX and Power Pivot, and be willing to get proficient at them
Ability to develop business models and perform analysis in MS Excel
Proficiency in R and Python preferred

Location

This position is based at our Houston, TX office. Occasional travel is required.

Availability

This is a full-time position."
Data Engineer,"Responsibilities for Data Engineer:

â Create and maintain optimal data pipeline.

â Assemble large, complex data sets that meet functional / non-functional business requirements.

â Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, etc.

â Build analytics tools/dashboards that utilize the data pipeline to provide actionable insights into operational efficiency and other key business performance metrics using Tableau.

â Work with stakeholders including the business owners, Data and Design teams to assist with data-related technical issues and support their data needs.

â Create data tools for analytics and data scientist team members that assist them in building and optimizing models/visualizations.

Required Qualifications for Data Engineer

We are looking for a candidate with 2+ years of experience in a Data Engineer role, who has attained a bachelor's degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience in following:

â Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL).

â Experience building and optimizing 'big data' data pipelines, architectures and data sets.

â knowledge with data visualization best practices.

â Minimum 1-year experience in Tableau development

â Experience with various data prep/pipeline/integration tools like Alteryx, Azure Data Factory etc.

â In-depth knowledge of relational databases (e.g. Oracle and SQL Server), including data warehousing concepts and best practices

â Proficient is SQL

â Experience in at least one the following languages â R, Python, SCALA

â Experience working in an agile or Scrum based environment

â Ability to test and document end-to-end processes.

Experience working with Microsoft Azure or AWS platform will be preferred"
Data Engineer,"What you'll do

Skills: Big Data - Impala, HDFS, HBase, Hive, Spark is a plus (including as provided in description below)

Level: Advanced/Expert

Location: Houston , TX

Banking

What you bring

While others say it, we do it: we care. We have great people and we do great work. Just as importantly, we have great relationships with an impressive clientele. Over 1,000 talented, diverse, and career-minded professionals are carving out their role and experiencing a good mix of challenges and opportunities – and we're rooting for them along the way, every day. For more, click: https://www.mindteck.com/career/life-at-mindteck.html
Mindteck is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, religion, color, national origin, sex, sexual orientation, gender identity, age, status as a protected veteran, status as a qualified individual with a disability, or any other trait protected by law."
Data Engineer,"Data Engineer

Share

Job ID: FA-0100-322

Open Since: 2019-05-28

City: Houston
State: Texas
Country: United States of America

Job Description:


Need for a well-demonstrated Data Engineer who will work with the Data Science team to complete a major data project.

Mode of Interview : Telephonic/F2F

Job Skills:
Experience working on Hadoop platform components
Knowledge of Big Data tools, such as zookeeper, Kafka Streaming.
Shell scripting experience
Experience with integration of data from multiple data sources (NoSQL, Mongo, SQL)
Experience working with Structured/Unstructured data.
Experience creating ETL pipelines
Experience in Docker builds and Git file versioning
Demonstrated ability to quickly learn new tools and paradigms to deploy cutting edge solutions.
Knowledge of programming in Python
Knowledge of MapR
Knowledge of Scala framework
Experience with Spark, Storm or Flink
Minimum Experience: 8 Yrs

Roles & Responsibilities:
Integrate Data from multiple data sources
Create ETL Pipelines
Work under the guidance of Lead to develop based on design/architecture.
Education:

Bachelor’s Degree in Computer Science or equivalent work experience. Masters preferred"
Data Engineer,"LyondellBasell (NYSE: LYB) is one of the largest plastics, chemicals and refining companies in the world. Driven by its employees around the globe, LyondellBasell produces materials and products that are key to advancing solutions to modern challenges like enhancing food safety through lightweight and flexible packaging, protecting the purity of water supplies through stronger and more versatile pipes, improving the safety, comfort and fuel efficiency of many of the cars and trucks on the road, and ensuring the safe and effective functionality in electronics and appliances. LyondellBasell sells products into more than 100 countries and is the world's largest producer of polypropylene compounds and the largest licensor of polyolefin technologies. In 2020, LyondellBasell was named to Fortune Magazine's list of the ""World's Most Admired Companies"" for the third consecutive year.Basic FunctionThe Senior Data Engineer is a core member of the team responsible for providing solutions to digital age data challenges through design and implementation of an ecosystem of Cloud solutions to support AI, Machine Learning and other modern data technologies. This role will be responsible for designing, developing, optimizing, standardizing data engineering pipelines along with creating robust data models for data publishing while complying with, and adding value to, the data architecture. The Data Engineer will also be responsible for guiding the existing data engineering team and developing cloud-native solutions with low time to market by leveraging DevOps methodologies. This individual will evangelize and implement the modern practices in data engineering that address scale and are essential for digital transformation through high value driven projects.Roles & ResponsibilitiesWork closely with data scientists, platform engineers, data architects, and data source owners to deliver foundational data sets, enabling analytics solutions driving successful business outcomes.Provide leadership in designing and implementing best practices for efficient sourcing and processing large data sets from Analytics data stores.Build real-time, reliable, scalable, high-performing, distributed, fault tolerant systems.Design and develop code, scripts and data pipelines that leverage structured and unstructured data.Implement measures to address data privacy, security, compliance and ensure robust data governance.Industrializing data lakes or real-time platforms for an enterprise enabling business applications and usage at scaleMonitor, maintain and optimize production systems. Investigate and resolve incidents reported by users. Identify opportunities to automate, consolidate and simplify platform.Work with Enterprise Architecture, Digital, and other IT teams to develop and maintain data integrity, integration and governance standards.Contribute to the selection of platforms, data management, libraries, tool chain and OSS for software development. Stay on top of evolving technology to suggest and prototype and implement improvements to the data architecture.Collaborate with cross-functional teams to help utilize and drive adoption of new big data tools / models.Guide business stakeholder and mentor members of the data and analytics teams regarding technology and best practices.Manage assigned activities within time, cost and technical objectives. May manage small projects with internal or external resources.Min. QualificationsBachelor's degree required10+ years hands-on experience in architecting, developing, and successfully operationalizing complex/large scale data management projectsAt least 5+ years of data warehouse and ETL design and development using SQL on RDBMS and MPP databasesExperience in big data development on Hadoop or Spark frameworks using Python, Java, noSQL, TimeSeries DBs, HDFSExperience in development on Azure Cloud (PaaS) Data SolutionsExperience working with LAMBDA architecture using real-time Kafka ingestion and high-volume batch loadsPreferred QualificationsDegree in Computer Science, Engineering, Technical Science or related disciplines, preferred.Experience building data management (metadata, lineage, tracking etc.) and governance solutions for modern data platformsExperience securing Cloud based modern data platformsUnderstanding of statistics and mathematical techniques to solve real business problems.Experience of working with models in SAP HANA, manufacturing historians, various IoT, subscription and public data sources.Experience of ETL tools such as SAP BusinessObjects Data Services.Experience of Reporting and Analytics tools such as Tableau, Power BI, and AlteryxFunctional experience in one of more business functions.Experience with global enterprise environment or major consulting firm is a plus.CompetenciesBuilds effective teamsCollaboratesCultivates innovationCustomer focusDemonstrates courageDrives resultsEnsures accountabilityInstills trust and exemplifies integrityMust be at least 18 years of age and must be legally authorized to work in the United States (US) on a permanent basis without visa sponsorship.LyondellBasell does not accept or retain unsolicited résumés or phone calls and/or respond to them or to any third party representing job seekers.LyondellBasell is an equal opportunity employer. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, age, veteran status, and other protected characteristics. The US EEO is the Law poster is available here.Nearest Major Market: Houston"
Data Engineer,"DATA ENGINEER – HOUSTON, TX

Who We Are

At m1neral, we are building the next evolution of mineral and royalty investing. Our built-for-purpose Mineral Intelligence Platform is an industry first, and provides enriched data, actionable AI insights, and transaction automation to buyers, operators, and sellers. Our data ingestion pipelines, processing power, and geospatial displays allow investors to target, research, and track the mineral marketplace trends across the United States.

In short, we are disrupting the status quo and are looking for hard-working, top-tier individuals to join our high-powered team.

Role Description

As a Data Engineer, you will be building out the back-end of our core product. You will be responsible work with our team to architect how we ingest, store, and query data across the entire platform. You will be making pivotal decisions on how we manage large datasets and how to make the data easily available in real-time across our platform. This will require close communication with product, design, and other engineers on the team to coordinate deliverables and navigate changes in priorities. Our team is committed to constantly improving, and as a key team member, you will have the opportunity to grow individually, and be expected to contribute to our engineering practices and culture.

The ideal candidate will have experience working with a early-stage SaaS start-ups and/or in fluid, dynamic, and fast-paced environments.

What will you be doing?
Architecting, implementing and deploying data solutions and pipelines
Working with business team members to understand problems and propose technical solutions
Developing expertise with our product, tech stack, and overall architecture
Automating solutions to improve data quality and improve overall productivity of end users
Delivering on-time release of well-tested and reliable code
Development of ETL logic, logging, monitoring, and maintenance of the central database
Integrating multiple data storage solutions using databases such as SQL Server, Mongo DB, and Azure Cognitive Search
Most importantly

We move fast, constantly innovate and you will thrive if you are someone who learns quickly, works well independently (and with others), and above all gets stuff done.

Job Requirements
BS/BA in Computer Science, Engineering, or equivalent preferred
3+ years of experience in Azure cloud architecture and relational database management (SQL Server)
Experience on Azure Data Factory (creating pipelines, loading and moving data) , Logic Apps, and Kusto preferred
Familiarity with Python and Jupyter notebooks a plus
Experience building large scale applications within oil and gas sector preferred
Familiarity with build systems, such as Github/Gitlab, Azure DevOps
Experience implementing, testing, and deploying code to a production environment
Ability to work effectively in a team environment
Capable of knowledgeably discussing performance trade-offs when evaluating different approaches
Great at solving problems, debugging, troubleshooting, and designing & implementing solutions to complex technical issues
Ability and eagerness to independently learn new technologies, prototype and propose solutions to the team
What We Offer
Competitive package (salary & equity)
Flexible work environment
Unlimited PTO
Volunteer time off
Vacation stipend (after 1yr)
High growth potential with an enterprising and energetic team

Notes
For the purposes of the interviewing process, you can expect to be rated on the results of a coding challenge that will be sent to you and both technical and team oriented in-person interviews.
Job Type: Full-time

Pay: $70,000.00 - $90,000.00 per year

Schedule:
Monday to Friday
Education:
Bachelor's (Preferred)
Work Location:
One location
Visa Sponsorship Potentially Available:
No: Not providing sponsorship for this job"
Data Engineer,"As a Data Engineer you will develop and maintain scalable data pipelines while collaborating with analytical and business teams to improve data models. Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it. Writes unit/integration tests, contributes to engineering wiki, and documents work. Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues. Defines company data assets (data models), and other jobs to populate data models and designs data integrations and data quality framework. Works closely with all business units and engineering teams to develop strategy for long term data platform architecture.

Qualifications:
• Preferable to have a Degree in an analytical field (e.g. Computer Science, Mathematics, Statistics, Engineering, Operations Research, Management Science) and 4+ years of professional experience.
• At least 4 years of data analytics experience in a distributed computing environment
• Strong experience with products or similar products like Apache Spark, Big Data
• Building and analyzing dashboards and reports
• Empower and assist operation and product teams through building key data sets and data-based recommendations
• Automating analyses and authoring pipelines via SQL/python based ETL framework
• Superb SQL programming skill with understanding of ETL tools and database architecture.
• Demonstrable familiarity with code and programming concepts. Experience with Python is highly preferred but not required.
**For confidential searches, please send resume directly to jelle.dejong@rht.com***
Scala, Python, Apache Spark

Robert Half Technology matches IT professionals with some of the best companies on a temporary, project or full-time basis. From roles in software and applications to IT infrastructure and operations, we provide you unparalleled access to exciting career opportunities. Our personalized approach, innovative matching technology and global network with local market expertise help you find the technology jobs that match your skills and priorities fast. By working with us, you have access to challenging opportunities, competitive compensation and benefits, and training to enhance your skill sets.

From philanthropy to environmental stewardship to employee programs, Robert Half is proud to have an active role in the communities in which we live and work. Our company has appeared on FORTUNEs Most Admired Companies list every year since 1998.

Download our mobile app to take your job search on the go!

Contact your local Robert Half Technology office at 888.490.4429 or visit www.roberthalf.com/jobs/technology to apply for this job now or find out more about other job opportunities.

All applicants applying for U.S. job openings must be authorized to work in the United States. All applicants applying for Canadian job openings must be authorized to work in Canada.

© 2020 Robert Half Technology. An Equal Opportunity Employer M/F/Disability/Veterans.

By clicking 'Apply Now' you are agreeing to Robert Half Terms of Use ."
Data Engineer,"Job Description
Big Data Engineer
JPMC
Houston , TX
6-12 Months CTH role.
Rate: $66/hr w2 Max or 115K Salary range.
JOB :
Skills: Big Data - Impala, HDFS, HBase, Hive, Spark is a plus (including as provided in description below)
Level: Advanced/Expert
Thanks
Farzan"
Data Engineer,"Tokio Marine HCC is a leading specialty insurance group with offices in the United States, the United Kingdom and Continental Europe, transacting business in approximately 180 countries and underwriting more than 100 classes of specialty insurance. Tokio Marine HCC products and capabilities set the standard for the industry, and many of the Company's almost 3,000 employees are industry-leading experts.Position Summary:The Senior Data Engineer is an important member of our organization that is responsible for developing and supporting best-in-class Actuarial data capabilities and a high-quality analytics environment for Tokio Marine HCC. This includes the creation of analytical databases and tools, provisioning datasets to be used for advanced analytics, implementation of information security best practices & policies, and building out automated data visualization and reporting tools.The Senior Data Engineer will have an eye for building and optimizing data systems and will work closely with various stakeholders such as actuaries, underwriters, IT, and data scientists to develop data pipelines and ensure consistency and compatibility of data development across multiple projects. The Senior Data Engineer will support our group companies in developing new insights that leverage data assets.Performance Objectives:* Create actuarial data and analytics roadmap based on best of breed technology aligned to business needs and priorities - including but not limited to:* Data Visualization* Predictive Modeling* Public Cloud Computing & Storage* AI & Machine-Learning* Help design data flows, reporting, and visualization tools to support actuarial pricing, reserving, and modeling processes.* Collaborate with corporate IT and company management on the development of a corporate-wide data hub* Define requirements for sourcing, analysis, and standardization of data required for Actuarial analysis and work with individual business units to enhance underlying data systems and feeds* Collect big data to be used in pricing, underwriting, and reserving from external data sources (such as websites) to be shared and utilized on a group wide basis* Develop data assets that support organizational decision making and create pilot cases for actual usage of big data. Some examples include:* Text mining to utilize unstructured data in predictive models* Provide expert advice in cutting edge modeling tools and techniques* A common mapping/geocoding solution* Interface with Corporate IT management on end user hardware, software, and support requirements* Implementation of TMHCC information security best practices & policies, including data protection and data privacy standards* Ensure data quality and compliance with data governance standardsQualifications:* Minimum 5-10 years' real project experience as a data engineer or similar role, including background and experience in the insurance industry (property/casualty preferable)* Experience and understanding of modern data technologies and analytical tools (HIVE/ Impala/R/Python/Scala) as well as experience with cloud computing services (Azure and/or AWS), and business reporting tools such as Power BI and Microstrategy.* Experience working closely with actuaries and data scientists in designing datasets and analytical and visualization tools and processes* Strong interpersonal skills and ability to project manage and work with cross-functional teams* Flexible and responsive with ability to adapt to rapid change in direction or business priority* Ability to work independently with limited supervision as well as contribute to team efforts is required* Ability and desire to communicate technical matters in order to train others and lead adoption of new tools and technologies* Demonstrated experience in working on development processes and agile methodologies* Sound analytical skills, as well as strong problem-solving aptitude. Able to proactively respond to a changing diverse business environment* Skilled at optimizing of work processes. Knows the most effective and efficient processes to get things done, with a focus on continuous improvement* Excellent oral and written communication skills* Ability to innovate and apply new ideas* Some domestic and international travel is possible* Bachelor's Degree or higher (desired majors include, but are not limited to: Computer Science/Engineering, Management Information Systems, or Data Analytics)The Tokio Marine HCC Group of Companies offer a competitive salary and employee benefit package. We are a successful, dynamic organization experiencing rapid growth and are seeking an energetic and confident individual to join our team of professionals. The Tokio Marine HCC Group of Companies are equal-opportunity employers. Please visit www.tmhcc.com for more information about our companies.#LI#GD#CB"
Data Engineer,"No need to be in Houston or relocate! Wersquore looking for an enthusiastic Data Engineers with MapR skills to provide brilliant solutions for our clients. Herersquos the deal The Data Engineer will be responsible for transforming data into a format that can be easily analyzed. Principle Accountabilities Installation of clusters across multiple nodes Security assessment and guidance. Skills Needed MapR cluster administration (preferably with the latest version of MapR + MapR 5.2) MapR security, monitoring, DB and Hbase administration MapR ES and Kafka administration MapR Data Governance story Hive, Drill, Spark, Sqoop, Oozie installation, administration, and tuning Automationscripting using Ansible, Puppet, Chef or Salt Experience with NFS or FUSE Basic ETL experience Cloud experience Docker and Kubernetes experience a big plus Qualifications and Requirements Strong work ethic, attitude and follow through ability. Excellent communicative, presentation and interpersonal skills. Makes compelling presentations to a variety of audiences using visual aids, PowerPoint presentations and software demos. Is adept at getting the attention and involvement of the most sophisticated and difficult audiences. Takes initiative and pursues opportunities. Self-motivated and excellent multi-tasking skills. Prioritizes and performs a variety of concurrent tasks with minimal direction. Projects a professional and polished image that inspires confidence and trust. PREDICTif Solutions is an Equal Opportunity Employer. This company does not discriminate in employment and personnel practices on the basis of race, sex, age, handicap, religion, national origin or any other basis prohibited by applicable law."
Data Engineer,"Do you want your voice heard and your actions to count?

Discover your opportunity with Mitsubishi UFJ Financial Group (MUFG), the 5th largest financial group in the world (as ranked by S&P Global, April 2018). In the Americas, were 14,000 colleagues, striving to make a difference for every client, organization, and community we serve. We stand for our values, developing positive relationships built on integrity and respect. Its part of our culture to put people first, listen to new and diverse ideas and collaborate toward greater innovation, speed and agility. Were a team that accepts responsibility for the future by asking the tough questions and owning the solutions. Join MUFG and be empowered to make your voice heard and your actions count.

Job Summary

We're seeking a Data Engineer to support the Core Banking Transformation (CBT) Program. This is a multi-year effort to modernize our deposits platform with a digitally-led and simplified ecosystem for consumer, small business, commercial, and transaction banking to deliver exceptional customer experience.

As the Data Engineer, you need to be collaborative and passionate about solving complex data engineering problems. You will be responsible for the design, build, implementation, monitoring, and management of the MUFG Core Banking data services gateway that provides the foundations for the technology modernization and digital transformation.

You will focus on building the firms next-generation data environment and be a key player in creating a data services platform that drives real-time decision-making in service of our customers. You will develop, build, and operate the platform using DevSecOps and System Reliability Engineering (SRE) methods.

Major Responsibilities:
Gather and process large, complex, raw data sets at scale.
Build processes to support data transformation, data structures, metadata, dependency, and workload management.
Build the infrastructure required for optimal extraction, transformation, and loading of data.
Partner with risk management and security teams to identify the standards and lead the design, build, and rollout of secured and compliant data services.
Embrace Infrastructure-as-Code, and use Continuous Integration / Continuous Delivery Pipelines to handle the full data service lifecycle.
Write infrastructure, application, and data automated test cases and participate in code review sessions.
Provide Level 3 support for troubleshooting and services restoration in Production.



The right candidate will have:
8+ years of technical experience with data services solution design and implementation in a cloud-native environment, possessing expert-level skills in four or more of the following areas:
Data field encryption, tokenization and metadata management
SQL and NoSQL databases, including Postgres, DynamoDB etc.
Experience with data pipeline and workflow tools: Wherescape Streaming, Wherescape RED, StreamSets Data Collector etc.
Experience with stream-processing systems: Kafka, AWS Kinesis, Apache Storm, Spark-Streaming, etc.
History of manipulating, processing and extracting value from large disconnected datasets with ETL and Data engineering
Know-how of SQL, Informatica PowerCenter or similar.
Experience with secure cloud services for data management and integration
Developing automation with python, bash, java, powershell or similar languages
Familiar with DevOps toolchain, i.e. BitBucket, JIRA, Jenkins Pipeline, Artifactory or Nexus, and experienced in deploying n-tier application stacks in AWS
Excellent data and system analysis, data mapping, and data profiling skills
Good understanding of cloud-native application models and patterns
Able to work alternative coverage schedules when necessary
Ability to find a solution with limited guidance
Bachelor's degree in computer science or related field, or equivalent professional experience
Desired Knowledge, Skills, and Experience:
Experience with container orchestration technologies such as Docker, Kubernetes, Openshift
AWS professional level certifications is preferred but not required
The above statements are intended to describe the general nature and level of the work being performed. They are not intended to be construed as an exhaustive list of all responsibilities, duties, and skills required of personnel so classified.

We are proud to be an Equal Opportunity / Affirmative Action Employer and committed to leveraging the diverse backgrounds, perspectives, and experience of our workforce to create opportunities for our colleagues and our business. We do not discriminate in employment decisions on the basis of any protected category.

A conviction is not an absolute bar to employment. Factors such as the age of the offense, evidence of rehabilitation, seriousness of violation, and job relatedness are considered in all employment decisions. Additionally, its the banks policy to only inquire into a candidates criminal history after an offer has been made. Federal law prohibits banks from employing individuals who have been convicted of, or received a pretrial diversion for, certain offenses."
Data Engineer,"Job Description


Here at Discount Tire, we celebrate the spirit of our people with extraordinary pride and enthusiasm. As America’s largest independent tire retailer, specializing in tires & wheels, we have over 1,000 store locations and continue to grow every year. Our consistent growth over the last 60 years, the loyalty of our customers and passion of our people makes Discount Tire a great place to work.

We recognize that to remain the industry leader we must continue to grow and evolve our business in a rapidly changing industry. We are achieving this, not only by opening new stores, but by transforming our technological landscape and making data a central component of our strategy. The Business Analytics team, one of the fastest growing teams in the company, is leading this change. We are responsible for driving the insights, recommendations, and developing the decision support tools that influence the strategic direction of the company.

Responsible for the design, build and implementation of cloud-based analytics platform which includes an MPP Enterprise Data Warehouse and other Big Data technologies.

Essential Duties and Responsibilities:
Designs and develops high performance distributed data warehouse, distributed analytic systems and cloud architecture
Develops, launches and maintains efficient and fault tolerant, batch and streaming, data pipelines (ETL/ELT) to populate databases and object stores from multiple disparate data sources
Performs complex data calculations through data integration tools and scripting languages
Designs and implements data quality metrics, standards, guidelines; automates data quality checks / routines as part of data processing frameworks; validates flow of information
Determines Data Warehousing and Big Data infrastructure needs, including but not limited to, automation of system builds, security requirements, performance requirements and logging/monitoring in collaboration with DevOps engineers
Troubleshoots complex data and performance related issues; implements adjustments, documents root cause and corrective measure; transfers knowledge to operations support team
Documents technical specifications and participates with peers in design and code review sessions
Develops complex cross application architectures in collaboration with cross functional teams
Stays current on the latest industry technologies, trends and strategies
Assists employees, vendors or other customers by answering questions related to Data Warehousing and Big Data processes, procedures and services
Completes work in a timely and accurate manner while providing exceptional customer service
Other duties as assigned

Job Requirements


Qualifications:
This position requires a minimum of eight years of progressive database development and integration experience.
Proven understanding of logical and physical data modeling is imperative.
Ability to translate a logical data model into a relational or non-relational solution is necessary.
Understanding of multiple relational (RDMS) and non-relational (NoSQL) data platforms is needed.
Expert level SQL experience is required.
Scripting knowledge with SQL, Python, Java or R is necessary. Proven experience in SQL tuning, indexing, partitioning, data access patterns and scaling strategies is needed.
Proven experience with data integrations and data processing for business intelligence and analytics workloads is required.
Experience with AWS S3 or other distributed object stores, AWS Redshift, Elastic MapReduce a plus.
Hands-on experience in database development using views, SQL scripts and transformations is needed.
Proficient with Microsoft office, including skills with Word and Excel, is necessary.
Experience working with large complex data sets.
Understanding of Software Development Life Cycle (SDLC) methodologies such as Agile and Waterfall is needed. Proven analytical problem solving and decision-making skills is critical.
Demonstrated ability to communicate across all levels of the organization is necessary; must be able to clearly articulate technical ideas to a non-technical audience both verbally and in writing.
Ability to work independently and in a team is vital.
Customer service skills including the ability to manage and respond to different customer situations while maintaining a positive and friendly attitude is essential.
The ability to multi-task and manage multiple projects to meet various deadlines simultaneously is required.
The ability to work efficiently and accurately under pressure, meet deadlines and present a professional demeanor is essential.
In addition, troubleshooting and organizational skills with a can-do attitude and the ability to adjust to changing requirements are essential.
Educational Requirements:

This position requires a Bachelors Degree in Computer Science, Computer Information Systems or related or equivalent experience. Data or cloud related certifications are a plus.

Work Days:

Normal work days are Monday through Friday. Occasional Saturdays and Sundays may be necessary.

Work Hours:

Normal work hours are 8:00 a.m. to 5:00 p.m. Additional hours may be necessary.

Apply
Not ready to"
Data Engineer,"Our mission is to protect life.
We’re out to make the world a safer place by solving big problems and taking on the public safety challenges of our time. From our company's inception building the TASER device to a full suite of hardware and software solutions, we are focused on providing police agencies with the state-of-the-art devices and services they need to successfully serve and protect us. In the next few years, we're going to eliminate the burden of paperwork in policing, so officers can increase the time they spend building relationships and serving in their communities. We’ll put video at the heart of the police record so our justice system can get to the truth faster. And we won't stop innovating until the bullet is rendered obsolete.

It’s a big mission, but it’s one we’ll pursue relentlessly every single day.

Your Impact
Axon’s Data and Integrations team is looking for a senior data engineer to build scalable, durable, and extensible data models and pipelines—optimizing ingestion and management of data from multiple sources to help public safety agencies protect life and truth.

As one of the founding engineers on the team, you will make key design decisions that will shape a series of data products and services. You’ll create and maintain data ingestion, management, and business intelligence tools alongside Axon’s newest public safety technology products.

You will join our team to work on complex data engineering problems. We’ll look to you to help us design, create, and maintain low-latency, near-real time data systems that are mission critical for public safety. You will be responsible for creating scalable ETL and streaming processes, efficient data pipelines, and a data warehouse that delivers value to technical users in public safety agencies.

Your Day-to-Day
Design, develop, and maintain data pipelines, warehouses, and reporting systems to support Axon’s public safety products.
Build the data products that technical users will depend on for business intelligence and ad-hoc access.
Partner with internal teams and agencies to make public safety data accessible and actionable.
Influence peers, advise senior leaders, coach and mentor junior team members.
Facilitate cross-team collaboration among engineers and contribute to the broader community of senior engineers.
Basic Qualifications
Bachelor’s degree in CS, engineering, or other quantitative field
5+ years of industry experience in data warehousing and modeling on highly available SQL and non-relational (NoSQL and distributed database management systems)
Fluent in writing and optimizing SQL with demonstrated strength in writing complex, high-optimized queries across large data sets
Proficiency in at least one scripting language, Python, R, or similar
Demonstrated strength in design, development, and optimization of low latency pipelines for both stream and batch data in Apache Spark or similar
Ability to make tough technical decisions based on requirements, constraints, and trade-offs
Preferred Qualifications
Experience with big data technologies like Hadoop, Spark, Cascading, Hive, PrestoDB, Zookeeper, etc.
Knowledge of distributed systems and resource optimization for data storage and processing
Backend engineering experience (Java, Scala, C++, or similar)
Experience with BI tools like Tableau, PowerBI, etc.
Compensation and Benefits
Competitive salary and 401K with employer match
Discretionary paid time off
An encouraging parental leave policy
An award-winning office/working environment
Ride along with real police officers in real life situations, see them use technology, get inspired
And more...
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
Data Engineer,"Data Engineer
9920
Scottsdale,
3/27/2019 1:46:38 PM

Application Development
Contractor - W2

Job Description
Data Engineer / Lead

4-5 years of experience in ETL, SQL, Python, Data Management and Spark and strong fundamentals in distributed environments

Real project implementations with Big Data technologies based on Spark

Experience working on Serverless technologies

Experience implementing NoSQL technologies – Mongo or Cassandra

Experience with AWS cloud services: Lambda, S3, Glue, Redshift, and Athena, or their open source equivalent (Zeppelin, Presto, etc)

Data storage formats – Parquet, JSON, AVRO etc.

Experience with real-time data sources and message ingestion for processing by filtering, aggregating, and preparing the data for analysis using technologies such as Spark Streaming and Kafka, AWS Kinesis, Firehose etc.

Experience with data pipelining

Understanding of best practices within the development process

Build processes supporting data transformation, data structures, metadata, dependency and workload management

CI/CD and DevOps tools such as BitBucket/Git, Bamboo, and Maven

AWS technologies – Cloudwatch, CloudFormation, Security (IAM)

AWS certification

Job Requirements"
Data Engineer,"Location: Phoenix (AZ)
Location: Pittsburgh (PA)

Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we've been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers.

The Data Science team lies at the foundation of Zoom's success - you'll be working cross-functionally with teams of engineers, scientists, marketers, and product professionals on some of the most critical projects in the company - whether it's exploratory research to predict user behavior, or running experiments to optimize untapped areas of growth, or developing machine learning models that deliver ""happiness"" to our users more consistently and at scale. If you are passionate about data engineering and looking to join a fun and fast-moving team, we'd love to meet you! Our team is taking Zoom's data culture to the next level by integrating predictive models into our infrastructure, and we are looking for someone like you to help us get there! This role is based in Pittsburgh or Phoenix.

Responsibilities
Own and optimize Zoom's data architecture to address the data needs of our rapidly-growing business
Join a group of passionate people committed to delivering ""happiness"" to our users and to each other
Partner with data scientists, sales, marketing, operation, and product teams to build and deploy machine learning models that unlock growth
Build custom integrations between cloud-based systems using APIs
Write complex and efficient queries to transform raw data sources into easily accessible models by coding across several languages such as Java, Python, and SQL
Architect, build, and launch new data models that provide intuitive analytics to the team
Build data expertise and own data quality for the pipelines you create
Requirements
Three or more years of relevant software engineering experience (Python, Scala and Java) in a data-focused role
Passion for creating data infrastructure technologies from scratch using the right tools for the job
A knack for writing, clean, readable, maintainable code
Comfort with open source technologies like Kafka, Hadoop, Hive, Presto, and Spark
Expertise in building out data pipelines, efficient ETL design, implementation, and maintenance
Experience with AWS tools
Proven track-record of solving complex data processing and storage challenges through scalable, fault-tolerant architecture"
Data Engineer,"Job Summary:

As part of Daman’s Data Engineering team, you will be architecting and delivering highly scalable, high performance data integration and transformation platforms. The solutions you will work on will include cloud, hybrid and legacy environments that will require a broad and deep stack of data engineering skills. You will be using core cloud data warehouse tools, Python, spark, events streaming platforms and other data management related technologies. You will also engage in requirements and solution concept development, requiring strong analytic and communication skills.

Responsibilities:

· Function as the solution lead for building the data pipelines to support the development / enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into data warehouse, data marts and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.

· Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging and integration. Develop overall design and determine division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in development of task plans including schedule and effort estimation
Skills and Qualifications:

· Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required

· Experience building high-performance, and scalable distributed systems

· Good experience in migration from Netezza/ DB2 to Snowflake.

· AWS cloud experience (EC2, S3, Lambda, EMR, RDS, Redshift)

· Experience in ETL and ELT workflow management

· Familiarity with AWS Data and Analytics technologies such as Glue, Athena, Spectrum, Data Pipeline

· Experience building internal cloud to cloud integrations is ideal
Experience with streaming related technologies ex Spark streaming or other message brokers like Kafka is a plus
· 3+ years of Data Management Experience

· 3+ years of batch ETL tool experience (DataStage / Informatica / Talend)

· 3+ years’ experience developing, deploying and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)

· 2+ years’ experience with Hadoop Ecosystem (HDFS/S3, Hive, Spark)

· 2+ years’ experience in a software engineering, leveraging Java, Python, Scala, etc.

· 2+ years’ advanced distributed schema and SQL development skills including partitioning for performance of ingestion and consumption patterns

· 2+ years’ experience with distributed NoSQL databases (Apache Cassandra, Graph databases, Document Store databases)
Experience in the financial services, banking and/ or Insurance industries is a nice to have
Daman Is an Equal Opportunity Employer and All Qualified Applicants Will Receive Consideration for Employment Without Regard to Race, Color, Religion, Sex, National Origin, Disability Status, Protected Veteran Status, Or Any Other Characteristic Protected by Law.

Job Type: Full-time

Pay: $100,000.00 - $130,000.00 per year

Benefits:
401(k)
Dental Insurance
Health Insurance
Paid Time Off
Vision Insurance
Additional Compensation:
Bonuses
Schedule:
Monday to Friday
Work Remotely:
Temporarily due to COVID-19"
Data Engineer,"About Freestar:

Freestar engineers cutting-edge monetization solutions for websites. By combining industry-leading technology, data, and massive scale, we enable busy site owners to seamlessly maximize revenue while freeing themselves of the hassles of ad operations. Publishers then have more time to do what they do best: create content.

Data Engineer Job Responsibilities:

Joining our data team, you will have an opportunity to learn and work with modern tools like Airflow and Druid to ensure a seamless stream of data where we need it. As we are a startup environment, you'll likely pick up some software engineering skills too, however the primary focus on the role is on data engineering. We're looking for someone who:
Develops and maintains scalable data pipelines and builds out new API integrations to support continuing increases in data volume and complexity.
Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.
Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.
Writes unit/integration tests, contributes to engineering wiki, and documents work.
Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues.
Works closely with a team of frontend and backend engineers, product managers, and analysts.
Defines company data assets (data models), SQL, Airflow to populate data models.
Designs data integrations and data quality framework.
Build dashboards that concisely and succinctly convey business metrics.
Designs and evaluates open source and vendor tools for data lineage.
Works closely with all business units and engineering teams to develop strategy for long term data platform architecture.
Data Engineer Qualifications / Skills:
Knowledge of best practices and IT operations in an always-up, always-available service
Experience with or knowledge of Agile Software Development methodologies
Excellent problem solving and troubleshooting skills
Process oriented with great documentation skills
Excellent oral and written communication skills with a keen sense of customer service
Education, Experience
Ample relevant knowledge and experience. You either have a BS or MS degree in Computer Science or a related technical field, OR certification from a data science bootcamp + 2 years of experience in a role as a data engineer
Proficiency in Python and Java, Scala, or Go development experience
4+ years of SQL experience (Strong SQL required)
Familiarity with BI reporting tools like Tableau, Looker.
4+ years of experience with schema design and dimensional data modeling
Ability in managing and communicating data warehouse plans to internal clients
Experience designing, building, and maintaining data processing systems
Experience working with either a Map Reduce or an MPP system on any size/scale
Experience/knowledge of cloud computing platforms like AWS/GCP would be a plus
We'd also like to see:
Excellent interpersonal and problem solving skills with the ability to communicate with team members to deliver actionable results
Comfortable interacting across multiple teams and management levels within the organization
Previous background in the ad tech or media landscape (linear, digital, or social) is a plus
What you can expect in return:
Full-Time, Salaried Position
Medical, Dental, and Vision benefits
401K with company match, vested immediately
The opportunity to be part of something BIG
Freestar is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.

This role is not eligible for visa sponsorship"
Data Engineer,"Role Data Engineer Location Phoenix AZ Duration 6 month contract to start, go possibility to extend longer (3 yr project) Job Description Skill set that the manager is looking for Mid ndash senior developer, at least 4-5 years exp in this space is ideal MUST HAVE STRONG Python and java Aws datalakes exp Data engineering background, Hadoop is a plus Project datalakes building in aws and building data processing technologies within aws No architecture, this is 100 engineering hands on role Must start immediately"
Data Engineer,"Title Data Engineer Location Chandler, AZ, USA Length Long term Restriction W2 or C2C Description Interview Type WebEx Video interview Long term project Initially start to Remote till covid19 settles The ideal candidate must have a strong J2ee background (UI DEVELOPMENT USING J2EE) and currently performing a Hadoop Data Engineer role. Job Description bull Data model development and Model scoring bull Work with Data Scientists and build scripts to meet their data needs Required Qualifications bull 10+ years of overall experience bull 3+ yearsrsquo experience with Big Data ( HADOOP platforms) ndashHive, Spark ( needs to be currently hands-on on Hadoop cluster) bull 4+ years of overall experience in UI development using J2ee"
Data Engineer,"Senior Data Engineer

Locations: Phoenix AZ

Long term

Job Description:

Data model development and Model scoring

Work with Data Scientists and build scripts to meet their data needs

Required Qualifications

· 7+ years of overall experience

· 3+ years’ experience with Big Data ( HADOOP platforms) –Hive, Spark ( needs to be currently hands-on on Hadoop cluster)

· 5+ years of ETL (Extract, Transform, Load) - Scoop, INFA RDBMS Teradata, Oracle

· Experience in Python

· Reproduce issues faced by Data Scientists

· Knowledge of Agile is a must

Job Type: Contract

Salary: $65.00 - $70.00 per hour

Experience:
HADOOP: 5 years (Required)
Big Data: 5 years (Required)
ETL: 8 years (Required)
Education:
Bachelor's (Preferred)"
Data Engineer,"Job Description
Full Time Data Engineer Position

***Local Phoenix Candidates Only***

***No Sponsorship Available - Must be legally authorized to work in US without sponsorship***
Create and maintain ETL pipelines
Build complex data sets to meet business requirements
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and NoSQL technologies

Company Description
Headfarmer is a boutique recruiting firm specializing in the permanent and contract placement of the upper echelon of talent in the greater Phoenix area. We offer a unique process of ""headfarming"" which provides a level of professional support to both candidates and clients that exceeds recruiting industry standards."
Data Engineer,"Do you want to work with one of the fastest-growing data services company? Do you consider yourself a superstar in Data Engineer? Then you have reached the right job opportunity posting.
This Data Engineer role is a critical project position with one of our marquee eCommerce client company at Tempe, AZ.
Role: Data Engineer
Duration: 9 Months with possibility of extension
Location: Remote (During COVID-19 Crisis). Post COVID-19 crisis, work would need to be onsite at Fort Mill, SC or Tempe, AZ
Experience Requirements:
Proven expertise in MemSql, Mysql, and Oracle/PostgreSQL. Prior experience with MemSQL is highly preferred.
Advanced experience in working with relational databases and ANSI-SQL
Demonstrated flexibility in working with large, complex, and ambiguous datasets
Proficient in one or more programming languages such as Python or Java
Good familiarity with AWS ecosystem including AWS EMR (preferred)
Familiar with one or more machine learning or statistical modeling tools such as R, scikit learn, and Spark MLlib (preferred)
Enhance our machine learning software with the latest in machine learning algorithms
Work successfully in a highly cross-functional environment
Strong analytical and quantitative problem-solving ability.
Experience on Data visualization tools such as Tableau and Power BI
Excellent communication, relationship skills, and a strong teammate.
Job Types: Full-time, Contract
Pay: $60.00 - $65.00 per hour
Schedule:
Monday to Friday
COVID-19 considerations:
Our company supports working from home/remotely.
Experience:
Memsql: 3 years (Preferred)
SQL: 5 years (Required)
Python: 3 years (Required)
Contract Length:
7 - 11 months
Contract Renewal:
Possible
Work Location:
One location
Fully Remote
This Job Is Ideal for Someone Who Is:
Adaptable/flexible -- enjoys doing work that requires frequent shifts in direction
Detail-oriented -- would rather focus on the details of work than the bigger picture
Autonomous/Independent -- enjoys working with little direction
Innovative -- prefers working in unconventional ways or on tasks that require creativity
This Company Describes Its Culture as:
Innovative -- innovative and risk-taking
Outcome-oriented -- results-focused with strong performance culture
Team-oriented -- cooperative and collaborative
Benefit Conditions:
Only full-time employees eligible
Work Remotely:
Temporarily due to COVID-19"
Data Engineer,"RESPONSIBILITIES Kforce has a client in search of a Data Engineer in Phoenix, AZ. Summary We are looking for a Senior Software Engineer with experience as a Full stack developer skilled primarily with middleware and back end data engineering technologies with a minimum of 7+ years of experience in designing and developing enterprise solutions with cloud. Key Tasks Design, build and optimize data applications and data pipelines to extract, transform and load data from various data sources to internal and cloud targets Participate in proactively identifying issues and resolving issues concerning data pipeline jobs Identify and implement process improvements automate data processing, build frameworks, design and implement near real-time data ingestion capabilities Evaluate tools and emerging technologies and carry out POCs to identify tools that would optimize data processing and data pipelines Design and implement data sharing tools and solutions including batch sharing and API capabilities Design and implement data analytic tools and solutions that provides actionable insights to stakeholders Work with product, application and business teams to assist with ad hoc data requests, data exploration requests and building business knowledge models Work with data science teams to assist with assist with data and data processing requests Contribute by the way of your past experiences and industry knowledge to the Data Engineering function that is driving significant changes to the tool stack to drive critical initiatives for the organization REQUIREMENTS A minimum of 5 years of software development experience, covering the stages of the SDLC A minimum of 3 years of experience working with data technologies and relational (oracle, MySQL, SQL server, PostgreSQL) as well as NoSQL databases 3 years of experience developing solutions on JavaJ2EE platform 3 years of experience developing solutions with Amazon AWS data tools and technologies 2+ years of experience with Python or JavaScript (server side - node.js or other js frameworks) or R scripting 2+ years of experience with Python or JavaScript (server side - node.js or other js frameworks) or R scripting 2+ years of experience working with Hadoop and spark eco system Advanced working knowledge of current and emerging data technologies Vast knowledge of open source tools technologies Experience working with agile software development methodologies Experience with spring framework using spring boot specifically is preferable Experience working on cloud migration projects is preferable Experience in developing and consuming REST based web services and working in and integrating with microservices based environment is preferable Experience working with event driven architectures and distributed messaging broker such as Kafka Experience working with one or more data streaming technologies is preferable Experience working with Mongo DB or Elasticsearch andor Redis technologies is preferable Experience working with Informatica, HVR is preferable Kforce is an Equal OpportunityAffirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status."
Data Engineer,"Overview


The Data Engineer will work on all phases of the development lifecycle. This will be considered a leader position, with responsibilities that include design, development, implementation, documentation, and optimization of an assortment of solutions. Candidates for this position will have prior experience delivering efficient, secure, high performance, and easy-to-support solutions primarily within the Microsoft suite of technologies. They will also have prior experience in providing technical leadership and direction to junior team members.

Essential Functions
Design, document, develop, test, debug, deploy, and support a variety of custom SSIS packages, SQL databases, indexing, and performance tuning.
Provides technical leadership, creates standards for technical approaches and/or software development practices for highly performant ETL processes and database schema.
Maintains awareness of development trends, new tools, efficiencies - - shares with entire team on routine basis.
Proactive solution detection, analysis, and guidance
Gather, analyze, and document user reporting and/or data feed requirements
Code reviews
Provide escalated support for junior team members
Perform any other assigned tasks deemed necessary by management
SM123

Knowledge, Skills, Abilities and Physical Requirements
Bachelor’s Degree in Computer Science or related field preferred
6+ years of experience with Microsoft technologies, application design, development, and administration.
3+ years of experience with cloud-based solutions and/or hybrid connectivity
Strong understanding of secure application development with the ability to perform application security reviews
Prior experience with data visualization tools, dashboard and web based reporting tools
Strong requirements gathering and documentation skills
Strong analytical and problem solving skills
Retail environment experience is preferred but not required
Technical Skills:
Databases: MS SQL Server
Languages/Libraries: T-SQL, Powershell
Operating Systems: Windows 2012 R2, Windows 10, Mac OS X
Tools: Visual Studio, VSCode, GitHub, SQL Server Management Studio, SQL Server Reporting Services, Business Intelligence Design Studio, Team Foundation Server
Other: SharePoint, Microsoft Office, Visio
Personality Traits:
Maintain high levels of integrity and dependability
Maintain a focus on results, quality and customer satisfaction
Works well in a team environment and effectively manage work activities
Project a professional demeanor and appearance
Be extremely flexible and adaptable
Demonstrates the ability to function and stay focused in a constant pressure, fast growing and ever-changing organization
Communication Skills:
Ability to competently understand, speak, read and write English.
Ability to effectively present information and respond to questions from groups of managers, project steering committees and customers.
Requires excellent interpersonal communications skills.
SM123
Benefits


In addition to a rewarding career, Sprouts offers a comprehensive program to help support you and your family. These programs include:
Competitive pay
Opportunities for career growth
15% discount for you and one other family member in your household on all purchases made at Sprouts
Flexible schedules
Employee Assistance Program (EAP)
Eligibility requirements may apply for the following benefits:
401(K) Retirement savings plan with a generous company match
Affordable benefit coverage, including medical, dental vision
Pre-tax Flexible Spending Accounts for healthcare and dependent care
Company paid life insurance and short-term disability coverage
Why Sprouts


Grow with us!

If you have a passion for inspiring people and a flair for fresh food, consider applying for a job at Sprouts! With a focus on customer service, our neighborhood grocery stores offer high-quality, farm fresh produce, natural meats, plenty of scoop-your-own bulk goods and much more in a fun, friendly, old-fashioned farmer’s market setting. Come grow your career in healthy living with a fast-paced, rapidly growing company and teams that pride themselves on empowering others along their journey.

The above statements are intended to describe the general nature and level of the work being performed by people assigned to this work. This is not an exhaustive list of all duties, responsibilities, and requirements. Sprouts’ management reserves the right to amend and change duties, responsibilities, and requirements to meet business and organizational needs as necessary.

California Residents: We collect information in accordance with California law, please see here for more information."
Data Engineer,"Big Data Engineer
9321
Phoenix, AZ
10/9/2018

Application Development
Contractor - W2

Job Description
Job Description:
You won’t just shape the world of software.

You’ll shape the world of life, work and play.

Our Software Engineers not only understand how technology works, but how that technology intersects with the people who count on it every day. Today, innovative ideas, insight and new perspectives are at the core of how we create a more powerful, personal and fulfilling experience for all our customers.

This position requires a mix of strategic engineering and design along with hands-on, technical work. We seek a self-starter, visionary person with strong leadership capabilities. Exceptional communication skills, for collaborating across many teams. You will lead teams and deliver best-in-class products in an exciting fast-paced environment with the focus on reliability and automation. Dynamic, smart people and inspiring, innovative technologies are the norm here.

The successful candidate will be highly self-motivated with a passion for quality and automation coupled with an ability to understand complex systems and a desire to constantly make things better. Will you join us in crafting solutions that do not yet exist?

You won’t just keep up, you’ll break new ground.


There are hundreds of opportunities to make your mark on technology and life at American Express. Here’s just some of what you’ll be doing:
Build & run the platform that delivers services to all of our customers around the world
You will be on a team of Operational Engineering warriors whose mission it is to build and improve Operational/Support Capabilities for our most critical services
Building software and systems to manage/support applications through automation, support and monitoring
Explore and evaluate new technologies and solutions to push our capabilities forward, transform, innovate and continually improving the platform
You will troubleshoot the platform to achieve optimal support performance, stability and availability.
You will support operations and work closely with the development engineers to assist with architectural design, and implementation of complex features
Continuously Improve the platform through automation & building new operational capabilities.
Are you up for the challenge?

Overall 5+yrs experience in a large scale *nix environment
5+ yrs experience handling BigData Environment
Software development experience with on or more of: Python, Java or Scala
Experience with relational databases such as PostgreSQL, MySQL
Experience with core Hadoop: HDFS, MapReduce, Yarn
Experience with Streaming Data Platform (Kafka, Storm)
Experience with distributed/NoSQL databases: HBase, MySQL
Solid understanding of *nix systems and networking fundamentals
Experience as a software developer with MapReduce or Spark
Experience with extended Hadoop ecosystem: Hive, Pig
Solid Scripting Skills in languages like Python, Shell
Oncall Experience
Excellent communication skills, written and spoken; troubleshooting skills
Evidence of self-learning and commitment to personal development
Bachelor's degree or higher in Computer Science, or equivalent experience\
Continues integration, testing and deployment using Git, Jenkins
At the core of Software Engineering

Every member of our team must be able to demonstrate the following technical, functional, leadership and business core competencies, including:
Agile Practices
Porting/Software Configuration
Programming Languages and Frameworks
Business Analysis
Analytical Thinking
Business Product Knowledge

Job Requirements"
Data Engineer,"Candidate Description Blue Rose Technologies (BRT) is actively seeking an experienced Data Engineer, with AWS Big Data experience. Our ideal candidate needs to have demonstrated expertise in building data pipelines and data systems at scale, using Apache Open Source stack and Hadoop ecosystem. Candidates should have strong familiarity working in an AWS cloud environment, as well as, working with other data engineers, product managers, and product delivery teams when required. Qualifications Experience Candidates with 6+ yearsrsquo experience in data engineering, who have either obtained a Graduate degree in the field of Computer Science or related field, or Bachelor's degree with 8+ years of relevant experience in the above fields. Key Responsibilities Provide technical solution leadership in data engineering team, driving technology decisions, mentoring others, and contributing significantly on an individual level Build frameworks to handle data at high scale using Apache Spark and data cataloging tools like Apache Hive, AWS Glue on top of a multi-tiered data lake storage Use exploration and analytic tools like AWS AthenaPresto to probe and validate data Build robust data processing pipelines using AWS Services and integrate with multiple data sources Collaborate with product owners and stakeholders to plan and define requirements Desired Skills AWS Services RDS, AWS Lambda, AWS Glue, Apache Spark, Kafka, Hive, etc. SQL and NoSQL databases like MySQL, Postgres, and Elasticsearch AWS EMR Familiarity with Spark programming paradigms (batch and stream-processing) Strong programming skills in at least one of the following languages Java, Scala. Familiarity with a scripting language like Python, as well as, UnixLinux shells AWS Athena Strong analytical skills and advanced SQL knowledge, indexing, query optimization techniques. Good to have ETL skills Ability to translate data needs into detailed functional and technical designs for development, testing, and implementation Ability to serve as a liaison between technical, quality assurance, and non-technical stakeholders throughout the development and deployment process Project Duration 12 months+ initiative, and All candidates will be required to work on-site at Tempe, AZ location. SalaryBenefits Details This is an hourly W2 or 1099 contracting position with Blue Rose Technologies, with optional United Health Insurance packages."
Data Engineer,"Job Description
Data Engineer– Emphasis on Elasticsearch / Kafka / JSON / XML

The Opportunity:

We are looking for a Data Engineer based in Phoenix Az with 2-3 years software engineering experience.

You'll be developing and deploying tools for the processing and import/export of data into and out of large scale Elasticsearch and Kafka environments.

The Day to Day:
Work to customer requirements for the import and export of data into various formats
Develop tools to automate this processing on a regular basis
Build back-end frameworks that are maintainable, flexible and scaleable
Work with the core Teraslice development and DevOps team to enhance our data processing platform
Requirements:
2-3 years of programming experience in Javascript (Node.js), Python, Ruby or Go
Experience working with any of Elasticsearch, Kafka, Hadoop (HDFS, Hive, Spark), MongoDB, MySQL or PostgreSQL
Strong preference for Elasticsearch experience
Experience working with data in JSON, XML and CSV data formats
Comfort doing development work on the Linux platform
Exposure to compute clusters and working with many terabytes of data
US Citizenship / Work Authorization
Bonus Points:
Operational experience with Hadoop, MongoDB, Redis, Cassandra, or other distributed big data systems
Mac OS X familiarity
BS or MS in a technology or scientific field of study
High energy level and pleasant, positive attitude!
Evidence of working well within a diverse team
Compensation:
Salary commensurate with experience, generally higher than competitive industries
Comprehensive benefits package
Opportunities for advancement and a clear career path
Relocation assistance provided (if required)
About Us:

Terascope provides software and technical services to assist companies deploying Elasticsearch at scale. We assist customers with design, development and operations and through our Open Source efforts are developing the Teraslice distributed processing platform for working with data stored in Elasticsearch and Kafka.

Powered by JazzHR

PzsSBHiFSF"
Data Engineer,"Data Engineer ||Phoenix,AZ or San Antonio,TX, or Plano, TX

Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company."
Data Engineer,"A leading and a big data analytics software products and services organization in Deer Valley, who specializing in helping fortune 100 companies in their digital transformation journey is seeking to add to a Data Engineer their talented team of developers. This is a great opportunity for whoever wants to work on the cutting- edge technologies in the global IT industry today. This is a 12-month contracting position ending in July 2021, paying up to $55.00 per hour with benefits.In this position, you will join a team that will be enhancing and modernizing legacy technology. This team develops Big Data and batch/real-time analytical solutions, creating insights about different Members across the full spectrum of digital channels, including search, mobile, email, social, and web. If you have the desire to tell a story with data, and the talent to seamlessly integrate customer information across physical, digital, mobile, and social media, this is the team for you!Required Skills & Experience* 4+ years of IT experience* Very good experience in Hadoop, Hive, Spark Batch and streaming.* Good to have experience with 1 NoSQL - HBase/ Cassandra.* Experience with Java/J2EE & Web Services, Scala/ Python is good to have* Writing utilities/program to enhance product capability to fulfill specific customer requirementDesired Skills & Experience* Experience with Jupyter is a plus* Experience with Apache Spark or other streaming data frameworks is a plus* Experience with Talend and Talend Master Data Management* AWS CertificationsWhat You Will Be DoingTech Breakdown* Java, Scala, Python* Hadoop, Hive, Spark Batch and streaming* NoSQL - HBase/ Cassandra* AWSThe Offer* Competitive Pay: Up to $55/hour, DOE* Contract Duration: X - 12 MonthsYou will receive the following benefits:* Medical & Dental Insurance* Health Savings Account (HSA)* 401(k)* Paid Sick Time Leave* Pre-tax Commuter Benefit* A culture of empowerment where you have the opportunity to be mentored by leading tech gurus* Paid holidays* Have fun while you work in a diverse environment* Participate in summits, hackathons and ""techfest"" events; collaborate and learn from peersApplicants must be currently authorized to work in the United States on a full-time basis now and in the future."
Data Engineer,"Primary Purpose and Essential Functions: Prepare and maintain data flows specific to our reporting, analysis and machine learning projects.* Collaborate with Data Science and Business Intelligence teams to identify, design, develop, and implement data applications such as truck arrival intelligence, network balance recommendations, service failure mitigation, and driver hours optimization.* Extensive elastic search development.* Provide troubleshooting, coding, and data pipeline expertise to Data Science and Business Intelligence teams.* Identify and implement outside data sources and new technologies to enhance analysis and reporting impact on business problems.* Collaborate with Data Warehouse/ ETL team to transition data sets into production/core environment and implement new data technologies for analysis and reporting use.* Maintain a positive work atmosphere by acting and communicating in a manner which facilitates the success of business operations in order to meet company demands and expectations and perform other duties as assigned by leadership.* Proactively work to assist others in achieving the organization's objectives.* Skills:Must possess excellent interpersonal skills.Must be able to collaborate with others on team and across the organization.Must be able to present recommendations and/or findings to others including senior leadership.* Education: Bachelors in computer science or related field or equivalent combination of education and experience required.* Experience Required: 3+years related hands on experience required. Previous experience with Java and SQL required. Proven problem solver, creative thinker capabilities required. Experience with Alteryx, Elastic Search, Hive/Impala, Scala, Spark, Python, HTML, Groovy preferred."
Data Engineer,"Title: Data Engineer
Location: Phoenix, AZ
Duration: Contract

Job Description:
The data engineer will be primarily attached to the data management team and provide much needed support to migrate data (and reports) from an existing legacy system to a new enterprise application.
In addition, this resource will be deeply involved in the designing, development and documentation of a new enterprise data warehouse.
Roles & Responsibilities:
Reverse engineering of existing reports to retrieve and document technical specifications (data source, column definitions, etc.).
Data migration and mapping from legacy systems to a new transactional system.
Building of a new data warehouse.
Exposure to cross-functional teams to facilitate solutions to meet business needs.
Assisting in the preparation of strategic analysis and findings for department leadership and key stakeholders.
Developing metrics and creating ad-hoc reporting as needed. Assist internal customers with acceptance and adoption of implemented solutions.
Adhering to best practices for problem analysis, concept evaluation, systems design, database development, modification, testing, and evaluation to ensure quality and consistency.
Conducting training to various audiences for data related issues and technical applications as needed.
Recommending solution options for data and reporting needs, and provide technical advice where needed to support companywide objectives and goals and resolve problems.
Interact with the business to understand change in processes, data implications and potential reporting modifications.
Documenting the types and structure of the business data (logical modeling).
Analyzing and mining business data to identify correlations among the various data points.
Mapping and tracing data from source to target systems in order to solve a given business or system problem.
Designing and creating data reports and dashboards to help the business in their decision making.
Perform statistical analysis of business data.
Ability to perform root cause analysis.
Required Skills:
SQL Development
Microsoft SQL Server, Microsoft Access, Microsoft Excel
SQL Server Integration Services (SSIS)
SQL Server Reporting Services (SSRS), Tableau

Preferred Skills:
OLTP and OLAP modelling and design
Data warehousing concepts
Power BI
C#, PowerShell, DAX, MDX
About our Company:
Connecting people’s aptitude & ambitions with our opportunities to deliver results.
22nd Century Staffing is a Minority & Woman Owned Business Enterprise (MWBE) that supports demanding staffing programs for Corporations and State and Local Government Agencies. Our journey began in 1997 by supporting large Federal contracts which nudged us in the direction of creating large candidate pools across the country. Over the last 20 years, we have built a strong business model that is carefully constructed to deliver on multiple facets. We have proven past performance of providing services that exceed our clients' expectations. Today 22nd Century supports clients in all 50 states and has grown to be a company that is trusted and sought for providing a complex mix of workforce solutions.
Our Global Delivery model with over 110 recruiters, data miners and research analysts working across multiple time zones is backed by an internal database of 800,000 resources across all major industries.
With a firm grip on the entire spectrum of staffing solutions, we have placed more than 500,000 skilled resources and delivered 15 million+ man-hours.
“22nd Century Staffing is an Equal Opportunity Employer"" and “US Citizens & all other parties authorized to work in the US are encouraged to apply."
Data Engineer,"Job Description
GTN is looking for a Data Engineer to expand and optimize our client's data storage and data pipeline architecture, and optimize data flow/collection for their cross functional teams. The successful candidate will design, implement, and maintain data storage and data flow solutions for structured and non-structured multi-model data in support of data science and machine learning pipelines. Additionally, the ideal candidate will be an experienced data pipeline builder and data wrangler, who enjoys optimizing data systems and building them from the ground up. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.

Job Responsibilities:
Create and maintain optimal data pipeline architecture.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Create data tools for analytics and data science team members that assist them in building and optimizing data science products.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Job Requirements:
Masters in Computer Science, Engineering or a related field (Specifically, with exposure to cancer biology studies/data/research/etc. being highly desired)
A successful history of manipulating, processing, and extracting value from large disconnected datasets.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience with relational SQL and NoSQL databases, including MongoDB, Cassandra, etc.
Strong analytic skills related to working with unstructured datasets
Experience with microservices architecture
Experience with data pipeline and workflow management tools: Luigi, Airflow, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with object-oriented/object function scripting languages: Python, Java, Scala, etc.
Proficiency in Python, Pandas, PySpark, Dask, Ray, etc.
Experience writing RESTful APIs
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Proficient verbal and written communication skills to explain complex technical details in clear language
Commitment to the successful achievement of team and organizational goals through a desire to participate with and help other members of the team
Demonstrate a focus on listening to and understanding user needs and then delighting the customer by exceeding service and quality expectations

Company Description
GTN provides Scalable Technical Staffing solutions encompassing SOW, staff augmentation, and direct hire placement for Fortune 2000 companies, with niche service offerings in Cyber Security, Digital, Payroll Management, and Professional Services."
Data Engineer,"Our client in Phoenix, AZ is looking to add dynamic Data Engineer within the organization.Qualifications

• Experience building data pipelines to automate batch and real-time data delivery to the AWS data lake, warehouses, analytical and machine learning applications
• Experience building and utilizing APIs and data delivery services that support critical operational and analytical applications for our internal business operations, customers and partners
• Experience integrating and shipping code into AWS cloud Production environments
• Strong experience with all phases of the software development life cycle (SDLC) using Agile methods
• Strong experience with using Amazon Cloud services
• Experience with programming languages: including Java, Node.js, Python

Job Requirements:"
Data Engineer,"RESPONSIBILITIES:

Kforce has a client in search of a Data Engineer in Phoenix, AZ.

Summary:
We are looking for a Senior Software Engineer with experience as a Full stack developer skilled primarily with middleware and back end data engineering technologies with a minimum of 7+ years of experience in designing and developing enterprise solutions with cloud.

Key Tasks:
Design, build and optimize data applications and data pipelines to extract, transform and load data from various data sources to internal and cloud targets
Participate in proactively identifying issues and resolving issues concerning data pipeline jobs
Identify and implement process improvements: automate data processing, build frameworks, design and implement near real-time data ingestion capabilities
Evaluate tools and emerging technologies and carry out POCs to identify tools that would optimize data processing and data pipelines
Design and implement data sharing tools and solutions including batch sharing and API capabilities
Design and implement data analytic tools and solutions that provides actionable insights to stakeholders
Work with product, application and business teams to assist with ad hoc data requests, data exploration requests and building business knowledge models
Work with data science teams to assist with assist with data and data processing requests
Contribute by the way of your past experiences and industry knowledge to the Data Engineering function that is driving significant changes to the tool stack to drive critical initiatives for the organization
REQUIREMENTS:
A minimum of 5 years of software development experience, covering the stages of the SDLC
A minimum of 3 years of experience working with data technologies and relational (oracle, MySQL, SQL server, PostgreSQL) as well as NoSQL databases
3 years of experience developing solutions on Java/J2EE platform
3 years of experience developing solutions with Amazon AWS data tools and technologies
2+ years of experience with Python or JavaScript (server side - node.js or other js frameworks) or R scripting
2+ years of experience with Python or JavaScript (server side - node.js or other js frameworks) or R scripting
2+ years of experience working with Hadoop and spark eco system
Advanced working knowledge of current and emerging data technologies
Vast knowledge of open source tools & technologies
Experience working with agile software development methodologies
Experience with spring framework using spring boot specifically is preferable
Experience working on cloud migration projects is preferable
Experience in developing and consuming REST based web services and working in and integrating with microservices based environment is preferable
Experience working with event driven architectures and distributed messaging broker such as Kafka
Experience working with one or more data streaming technologies is preferable
Experience working with Mongo DB or Elasticsearch and/or Redis technologies is preferable
Experience working with Informatica, HVR is preferable
Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status."
Data Engineer,"About thredUP
thredUP is the world’s largest fashion resale platform, inspiring a new generation to think secondhand first. The company has spent the past 10 years reinventing resale, building a marketplace and infrastructure now poised to power the $50B resale economy and usher in a more sustainable fashion future. Millions of consumers use thredUP as the easiest way to sell their clothes and shop over 35,000 brands at up to 90% off — online, in stores or via “try-before-you-buy” Goody Boxes. Backed by world-class investors, thredUP designed a resale engine that has redistributed nearly 100 million unique garments from closets across America and is now powering resale for the broader fashion industry via its Resale-As-A-Service (RAAS) platform.

About the Role
At thredUP, we’re working to revolutionize the clothing industry and we're looking for an experienced Data/BI Engineer to help grow the foundation of our data warehouse and Business Intelligence infrastructure., You will work in a highly collaborative environment of data engineers, data analysts, domain experts and business leaders to deliver data solutions that alter how the secondhand clothing industry is perceived. If you are excited by recent developments in cloud data technologies and the ability to solve complex data problems with an engineering team that is passionate about our culture, the quality of our code, and building a successful company, we'd love to hear from you.

Responsibilities
Design, implement and maintain data warehouse/ business intelligence solutions which will handle growing business needs
Translate business questions and concerns into specific quantitative questions that can be answered with available data using sound methodologies. In cases where questions cannot be answered with available data, partner with engineers to produce the required data
Automate, optimize, and maintain ETL flows in a repeatable, scalable manner
Handle and respond to ad-hoc data support/analytical requests
Explore/analyze data and work with Supple Chain Operations, Data Scientists and Product Managers
Requirements
5+ years of experience in data modeling, ETL, data warehousing, and transformation of large scale data sources using SQL, Redshift or other Big Data technologies
Proficiency with analytical SQL and knowledge of how to optimize and troubleshoot
BI Reporting experience with tools such as Looker or Tableau
Bachelor's or Master’s degree in computer science, information systems, or a related technical discipline
Experience with AWS Data technologies (such as Redshift, S3,Glue)
Familiarity with Python or any other scripting language
Excellent communication and problem solving skills
(Bonus) Experience with Spark, Hadoop or other Big Data technologies.
(Bonus) Experience working in an e-commerce business
(Bonus) Experience working with Supply Chain data
And in return, you'll have...
The opportunity to make a massive impact & influence outcomes for our business and customers alongside passionate coworkers
Autonomy. The ability to make, own, and carry out decisions
Competitive salary, equity and full benefits (health/dental/vision insurance & 401k)
Work from anywhere most Tuesdays and Thursdays meeting free (what we call Maker Days)
Flexible PTO

At thredUP, our mission has been built on extending the lives of millions of unique clothing items. Much like our inventory, we believe diversity is key. As a diverse and inclusive workplace, we are committed to ensuring our employees are comfortable bringing their authentic selves to work every day. A unique perspective is critical to solving complex problems and inspiring a new generation to think secondhand first. Everyone is welcome - be you."
Data Engineer,"Position: Data Engineer
Location: currently remote ( candidate needs to move to Chandler, AZ a later point in time)
Duration: Long Term

• The ideal candidate must have a strong J2ee background (UI DEVELOPMENT USING J2EE) and currently performing a Hadoop Data Engineer role.

Job Description:
• Data model development and Model scoring
• Work with Data Scientists and build scripts to meet their data needs
Required Qualifications
• 10+ years of overall experience
• 3+ years' experience with Big Data ( HADOOP platforms) –Hive, Spark ( needs to be currently hands-on on Hadoop cluster)
• 4+ years of overall experience in UI development using J2ee"
Data Engineer,"Job Description
Data Engineer
Scottsdale, AZ, USA
USC, GC only
6+ months

MINIMUM QUALIFICATIONS: •
2+ years of experience in a Data Engineer role, who has attained a bachelors degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
• AWS: 1 year experience
• DevOps Practices: 1 year experience
• 2+ years experience working with data warehousing, ETL development and ETL architecture.
• 2+ years experience combined experience with any of the following database technologies (RDBMS: MSSQL, MySQL Oracle; NoSQL: MarkLogic, Snowflake, DynamoDB, Redis).
• 2 years experience working on large data initiatives (?5 terabytes).
• 1 years experience as a JavaScript"
Data Engineer,"Location: Tempe, AZDescription: Global retail partner of the Phoenix-local JUDGE delivery office is currently seeking a Data Engineer for a contract to hire role in Tempe!
This is a CONTRACT TO HIRE position; The selected individual should be fully prepared to expect conversion to a direct employee of the end client at the conclusion of the initial contract timeline. This is not a position for a career consultant; our client is seeking a background reflecting commitment to long tenure and seeking to resume in kind.
Please follow-up with Sky Donovan / sdonovan@judge.com / following submission of application and resume to coordinate a complete qualification discussion | BE ADVISED: Resumes will not be presented to the end-client without a full conversation with a JUDGE associate.

This job will have the following responsibilities:
• Build, deploy and manage data engineering pipelines.
• Contribute to design and creation of high-quality solutions.
• Work with other data engineers, business intelligence and machine learning experts to solve real-life, challenging business problems.
• Work with languages such as python and SQL.
• Handle batch and real-time data processing utilizing different tools and technologies.
Qualifications & Requirements:
• Degree in computer science or related.
• 3+ years of relevant professional experience.
• Extensive experience in Python and SQL.
• Experience with batch and real-time data processing tools and technologies (Databricks, Spark, Kafka)
• Knowledge of distributed data solutions, storage systems and columnar databases.
• Knowledge of Cloud Computing on Microsoft Azure or any other public cloud offering
• Familiarity with Continuous Integration/Continuous Deployment, Git.
• Knowledge about Agile development methods like Scrum and Kanban.
• Knowledge of key machine learning concepts & ML frameworks (like scikit-learn, H2O.ai, Keras, etc.) is a plus.
Please follow-up with Sky Donovan / sdonovan@judge.com / following submission of application and resume to coordinate a complete qualification discussion | BE ADVISED: Resumes will not be presented to the end-client without a full conversation with a JUDGE associate.

Contact: sdonovan@judge.com
This job and many more are available through The Judge Group. Find us on the web at www.judge.com

Job Requirements:"
Data Engineer,"Net2Source is a Global Workforce Solutions Company headquartered at NJ, USA with its branch offices in Asia Pacific Region.We are one of the fastest growing IT Consulting company across the USA and we are hiring "" Data Engineer for one of our clients.We offer a wide gamut of consulting solutions customized to our 450+ clients ranging from Fortune 500/1000 to Start-ups across various verticals like Technology, Financial Services, Healthcare, Life Sciences, Oil & Gas, Energy, Retail, Telecom, Utilities, Technology, Manufacturing, the Internet, and Engineering.

Company: One of Our Clients
Role: Data Engineer
Location: Phoenix AZ
Contract : 12+ months

Job Description:

Mandatory Skills: AWS Lambda, Hive, Spark and Python

Role & Responsibilities:

About Net2Source, Inc.
Net2Source is an employer-of-choice for over 2200+ consultants across the globe. We recruit top-notch talent for over 40 Fortune and Government clients coast-to-coast across the U.S. We are one of the fastest-growing companies in the U.S. and this may be your opportunity to join us!
Want to read more about Net2Source?, Visit us at

Equal Employment Opportunity Commission
The United States Government does not discriminate in employment on the basis of race, color, religion, sex (including pregnancy and gender identity), national origin, political affiliation, sexual orientation, marital status, disability, genetic information, age, membership in an employee organization, retaliation, parental status, military service, or other non-merit factor.

Net2Source Inc. is one of the fastest growing Global Workforce Solutions company with a growth of 100% YoY for last consecutive 3 years with over 2200+ employees globally and 30 locations in US and operations in 20 countries. With an experience of over a decade we offer unmatched workforce solutions to our clients by developing an in-depth understanding of their business needs. We specialize in Contingent hiring, Direct Hires, Statement of Work, Payroll Management, IC Compliance, VMS, RPO and Managed IT Services.

Fast Facts about Net2Source:
Inception in 2007, privately held, Debt free
2200+ employees globally
375+ In- house Team of Sales, Account Management and Recruitment with coast to coast COE.
30 offices in US and 50+ Offices globally
Operations in 20 countries (US, Canada, Mexico, APAC, UK, UAE, Europe, , Europe, Latin America, Japan, Australia)

Awards and Accolades:
2018 Fastest Growing IT Staffing Firm in North America by Staffing Industry Analysts
2018 Fastest-Growing Private Companies in America as a 5 times consecutive honoree Inc. 5000
2018 Fastest 50 by NJBiz
2018 Techserve Excellence Award (IT and Engineering Staffing)
2018 Best of the Best Platinum Award by Agile1
2018 40 Under 40 Award Winner by Staffing Industry Analysts
2018 CEO World Gold Award by SVUS
2017 Best of the Best Gold Award by Agile1

Stay safe and healthy!

Regards,
Divyansh Srivastava (Dave)
Net2Source Inc.
Client Delivery Manager Enterprise Business
Global HQ Address 7250 Dallas Pkwy, Suite 825 Plano, Texas 75024
Office: (201) 340-8700 x 477|Cell: (201) 479-3334|Fax: (201) 221-8131|Email: divyansh@net2source.com
https://www.linkedin.com/in/divyansh-srivastava-11563041/
Web: www.net2source.com | Social: Facebook | Twitter | LinkedIn"
Data Engineer,"Job Description


Here at Discount Tire, we celebrate the spirit of our people with extraordinary pride and enthusiasm. As America’s largest independent tire retailer, specializing in tires & wheels, we have over 1,000 store locations and continue to grow every year. Our consistent growth over the last 60 years, the loyalty of our customers and passion of our people makes Discount Tire a great place to work.

We recognize that to remain the industry leader we must continue to grow and evolve our business in a rapidly changing industry. We are achieving this, not only by opening new stores, but by transforming our technological landscape and making data a central component of our strategy. The Business Analytics team, one of the fastest growing teams in the company, is leading this change. We are responsible for driving the insights, recommendations, and developing the decision support tools that influence the strategic direction of the company.

Responsible for the design, build and implementation of cloud-based analytics platform which includes an MPP Enterprise Data Warehouse and other Big Data technologies.

Essential Duties and Responsibilities:
Designs and develops high performance distributed data warehouse, distributed analytic systems and cloud architecture
Develops, launches and maintains efficient and fault tolerant, batch and streaming, data pipelines (ETL/ELT) to populate databases and object stores from multiple disparate data sources
Performs complex data calculations through data integration tools and scripting languages
Designs and implements data quality metrics, standards, guidelines; automates data quality checks / routines as part of data processing frameworks; validates flow of information
Determines Data Warehousing and Big Data infrastructure needs, including but not limited to, automation of system builds, security requirements, performance requirements and logging/monitoring in collaboration with DevOps engineers
Troubleshoots complex data and performance related issues; implements adjustments, documents root cause and corrective measure; transfers knowledge to operations support team
Documents technical specifications and participates with peers in design and code review sessions
Develops complex cross application architectures in collaboration with cross functional teams
Stays current on the latest industry technologies, trends and strategies
Assists employees, vendors or other customers by answering questions related to Data Warehousing and Big Data processes, procedures and services
Completes work in a timely and accurate manner while providing exceptional customer service
Other duties as assigned

Job Requirements


Qualifications:
This position requires a minimum of eight years of progressive database development and integration experience.
Proven understanding of logical and physical data modeling is imperative.
Ability to translate a logical data model into a relational or non-relational solution is necessary.
Understanding of multiple relational (RDMS) and non-relational (NoSQL) data platforms is needed.
Expert level SQL experience is required.
Scripting knowledge with SQL, Python, Java or R is necessary. Proven experience in SQL tuning, indexing, partitioning, data access patterns and scaling strategies is needed.
Proven experience with data integrations and data processing for business intelligence and analytics workloads is required.
Experience with AWS S3 or other distributed object stores, AWS Redshift, Elastic MapReduce a plus.
Hands-on experience in database development using views, SQL scripts and transformations is needed.
Proficient with Microsoft office, including skills with Word and Excel, is necessary.
Experience working with large complex data sets.
Understanding of Software Development Life Cycle (SDLC) methodologies such as Agile and Waterfall is needed. Proven analytical problem solving and decision-making skills is critical.
Demonstrated ability to communicate across all levels of the organization is necessary; must be able to clearly articulate technical ideas to a non-technical audience both verbally and in writing.
Ability to work independently and in a team is vital.
Customer service skills including the ability to manage and respond to different customer situations while maintaining a positive and friendly attitude is essential.
The ability to multi-task and manage multiple projects to meet various deadlines simultaneously is required.
The ability to work efficiently and accurately under pressure, meet deadlines and present a professional demeanor is essential.
In addition, troubleshooting and organizational skills with a can-do attitude and the ability to adjust to changing requirements are essential.
Educational Requirements:

This position requires a Bachelors Degree in Computer Science, Computer Information Systems or related or equivalent experience. Data or cloud related certifications are a plus.

Work Days:

Normal work days are Monday through Friday. Occasional Saturdays and Sundays may be necessary.

Work Hours:

Normal work hours are 8:00 a.m. to 5:00 p.m. Additional hours may be necessary.

Apply
Not ready to"
Data Engineer,"Need minimum 10 yrs of exp. Job Requirements Data Engineer with expertise in Python and Big data technologies like Spark, Hive, Presto etc. Experience with AWS services ndash S3, EC2, EMR, Lambda Functions and Step Functions. Experience with both SQL and NoSql DB Preferred Experience with NoSQL database (Hive, HBase, MongoDB, ElasticSearch etc.) Knowledge and expertise with Python and other Big data technologies like Spark, Hive, Presto etc Proficient writing Spark jobs in Python and Scala Developing Hive UDF and Hive jobs Proven hands-on Software Development experience Experience with test-driven development Exposure to CICD processes using Maven and Jenkins, familiarity with GIT. Exposure to Scrum Agile framework Preferred experience in core Java technologies ( Java 1.8, Spring Boot, Spring MVC, Java EE fundamentals, Hibernateany Object relation mappers, OracleMySQL etc ) Good communication and collaborative skills with internal and external teams Flexibility and ability to work in onshoreoffshore model involving multiple agile teams Strong analytical and problem-solving skills Regards Chandrakala Doddi 732-898-6795"
Data Engineer,"Job Description
3+ year of Industry experience. Minimum 2+ years of Big Data/Hadoop experience Must have extensive Hadoop working knowledge and hands on experience in HDFS Experience in developing real time streaming applications using Flume and Kafka Good knowledge spark configurations and performance tuning Develop highly scalable and extensible Big Data platform."
Data Engineer,"If you’re bright, highly motivated and want to make a difference in education, then join our trailblazing edtech team! We’re catapulting secondary education into the digital age with our award-winning online curriculum and learning platform.

Privately owned and operated since 2001, StrongMind is stable, profitable and rapidly growing. Our forward-thinking teammates and leaders foster an innovative, creative and collaborative environment where our top priority is making our clients happy.

StrongMind is seeking a dynamic individual to join our innovative team as a Data Engineer with experience in Big Data. We have a vision to transform the way we deliver actionable data insights to our customers. We’re reimagining the typical report writing delivery model and building a brand-new business intelligence and data platform… one which puts the power and flexibility into the hands of our customers to get exactly the data they need, when they need it. We’re looking for talented people to help us make this dream a reality. Working within a team setting, you will lead the effort to organize and deliver diverse amount of data to our customers in a timely manner with actionable insights into their business. A positive consequence of this will be that our product teams will be able to better serve our end-users and customers on a daily basis.

Here’s what to expect:
You will be our go-to person when it comes to Data, whether it’s data manipulation, data warehousing, or engineering
Ability to pitch the right solution when it comes to Data be it a relational databases or non-relational databases
Build magical product experiences. Make the technical& complex, simple and effortless.
Be empowered to contribute to growing StrongMind's technical excellence, culture, and customer delight.
Participate in all aspects of the software life-cycle, including ideation, development, and production support.
Mentor and be mentored, educate and learn, lead, and be led by other engineers in problem solving and solutioning.
Drive improvements to enable your team to deliver quality outcomes that lead to customer success.
Collaborate deeply with a diverse, cross functional team of Engineers, UX, Product, and Operations.
Leverage engineering best practices such as CI/CD, Pairing, and Test-Driven Development to deliver early and often.
Actively engage with the education community to understand product needs.
Apply Lean Startup/Agile approaches to software development
Work in an open space environment (no cube walls)
The Successful Candidate possesses:
BS in a related field, bootcamp, self-taught or equivalent experience
Experience working with both SQL and NoSQL databases, knowing which paradigm works best for different use cases
Demonstrated experience working with various components of Big Data ecosystem: Hadoop, Spark/Spark Streaming, Hive, Kafka
Ability to use industry standard dimensional modeling techniques to organize data from disparate systems into Data Marts and Warehouses
Experience with both OLTP and OLAP environments
Familiarity with ETL and ELT data transformation best practices
Mastery of Business Intelligence is a must
Experience to a variety of data technologies (Relational, Non-Relational, NoSQL, Data Warehouses, Big Data)
Ability to deliver iterative vertical slices of business value consisting of data visualization, transformation, and persistence.
Data security, e.g., HIPPA or FERPA compliance, experience helpful, but not required
Experience developing in AWS, Azure, or other cloud services
Preference for working within an Agile methodology (i.e. Scrum, Kanban, XP)
Prior knowledge of Snowflake is a bonus
Familiarity with software engineering best practices
Passion for self-driven, continuous learning, both in and out of the office
Leadership qualities and capabilities
Opinionated on technology, in theory, but flexible in practice
Passion for Education is a must; experience in Ed-Tech helpful, but not required
StrongMind’s culture is supportive, lively and fun. We offer great benefits including a competitive salary, health and dental, 401K, an onsite gym, generous PTO, and holidays"
Data Engineer,"Maybe youve stopped by for a coffee, fueled up your car or grabbed something to eat on the go. Then you know what Circle K is all about. Making everyday life easier for people all over the world. Weve grown into a successful global company with over 15,000 stores in 24 countries, serving more than 6 million customers each day. In all, we have more than 120,000 people working at our stores and support offices.To support our future growth ambitions we are building a global data platform and machine learning solutions. We are now looking for an experienced Data Engineer to join our Data Engineering team in the Global Tech Data and Analytics Department. For the right candidate, Circle K can offer a challenging and rewarding role in a talented, multinational and diverse team.

RESPONSIBILITIES

• Prototype, build, deploy and manage data engineering pipelines.
• Contribute to design and creation of high-quality solutions.
• Work with other data engineers, business intelligence and machine learning experts to solve real-life, challenging business problems.

REQUIRED QUALIFICATIONS

• Degree in computer science or related.
• 3+ years of relevant professional experience.
• Extensive experience in Python and SQL
• Experience with batch and real-time data processing tools and technologies (Databricks, Spark, Kafka)
• Knowledge of distributed data solutions, storage systems and columnar databases.
• Knowledge of Cloud Computing on Microsoft Azure or any other public cloud offering
• Familiarity with Continuous Integration/Continuous Deployment, Git.
• Fluent in spoken and written English.
• Knowledge about Agile development methods like Scrum and Kanban.

Technologies we use: Microsoft Azure Databricks (Spark), Azure SQL Datawarehouse, Azure Tabular, Azure Data Factory, Azure Functions, Azure Containers, Docker, DevOps, Python (3.x), PySpark, Scripting (Powershell, Bash), Git, Terraform, Power BI.

We know great companies are built from within, by great people like you. Come grow with us! We´re looking forward to reviewing your application.

Circle K is an Equal Opportunity Employer.
The Company complies with the Americans with Disabilities Act (the ADA) and all state and local disability laws. Applicants with disabilities may be entitled to a reasonable accommodation under the terms of the ADA and certain state or local laws as long as it does not impose an undue hardship on the Company. Please inform the Companys Human Resources Representative if you need assistance completing any forms or to otherwise participate in the application process.

Click below to review information about our company's use of the federal E-Verify program to check work eligibility:

In English

In Spanish

Job Requirements:"
Data Engineer,"Data Engineer Phoenix, AZ Note Required only 10+ years candidates with strong exp in python coding. Job Requirements Data Engineer with expertise in Python and Big data technologies like Spark, Hive, Presto etc. Experience with AWS services ndash S3, EC2, EMR, Lambda Functions and Step Functions. Experience with both SQL and NoSql DB Preferred Experience with NoSQL database (Hive, HBase, MongoDB, ElasticSearch etc.) Knowledge and expertise with Python and other Big data technologies like Spark, Hive, Presto etc Proficient writing Spark jobs in Python and Scala Developing Hive UDF and Hive jobs Proven hands-on Software Development experience Experience with test-driven development Exposure to CICD processes using Maven and Jenkins, familiarity with GIT. Exposure to Scrum Agile framework Preferred experience in core Java technologies ( Java 1.8, Spring Boot, Spring MVC, Java EE fundamentals, Hibernateany Object relation mappers, OracleMySQL etc ) Good communication and collaborative skills with internal and external teams Flexibility and ability to work in onshoreoffshore model involving multiple agile teams Onshore position ndash Person should be able to work from Marsh Phoenix office Strong analytical and problem-solving skills Thanks, Sandeep Pedhi 732-898-6796"
Data Engineer,"Passionate about writing Quality Code/Best Practices

Knowledge of Agile principles

Experience with contemporary tools and frameworks commonly utilized by agile developers such as- Java web components, REST, Web services, Struts, Spring boot and Angular JS

Good understanding of
Microservices architecture
Solid principles
Refactoring and unit test practices"
Data Engineer,"Skills:
8-10+ years of experience
Big Data Sr. Engineer
Java Spark, Elastic Search, Hbase, HIVE , REST API and SQL
Solids hands on
Great communicator
Leadership skills"
Data Engineer,"Data Engineer
9920
Scottsdale,
3/27/2019 1:46:38 PM

Application Development
Contractor - W2

Job Description
Data Engineer / Lead

4-5 years of experience in ETL, SQL, Python, Data Management and Spark and strong fundamentals in distributed environments

Real project implementations with Big Data technologies based on Spark

Experience working on Serverless technologies

Experience implementing NoSQL technologies – Mongo or Cassandra

Experience with AWS cloud services: Lambda, S3, Glue, Redshift, and Athena, or their open source equivalent (Zeppelin, Presto, etc)

Data storage formats – Parquet, JSON, AVRO etc.

Experience with real-time data sources and message ingestion for processing by filtering, aggregating, and preparing the data for analysis using technologies such as Spark Streaming and Kafka, AWS Kinesis, Firehose etc.

Experience with data pipelining

Understanding of best practices within the development process

Build processes supporting data transformation, data structures, metadata, dependency and workload management

CI/CD and DevOps tools such as BitBucket/Git, Bamboo, and Maven

AWS technologies – Cloudwatch, CloudFormation, Security (IAM)

AWS certification

Job Requirements"
Data Engineer,"Our mission is to protect life.
We’re out to make the world a safer place by solving big problems and taking on the public safety challenges of our time. From our company's inception building the TASER device to a full suite of hardware and software solutions, we are focused on providing police agencies with the state-of-the-art devices and services they need to successfully serve and protect us. In the next few years, we're going to eliminate the burden of paperwork in policing, so officers can increase the time they spend building relationships and serving in their communities. We’ll put video at the heart of the police record so our justice system can get to the truth faster. And we won't stop innovating until the bullet is rendered obsolete.

It’s a big mission, but it’s one we’ll pursue relentlessly every single day.

Your Impact
Axon’s Data and Integrations team is looking for a senior data engineer to build scalable, durable, and extensible data models and pipelines—optimizing ingestion and management of data from multiple sources to help public safety agencies protect life and truth.

As one of the founding engineers on the team, you will make key design decisions that will shape a series of data products and services. You’ll create and maintain data ingestion, management, and business intelligence tools alongside Axon’s newest public safety technology products.

You will join our team to work on complex data engineering problems. We’ll look to you to help us design, create, and maintain low-latency, near-real time data systems that are mission critical for public safety. You will be responsible for creating scalable ETL and streaming processes, efficient data pipelines, and a data warehouse that delivers value to technical users in public safety agencies.

Your Day-to-Day
Design, develop, and maintain data pipelines, warehouses, and reporting systems to support Axon’s public safety products.
Build the data products that technical users will depend on for business intelligence and ad-hoc access.
Partner with internal teams and agencies to make public safety data accessible and actionable.
Influence peers, advise senior leaders, coach and mentor junior team members.
Facilitate cross-team collaboration among engineers and contribute to the broader community of senior engineers.
Basic Qualifications
Bachelor’s degree in CS, engineering, or other quantitative field
5+ years of industry experience in data warehousing and modeling on highly available SQL and non-relational (NoSQL and distributed database management systems)
Fluent in writing and optimizing SQL with demonstrated strength in writing complex, high-optimized queries across large data sets
Proficiency in at least one scripting language, Python, R, or similar
Demonstrated strength in design, development, and optimization of low latency pipelines for both stream and batch data in Apache Spark or similar
Ability to make tough technical decisions based on requirements, constraints, and trade-offs
Preferred Qualifications
Experience with big data technologies like Hadoop, Spark, Cascading, Hive, PrestoDB, Zookeeper, etc.
Knowledge of distributed systems and resource optimization for data storage and processing
Backend engineering experience (Java, Scala, C++, or similar)
Experience with BI tools like Tableau, PowerBI, etc.
Compensation and Benefits
Competitive salary and 401K with employer match
Discretionary paid time off
An encouraging parental leave policy
An award-winning office/working environment
Ride along with real police officers in real life situations, see them use technology, get inspired
And more...
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
Data Engineer,"Role: Big Data Engineer

Location: Phoenix, AZ

Job Type: Contract

Job Description

Need Big Data, Spark.

Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company."
Data Engineer,"Return to Job SearchSenior Data EngineerU-Haul Intl. is seeking a Senior Data Engineer in the Phoenix area. Responsible for developing, debugging & supporting applications for big data analytics platforms. Bachelor's degree or foreign equivalent in Computer Science, Data Science, Statistics, Mathematics, Engineering, Bioinformatics, Physics, or related field. + 5yrs wrk exp. w/ Java or Scala, Hadoop, Kafka, Data Lake or Databricks environments, SQL, Apache Spark, Linux, Unix, HDFS or DBFS, Data science, data analytics, or machine learning, scalability analysis & performance monitoring & measuring techniques, Hive & HBase, Zookeeper. U-Haul Intl. is a drug free environment & an EEO employer."
Data Engineer,"Job Title: Big Data Engineer
Location: Phoenix, AZ
Duration: Full Time

Job Description:
The position is for a Hadoop Developer for Financial Crimes Technology Group within a Banking and Financial Services firm.
7-8 years of experience working on Spark, Hadoop, relational databases.
Good understanding of Data warehousing concepts.
Strong in concurrent programming.
Experience with frameworks like Spark, MapReduce, Hive, Pig, HBase.
Job Type: Full-time"
Data Engineer,"Role: Senior Data Engineer

Job Type: Contract

Location: Chandler, AZ / San Francisco, CA / Bridgewater, NJ

Job Description:

Key Responsibilities:

End to end ownership of ETL data pipelines, from ingestion of data to consumption by business intelligence and advanced analytics teams.

Design and build an automated, self-service data platform, freeing teams to focus on customer features and analysis.

Evolve existing tools and framework to support new scalability requirements as well new functionality as needed.

Identify and drive new solutions to enhance the development cycle to increase development productivity.

Work with product owners to identify and mature upcoming business needs and develop technical backlog to answer those needs in a timely manner.

Work with team to identify and resolve technical debt to improve the team's throughput.

Skills:

Strong communication skills.

Deep experience designing and implementing highly scalable, distributed application systems.

5+ years' experience building data pipelines.

5+ years' experience programming in Python

Extensive knowledge in fine tuning SQL, understanding optimizers, and execution plans.

Extensive experience architecting complex data models to handle millions of transactions.

Experience in application design and Implementation using agile practices & TDD.

Experience leveraging open source data infrastructure projects, such as Apache Spark, Kafka, Flink.

Strong understanding of software development life cycle and release management

Past experience integrating with Oracle, Microsoft SQL Server

Self-motivated, independent, team-player

Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company."
Data Engineer,"2 to 3 years' experience designing and developing in Python, Scala, Spark, HDFS.
1 to 2 years' experience with Unix shell scripting
3 to 5 years' experience with SQL
Experience with version control tools and processes.
Good to have understanding of regulatory requirement, Basel-III, CCAR, CECL etc"
Data Engineer,"Role Big Data Engineer Location Phoenix, AZ Atleast 7-8 years of experience. Experience in Spark Good experience in kafka, streaming data. Hands on experience on Cassandra and elastic search. Should have experience in handling huge data in production Worked on at least 1 or 2 production usecase 2+ years of experience working in enterprise using Big Data Tool stack. Hive concepts and writing queries experience is must. Can work independently (without much support due to total remote work these days) on creating, running and debugging jobs hive queries on Hadoop platform ( preferably HDP )"
Data Engineer,"Location:
Phoenix, AZ

Qualification:
BE/ B.Tech

Experience:
7 Years

Technology:
Big Data

Role/Skills:
Very strong server-side Java experience, especially in an open source, data-intensive, distributed environments.
Strong previous professional experience building Distributed Solutions dealing with high volumes of data.
Hands on experience on HDFS, Hive, Pig, Sqoop and NOSQL.
Experience/ knowledge working with batch processing/ real-time systems using various open source technologies like Solr, Storm, Kafka, etc.
Experience in Apache Spark Batch and/or Spark Streaming (at least 6 months)
Good understanding of algorithms, data structure, performance optimization techniques and exposure to complete SDLC and PDLC
Well aware of architectural concepts (Multi-tenancy, SOA, SCA etc.) and NFR’s (performance, scalability, monitoring etc.)

Responsibilities:
Implementing various solutions arising out of large data processing (GB’s/ PB’s) over various NoSQL, Hadoop and MPP based products—both on-premise and in the cloud
Actively participating in various architecture and design calls with Big Data customers
Developing Hive scripts and being involved in writing MapReduce jobs
Implementing complex projects dealing with considerable data size (GB/PB) and with high complexity
Leveraging your experience with Hadoop and software engineering to help our customers drive value from their data
Working with Sr. Architects and providing implementation details to the Offshore team
Conducting sessions/ writing whitepapers/ Case Studies pertaining to Big Data
Being responsible for timely and quality deliveries
Fulfilling organizational responsibilities – sharing knowledge and experience with other passionate Impetus professionals, conducting various technical development sessions and training on new technologies"
Data Engineer,"Big Data Engineer
9321
Phoenix, AZ
10/9/2018

Application Development
Contractor - W2

Job Description
Job Description:
You won’t just shape the world of software.

You’ll shape the world of life, work and play.

Our Software Engineers not only understand how technology works, but how that technology intersects with the people who count on it every day. Today, innovative ideas, insight and new perspectives are at the core of how we create a more powerful, personal and fulfilling experience for all our customers.

This position requires a mix of strategic engineering and design along with hands-on, technical work. We seek a self-starter, visionary person with strong leadership capabilities. Exceptional communication skills, for collaborating across many teams. You will lead teams and deliver best-in-class products in an exciting fast-paced environment with the focus on reliability and automation. Dynamic, smart people and inspiring, innovative technologies are the norm here.

The successful candidate will be highly self-motivated with a passion for quality and automation coupled with an ability to understand complex systems and a desire to constantly make things better. Will you join us in crafting solutions that do not yet exist?

You won’t just keep up, you’ll break new ground.


There are hundreds of opportunities to make your mark on technology and life at American Express. Here’s just some of what you’ll be doing:
Build & run the platform that delivers services to all of our customers around the world
You will be on a team of Operational Engineering warriors whose mission it is to build and improve Operational/Support Capabilities for our most critical services
Building software and systems to manage/support applications through automation, support and monitoring
Explore and evaluate new technologies and solutions to push our capabilities forward, transform, innovate and continually improving the platform
You will troubleshoot the platform to achieve optimal support performance, stability and availability.
You will support operations and work closely with the development engineers to assist with architectural design, and implementation of complex features
Continuously Improve the platform through automation & building new operational capabilities.
Are you up for the challenge?

Overall 5+yrs experience in a large scale *nix environment
5+ yrs experience handling BigData Environment
Software development experience with on or more of: Python, Java or Scala
Experience with relational databases such as PostgreSQL, MySQL
Experience with core Hadoop: HDFS, MapReduce, Yarn
Experience with Streaming Data Platform (Kafka, Storm)
Experience with distributed/NoSQL databases: HBase, MySQL
Solid understanding of *nix systems and networking fundamentals
Experience as a software developer with MapReduce or Spark
Experience with extended Hadoop ecosystem: Hive, Pig
Solid Scripting Skills in languages like Python, Shell
Oncall Experience
Excellent communication skills, written and spoken; troubleshooting skills
Evidence of self-learning and commitment to personal development
Bachelor's degree or higher in Computer Science, or equivalent experience\
Continues integration, testing and deployment using Git, Jenkins
At the core of Software Engineering

Every member of our team must be able to demonstrate the following technical, functional, leadership and business core competencies, including:
Agile Practices
Porting/Software Configuration
Programming Languages and Frameworks
Business Analysis
Analytical Thinking
Business Product Knowledge

Job Requirements"
Data Engineer,"Hi,

Hope you're doing great!!

I'm Manvitha from Adwait Algorithm Please let me know if you're interested i
the below job description. I would appreciate if you refer someone for this
position

Role: Data Engineer
Location: Chandler, AZ
Duration: Long term

Job Description:
Data model development and Model scoring
Work with Data Scientists and build scripts to meet their data needs
Required Qualifications:
10+ years of overall experience
3+ years experience with Big Data ( HADOOP platforms) Hive, Sp
( needs to be currently hands-on on Hadoop cluster)
5+ years of ETL (Extract, Transform, Load) - Scoop, INFA RDBMS Teradata,
Oracle
Experience in Python
Reproduce issues faced by Data Scientists
Knowledge of Agile is a must
My Best,
Manvitha
Ph:
E:
Show moreShow less"
Data Engineer,"Join a team of financial software specialists responsible for the acquisition and management of financial market and related data required for research implementation of systematic trading strategies.
Primary Responsibilities:
Implement robust solutions for adoption, storage and management of large volumes of data
Develop quality assurance systems in support of high integrity data sets
Assist quantitative analysts in crafting custom, bespoke data sets
Support timely and fault tolerant data systems in support of production trading algorithms
Meet tight deadlines in an efficient manner
Requirements of the Candidate include:
Bachelor’s degree in Computer Science or applicable degree and very strong exposure to programming and computer systems
Experience with Python and C++
Experience with data storage and manipulation using approaches such as HDF5, Pandas, and RDBMS/SQL
2+ years professional experience in data management role
The desire to work in a fast-paced, hardworking and committed environment with a talented team
Strong problem solving skills, critical thinking and clear written and verbal communications
Financial industry, market data and cloud data environment experience are beneficial"
Data Engineer,"Overview

Required Skills
• Experience in building Big Data technologies and utilities is required
• Experience with MPP databases. Ability to troubleshoot issues and develop functions in an MPP environment is highly desired
• Very good knowledge of the software development life cycle, agile methodologies, and test-driven development
• Experience utilizing and extending ETL solutions in a complex, high-volume data environment is highly desired
• 2+ years of SQL experience and ETL development is required
• 2+ year of programming experience in either Spark, C, Java, Python, R and/or other functional programming skills
• Sound understanding of continuous integration & continuous deployment environments
• Experience in connecting and building application program interfaces (APIs) or messaging software and interoperability techniques and standards
• Strong analytical skills with a passion for testing
• Excellent problem solving and debugging skills
• Exposure in Data Management, Governance and Controls functions
• 4+ years of certified learning in Information Management, Computer Science, Engineering or Application/Platform Development

Preferred / Complementary Skills
• Technical and quantitative reasoning skills with regard to marketing data
• Experience in Informatica, Talend, Pentaho, Ab Initio
• Experience in Hadoop, Hive, Impala, Pig or Kafka
• Experience in Greenplum or Netezza

Required Skills • Experience in building Big Data technologies and utilities is required • Experience with MPP databases. Ability to troubleshoot issues and develop functions in an MPP environment is highly desired • Very good knowledge of the software development life cycle, agile methodologies, and test-driven development • Experience utilizing and extending ETL solutions in a complex, high-volume data environment is highly desired • 2+ years of SQL experience and ETL development is required • 2+ year of programming experience in either Spark, C, Java, Python, R and/or other functional programming skills • Sound understanding of continuous integration & continuous deployment environments • Experience in connecting and building application program interfaces (APIs) or messaging software and interoperability techniques and standards • Strong analytical skills with a passion for testing • Excellent problem solving and debugging skills • Exposure in Data Management, Governance and Controls functions • 4+ years of certified learning in Information Management, Computer Science, Engineering or Application/Platform Development Preferred / Complementary Skills • Technical and quantitative reasoning skills with regard to marketing data • Experience in Informatica, Talend, Pentaho, Ab Initio • Experience in Hadoop, Hive, Impala, Pig or Kafka • Experience in Greenplum or Netezza
This is Company overview ...

Which company? Recruiting Company or Client Company???"
Data Engineer,"Description: Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform (ETL) workflows.

71635

Fundamental Components:
Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs.
Writes ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processing.
Collaborates with International data/reporting and business teams to transform data and integrate algorithms and models into automated processes.
Uses knowledge in data transformation and storage experience designing & optimizing queries to build data pipelines.
Uses strong programming skills in SAS, Python, Java or any of the major languages to build robust data pipelines and dynamic systems.
Builds data marts and data models to support internal customers.
Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards.
Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions.
Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case.
Background Experience:
Strong problem solving skills and critical thinking ability.
Strong collaboration and communication skills within and across teams.
5 or more years of progressively complex related experience.
Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources.
Ability to understand complex systems and solve challenging analytical problems.
Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, MySQL or NoSQL or similar.
Experience building data transformation and processing solutions.
Has strong knowledge of large scale search applications and building high volume data pipelines.
Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline.
Clinical Licensure: N/A Potential Telework Position: Yes Percent of Travel Required: 0 - 10% EEO Statement: Aetna is an Equal Opportunity, Affirmative Action Employer Benefit Eligibility: Benefit eligibility may vary by position. Candidate Privacy Information: Aetna takes our candidate's data privacy seriously. At no time will any Aetna recruiter or employee request any financial or personal information (Social Security Number, Credit card information for direct deposit, etc.) from you via e-mail. Any requests for information will be discussed prior and will be conducted through a secure website provided by the recruiter. Should you be asked for such information, please notify us immediately."
Data Engineer,"Title: Data Engineer
Location: Philadelphia - Remote for the forseeable future
Type: 6+ Month Contract
Compensation: DOE

3 Must haves:

Python
SQL
Big Data framework

Job Description (no firm JD created yet):

We are looking for a motivated Data Engineer who would help build data pipelines to ingest and transform the data into our Data platform (on-prem/cloud). Candidate should be passionate about Python coding.

Job Responsibilites:
Develop and enhance data pipelines, mostly batch (if necessary ""streaming"" processes)
Apply best approaches for large scale data movement, capture data changes and apply incremental data load strategies.
Build automated test pipelines to ensure data integrity and completion.
Identify and improve current data pipelines through automation and optimization.
Skills:

Understand and comply with all enterprise and IS departmental information security policies, procedures and standards.
Support the integration of information security in the development, design, and implementation of Hospital Technology Resources that process, transmit, or store information.
Support all compliance activities related to state, federal regulatory requirements, healthcare accreditation standards, and all other applicable regulations that govern the use and disclosure of patient, financial, or other confidential information.
General
Basic knowledge on structured operational processes, conformance with SLAs, and metrics based reporting.
Foundational knowledge in Change Control Mgt. processes
Experience with process documentation and communication tools including MS Word, Project, Excel, Visio and PowerPoint.
Basic experience and proven use of one or more of the subject areas listed below:

SQL and Database Knowledge – Understanding SQL, Relational and Multidemensional Databases and Designs
Knowledge of relational database structures (tables, data types, data model schemas), SQL Syntax & SQL Functions, develop Views and SQL Optimization
Moderate experience and proven use of one or more of the subject areas listed below:

Tableau, Qliksense, Power BI, or any other data visualization application.

Data Warehouse Support and Design
Creating/Maintaining Tables, views, & indexes
• Proficiency in appropriate Business Intelligence/Data Warehousing technology or subject domain.

Education:

• Bachelor's degree in computer related field required.

• 2-4 years of Business Intelligence/Data Warehousing experience, preferably in an academic research (Administration) environment.

Preferred experience

5+ years of experience with Python

3+ years of experience working with BigData platform, large and complex file types such as XML, JSON, AVRO, PARQUET, ORC etc.,

Should be comfortable using SQL, Git, JIRA, Docker, CI/CD for testing/deployment."
Data Engineer,"Job Description
The Data Science and Engineering team is acquiring, studying, simulating, and modeling to enable data as a key driver and core functional component toward better understanding, predicting, and dynamically optimizing the access network to improve overall user experience. Success in this role is best enabled by a broad mix of skills and interests ranging from traditional distributed systems software engineering prowess to the multidisciplinary field of data science.

What you will will be doing as a Data Engineer:
Developing large scale data pipelines exposing data sources within our team of data analysts and data scientists.
Heavy Python programming involved in the day-to-day, must be able to hit the ground running with strong Python experience.
Utilizing AWS lambda and API Gateway.
Developing Spark streaming and batch jobs to clean and transform data.
Writing build automation to deploy and manage cloud resources.
Writing unit and integration tests.
What you will bring to the table as a Data Engineer:
Strong development experience coding in Python - a Python coding challenge will be given in the interview.
Experience transferring data using queries and working inside an Oracle database
Hands-on experience with AWS, specifically Lambda, and Neptune
Experience with Cloud Computing
Nice to have: experience with Kinesis, Kafka, Spark, and Redis

Company Description
NextGeners are Connectors.
We thrive on ‘quality over quantity’ and put in the work building strong relationships. We create connections, discover qualities, uncover skills, and place people with accuracy. We are your true partner!

NextGeners are Collaborators.
You’ll be working with an award-winning company that gives you direct connections to hiring managers. Our services don’t stop at the standard recruiting process. We use our expertise to tailor your resume, help improve your interview skills and provide ongoing career support.

NextGeners give 24/7 Support.
We are in this together. We provide around the clock availability, competitive employee benefits, and continuously check-in to make sure things are going smoothly. Check out our Glassdoor page!"
Data Engineer,"Site Name: UK - London - Brentford, USA - North Carolina - Research Triangle Park, USA - Pennsylvania - Philadelphia
Posted Date: Mar 17 2020

Data Engineer is accountable for developing and delivering cloud-based data ingestion solutions across the Pharma Commercial. You will be working on fully up to date technologies in the Data & Analytics environment and in a team, which is fully committed to remain at the leading edge of this skill set. Therefore, the impetus to keep improving skills and acquiring skill sets in new technologies will be very strong. If you are a top-flight developer who wants to continue to keep learning and remain at the cutting edge of the very fast-moving Data Analytics technology environment, then this role is for you.

This is very hands-on development role which includes developing & delivering code through from origin to production, plus working in partnership with 3rd party development service providers to help ensure that code comes in on time, to quality and in line with the overall ecosystem being established. The Data engineer will directly contribute to the extensive and varied build and deployment activities involved in establishing the new platform then continue to work on the already significant and growing pipeline of future build-outs on the platform.

This role will provide YOU the opportunity to lead key activities to progress YOUR career, these responsibilities include some of the following
Development: Hands on, sleeves up development and delivery expected as a matter of course.
Delivery: Ensure project goals are achieved on time in alignment with the stakeholders expectation. Ability to work on complex projects and in a distributed environment. Escalate to other Data & Analytics leadership team when needing support. Work in close collaboration with other team members in the CH BI Tech team, to ensure Development/Delivery aspects are well represented in the projects requirements and deliverables.
Methodology: . Incorporate agile ways of working into the delivery process thru use of DABL (Discovery, Alpha, Beta, Launch) framework to show value periodically. Individuals will work as part of product-centric delivery team(s) that will focus on delivering value independently while fully embracing integrated DevOps approaches.
Ownership: Take ownership for the delivery/development projects and help steer until completion.
Governance: Follow governance that allows projects and stakeholders to manage overall project performance and manage programme risks within the global nature of some of the programmes.
Forward looking: Remain flexible towards technology approaches to ensure that the best advantage is being taken of new technologies. Keep abreast of industry developments in analytics and be able to interpret how these would impact services and present new opportunities.
Quality, Risk & Compliance: Ensure all risk and issues associated with owned projects are recorded and managed in the appropriate Risk & Issue logs in a timely manner. Ensure all Risks and Issues have clear action/mitigation/contingency plans defined, with named action owners and timelines for completion.
Technical Architecture: Be conversant with technical architecture to contribute to design discussions in partnership with the Delivery/Development Director and dedicated Analytics & Data Architect.
Why you?
Basic Qualifications:


We are looking for professionals with these required skills to achieve our goals:
MS/BS degree in Computer Science, Engineering, Design or equivalent experience.
6-8 years as a Developer in the Data & Analytics arena with demonstrated expertise in emerging technologies and data technology platforms and management.
Fully conversant with agile and DevOps development methodology and concepts. Must have worked in CI/CD ways of working using tools like Azure DevOps.
Preferred Qualifications:


If you have the following characteristics, it would be a plus:
Ideal candidate would have built an impressive hands-on career to date in an advanced, recognized and innovative environment around Data & Analytics.
Fully conversant with big-data processing approaches and schema-on-read methodologies is a must and knowledge of Azure Data Factory/DataBricks/Azure Data Lake/Azure DW/Analysis Services is highly preferred. Understanding AWS and GCP cloud platforms is a plus.
Experience of the Azure analytics components, Power BI, Power Apps & Microsoft Visual Studio is desirable.
Good to have an excellent development skills & extensive hand-on development & coding experience in a variety of languages, e.g. C#, Python, SQL, DAX, etc.
Have a highly innovative mind-set and experience with analytics in a healthcare or CPG company.
Ability to work in close partnership with other IT functions such as IT security, compliance, infrastructure, etc. as well as partner closely with business stakeholders in the commercial and digital organizations.
Experience in executing Data Analytics projects in an Agile manner, articulation of Value depending on the project life cycle stage, Creating MVPs, developing plans for scale up are all very important experience to be successful in this role.
Great communication skills and ability to communicate inherently complicated technical concepts to non-technical stakeholders.
Why GSK?


Our values and expectations are at the heart of everything we do and form an important part of our culture.

These include Patient focus, Transparency, Respect, Integrity along with Courage, Accountability, Development, and Teamwork. As GSK focuses on our values and expectations and a culture of innovation, performance, and trust, the successful candidate will demonstrate the following capabilities:
Agile and distributed decision-making using evidence and applying judgement to balance pace, rigour and risk
Managing individual and team performance.
Committed to delivering high quality results, overcoming challenges, focusing on what matters, execution.
Implementing change initiatives and leading change.
Sustaining energy and well-being, building resilience in teams.
Continuously looking for opportunities to learn, build skills and share learning both internally and externally.
Developing people and building a talent pipeline.
Translating strategy into action - a compelling narrative, motivating others, setting objectives and delegation.
Building strong relationships and collaboration, managing trusted stakeholder relationships internally and externally.
Budgeting and forecasting, commercial and financial acumen.
If you require an accommodation or other assistance to apply for a job at GSK, please contact the GSK Service Centre at 1-877-694-7547 (US Toll Free) or +1 801 567 5155 (outside US).

GSK is an Equal Opportunity Employer and, in the US, we adhere to Affirmative Action principles. This ensures that all qualified applicants will receive equal consideration for employment without regard to race, color, national origin, religion, sex, pregnancy, marital status, sexual orientation, gender identity/expression, age, disability, genetic information, military service, covered/protected veteran status or any other federal, state or local protected class.

Important notice to Employment businesses/ Agencies

GSK does not accept referrals from employment businesses and/or employment agencies in respect of the vacancies posted on this site. All employment businesses/agencies are required to contact GSK's commercial and general procurement/human resources department to obtain prior written authorization before referring any candidates to GSK. The obtaining of prior written authorization is a condition precedent to any agreement (verbal or written) between the employment business/ agency and GSK. In the absence of such written authorization being obtained any actions undertaken by the employment business/agency shall be deemed to have been performed without the consent or contractual agreement of GSK. GSK shall therefore not be liable for any fees arising from such actions or any fees arising from any referrals by employment businesses/agencies in respect of the vacancies posted on this site.

Please note that if you are a US Licensed Healthcare Professional or Healthcare Professional as defined by the laws of the state issuing your license, GSK may be required to capture and report expenses GSK incurs, on your behalf, in the event you are afforded an interview for employment. This capture of applicable transfers of value is necessary to ensure GSKs compliance to all federal and state US Transparency requirements. For more information, please visit GSKs Transparency Reporting For the Record site."
Data Engineer,"Data Engineer Location Philadelphia, PA 19139 Duration 6+ Months Initial 3 months remote, then 3 months onsite in Philadelphia, PA. Duties We are looking for a motivated Data Engineer who would help build data pipelines to ingest and transform the data into our Data platform (on-premcloud). Candidate should be passionate about Python coding. Job Responsibilities 1. Develop and enhance data pipelines, mostly batch (if necessary ""streaming"" processes) 2. Apply best approaches for large scale data movement, capture data changes and apply incremental data load strategies. 3. Build automated test pipelines to ensure data integrity and completion. 4. Identify and improve current data pipelines through automation and optimization. Skills Understand and comply with all enterprise and IS departmental information security policies, procedures and standards. Support the integration of information security in the development, design, and implementation of Hospital Technology Resources that process, transmit, or store CHOP information. Support all compliance activities related to state, federal regulatory requirements, healthcare accreditation standards, and all other applicable regulations that govern the use and disclosure of patient, financial, or other confidential information."
Data Engineer,"Slyce is the market leader in the emerging technology of image recognition and visual search. Over the past few years, we've been busy raising capital, growing our team, and signing deals with 25+ of the leading retailers in the US including Home Depot, Bed Bath & Beyond, Neiman Marcus, and Macy's. We are a close-knit team with ambitious goals and we're excited to drive our cutting-edge technology further into the marketplace and have fun doing it.

Our technology allows consumers to submit a photo or scan an image using their mobile device and Slyce will recognize what it contains and match it to products sold by retailers that have integrated our technology. The focus of our approach is to take our core services and white label our technology into retailers' apps and mobile web. We also drive experiences with our core consumer apps, including SnipSnap, which reaches more than 5 million monthly users.

About the role
On a typical day, you may work on a new feature, develop a new algorithm, help construct a dataset, discover and fix a software bug, message a customer to understand the problem they're trying to solve, or improve your team's tools. PS: There is no typical day.
You will perform data and concrete analysis with human intuition, build data pipelines and develop tools to enable machine learning models to learn from data.
As you get to know our existing data streams and customer goals, you will recommend new types and methods of data collection, new ways to process data, and discover new patterns between many different types of data that provide value to customers.
As you get to know our existing data streams and customer goals, you will recommend new types and methods of data collection, new ways to process data, and discover new patterns between many different types of data that provide value to customers.
You will regularly deploy solutions to enable the team to release small, frequent iterations to customers via mobile apps, the web, and embedded systems.
Effective communication is extremely important. You will work with a diverse array of people, both inside and outside of Slyce to create the best products and solutions possible.
About You
MS in Computer Engineering, Computer Science, Electrical Engineering or equivalent practical experience.
Proficient in Python, C/C++ or Rust
Deep understanding of software systems fundamentals
You are actively interested in developing and deploying AI on a fleet of devices to improve people's shopping experience.
Experience with or exposure to creating highly available, scalable, low-latency, global systems.
A track record of pursuing self directed side projects, research, or open source projects.
Innate curiosity and a desire to explore solutions in a small, highly focused team.
You take pride in your work and ownership of the solutions you build.
Technologies we use:
Python, C++, Rust
Javascript, React, Angular-JS
TensorFlow, TensorRT
MySQL, MongoDB, Kafka, Kubernetes
AWS, GCP"
Data Engineer,"Nearly half of U.S. households struggle to handle an unexpected expense of $400 or more. When it comes to managing household expenses, things like a broken appliance or a growing family can be financially burdensome. Ranked 5th on Inc. Magazine's annual list of fastest-growing private companies in the United States, our team is on a mission to make life's purchases more accessible and affordable.Located in the heart of vibrant Philadelphia, we're building a FinTech-enabled e-commerce marketplace that combines quality brands with sensible financing to improve the lives of our neighbors and communities.Position OverviewWe are searching for a Data Engineer who is a quantitative, critical thinker with a passion for data and the capacity to work in a fast-paced, entrepreneurial environment. We are looking for an individual who desires experience in serving data-driven solutions at scale, crossing multiple functional areas and driving organizational efficiency. Applicants should be highly motivated and comfortable with taking on and adapting to a diverse array of subject matter. This opportunity is both unique and pivotal, as it provides the chance to contribute greatly to a rapidly-growing team.Initial Responsibilities* Analyze and resolve complex challenges around data and tools. Optimize analytical workflows by identifying opportunities and automating them* Implement solutions to bring together application data generated by distributed systems, third-party data, and real-time user data needed to make key business decisions* Work within the Data Science team to serve machine learning solutions at scale* Work on projects of growing responsibility, both individually and as part of a team, to build experience and skills at a pace matched to your shown ability* Learn more about the industry and Perpay, establishing a solid foundation to be better positioned for long-term career successBasic Qualifications* Bachelor's degree or higher in a quantitative/technical field (Computer Science, Statistics, Engineering)* Minimum two years work experience in related field required* Working knowledge of data design, architecture and warehousing* Understanding of data management fundamentals and data storage principles* Knowledge of distributed systems as it pertains to data storage and cloud computing* Understanding and administration of AWS, Docker and Linux-based systems* Experience in custom ETL design, implementation and maintenance* Experience in large scale data processing using traditional and distributed systems like Hadoop, Spark, Dataflow, and Airflow.* Strong working knowledge of SQL/NoSQL, relational databases and Python is required (2+ years experience)Preferred Qualifications* Knowledge and practical experience in machine learning and AI fundamentals* Experience implementing machine learning solutions at scale* Experience working with both Batch and Real Time data processing systems* Ability to work and communicate effectively with stakeholders.* Effective project management, problem solving, analytical and troubleshooting skills.Benefits* Opportunity to work with one of the fastest growing financial startups* Competitive salary + equity* Health/dental/vision insurance + 401K* Gym + public transportation subsidy* Relocation assistance* Centrally located in downtown PhiladelphiaWe are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.We don't work with external recruiting agencies, please contact us or apply directly!"
Data Engineer,"Overview

goPuff is seeking a Data Engineer which will be responsible for supporting the data ingest and integration needs at goPuff. The data engineer must provide technical expertise and leadership on architecture, design, and integration of multiple datasets into goPuff’sever evolving, large scale data environment. Emphasis of this position will be in developing and deploying a robust data processing pipeline and streams.

Responsibilities
Responsible for designing, deploying, and maintaining analytics about goPuff’s environment that process data at scale
Contributes design, configuration, deployment, and documentation for components that manage data ingestion, real time streaming, batch processing, data extraction, transformation, enrichment, and loading of data into a variety of cloud data platforms, including AWS and Microsoft Azure
Identifies gaps and improves the existing platform to improve quality, robustness, maintainability, and speed
Evaluates new and upcoming big data solutions and makes recommendations for adoption to extend our platform to meet advanced analytics use cases, such as predictive modeling and recommendation engines
Performs development, QA, and dev-ops roles as needed to ensure total end to end responsibility of solutions
Qualifications
4-6 years of experience in a Data Engineering role
Experience building, maintaining, and improving Data Processing Pipeline / Data routing in large scale environments
Fluency in common query languages, API development, data transformation, and integration of data streams
Strong experience with large dataset platforms such as (e.g. Amazon EMR, Amazon Redshift, Elasticsearch, Amazon Athena, Azure SQL Database, Azure Database for PostgreSQL, Azure Cosmos DB)
Experience in producing and consuming topics to/from Apache Kafka, AWS Kinesis, or Azure Event Hubs
Experience with data flow monitoring, batching, and streaming
Experience working with variety of databases and expert using SQL
Experience data storage performance analysis and enhancements such as data replication, distribution, and compression
Experience with acquiring data from varied sources such as: API, data queues, flat-file, remote databases
Creativity to go beyond current tools to deliver the best solution to the problem
Data QA experience is a plus
Experience with event-based and transaction-based system is a plus
About Us

The only predictable thing about life is that it’s wildly unpredictable. That’s where we come in.

When life does what it does best, customers turn to goPuff to deliver their everyday essentials, and to get through their day & night, work day and weekend.

We’re assembling a team of thinkers, dreamers & risk-takers...the kind of people who know the value of peace of mind in an unpredictable world. (And people who love snacks.)

Like what you’re hearing? Welcome to goPuff.

The goPuff Fam is committed to an inclusive workplace where we do not discriminate on the basis of race, sex, gender, national origin, religion, sexual orientation, gender identity, marital or familial status, age, ancestry, disability, genetic information, or any other characteristic protected by applicable laws. We believe in diversity and encourage any qualified individual to apply. We are an equal employment opportunity employer."
Data Engineer,"Description: Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform (ETL) workflows.

71635

Fundamental Components:
Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs.
Writes ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processing.
Collaborates with International data/reporting and business teams to transform data and integrate algorithms and models into automated processes.
Uses knowledge in data transformation and storage experience designing & optimizing queries to build data pipelines.
Uses strong programming skills in SAS, Python, Java or any of the major languages to build robust data pipelines and dynamic systems.
Builds data marts and data models to support internal customers.
Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards.
Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions.
Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case.
Background Experience:
Strong problem solving skills and critical thinking ability.
Strong collaboration and communication skills within and across teams.
5 or more years of progressively complex related experience.
Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources.
Ability to understand complex systems and solve challenging analytical problems.
Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, MySQL or NoSQL or similar.
Experience building data transformation and processing solutions.
Has strong knowledge of large scale search applications and building high volume data pipelines.
Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline.
Clinical Licensure: N/A Potential Telework Position: Yes Percent of Travel Required: 0 - 10% EEO Statement: Aetna is an Equal Opportunity, Affirmative Action Employer Benefit Eligibility: Benefit eligibility may vary by position. Candidate Privacy Information: Aetna takes our candidate's data privacy seriously. At no time will any Aetna recruiter or employee request any financial or personal information (Social Security Number, Credit card information for direct deposit, etc.) from you via e-mail. Any requests for information will be discussed prior and will be conducted through a secure website provided by the recruiter. Should you be asked for such information, please notify us immediately."
Data Engineer,"SUMMARYData Analytics team is looking for a smart, data-savvy data engineer to help us re-invent our data and analytics platforms. You should have a deep passion for understanding and solving data issues and bring logic, enthusiasm and an analytical approach to work issues.

RESPONSIBILITIES

Drive engineering efforts to design, develop, and test data-driven solutions
Drive the optimization, testing and tooling that will improve data quality
Improve performance, availability and scalability of our backend systems
Partner with engineering, product and operations teams to design tech solutions
Utilize CI/CD throughout the development, automation and testing lifecycle
Deliver updates in different backend components and
Contribute strategy and tactics to the long-term roadmap for enterprise data strategy

REQUIREMENTS

3+ years(TM) experience building large scale big data applications in a business setting
Excellent SQL and database knowledge
Ability to pull data from disparate sources, (RDBMS, Excel), and deliver to Cloud data lake
Hands-on experience with Python, Ruby, Scala, and/or Java.
Knowledge/experience using AWS, Snowflake or other Cloud technology
Proficient with Linux commands and environments
Demonstrated ability to learn new languages.
Bachelor(TM)s degree in an analytic or technical field, (Math, Stat, Computer Science, etc.)

PREFERENCES

Hands-on experiences with data processing, reporting or analysis tools
Experiences with Hadoop/Spark/Kafka/Presto/etc."
Data Engineer,"Job Responsibilites Develop and enhance data pipelines, mostly batch (if necessary ""streaming"" processes) Apply best approaches for large scale data movement, capture data changes and apply incremental data load strategies. Build automated test pipelines to ensure data integrity and completion. Identify and improve current data pipelines through automation and optimization. Skills Understand and comply with all enterprise and IS departmental information security policies, procedures and standards.2. Support the integration of information security in the development, design, and implementation of Hospital Technology Resources that process, transmit, or store client information.3. Support all compliance activities related to state, federal regulatory requirements, healthcare accreditation standards, and all other applicable regulations that govern the use and disclosure of patient, financial, or other confidential information. General Basic knowledge on structured operational processes, conformance with SLAs, and metrics based reporting.2. Foundational knowledge in Change Control Mgt. processes3. Experience with process documentation and communication tools including MS Word, Project, Excel, Visio and PowerPoint. Basic experience and proven use of one or more of the subject areas listed below SQL and Database Knowledge ndash Understanding SQL, Relational and Multidemensional Databases and Designs1. Knowledge of relational database structures (tables, data types, data model schemas), SQL Syntax SQL Functions, develop Views and SQL Optimization Moderate experience and proven use of one or more of the subject areas listed below Tableau, Qliksense, Power PI, or any other data visualization application. Data Warehouse Support and Design1. CreatingMaintaining Tables, views, indexes bull Proficiency in appropriate Business IntelligenceData Warehousing technology or subject domain. Soft Skills Comfortable working in a collaborative environment. Ability to self-organize one's priorities and schedule. Have mindset to perform necessary documentation. Should be a self-starter to work independently or in a team. Keywords Education bull Bachelor's degree in computer related field required.bull 2-4 years of Business IntelligenceData Warehousing experience, preferably in an academic research (Administration) environment. Preferred experience 5+ years of experience with Python 3+ years of experience working with BigData platform, large and complex 3+ file types such as XML, JSON, AVRO, PARQUET, ORC etc., Should be comfortable using SQL, Git, JIRA, Docker, CICD for testingdeployment. Skills and Experience Required Skills PYTHON"
Data Engineer,"Data EngineerData Engineer

You will support the engineering team's data endeavors, diving in to fix issues, optimize processes, and automate what you do more than once. You'll use the best tools for the job, whether modern and revolutionary or time tested and proven, to deliver elegant, scalable solutions that meet business and technical needs.

• Work with internal stakeholders to load data into the data warehouse
• Troubleshoot and resolve issues relating to data integrity
• Help establish procedures and best practices for transforming and storing data
• Lead requirements gathering around data pipeline automation improvements
• Work with some of the most exciting open-source tools like Spark, Hadoop, Docker, Airflow, Zeppelin
• Leverage distributed computing and serverless architecture such as AWS EMR & AWS Lambda, to develop pipelines for transforming data
• Enjoy the peace that comes with working in a mature software development environment
• Marvel at the speed with which your creation makes it into production
• Research and implement new technologies with a team of developers to execute strategies and implement solutions
• Produce peer reviewed quality software
• Solve complex problems related to the real-time discovery of large data

• Experienced in writing scalable applications on distributed architectures
• Data driven, testing and measuring as much as you can
• Eager to both review peer code and have your code reviewed
• Comfortable on the command line and consider it an essential tool
• Confident in SQL, you know it, write smart queries

Required skills and experience

• 5+ years of work experience
• 3+ years of experience with Python
• 3+ years of experience with PySpark and Spark-SQL (writing, testing, debugging spark routines)
• 1+ years of experience with AWS EMR, AWS S3 service. Comfortable using AWS CLI and boto3
• Comfortable working in remote environments
• Comfortable using *nix command line (shell scripting, AWK, SED)
• Experience with MySQL and Postgres
Desired experience

• Experience with Apache Airflow
• Experience with Apache Zeppelin
• Experience with healthcare data

Interested candidates please send resume in Word format Please reference job code 528434 when responding to this ad.

Job Requirements:"
Data Engineer,"Data Engineer in Philadelphia, PA 19103

Interview Logistics:

WebEx Interview

Required Skills Set:

Years of Experience:4+

Education Required:Bachelors Degree
BS/MS degree in Computer Science, Mathematics, or other relevant science and engineering discipline.
4+ years working as a software engineer.
2+ years working within an enterprise data lake/warehouse environment or big data architecture.
Excellent programming skills with experience in at least one of Python, Scala, Java, Node.js.
Great communication skills.
Proficiency in testing frameworks and writing unit/integration tests
Proficiency in Unix-based operating systems and bash scripts.
Additional Preferred Skills:
Experience with working in Spark
Experience with AWS
Experience with monitoring and visualization tools such as Grafana, Prometheus, Data Dog, and Cloudwatch.
Experience with NoSQL databases, such as DynamoDB, MongoDB, Redis,Cassandra, or HBase
Project Description:

Join a multi-disciplinary team of devops engineers, software engineers, data analysts, and data scientists working together to improve the user experience.

Do you likebigchallenges and working within ahighly motivatedteam environment?As a data software engineer on the Data Science and Engineering team within the organization, you will be part of a team that thrives onbigchallenges, results, quality, and agility. You will work closely with business stakeholders, data analysts, and data scientists within the organization developing software solutions helping to deliver insights into customer and network behavior that drive business decisions shaping the future of the company.

Who does the Data Engineer work with?

You will collaborate with a diverse set of professionals ranging from software engineers whose software integrates with analytics services, network architects and engineers who are tasked with evolving the network, service delivery engineers who provide support for our products, data analysts and data scientists distilling key insights from massive amounts of raw data, operational stakeholders with all manner of information needs,and executives who rely ondatafor fact-based decision making.

What are some interesting problems you'll be working on?

Develop large scale, cloud based data pipelines for the collection and processingof device telemetry and network events, providing both a real time and historical view into the operation of our products and services.Work on high performance, real time data stores and a massive historical data sets usingbest-of-breed and industry leading technology. Expose services over REST APIs. Work closely with various engineering teams to solve key optimization, insight and access network data challenges.

Where can you make an impact?

The Data Science and Engineering teamis acquiring, studying, simulating, and modeling to enable data as a key driver and core functional component toward better understanding, predicting, and dynamically optimizing the access network to improve overall user experience. Success in this role is best enabled by a broad mix of skills and interests ranging from traditional distributed systems software engineering prowess to the multidisciplinary field of data science.

Responsibilities:
Developing large scale data pipelines exposing data sources within the company to our team of data analysts and data scientists.
Developing REST APIs utilizing AWS lambda and API Gateway.
Developing Spark streaming and batch jobs to clean and transform data.
Writing build automation to deploy and manage cloud resources.
Writing unit and integration tests.
Some of the specific technologies we use:
Programming Languages (Python, Scala, Golang, Node.js)
Build Environment: GitHub Enterprise, Concourse CI, Jira, Serverless, SAM
Cloud Computing (AWS Lambda, EC2, ECS)
Spark(AWS EMR, Databricks)
Stream Data Platforms: Kinesis, Kafka
Databases: S3, MySQL, Oracle,MongoDB, DynamoDB
Caching Frameworks (ElasticCache/Redis)
Physical Environment and Working Conditions:

Must be able to work on site in Philadelphia after coronavirus restrictions are lifted."
Data Engineer,"Job Description US Tech Solutions is seeking a ldquoData Engineerrdquo for a 06+ Months Contract with a client in Philadelphia, PA Job Poster Akhil Kumar Description We are looking for a motivated Data Engineer who would help build data pipelines to ingest and transform the data into our Data platform (on-premcloud). Candidate are to be passionate about Python coding. Job Responsibilities Develop and enhance data pipelines, mostly batch (if necessary ""streaming"" processes) Apply best approaches for large scale data movement, capture data changes and apply incremental data load strategies. Build automated test pipelines to ensure data integrity and completion. Identify and improve current data pipelines through automation and optimization. Skills Understand and comply with all enterprise and IS departmental information security policies, procedures and standards. Support the integration of information security in the development, design, and implementation of Hospital Technology Resources that process, transmit, or store client information. Support all compliance activities related to state, federal regulatory requirements, healthcare accreditation standards, and all other applicable regulations that govern the use and disclosure of patient, financial, or other confidential information. General Basic knowledge on structured operational processes, conformance with SLAs, and metrics based reporting. Foundational knowledge in Change Control Mgt. processes Experience with process documentation and communication tools including MS Word, Project, Excel, Visio and PowerPoint. Basic experience and proven use of one or more of the subject areas listed below SQL and Database Knowledge ndash Understanding SQL, Relational and Multidimensional Databases and Designs Knowledge of relational database structures (tables, data types, data model schemas), SQL Syntax SQL Functions, develop Views and SQL Optimization Moderate experience and proven use of one or more of the subject areas listed below Tableau, Qliksense, Power PI, or any other data visualization application. Data Warehouse Support and Design CreatingMaintaining Tables, views, indexes Proficiency in appropriate Business IntelligenceData Warehousing technology or subject domain. Education Bachelorrsquos degree in computer related field required. 2-4 years of Business IntelligenceData Warehousing experience, preferably in an academic research (Administration) environment. Preferred experience 5+ years of experience with Python 3+ years of experience working with BigData platform, large and complex file types such as XML, JSON, AVRO, PARQUET, ORC etc., Should be comfortable using SQL, Git, JIRA, Docker, CICD for testingdeployment. About US Tech Solutions Your talent, our opportunities - This is the premise behind US Tech Solutions. You have the skill we have the opportunity. As a team, we work passionately for you to get the right career opportunity across industry verticals and functions. For past sixteen years, leading Global Companies and Fortune 500 come to us to get the right talent. Whether you want to work as full-time, contractor or part-time, technical or non-technical our talent consultants will connect with the right career opportunity globally. Connect with our talent team today. USTECH was founded in 2000 by Manoj Agarwal. Today, we are a global firm offering talent solutions to 150 customers including 20 of Fortune 500 across Financial Services, Healthcare, Life Sciences, Aerospace, Energy, Retail, Telecom, Technology, Manufacturing, and Engineering. We are headquartered in New Jersey with 40 global locations across the USA, Canada, Europe, and India. Deloitte has recognized USTECH as one of the fastest growing private businesses for the past five consecutive years and INC 500 for the past three. We have also been rated ldquoThe Top Business in the US"" by Diversity Business since 2011. To learn more about how US Tech Solutions visit our website www.ustechsolutions.com. ldquoUS Tech is an Equal Opportunity Employer"" and ldquos all other parties authorized to work in the US are encouraged to apply."" Apply Interested candidates are requested to send their resume to Akhil at AkhilKustechsolutionsinc.com"
Data Engineer,"Job Description
Hands on experience in developing big data pipelines end to end

· 5-8 years of Java and Python experience

· Experience in software development of largescale distributed systems including proven track record of delivering backend systems that participate in a complex ecosystem

· Experience working with imperfect data sets that, at times, will require improvements to process, definition and collection

· Experience with real-time data pipelines and components including Kafka, Spark Streaming

· Proficient in Unix/Linux environments

· AWS experience developing data streaming pipelines

· Deep Spark understanding

· Must Have Skills : Spark, SQL,Kinesis/Kafka, python for scripting on AWS and Java for APIs"
Data Engineer,"The Philadelphia District Attorneys Office (DAO), led by District Attorney Larry Krasner, is committed to the mission of providing fair and just prosecution to all Philadelphians. DA Krasner believes that justice is best achieved through policies grounded in research, data, and science. Holding true to these commitments, the DAO has improved its capabilities to process data, craft policy, and conduct research. As we continue to modernize a progressive and fast-paced office, we aim to be responsive, forward thinking, and collaborative.

The Data Engineer will join an innovative team of researchers, lawyers, analysts, and software engineers in the development and deployment of applications to support prosecution, analytics, data-driven policy, and public transparency. They will support DA Krasners targeted areas for informed reform, including charging, diversion, sentencing, focusing on the most serious crimes, preventing violence, enhancing harm reduction, and ameliorating disparities in the criminal justice process. The Data Engineer will work with both the development and research and analytics teams to create tools to help transform and structure our data to help developers and analysts to access our data quickly, accurately, and efficiently. The Data Engineer will be expected to:
Build and maintain the DAO data warehouse to house both internally created data, administrative data, and other data from external partners about the criminal justice system and the welfare of individuals associated with the criminal justice system.
Create and build tools and libraries, to help our developers and analysts query the data warehouse expertly
Maintain a cloud-based development environment for a team of analysts to work in
Evangelize and educate teams across the DAO on standard methodologies around data services development, data privacy and security, as well as improving efficiency
Help the DAO improve our data operations by automating manual processes, optimizing data storage and delivery, re-designing infrastructure for scalability
Work with our DATA Lab to enable and optimize their data and data services usage
Qualifications

A Bachelors or Masters degree in Computer Science, Software Engineering, Mathematics or related technical discipline is preferred, but not required. Additional years of experience exceeding the minimum requirement as well as a proven track record of work may stand in for a degree. The preferred candidate will also have or demonstrate:
Ability to professionally deal with changing environments, be self-directed; and given broad direction, be able to prioritize work and allocate time and resources effectively;
Experience with or knowledge of: web application security concerns and solutions to mitigate those concerns;
Familiarity with Bitbucket, Git, or similar version control systems;
Experience with developing and maintaining custom and non-proprietary software based data platform infrastructure services
Experience with a systems language such as C, C++, C#, Go, Java or Scala
Experience with a scripting language such as Python, PHP, or Ruby
Experience developing data solutions using storage platforms like S3, Cassandra, Hbase, and using data warehouse systems like Snowflake, Redshift, Cosmos DB, or BigQuery
Experience developing applications and solutions using data ecosystem components such as Spark, Flink, Solr, Redis, InfluxDB, Elasticsearch
Experience working with cloud services such as AWS, Google Cloud, Azure
Experience with building and integrating logging, alerting, and monitoring as an integral part of data platform services
Enthusiasm to collaborate, innovate, and solve problems;
A proven ability to prioritize work in a team and independently, and handle multiple complex tasks simultaneously;
Experience working in the legal field/with attorneys and a knowledge of the criminal justice system are both a plus."
Data Engineer,"Location Philadelphia, PA Description Our client is currently seeking a Data Engineer for our client in Philadelphia! This job will have the following responsibilities Work closely with stakeholders, analysts, and data scientists within the organization developing software solutions helping to deliver insights into customer and network behavior that drive business decisions. Develop large scale, cloud based data pipelines for the collection and processing of device telemetry and network events, providing both a real time and historical view into the operation of our products and services. Expose services over REST APIs and work closely with various engineering teams to solve key optimization, insight and access network data challenges. Develop large scale data pipelines exposing data sources Develop REST APIs utilizing AWS lambda and API Gateway. Develop Spark streaming and batch jobs to clean and transform data. Write build automation to deploy and manage cloud resources and unit and integration tests. Qualifications Requirements Strong python development experience required Experiences with Oracle database, especially in AWS RDS, Cloud Computing (AWS Lambda, EC2, ECS), and Spark (AWS EMR, Databricks). BSMS degree in Computer Science, Mathematics, or other relevant science and engineering discipline. 4+ years working as a software engineer and 2+ years working within an enterprise data lakewarehouse environment or big data architecture. Proficiency in testing frameworks and writing unitintegration tests and in Unix-based operating systems and bash scripts. Preferred Additional Skills Experience with working in Spark Experience with AWS Experience with monitoring and visualization tools such as Grafana, Prometheus, Data Dog, and Cloudwatch. Experience with NoSQL databases, such as DynamoDB, MongoDB, Redis, Cassandra, or HBase If interested, please email LEltringhamjudge.com with an updated resume! Thanks! Contact leltringhamjudge.com mailtoleltringhamjudge.com?subjectData20Engineer This job and many more are available through The Judge Group. Find us on the web at www.judge.com httpwww.judge.com"
Data Engineer,"Job Description
How you will help
You will support the engineering team’s data endeavors, diving in to fix issues, optimize processes, and automate what you do more than once. You’ll use the best tools for the job, whether modern and revolutionary or time tested and proven, to deliver elegant, scalable solutions that meet business and technical needs.

What you will do
Work with internal stakeholders to load data into HealthVerity's data warehouse
Troubleshoot and resolve issues relating to data integrity
Help establish procedures and best practices for transforming and storing data
Lead requirements gathering around data pipeline automation improvements
Work with some of the most exciting open-source tools like Spark, Hadoop, Docker, Airflow, Zeppelin
Leverage distributed computing and serverless architecture such as AWS EMR & AWS Lambda, to develop pipelines for transforming data
Enjoy the peace that comes with working in a mature software development environment
Marvel at the speed with which your creation makes it into production
Research and implement new technologies with a team of developers to execute strategies and implement solutions
Produce peer reviewed quality software
Solve complex problems related to the real-time discovery of large data
About you
You are...
Experienced in writing scalable applications on distributed architectures
Data driven, testing and measuring as much as you can
Eager to both review peer code and have your code reviewed
Comfortable on the command line and consider it an essential tool
Confident in SQL, you know it, write smart queries, it’s no big deal

Required skills and experience
5+ years of work experience
3+ years of experience with Python
3+ years of experience with PySpark and Spark-SQL (writing, testing, debugging spark routines)
1+ years of experience with AWS EMR, AWS S3 service. Comfortable using AWS CLI and boto3
Comfortable working in remote environments
Comfortable using *nix command line (shell scripting, AWK, SED)
Experience with MySQL and Postgres
Desired experience
Experience with Apache Airflow
Experience with Apache Zeppelin
Experience with healthcare data
HealthVerity, based in Center City Philadelphia, is a venture-backed technology company that is transforming the way data-led organizations make critical decisions. Our technology platform serves as the foundation for the rapid creation, exchange and management of healthcare and consumer data in a fully-interoperable, privacy-protecting manner. Advantaged by highly sophisticated identity resolution and matching capabilities, HealthVerity is on a mission to increase transparency, forge interoperability and activate deeper insights.

Our company challenges
Empowering clients with highly rewarding data discovery and licensing tools
Ingesting and managing billions of healthcare records from a wide variety of partners
Standardizing on common data models across data types
Orchestrating an industry-leading HIPAA privacy layer
Innovating our proprietary de-identification and data science algorithms
Building a culture that supports rapid iteration and new possibilities
The infrastructure and culture we are building will provide an environment that cultivates innovation. We want to move fast knowing we can fix anything we break along the way. If a new need arises, we want to turn around a solution quickly. We want to solve our challenges in ways that create even more possibilities. We’re creating a platform that lets us discover what else we might do.

We have big plans
We are building a platform that will scale to support an ever-growing array of data providers and innovative products. You must be able to think big while still delivering on near-term requirements.

HealthVerity is an equal opportunity employer."
Data Engineer,"IntePros is looking to hire a Sr. Data Engineer for a premier client in Philadelphia, PA
suggest, prototype and deploy machine learning solutions.
expose your models as service interfaces for the application integration.
build data pipelines for sourcing data from disparate sources which later can be analyzed, cleaned and transformed for building intelligent systems.
Primary Requirements:
• Strong and proven software development skills
• Proficient in Python programming language
• Should be able to build interfaces for collecting data, data extraction, transformations through Python
• Should be able to work with data through SQL - Preparing data, cleanup, Aggregating, Slicing/Dicing Data
• Should have worked in Python Stats, Machine learning packages - Pandas, Data frames, Scipy, NumPy, StatsModel, Scikit packages. Familiarity with frameworks such as MLlib, H2O, TensorFlow or similar.
• Should have experience in working with Kafka using Python or Java API
• Should have worked with a NoSQL database. Should be able to build interfaces for serving data-in/data-out of the database through API.
• Preferred Telecom and Billing
• Experience working in an Agile organization and understanding of Agile fundamentals, JIRA with CI/CD
• Excellence at formulating, understanding, and solving complex, non-routine problems
• Exposure in working with Big Data; experience with Hadoop, Kafka, and Spark preferred
• Familiarity with managed cloud-based options for building machine learning models"
Data Engineer,"Job Description
Although the role says SAS to python conversion, but look for more of big data hive, python AWS, Machine Learning, Artificial intelligence kind of profiles on this

Role: Data Engineer

Location: Malvern, PA

Duration: 07 Months +

VISA: No H1B & OPT & CPT

Job Description:
SAS to Python conversion,
HIVE, Python,
AWS,
Big Data Technologies,
Machine Learning, Artificial intelligence"
Data Engineer,"Data EngineerLocation: Philadelphia, PA Full-time/Permanent position Summary: You will support the engineering team’s data endeavors, diving in to fix issues, optimize processes, and automate what you do more than once. You’ll use the best tools for the job, whether modern and revolutionary or time tested and proven, to deliver elegant, scalable solutions that meet business and technical needs. What you will do -Work with internal stakeholders to load data into a data warehouse -Troubleshoot and resolve issues relating to data integrity -Help establish procedures and best practices for transforming and storing data -Lead requirements gathering around data pipeline automation improvements -Work with some of the most exciting open-source tools like Spark, Hadoop, Docker, Airflow, Zeppelin -Leverage distributed computing and serverless architecture such as AWS EMR & AWS Lambda, to develop pipelines for transforming data -Enjoy the peace that comes with working in a mature software development environment -Marvel at the speed with which your creation makes it into production -Research and implement new technologies with a team of developers to execute strategies and implement solutions -Produce peer reviewed quality software -Solve complex problems related to the real-time discovery of large data About you You are... -Experienced in writing scalable applications on distributed architectures -Data driven, testing and measuring as much as you can -Eager to both review peer code and have your code reviewed -Comfortable on the command line and consider it an essential tool -Confident in SQL, you know it, write smart queries, it’s no big deal Required skills and experience -5+ years of work experience -3+ years of experience with Python -3+ years of experience with PySpark and Spark-SQL (writing, testing, debugging spark routines) -1+ years of experience with AWS EMR, AWS S3 service. Comfortable using AWS CLI and boto3 -Comfortable working in remote environments -Comfortable using *nix command line (shell scripting, AWK, SED) -Experience with MySQL and Postgres Desired experience -Experience with Apache Airflow -Experience with Apache Zeppelin -Experience with healthcare data"
Data Engineer,"Title: SSIS ConsultantLocation : Minneapolis MN Duration : 3 month contract (strong possibility to extend) Top 3 Skills: 1) SQL ETL, SSIS, SSIS catalogue to deploy package - how they are moving data from point a to point b - bringing it into Azure ++ SQL based ERP system 2) Analysis Services - to use Tabular cubes - supplement invoicing data with delivery information 3) experience with Data viz - QLIK Description: Looking for a strong MS ETL developer to assist with migrating data from an old ERP/DW system into the central source. The bulk of the work will be ancillary data onboarding and some data cleansing, so if you've connected to a data system in the past, it should be relatively straight forward work. Project Background: ASC acquired a company that uses ERP called Falcon - they created a looping script, so that when there is a new acquisition, if they are connecting to a known system, they could onboard data in a day. Title: SSIS Consultant Location : Minneapolis MN Duration : 3 month contract (strong possibility to extend) Top 3 Skills: 1) SQL ETL, SSIS, SSIS catalogue to deploy package - how they are moving data from point a to point b - bringing it into Azure ++ SQL based ERP system 2) Analysis Services - to use Tabular cubes - supplement invoicing data with delivery information 3) experience with Data viz - QLIK Description: Looking for a strong MS ETL developer to assist with migrating data from an old ERP/DW system into the central source. The bulk of the work will be ancillary data onboarding and some data cleansing, so if you've connected to a data system in the past, it should be relatively straight forward work. Project Background: ASC acquired a company that uses ERP called Falcon - they created a looping script, so that when there is a new acquisition, if they are connecting to a known system, they could onboard data in a day."
Data Engineer,"Aetea has been asked by their valued client, to assist with their need for a Data Engineer. The client and the team yoursquoll be working in are great and offer positive challenges in a dynamic environment. Specific Responsibilities Design and develop Extraction, Transformation and Loading (ETL) processes using standard ETL tools to streamline and automate the collection of data from source systems using SQL, Snowflake and AWS technologies Assemble large, complex data sets that meet functional non-functional business requirements Identify, design, and implement internal process improvements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Work with stakeholders including the Marketing, CRM, Finance, Operations, Product, and Development teams to assist with data-related technical issues and support their data infrastructure needs. Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader. Develop repeatable and scalable data quality audits Requirements Design and develop Extraction, Transformation and Loading (ETL) processes using standard ETL tools to streamline and automate the collection of data from source systems using SQL, Snowflake and AWS technologies Assemble large, complex data sets that meet functional non-functional business requirements Identify, design, and implement internal process improvements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Work with stakeholders including the Marketing, CRM, Finance, Operations, Product, and Development teams to assist with data-related technical issues and support their data infrastructure needs. Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader. Develop repeatable and scalable data quality audits Requirements Bachelorrsquos Degree in Computer Science, MIS or related field 5+ years Advanced SQL knowledge and experience working with relational databases, query authoring (T-SQL) as well as working familiarity with a variety of databases. Able to write advanced SQL queries with multi-table joins, group functions, subqueries, set operations, functions, Stored Procedures and basic Java Scripting. 2+ years of Experience with Snowflake Cloud Datawarehouse Platform 2+ Years Experience with object-orientedobject function in one of scripting languages (Python, PySpark) Experience with AWS DatabasesTools a plus (S3, Glue, Lambda) Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. Strong understanding of data modeling and data warehousing principles Excellent documentation and general organization of ongoing tasks, including the ability to evaluate and question business rules. Ability to solve problems and work directly with business SMErsquos Financial Experience is a plus JavaScripting is a plus This is a great opportunity to get in with a stable and growing organization that values technology and the people that drive it. There is a lot more we can share with you about the role, so please contact me (or apply) at gkumaraetea.com mailtogkumaraetea.com or through the ldquoapplyrdquo function listed. This is an urgent opportunity and the client committed to moving quick when they identify the right person. For more than 30 years, Aetea Information Technology has been the go-to partner for both premier clients and IT professionals alike. As a full spectrum IT Human Capital organization, we have experience in all industries, supporting clients ranging from emerging start-ups to Fortune 50. Our consultants value Aetea for their benefits, length of assignments, sequential employment opportunities, and compensation models not to mention the attentiveness and world-class support you will receive from the internal Aetea team."
Data Engineer,"Description Robert Half Technology is looking for a motivated Data Engineer in Philadelphia, PA who would help build data pipelines to ingest and transform data into an internal Data platform (on-premcloud). Candidate should be passionate about Python coding. Responsibilities bullDevelop and enhance data pipelines, mostly batch (if necessary ""streaming"" processes) bullApply best approaches for large scale data movement, capture data changes and apply incremental data load strategies. bullBuild automated test pipelines to ensure data integrity and completion. bullIdentify and improve current data pipelines through automation and optimization. Skills bullBasic experience and proven use of one or more of the subject areas listed below bullSQL and Database Knowledge - Understanding SQL, Relational and Multidemensional Databases and Designs bullKnowledge of relational database structures (tables, data types, data model schemas), SQL Syntax SQL Functions, develop Views and SQL Optimization bullModerate experience and proven use of one or more of the subject areas listed below bullTableau, Qliksense, Power PI, or any other data visualization application. bullData Warehouse Support and Design bullCreatingMaintaining Tables, views, indexes bullProficiency in appropriate Business IntelligenceData Warehousing technology or subject domain Soft Skills bullComfortable working in a collaborative environment. bullAbility to self-organize one's priorities and schedule. bullHave mindset to perform necessary documentation. bullShould be a self-starter to work independently or in a team. Requirements Python, Big Data, Business Intelligence and Data Warehousing (BI DW), Microsoft Word, Microsoft Excel, Microsoft PowerPoint, SQL Database, Tableau, XML, JSON, Avro, JIRA Robert Half Technology matches IT professionals with some of the best companies on a temporary, project or full-time basis. From roles in software and applications to IT infrastructure and operations, we provide you unparalleled access to exciting career opportunities. Our personalized approach, innovative matching technology and global network with local market expertise help you find the technology jobs that match your skills and priorities - fast. By working with us, you have access to challenging opportunities, competitive compensation and benefits, and training to enhance your skill sets. From philanthropy to environmental stewardship to employee programs, Robert Half is proud to have an active role in the communities in which we live and work. Our company has appeared on FORTUNE's ""Most Admired Companies"" list every year since 1998. Download our mobile app to take your job search on the go! Contact your local Robert Half Technology office at 888.490.4429 or visit www.roberthalf.comjobstechnology to apply for this job now or find out more about other job opportunities. All applicants applying for U.S. job openings must be authorized to work in the United States. All applicants applying for Canadian job openings must be authorized to work in Canada. 2020 Robert Half Technology. An Equal Opportunity Employer MFDisabilityVeterans. By clicking 'Apply Now' you are agreeing to Robert Half Terms of Use httpswww.roberthalf.comterms-of-use ."
Data Engineer,"Proficient in Big Data application development skills as well as multiple design
techniques
Working proficiency in Big Data development toolset to
design, develop, test, deploy, maintain and improve
software
Strong understanding of Agile methodologies with
ability to work in at least one of the common
frameworks
Strong understanding of techniques such as
Continuous Integration, Continuous Delivery, Test
Driven Development, Cloud Development,
application resiliency and security
Proficiency in one or more general purpose
programming languages
Working proficiency in a portion of software
engineering disciplines and demonstrates
understanding of overall software skills including
business analysis, development, testing,
deployment, maintenance and improvement of
software
Dress Codes:
Casual

Roles and Responsibilities: Qualifications
"" Bachelor's or Advanced Degree in Information Management, Computer Science, Mathematics, Statistics, or related fields desired
"" Financial Services background or experience preferred across different LOBs. Proven proficiency with data analysis and ability to troubleshoot data issues.
"" Proficiency across the full range of big data technologies and/or database and business intelligence tools; publishing and presenting information in an engaging way
"" Intensive, recent experience in assessing and sourcing data needs
"" Detail oriented with a commitment to innovation

Knowledge/Technical Skills
"" Strong experience in Big Data Hadoop stack Hive, Impala, Spark, Spark/SQL
"" Experience in Python, Java, Spark development is highly desired.
"" Experience with MPP databases like Teradata or Oracle is preferred.
"" Experience in Big Data technologies like Pig, Kafka is preferred
"" Exposure to machine learning is preferred
"" Exposure to ETL tools ( ABINTIO, INFORMATICA, DataStage or Others) a plus.
"" Good communication skills and ability to interact with users, make presentations and guide conversations.
"" Experience with scheduling & data integration tools like Control-M and Ni-Fi is highly desired.
"" Strong exposure in Data Management, Governance and Controls functions
"" Experience in data analysis, validation and reporting is a plus.
"" Ability to present complex information in an understandable and compelling mannr
Either skills or additional skills are required
Skills:
Category
Name
Required
Importance
Level
Last Used
Experience
No items to display.
Additional Skills: Role
the Data Engineer will be part of the core group responsible for the end to end data management on the Big Data platform. The Data Engineer will be responsible for data ingestion, data validation, code development on various big data technologies, data analysis troubleshooting and user interactions working in an agile team.

Responsibilities
"" Effectively partner with the Data and Analytics team to understand their data requirements, work on data ingestion, analysis, validation, transformation etc.
"" Partner closely with various analytics projects to provide value from a data engineering perspective.
"" Provide support for development work supporting the ingestion framework, SQL and anything else needed for generating validations.
"" Support & monitor various data feeds as required for the project.
"" Experience in working in Agile/Scrum framework and participate in all ceremonies and deliver
"" Apply quality assurance best practices to all work products

Remarks:"
Data Engineer,"Overview

Penn Interactive (PI) is a real-money interactive gaming company headquartered in Philadelphia. As the digital arm to Penn National Gaming (NASDAQ: PENN), the largest regional casino operator in the U.S., we are poised for fast-paced growth in the sports betting and online casino space and are looking for a Backend Engineer to join our expanding Sportsbook team!

The Senior Data Engineer works closely with the Director of Engineering in a small, cross-functional team to develop a one-of-a-kind, native sports betting experience. Successful candidates for this role will leverage data from the largest casino chain in the United States to assist in tasks such as affinity, personalization, bonusing, promotions, etc. Previous work in the gaming industry is not important but we do expect you to take the lead in developing a data warehouse and pipeline to enable novel and rewarding data discoveries.

Your daily responsibilities include
Design a big data stack and data processing infrastructure and platform
Architect and rearchitect multi-tenant databases to meet the needs of our enormous customer base
Improve data validation and data quality monitoring
Work with client and backend teams to ensure appropriate logging
Optimize and tune the databases to improve performance and reduce cost
To be successful in this position it will require the following skill set
Minimum 5+ years of experience building large scale, cost effective and robust data processing platforms
Strong experience in database schema design and data modeling for analytics purposes
Experience with Hadoop, Redshift, Pentaho, MySQL, etc.
Experience with AWS and/or Azure cloud
Good understanding of data security and encryption
Ability to work effectively as part of a small team, with strong interpersonal and communication skills
BONUS POINTS
BSc or MSc in Computer Science or another STEM field
A passion for sports or betting
Familiarity with Kotlin
Something to leave you with

Penn Interactive is committed to helping our team members live their best, healthy life. We offer unique and competitive benefits that help our employees, through a private exchange which allows our team members the ability to choose from several coverage levels and insurance carriers (both local and national carriers). Along with medical, prescription, dental, and vision coverage, there are also voluntary plans available to employees. PI also offers our employees office perks such as free catered lunches, snacks, and beverages in the office."
Data Engineer,"Company Description

CapTech is a team of master builders, creators, and problem solvers who help clients grow efficient, successful businesses. We unite diverse skills and perspectives to transform how data, systems, and ingenuity enable each client to advance what’s possible in a changing world.

As perceptive partners, our U.S-based consultants find inspiration in the unknown and enjoy getting our hands dirty solving our clients’ myriad of challenges. Across industries and business goals, we fuse technical depth and analytical prowess with creative savvy to move clients forward. This drive helps each organization use technology, management, and insight to turn ideas into action. Together, we create outcomes that exceed the expected — which is one of the reasons we’ve been on the Inc. 500/5000 list for over a decade.

Job Description

Data Engineers work in our Data & Analytics practice area at CapTech. Our DE consultants deliver Data, Pipeline, and Integration and solutions and services to our clients across many industries in support of crucial and diverse data strategies. Our consultants work to bridge the ever-evolving gap between value and technology for our clients. Through our core values of intellectual curiosity, enthusiasm, flexibility, servant leadership, and serving as trusted advisors, CapTech seeks to maintain the premier workplace and destination for technology consultants.

Specific responsibilities for the Senior Data Engineer position include:
Develop data solutions using data storage, integration, and pipeline platforms such as HDFS, Spark, Hive, Impala, Cassandra, Informatica, Ab Initio, SQL Server, Oracle, Python, and Kafka
Design/develop data models and consolidate data into them of various types/formats from various sources
Optimize data solutions for the right mix of performance, reliability, and maintainability
Collaborate with Quality Assurance resources and systems administrators to debug code and ensure the timely delivery of products
Prepare documentation conveying design and support information
Convey complex technical information in a clear, concise manner to all levels of stakeholders, including executives, business users, IT, application developers, and customers
Participate in requirements gathering sessions with business and technical staff
Fully understand clients’ business philosophies and IT Strategies. Recommend process improvements to increase efficiency and reliability of data solutions
Qualifications

Specific qualifications for the Senior Data Engineer, Analytics position include:
Bachelor's Degree in Computer Science, MIS, or equivalent combination of education and experience preferred.
Strong SQL and Python skills
Experience optimizing data solutions for strong performance
Development experience with Hadoop and related platforms
Development experience with database platforms (Redshift, Snowflake, MS or Azure SQL, Teradata, Oracle, etc.)
Development experience building ETL graphs using the Ab Initio GDE, EME and Co-Operating system
Preferred - development experience with programming languages
Preferred - development experience with Unix tools and shell scripts preferred
Minimum of 5 years’ experience designing, developing, and testing software aligned with defined requirements
Experience with version control (Git, TFS, JIRA, etc.) and test driven development
Exposure to Business Intelligence tools such as Tableau, Power BI, Qlik, Domo, Birst, Business Objects, SSRS, Cognos, MicroStrategy, etc.
Additional Information

We offer challenging and impactful jobs with professional career paths. All CapTechers can keep their hands-on technology no matter what position they hold. Our employees find their work exciting and rewarding in a culture filled with opportunities to have fun along the way.

At CapTech we offer a competitive and comprehensive benefits package including, but not limited to:
Competitive salary with performance-based bonus opportunities
Single and Family Health Insurance plans, including Dental coverage
Short-Term and Long-Term disability
Matching 401(k)
Competitive Paid Time Off
Training and Certification opportunities eligible for expense reimbursement
Team building and social activities
Mentor program to help you develop your career
CapTech is an equal opportunity employer committed to fostering a culture of equality, inclusion and fairness — each foundational to our core values. We strive to create a diverse environment where each employee is encouraged to bring their unique ideas, backgrounds and experiences to the workplace.

Candidates must be eligible to work in the U.S. for any employer directly (we are not open to contract or “corp to corp” agreements). At this time, CapTech cannot transfer nor sponsor a work visa for this position. Applicants must be authorized to work directly for any employer in the United States without visa sponsorship.

CapTech is a Drug-Free work place.
Candidates must have the ability to work at CapTech’s client locations.
All positions include the possibility of travel.
CapTech has not contracted/does not contract with any outside vendors in its recruitment process. If you are interested in this position, please apply to CapTech directly."
Data Engineer,"OverviewBaker Tilly Virchow Krause, LLP (Baker Tilly) is a leading advisory, tax and assurance firm whose specialized professionals guide clients through an ever-changing business world, helping them win now and anticipate tomorrow. Headquartered in Chicago, Baker Tilly, and its affiliated entities, have operations in North America, South America, Europe, Asia and Australia. Baker Tilly is an independent member of Baker Tilly International, a worldwide network of independent accounting and business advisory firms in 145 territories, with 34,700 professionals. The combined worldwide revenue of independent member firms is $3.6 billion. Visit bakertilly.com or join the conversation on LinkedIn, Facebook and Twitter.It's an exciting time to join Baker Tilly!Baker Tilly's principles of integrity, passion and stewardship define us as an organization and an employer. With offices consistently earning 'best place to work' honors and as a top-ranked firm, Baker Tilly recognizes that our approach, strategy and culture are driven by our people. Their focus and commitment have been fundamental in getting us to where we are today and where we will go in the future. Based on the growth we are planning, Baker Tilly is actively recruiting bright, talented individuals who have a passion to succeed.Due to the continued growth of our consulting practice, we are currently recruiting for a Senior Data Engineer to join our team. As a part of Baker Tilly Consulting, you will find that our global brand and entrepreneurial environment will give you the support you need to apply your industry and technical experience to build your career across a wide range of services to meet our client's most important needs. As a member of our team, you will also contribute to some of the most important activities in our firm which include operating and growing the business, serving the client, developing the best people, and shaping our culture.Baker Tilly Annual Report 2018ResponsibilitiesAs we continue to navigate the impact of COVID-19, you may encounter a longer job application, interview experience and/or a deferred offer of employment for this position.* Create and maintain optimal data pipeline architecture* Assemble large, complex data sets that meet functional / non-functional business requirements* Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.* Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS 'big data' technologies* Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics* Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs* Keep our data separated and secure across national boundaries through multiple data centers and AWS regions* Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader* Work with data and analytics experts to strive for greater functionality in our data systemsQualifications* Bachelor's degree in Computer Science, Statistics, Informatics, Information Systems or another related field* Minimum of five (5) years of experience in a Data Engineering* Advanced working SQL knowledge and experience working with relational databases* Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.* Experience with big data tools: Hadoop, Spark, Kafka, etc.* Query authoring (SQL) as well as working familiarity with a variety of databases* Experience building and optimizing 'big data' data pipelines, architectures and data sets* Deep experience with AWS cloud services: EC2, EMR, RDS, Redshift* Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement* Strong analytic skills related to working with unstructured datasets* Build processes supporting data transformation, data structures, metadata, dependency and workload management* Experience with relational SQL and NoSQL databases, including PostgreSQL and Cassandra* A successful history of manipulating, processing and extracting value from large disconnected datasets* Working knowledge of message queuing, stream processing and highly scalable 'big data' data stores* Experience with stream-processing systems: Storm, Spark-Streaming, etc.* Strong project management and organizational skills required* Experience supporting and working with cross-functional teams in a dynamic environment* Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow preferred"
Data Engineer,"Company DescriptionCapTech is a team of master builders, creators, and problem solvers who help clients grow efficient, successful businesses. We unite diverse skills and perspectives to transform how data, systems, and ingenuity enable each client to advance what's possible in a changing world.As perceptive partners, our U.S-based consultants find inspiration in the unknown and enjoy getting our hands dirty solving our clients' myriad of challenges. Across industries and business goals, we fuse technical depth and analytical prowess with creative savvy to move clients forward. This drive helps each organization use technology, management, and insight to turn ideas into action. Together, we create outcomes that exceed the expected - which is one of the reasons we've been on the Inc. 500/5000 list for over a decade.Job DescriptionData Engineers work in our Data & Analytics practice area at CapTech. Our DE consultants deliver Data, Pipeline, and Integration and solutions and services to our clients across many industries in support of crucial and diverse data strategies. Our consultants work to bridge the ever-evolving gap between value and technology for our clients. Through our core values of intellectual curiosity, enthusiasm, flexibility, servant leadership, and serving as trusted advisors, CapTech seeks to maintain the premier workplace and destination for technology consultants.Specific responsibilities for the Senior Data Engineer position include:* Develop data solutions using data storage, integration, and pipeline platforms such as HDFS, Spark, Hive, Impala, Cassandra, Informatica, Ab Initio, SQL Server, Oracle, Python, and Kafka* Design/develop data models and consolidate data into them of various types/formats from various sources* Optimize data solutions for the right mix of performance, reliability, and maintainability* Collaborate with Quality Assurance resources and systems administrators to debug code and ensure the timely delivery of products* Prepare documentation conveying design and support information* Convey complex technical information in a clear, concise manner to all levels of stakeholders, including executives, business users, IT, application developers, and customers* Participate in requirements gathering sessions with business and technical staff* Fully understand clients' business philosophies and IT Strategies. Recommend process improvements to increase efficiency and reliability of data solutionsQualificationsSpecific qualifications for the Senior Data Engineer, Analytics position include:* Bachelor's Degree in Computer Science, MIS, or equivalent combination of education and experience preferred.* Strong SQL and Python skills* Experience optimizing data solutions for strong performance* Development experience with Hadoop and related platforms* Development experience with database platforms (Redshift, Snowflake, MS or Azure SQL, Teradata, Oracle, etc.)* Development experience building ETL graphs using the Ab Initio GDE, EME and Co-Operating system* Preferred - development experience with programming languages* Preferred - development experience with Unix tools and shell scripts preferred* Minimum of 5 years' experience designing, developing, and testing software aligned with defined requirements* Experience with version control (Git, TFS, JIRA, etc.) and test driven development* Exposure to Business Intelligence tools such as Tableau, Power BI, Qlik, Domo, Birst, Business Objects, SSRS, Cognos, MicroStrategy, etc.Additional InformationWe offer challenging and impactful jobs with professional career paths. All CapTechers can keep their hands-on technology no matter what position they hold. Our employees find their work exciting and rewarding in a culture filled with opportunities to have fun along the way.At CapTech we offer a competitive and comprehensive benefits package including, but not limited to:* Competitive salary with performance-based bonus opportunities* Single and Family Health Insurance plans, including Dental coverage* Short-Term and Long-Term disability* Matching 401(k)* Competitive Paid Time Off* Training and Certification opportunities eligible for expense reimbursement* Team building and social activities* Mentor program to help you develop your careerCapTech is an equal opportunity employer committed to fostering a culture of equality, inclusion and fairness - each foundational to our core values. We strive to create a diverse environment where each employee is encouraged to bring their unique ideas, backgrounds and experiences to the workplace.Candidates must be eligible to work in the U.S. for any employer directly (we are not open to contract or ""corp to corp"" agreements). At this time, CapTech cannot transfer nor sponsor a work visa for this position. Applicants must be authorized to work directly for any employer in the United States without visa sponsorship.CapTech is a Drug-Free work place.Candidates must have the ability to work at CapTech's client locations.All positions include the possibility of travel.CapTech has not contracted/does not contract with any outside vendors in its recruitment process. If you are interested in this position, please apply to CapTech directly."
Data Engineer,"§ Work with the Data Engineers in creating the ingestion metadata for processing from Source to Raw-Composed.
§ Do integration testing for the ingestion pipelines, analyze and resolve data discrepancy issues between sources to compose which are not related to Ingestion Framework Code.
§ Validate, Test and provide improvement suggestions on the Data Processing and Data Lineage capture framework.
§ Work with the business users to identify and resolve the data/recon issues identified in the Raw Composed layer tables.
§ Analysis of Users, Systems Entitlement, Data Classifications, PII data elements identification
§ Create IAM Roles and Policies relevant to newly added sources and tables based on entitlements
§ Update S3 bucket policies relevant to newly added sources/tables"
Data Engineer,"Req ID: 71635BR

Job Description

Leads and participates in the design, built and management of large scale data structures and pipelines and efficient Extract/Load/Transform (ETL) workflows.

Fundamental Components included but are not limited to:

+ Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs.

+ Writes ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processing.

+ Collaborates with International data/reporting and business teams to transform data and integrate algorithms and models into automated processes.

+ Uses knowledge in data transformation and storage experience designing & optimizing queries to build data pipelines.

+ Uses strong programming skills in SAS, Python, Java or any of the major languages to build robust data pipelines and dynamic systems.

+ Builds data marts and data models to support internal customers.

+ Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards.

+ Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions.

+ Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case.

Qualifications Requirements and Preferences:

+ Strong problem solving skills and critical thinking ability.

+ Strong collaboration and communication skills within and across teams.

+ 5 or more years of progressively complex related experience.

+ Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources.

+ Ability to understand complex systems and solve challenging analytical problems.

+ Experience with bash shell scripts, UNIX utilities & UNIX Commands. Knowledge in Java, Python, Hive, Cassandra, Pig, MySQL or NoSQL or similar.

+ Experience building data transformation and processing solutions.

+ Has strong knowledge of large scale search applications and building high volume data pipelines.

+ Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline.

Benefit Eligibility

Benefit eligibility may vary by position.

Job Function: Data & Analytics

Aetna is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or protected Veterans status."
Data Engineer,"Job Title: Data Engineer Location: Wilmington, DE 19801 Duration: 6 months (Possibility of Extension) Bill Rate Range: $85-92 (Close candidates depending on experience) Work Authorization: Preferably USC/GC however open for all visas. C2C will also be considered with proper verification and video call Interview: Phone/Video (Candidate should be able come to nearby Collabera office for phone/video interview. We can look into exception for genuine cases) We can work with both W2/C2C will work (Attached is Tier 1 & 2 list of C2C vendor we can work) Ãâ"" If we need to work with anyone outside this list (Prefer to Exception below) Skills Required: ÃÂ Data Analysis ÃÂ SQL ÃÂ Python AWS Regards Vishal Choudhary Associate Delivery Manager T: (973) 841-2279 How am I doing? GiveÂfeedback at ourÂCenter of Business ExcellenceÂor call 1-866-398-6484 Privacy/Confidentiality From: Vishal Choudhary Sent: Thursday, January 16, 2020 9:39 AM To: avinash@tekpyramids.com Subject: Capital One Bulk Recs Software Engineers: LOB- Financial Services Manager Name- Sriram Srinivasan Security Standard and Applications. Will work on Legacy System & Modification work. Java Spring based application, API, Library APIÃââs Most Application are Built on Spring Boot, Standalone Teams & Tech Lead they are looking. Have to Rewrite Services , AngularJS bit, Kafka Backend-PostgreSQL. 200 not confirmed Yet. AWS is preferred but Strong AWS is mandate. First Week of Jan budget will get Approve. They have Deadline till Q1. Project will go to End of Next year Will hire Juniors, Mid-Level & C2H-Mention eligibility on top before submitting. C2H is preferred. Top 3: Java Basics, Cloud, CI/CD Most are going to be Backend, few with Full Stack Developers. Data Engineers: Spark Streaming, Snowflake Infra ETL & Metadata. SQL knowledge is good to have. Cloud is must Regards Vishal Choudhary Associate Delivery Manager T: (973) 841-2279 How am I doing? GiveÂfeedback at ourÂCenter of Business ExcellenceÂor call 1-866-398-6484 Privacy/Confidentiality"
Data Engineer,"STRATEGIC STAFFING SOLUTIONS HAS AN OPENING!Strategic Staffing Solutions (S3) prides itself on being an international IT Staffing and Solutions Company with 28 years experience recruiting and managing exceptional IT and Business consultants for customers in the Financial, Energy, Public Sector and Retail Distribution industries. A privately held, financially strong, woman-owned company, S3 is a full service IT firm, with 24 major market locations in the U.S!

Job Title: Data Engineer

Duration: 12 Month Contract

Location: Wilmington, DE

W2 ONLY

C2C/1099 NOT ALLOWED

To apply: Please email your resume in Word Format to Bruce Wallace at: bwallace@strategicstaff.com and Reference Job Order #: 159070 or Click the Apply Button.

Strategic Staffing Solutions (S3) seeks a Data Engineer for a 12 Month Contract at one of our top Fortune 500 companies. This position requires that the selected candidate be located in Wilmington, DE

Job Description:

We are looking for a savvy Data Engineer to join our growing team to build a modernized Data

Ecosystem. The hire will be responsible for expanding and optimizing our data and data pipeline

architecture, modernizing our data ecosystem with open source technologies as well as

optimizing data flow and collection for cross functional teams. The ideal candidate is an

experienced data pipeline builder and data wrangler who enjoys optimizing data systems and

building them from the ground up. The Data Engineer will support our software developers,

database architects, data analysts and data scientists on data initiatives and will ensure optimal

data delivery architecture is consistent throughout ongoing projects. They must be self-directed

and comfortable supporting the data needs of multiple teams, systems and products. The right

candidate will be excited by the prospect of optimizing or even re-designing our companys data

architecture to support our next generation of products and data initiatives.

Qualifications:

• We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has

attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems

or another quantitative field.

• Experience with big data tools: Hadoop, Apache Spark, Kafka, etc.
• Experience with relational SQL, Snowflake and NoSQL databases, including

Postgres and Cassandra.

• Experience with data pipeline and workflow management tools: NiFi, Kylo, Luigi,

Airflow, Azkaban etc.

• Experience with AWS cloud services: S3, EC2, EMR, RDS, Redshift
• Experience with stream-processing systems: Storm, Spark-Streaming, etc.
• Experience with object-oriented/object function scripting languages: Python,

Java, C++, PySpark, Scala, etc.

WE OFFER A REFERRAL INCENTIVE FOR ANYONE REFERRED & HIRED WITH S3!

Strategic Staffing Solutions (S3), based in Detroit, Michigan, prides itself on being an international, woman-owned, $260 million IT and Business Services Corporation. We are ranked 16th among the largest staffing firms in the US by Staffing Industry Report and are one of five companies nationally certified as a Charter Partner with Staffing Industry Analysts. S3 provides IT consulting, customized project solutions, vendor management programs and executive search services to financial institutions, insurance, energy, oil/gas, telecommunication, government, retail and health care industries worldwide. We have more than 2,700 consultants and 31 offices in the US and Europe.S3 is also proud to be nationally recognized as both a Military Friendly and Military Spouse Friendly Employer.

As an S3 employee, youre eligible for a full benefits package which may include: Medical Insurance, Dental Insurance, Vision Insurance, 401(k) Plan, Vacation Package, Life & Disability Insurance Plans, Flexible Spending Accounts, and Tuition Reimbursement.

The global mission of S3 is to build trusting relationships and deliver solutions that positively impact our customers, our consultants, and our communities. The four pillars of our company are to: Set the bar high for what a company should do, Create jobs, Offer people an opportunity to succeed and change their station in life, and improve the communities where we live and work through volunteering and charitable giving.

Strategic Staffing Solutions is an Equal Opportunity Employer

D1-JK*

Job Requirements:"
Data Engineer,"Startup in Center City focused on tackling a newly supported industry with few competitors. Legislation has opened up a large sector of the market doing billions a year in transactions. This platform has to be very high traffic, consumer friendly and run like a trading site. The team is hiring on their 2nd Data Engineer and is looking for a senior member. The ideal candidate has strong data pipeline skills, data warehousing and backend logic skills, AWS experience with any data specific services and strong Python skills. The team is very collaborative and culture driven. If you've ever said ""I don't want to be the smartest person in the room"" this is a great place. No ego's here and lots of exciting problems to solve.Desired Skills & Experience* 5+ years in general engineering* 2+ years in a data engineering role specifically (or where that was a large portion of your time)* Experience with Spark/PySpark* Python* AWSWhat You Will Be DoingDaily Responsibilities* 60% Hands On* 20% Architecture* 20% Product/Company workThe Offer* Competitive Salary: Up to $140K/year, DOEYou will receive the following benefits:* Medical Insurance plans offered with low deductible and premium* 401(k) with 3% match* 18 days PTO, 5 sick days* Fast growth track at this stageApplicants must be currently authorized to work in the United States on a full-time basis now and in the future.Workbridge Associates, part of the Motion Recruitment network, provides IT Staffing Solutions (Contract, Contract-to-Hire, and Direct Hire) in major North American markets. Our unique expertise in today's highest demand tech skill sets, paired with our deep networks and knowledge of our local technology markets, results in an exemplary track record with candidates and clients."
Data Engineer,"Senior Data Engineer

Senior Spark Engineer - Senior Data Engineer

At Berkadia we are redefining the industry with long-term investments in people and technology that deliver actionable insights and create the best customer experience. From acquisition to disposition, we service all commercial property types including multifamily, retail, office, industrial, affordable housing, seniors housing, hotels, student housing, and manufactured housing.

The Opportunity

As a member of the Data Pipeline team youll be at the center of building our platform that enables us to redefine the Commercial Real Estate industry. Were building a data pipeline that merges the tested design of data warehousing and the latest big data technologies. You will be designing and building data projects while securing core data elements.

Responsibilities

· Research new technology and share knowledge with team and peers

· Design, implement and release data applications in AWS using big data technologies

· Coordinate development efforts across the organization

· Influence team in development standards and processes

· Teach and mentor peers

· Other duties as assigned

Qualifications

· 2-4 years of hands on Apache Spark experience (PySpark preferred)

· 7-10 years of hands on Business Intelligence / Data Warehousing experience

· 1-2 years of experience building high-performance algorithms in scalable languages such as Scala, Python and R

Essential Duties
Will hold critical role in creating data pipelines using Spark and other technologies
Work closely with Platform Architects to achieve going from an Event Source data model, into Spark and out to SQL.
Other tasks as assigned.
What someone will need to be successful in this role
Excellent interpersonal, verbal and written communication skills
Experience mentoring junior engineers and performing code reviews
Strong logical, analytical, problem solving and reporting skills
Familiarity with Agile principles and methodologies
AWS Elastic Map Reduce (EMR) experience
Able to use version control (git) and other build, packaging & release management tools
Passionate about developing quality products
Employee Benefits
18 PTO days + 2 floating holidays & 10 paid holidays per year
Generous tuition reimbursement towards a Masters or Bachelors degree
401K match up to 6%
12 weeks of 100% paid paternity/maternity leave
Mentorship with industry professionals
About us

Berkadia, a joint venture of Berkshire Hathaway and Jefferies Financial Group, is a leader in the commercial real estate industry, offering a robust suite of services to our multifamily and commercial property clients. Powered by deep relationships and industry-changing technology, our people sell, finance, and service commercial real estate, providing support for the entire life cycle of our clients assets. Our unique ownership structure allows us to put the clients interests first and creates a marketplace that delivers a superior experience.

Berkadia is an equal opportunity employer and affords equal opportunity to all applicants and employees for all positions without regard to race, color, religion, gender, national origin, age, disability, veteran status or any other status protected under local, state or federal laws.

Berkadia does not share salary ranges in its job postings. Any salary-related information you see posted externally has not been provided or verified by Berkadia and may not be accurate.

Berkadia does not accept unsolicited resumes from search firms. All resumes submitted by search firms to Berkadia or any of its employees via-email, the internet or direct communication without a valid written Berkadia-approved search agreement will be deemed the sole property of Berkadia, and no fee will be paid in the event the candidate is hired by Berkadia."
Data Engineer,"Job Description
Tiger Analytics is a fast-growing advanced analytics consulting firm. Our consultants bring deep expertise in Data Science, Machine Learning and AI. We are the trusted analytics partner for several Fortune 100 companies, enabling them to generate business value from data. Our business value and leadership has been recognized by various market research firms, including Forrester and Gartner.

We are looking for top-notch talent as we continue to build the best analytics global consulting team in the world.We are looking for talented individuals to join our team in building the core components of our software platforms use machine learning and artificial intelligence. In this role, we are seeking an experienced Data Engineer. The ideal candidate needs to be passionate about their work not just in words, but in demonstrable actions, both contributing and leading development of select aspects of our platforms and solutions. Successful candidates for this position must have experience designing and building production-level data pipelines. The candidate should be able to :
Create / enhance data pipelines, including the creation, enhancement and automation of advanced ETL flows and processes.
Collaborate in the design, development, test and maintenance of scalable data management solutions.
Assist other teams with data analysis (i.e. feature extraction) where appropriate and applicable.
Provide commercial quality software processes in a professional and timely manner.
Author clear technical documentation.
Accurately scope work and perform to agreed times lines.
Requirements
3+ years of experience with Python Programming Language.
Bachelors Degree in Computer Science or related field.
Hands-on experience in data modeling.
Develop applications and services that run on a cloud infrastructure (Private and Public).
Experience working in a Linux/Unix environment. Note that the majority of the work will be within the Azure infrastructure.
Knowledge and experience in ETL tools and automating data processing workflows.
Good level of understanding of data warehousing, business intelligence, and application data integration solutions.
Working knowledge of Apache Spark.
Knowledge of Azure Databricks.
Ability to thrive in a fast-paced, ever changing environment.
Excellent problem-solving skills and troubleshooting issues in a large, complex environment.
Experience with container management and deployment, such as Docker and Kubernetes.
Experience in data wrangling software as an analyst.
Excellent communication skills, both verbal and written. This includes the ability to write technical documentation, user guides, etc. as appropriate
Benefits

Significant career development opportunities exist as the company grows. The position offers a unique opportunity to be part of a small, challenging, and entrepreneurial environment, with a high degree of individual responsibility."
Data Engineer,"Big Data Engineer
Location: King of Prussia, PA
Length: 1+ yearÂ
Â
Interview mode: Video/Skype
Â
Would be Remote until the Current Pandemic (Covid â 19) is settled
Â
Details
- Scala development
- Python scripting
- Experience with Hive
- AWS experience (DynamoDB, S3, Lambda, Kinesis, EMR, etc.)"
Data Engineer,"Job Title Big Data Engineer Location Wilmington, DE, USA Job Description BSBA degree or with 8+ years of experience Advanced knowledge of application, data and infrastructure architecture disciplines Understanding of architecture and design across all systems Working proficiency in developmental toolsets Ability to collaborate with high-performing teams and individuals throughout the firm to accomplish common goals Strong hands on experience in JavaJ2EE and Spark Experience in building Big Data technologies and utilities is required (Sprak, Hadoop, Hive), building applications using Spark frameworks. Understanding of AWS and at least hands on experience working in projects with AWS. Knowledge of the software development life cycle, agile methodologies, and test-driven development. Sound understanding of continuous integration continuous deployment environments. Experience in connecting and building application program interfaces (APIs) or messaging software and interoperability techniques and standards Strong analytical skills with a passion for testing. Excellent problem solving and debugging. Skills Exposure in Data Management, Governance and Controls functions a plus Technical and quantitative reasoning skills with regard to marketing data is a plus. Thanks Regards Gaurav Dhuware Sr. Technical Recruiter (732) 200-1307 Ext 5089 Gauravdaptask.com mailtoGauravdaptask.com Address 120 Wood Ave South, Iselin, NJ 08830"
Data Engineer,"Title Big Data Engineer Location Wilmington, DE GC GC-EAD Job Description middot BSBA degree or with 8+ years of experience middot Advanced knowledge of application, data and infrastructure architecture disciplines middot Understanding of architecture and design across all systems middot Working proficiency in developmental toolsets middot Ability to collaborate with high-performing teams and individuals throughout the firm to accomplish common goals middot Strong hands on experience in JavaJ2EE and Spark middot Experience in building Big Data technologies and utilities is required (Sprak, Hadoop, Hive), building applications using Spark frameworks. middot Understanding of AWS and at least hands on experience working in projects with AWS. Knowledge of the software development life cycle, agile methodologies, and test-driven development. middot Sound understanding of continuous integration continuous deployment environments. Experience in connecting and building application program interfaces (APIs) or messaging software and interoperability techniques and standards middot Strong analytical skills with a passion for testing. Excellent problem solving and debugging skills middot Exposure in Data Management, Governance and Controls functions a plus middot Technical and quantitative reasoning skills with regard to marketing data is a plus. Share Resume Raviomegasolutioninc.com"
Data Engineer,"Senior Data Engineer
Senior or Lead Data Engineer - Python, data - Philadelphia!

We are seeking a smart and engaged Senior Data Engineer to help us build our platform solutions addressing problems in the insurance space. We are developing a platform that supports data aggregation for our partner organizations to efficiently and effectively campaign for the increased quality and quantity of service offered in this country. Our current team is growing and needs to add a Data Engineer to help us build a data processing pipeline.

Ideally, this person will be passionate about data and engineering - Python, SQL, and AWS - and will have 5+ years of engineering experience to date.

What You Need for this Position
Python
AWS
SQL
deep data engineering and platform build outs
mentoring / lead experience
What You Will Be Doing
Developing our cloud based data warehouse.
Data processing pipeline development using Python and SQL.
New database solutions architecture.
Strategic planning and external client interaction.
This is ideal for a senior or lead level engineer - so ideally we're seeking someone with strong mentoring experience, interest, and enthusiasm.

What's In It for You
An opportunity to join a very smart, motivated team and to directly impact the product portfolio!
Excellent benefits and vacation policy.
Growth potential and a great working environment.
If you are a Senior or Lead Data Engineer looking to be on a collaborative team at an established organization, please apply today!
Applicants must be authorized to work in the U.S.

CyberCoders, Inc is proud to be an Equal Opportunity Employer
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.

Your Right to Work In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire."
Data Engineer,"Immediate need for a talented Big Data Engineerwith experience in the IT Industry. This is a 8+ Months contract opportunity with long-term potential and is located in Wilmington, DE. Please review the job description below.

Job ID: 20-09894

Note: ** This opportunity is REMOTE until COVID-19 crisis setttles down**
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have working experience using the following software/tools:
Experience with big data tools: Hadoop, Apache Spark, Kafka, etc.
Experience with relational SQL, Snowflake and NoSQL databases, including Postgres and Cassandra.
Experience with AWS cloud services: S3, EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, C++, PySpark, Scala, etc.
Qualifications for Data Engineer
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing 'big data' data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment
Our client is a leading Industry Type and we are currently interviewing to fill this and other similar contract positions. If you are interested in this position, please apply online for immediate consideration.

#san"
Data Engineer,"Group O is seeking a strong candidate with advanced analytics experience to fill an exciting Data Engineer role for an AI partner to Fortune 500 companies. In this role, you will be a valuable in managing large amounts of data and implement loading disparate data sets while managing the technical communication between the team and client.

Implementation including loading from disparate data sets, preprocessing using Hive and Pig.
Manage the technical communication between the team and client
Work with big data team to deliver cutting edge solutions

2-5 years of demonstrable experience designing technological solutions to complex data problems, developing & testing modular, reusable, efficient and scalable code to implement those solutions.
Ideally, this would include work on the following technologies:
Proficiency in at least one of the following: R, C++ or Python (preferred). Scala knowledge a strong advantage.
Strong understanding and experience in distributed computing frameworks, particularly Apache Hadoop 2.0 (YARN; MR & HDFS) and associated technologies -- one or more of Hive, Sqoop, Avro, Flume, Oozie, Zookeeper, etc..
Hands-on experience with Apache Spark and its components (Streaming, SQL, MLLib) is a strong advantage.
Operating knowledge of cloud computing platforms (AWS, especially EMR, EC2, S3, SWF services and the AWS CLI)
Experience working within a Linux computing environment, and use of command line tools including knowledge of shell/Python scripting for automating common tasks
In addition, the ideal candidate would have great problem-solving skills, and the ability & confidence to hack their way out of tight corners.
Education:
Bachelor's degree in Computer Science or related technical degree"
Data Engineer,"Job Summary As part of Damanrsquos Data Engineering team, you will be architecting and delivering highly scalable, high performance data integration and transformation platforms. The solutions you will work on will include cloud, hybrid and legacy environments that will require a broad and deep stack of data engineering skills. You will be using core cloud data warehouse tools, hadoop, spark, events streaming platforms and other data management related technologies. You will also engage in requirements and solution concept development, requiring strong analytic and communication skills. Responsibilities Function as the solution lead for building the data pipelines to support the development enablement of Information Supply Chains within our client organizations ndash this could include building (1) data provisioning frameworks, (2) data integration into data warehouse, data marts and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores. Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging and integration. Develop overall design and determine division of labor across various architectural components Deploy and customize Daman Standard Architecture components Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions Provide feedback and enhance Daman intellectual property related to data management technology deployments Assist in development of task plans including schedule and effort estimation Skills and Qualifications Bachelorrsquos Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required Experience building high-performance, and scalable distributed systems Good experience in migration from Netezza DB2 to Snowflake AWS cloud experience (EC2, S3, Lambda, EMR, RDS, Redshift) Experience in ETL and ELT workflow management Familiarity with AWS Data and Analytics technologies such as Glue, Athena, Spectrum, Data Pipeline Experience building internal cloud to cloud integrations is ideal Experience with streaming related technologies ex Spark streaming or other message brokers like Kafka is a plus 3+ years of Data Management Experience 3+ years of batch ETL tool experience (DataStage Informatica Talend) 3+ yearsrsquo experience developing, deploying and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing) 2+ yearsrsquo experience with Hadoop Ecosystem (HDFSS3, Hive, Spark) 2+ yearsrsquo experience in a software engineering, leveraging Java, Python, Scala, etc. 2+ yearsrsquo advanced distributed schema and SQL development skills including partitioning for performance of ingestion and consumption patterns 2+ yearsrsquo experience with distributed NoSQL databases (Apache Cassandra, Graph databases, Document Store databases) Experience in the financial services, banking and or Insurance industries is a nice to have Daman Is an Equal Opportunity Employer and All Qualified Applicants Will Receive Consideration for Employment Without Regard to Race, Color, Religion, Sex, National Origin, Disability Status, Protected Veteran Status, Or Any Other Characteristic Protected by Law."
Data Engineer,"Our direct end client in San Antonio, Texas is looking for 2 Data engineers for contract position. The candidate should be a data engineer who has build pipelines to ingest data into data lakes, understandanalyze data, script (SQL, UNIX), understand business process. A coder vs user of traditional ETL tools like Informatica. Soft skills are critical with this role - the manager is looking for a resource who is a problem solver, not just take direction. They need someone who can work independently, focus on the learning the data to relate it to the requirements. Key technical skills Big Data tools experience with SparkHiveImpala Datawarehouse experience with TeradatanetezzaGreenplum MPP Databases OR Hadoop Data Lake Experience Cloud Experience - AWS, Azure, or Google is a nice-to-have, but not required Build Data PipeLineELTETL using JAVA or Python or Linux scripting Advanced SQL development Experience The resource should be willing to relocate to San Antonio Job Summary Data Engineer. Responsible for developing and implementing data models for multiple business processes on client s data warehouse and big data analytics infrastructure within the Medical Solutions Division. This particular role will focus on developing datamarts and datasets in conjunction with analysts and data scientists. Principle Responsibilities (essential job duties and responsibilities) Design and implement data models, datamarts, and datasets on a modern data warehouse and data hub Interface directly with business and systems subject matter experts to understand analytic needs and determine logical data model requirements Develop and implement ETL processes across Hadoop, BI systems, and databases Work closely with data architects to identify common data requirements and develop shared solutions Develop close collaboration with senior analysts and data owners across multiple business domains Maintain data modeling standards and ETL best practices Support data model and ETL solutions in production Skills and Experiences Strong data warehouse and ETL background Advanced SQL programming capabilities. HiveImpala or Snowflake preferred Strong background in preparing data for analysis and reporting creating analytical datasets and working with others to define simple to use data models (i.e. star schema) Experience with analytical tools for data discovery modeling, visualization, and analysis Success in a highly dynamic technology demand driven environment with ability to shift priorities with agility Ability to go from whiteboard discussion to code Willingness to explore and implement new ideas and technologies Ability to effectively communicate with technical and non-technical audiences Ability to work independently with minimal supervision Minimum Qualifications 8+ years experience with SQL 6+ years experience with data modeling design and implementation 4+ years experience working directly with subject matter experts in both business and technology domains 4+ years experience with BI and analytic tools such as Tableau, Datameer, R, or similar Nice-to-have Hands-on experience with Cloudera or Snowflake Familiarity with Agile methodologies Education Bachelor s in Computer Science, Information Systems, Engineering, science discipline, or similar"
Data Engineer,"*Current employees and contingent workers click** **here** at https://wd5.myworkday.com/iheartmedia/d/task/3005$4482.htmld **to apply and search by the Job Posting Title.**
iHeartMedia
*Job Summary:**
iHeartMedia is the number one audio company in the United States, reaching nine out of 10 Americans every month - and with its quarter of a billion monthly listeners, has a greater reach than any other media company in the U.S. The company's leadership position in audio extends across multiple platforms including 850 live broadcast stations; streaming music, radio and on demand via its iHeartRadio digital service available across more than 250 platforms and 2,000 devices including smart speakers, digital auto dashes, tablets, wearables, smartphones, virtual assistants, TVs and gaming consoles; through its influencers; social; branded iconic live music events; and podcasts as the #1 commercial podcast publisher globally. iHeartMedia also leads the audio industry in analytics and attribution technology for its marketing partners, using data from its massive consumer base.

The Data Engineer will be responsible for developing expanding, testing and/or optimizing the infrastructure and architecture of existing and future data pipelines, as well as optimizing data collection, flow, and delivery for cross-functional teams including software engineers, data scientists, and business partners. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products
*Responsibilities:**
+ Assemble large, complex data sets that meet functional and non-functional business requirements

+ Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources

+ Develop and maintain standards for administration and operation including the scheduling, running, monitoring, logging, error management, failure recovery, and output validation

+ Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.

+ Work with cross functional teams to strive for greater functionality in our data systems, and recommend and implement ways to improve data reliability, efficiency, and quality

+ Collaborate with data scientists to build data products that ingest data from a variety of data sources, process it with sophisticated data science techniques, and produces results that are consumed by business partners for analysis or action

+ Contribute to the project planning process by estimating tasks and deliverables

+ Communicate complex solutions and ideas to a variety of stakeholders (other team members, IT leadership, and business leaders) in easily understandable language

+ Utilize and stay current in programming languages and software technology
*Qualifications:**
+ Bachelor's Degree in Computer Science, Information Technology, Informatics, or Applied Math - Graduate degree desired

+ 5+ years of commercial experience in a data engineer role with a proven record of manipulating, processing and extracting value from large disconnected datasets.

+ Strong understanding of ETL processes

+ Expert-level knowledge of SQL and experience with NoSQL databases

+ Strong analytic skills related to working with unstructured datasets

+ Solid programming skills and expertise in Python

+ Experience working with REST and SOAP APIs

+ Strong project management, organizational, communication, and presentation skills

+ Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement

+ Experience supporting and working with cross-functional teams in a dynamic environment
*Location**
San Antonio, TX: 20880 Stone Oak Parkway, 78258

Position Type

Regular

The Company is an equal opportunity employer and will not tolerate discrimination in employment on the basis of race, color, age, sex, sexual orientation, gender identity or expression, religion, disability, ethnicity, national origin, marital status, protected veteran status, genetic information, or any other legally protected classification or status.

Our organization participates in E-Verify. Click here at http://iheartmediacareers.com/Pages/EEO.aspx to learn about E-Verify.

Current employees and contingent workers click here at https://wd5.myworkday.com/iheartmedia/d/task/3005$1999.htmld to apply and search by the Job Posting Title.

iHeartMedia is the number one audio company in the United States, reaching nine out of 10 Americans every month - we specialize in radio, digital, social, podcasts, influencers, data, and events across the nation and provide premier opportunities for advertisers.

Visit iHeartMedia.com at https://iheartmedia.com/ to learn more about us."
Data Engineer,"Data Engineer

Chargify | San Antonio, TX, USA

Chargify, a Scaleworks portfolio business is seeking an individual that is driven, intensely curious, and smart to join our team of developers, engineers, and data scientists. One day you might work with our front-end development team strategizing how to coordinate APIs and workflows to serve up historical data to customers. The next day you might assist our Operations team in getting our Data Scientists the data they need to compare the effectiveness of pricing strategies. The ideal candidate has a strong software engineering background, but gravitates towards building scalable data products.

Role Responsibilities
Ensure the reliability, efficiency, and scalability of the ETL system
Work closely with the development and data science teams to redesign data warehouse schemas
Perform and automate data preparation for internal consumption upon request
You'll have opportunities to work on high impact projects that improve data availability/quality and provide reliable access to data for the analytics team and the rest of the business
Requirements
Experience with solution building and architecting with AWS
Expertise in data pipeline tools such as Airflow or Luigi
Expertise in SQL, SQL Tuning, schema design
Expertise in Ruby and Python, particularly with ensuring data quality across multiple datasets used for analytic purposes
Experience with database systems including MySQL, Elasticsearch, Postgres, Redshift, etc.
Great communication skills
Working knowledge of Jupyter
Bonus points for:
Bachelor's degree in Computer Science, Computer Engineering, or equivalent field
Elasticsearch expertise
Worked with data infrastructure in a SaaS product
Knowledge of statistical sciences
Have worked with marketing organizations or advertising technologies
Have worked with Data Scientists
No visa sponsorship is available for this position*
We are an equal opportunity employer and do not discriminate against protected characteristics. We guarantee that all candidates will be given the same consideration."
Data Engineer,"H-E-B Digital is seeking new team members (Partners)! Since our inception, we’ve been investing heavily in our customers’ digital experience, reinventing how they find inspiration from food, how they make food decisions, and how they ultimately get food into their homes. This is an exciting time to join H-E-B Digital, and we’re hiring across the stack: front-end web and mobile, full-stack, and backend engineering. We’re using the best available technologies to deliver modern, engaging, reliable, and scalable experiences to meet the needs of our growing audience. Our digital solutions are growing in popularity and adoption—like Curbside and Home Delivery—so you’ll get the opportunity to define the user experience for millions of customers and hundreds of thousands of Partners. If you’re someone who enjoys taking on new challenges, working in a rapidly changing environment, learning new skills, and applying it all to solve large and impactful business problems, we want you as part of our team.

Our Partners thrive The H-E-B Way. In the Data Engineer position, that means you have a…

HEART FOR PEOPLE… you’re willing to communicate with and learn from your manager and your team

HEAD FOR BUSINESS… you can follow technical guidance and understand why it’s important

PASSION FOR RESULTS… you’ll take the initiative to get familiar with technology / the software development process
What you’ll do
Work with HEB Digital teams to provide data solutions for e-commerce, supply chain, store operations, finance, and marketing reporting and analytics platforms
Contribute to existing data platforms and implement new technologies
Develop a deep understanding of HEB’s data and become a domain expert
Ensure data is distributed in a timely and accurate manner
Make data discoverable and accessible to business users

Who You Are
2 years of data engineering experience
Proficient with data technologies (e.g. Spark, Kinesis, Kafka, Airflow, Oracle, PostgreSQL, Redshift, Presto, etc.)
Experienced with designing and developing ETL data pipelines using tools such as Airflow, Nifi, or Kafka.
Strong understanding of SQL and data modeling
Understanding of Linux, Amazon Web Services (or other cloud platforms), Python, Docker, and Kubernetes
Experienced with common software engineering tools (e.g., Git, JIRA, Confluence, or similar)
Bachelor's degree in computer science or comparable field or equivalent experience
A proven understanding and application of computer science fundamentals: data structures, algorithms, design patterns, and data modeling

What are the Perks?
A robust Benefits plan with coverage starting Day One
Dental, vision, life, and other insurance plans; flexible spending accounts; short term / long term disability coverage
Partner Care Team, for any time you have healthcare or coverage questions
Telehealth offers 24/7 access to board-certified doctors by phone
Partner Guidance allows free counselor visits
Funeral leave, jury duty, and military pay (subject to applicable law)
Maternal / paternal leave for new parents, including adoptions
10"" off H-E-B brand products in-store and online
Eligibility to participate in 401(k)
Opportunity to become a “Partner-Owner” after 12 months
Who We Are
H-E-B is one of the largest, independently owned food retailers in the nation, operating over 400 stores throughout Texas and Mexico, with annual sales generating over $25 billion
We hire talented people (109,000 Partners), and give them autonomy to be creative in how they impact the business
We’re a Partner-driven company with a Bold Promise – Because People Matter
We embrace Diversity and Inclusion as core values, and support them with thriving company-wide programs
We’re a truly original Texas-based company that created the Spirit of Giving to help Texas communities every day
Once eligible, our Partners become Owners in the company. “Partner-owned” means our most important resources—People—drive the innovation, growth, and success that make H-E-B The Greatest Retailing Company
04-2019

DASO3232"
Data Engineer,"Job Title Data Engineer Location San Antonio, TX(Remote till Covid-19) Duration Long term Contract Job Description Requirements ndash MUST HAVE all of the bullets below 5+ years Data Engineering experience Big Data tools experience with SparkHiveImpala Must Have Data Warehouse builds with TeradataNetezzaGreenplum OR lsquoHadoop Data Lakersquo experience Building Data PipelinesELTETL using Java or Python or Linux scripting Cloud experience AWS or Azure OR GCP Advanced SQL development Preferred ndash The strongest candidates will have Experience with 1 or more Talend, Ab Initio, Datastage, Informatica Oracle EBS Agile methodology environments BI and Analytics tools Tableau, Datameer, R, or similar Education Qualification Bachelor's Degree in Computer Science, Information Technology (IT), or closely related field. Master s in Computer Science or related degree Preferred. Contact Details Pawan Ph 312-967-6667 Email Pawan.drsrit.com Reliable Software is an Equal Opportunity Employer. Reliable Software does not discriminate on the basis of race, religion, color, sex, gender identity, sexual orientation, age, non-disqualifying physical or mental disability, national origin, veteran status or any other basis covered by appropriate law. All employment is decided on the basis of qualifications, merit, and business need."
Data Engineer,"Great sales are the result of strong purpose, conviction and pride - pride in your ability and your product. UnitedHealth Group offers a portfolio of products that are greatly improving the life of others. Bring along your passion and do your life's best work.(sm)

Primary Responsibilities:
Excellent analytical and problem solving capabilities with special attention to accuracy and detail
Self-starter with a proven ability to take ownership of job responsibilities and ensure successful completion of all projects and requests
Designs, develops, tests, documents and maintains database queries and reports related to-but not limited to - clinical systems data
Works with customers to define and document additional requirements to enrich reporting capabilities
Develops systematic reporting processes and procedures to ensure timely delivery of daily, weekly, monthly, annual and ad hoc reporting to management
Troubleshoots data integrity issues, analyzes data for completeness to meet business needs, and proposes documented solution recommendations
Transfers data into meaningful, professional and easy to understand formats for various audiences
Combines various data sources into a comprehensive understanding of customer behaviors and feedback for service improvement opportunities
Manages reporting and analysis elements on all business initiative projects
Recommends and implements new or modified reporting methods and procedures to improve report content and completeness of information
Troubleshoots and coordinates resolutions to all system issues affecting clinical applications
Creates on-line tools to improve efficiency and effectiveness of staff in their role of servicing our customer
Youll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.

Required Qualifications:
Bachelor's degree in Business, Healthcare Administration, Information Technology or related field required. (4 additional years of comparable work experience beyond the required years of experience may be substituted in lieu of a bachelor?s degree)
Four or more years related experience in a reporting and analytic role
Two or more years of experience in Healthcare or Clinical environment
Proficient in SQL, SQL Reporting Services, , MS Access, and MS Excel
Preferred Qualifications:
MS Visual Studio
SSIS
Ability to effectively prioritize and multi-task in high volume workload situations.

Careers with WellMed. Our focus is simple. We're innovators in preventative health care, striving to change the face of health care for seniors. We're impacting 380,000+ lives, primarily Medicare eligible seniors in Texas and Florida, through primary and multi-specialty clinics, and contracted medical management services. We've joined Optum, part of the UnitedHealth Group family of companies, and our mission is to help the sick become well and to help patients understand and control their health in a lifelong effort at wellness. Our providers and staff are selected for their dedication and focus on preventative, proactive care. For you, that means one incredible team and a singular opportunity to do your life's best work.(sm)

WellMed was founded in 1990 with a vision of being a physician-led company that could change the face of healthcare delivery for seniors. Through the WellMed Care Model, we specialize in helping our patients stay healthy by providing the care they need from doctors who care about them. We partner with multiple Medicare Advantage health plans in Texas and Florida and look forward to continuing growth.

OptumCare is committed to creating an environment where physicians focus on what they do best: care for their patients. To do so, OptumCare provides administrative and business support services to both owned and affiliated medical practices which are part of OptumCare. Each medical practice part and their physician employees have complete authority with regards to all medical decision-making and patient care. OptumCares support services do not interfere with or control the practice of medicine by the medical practices or any of their physicians.

Diversity creates a healthier atmosphere: OptumCare is an Equal Employment Opportunity/Affirmative Action employers and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.

OptumCare is a drug-free workplace. Candidates are required to pass a drug test before beginning employment"
Data Engineer,"Overview Guidehouse is a leading management consulting firm serving the public and commercial markets. We guide our clients forward towards new futures that build trust in society and your professional skills along the journey. Join us at Guidehouse. Responsibilities The Data Engineer will need to demonstrate experience and knowledge of Activity Management, Requirement Identification, and Project Programming Programs or cross-Directorate programs which are still under development and need support to meet the end-state. Develop life-cycle sustainment and risk mitigation strategy recommendations for two FYDP's for each portfolio. Analyze built infrastructure data, aggregate requirements, assesses asset performance predictions from SMS, analyze statistics and trend across portfolios, validate data, perform data management, compile other summarizing reports for each AMP portfolio, and recommend enhancements to processes. Support AMP Enterprise Managers with the development of portfolio-specific SEED's to support Installation Development Plans (IDPs) and the Enterprise Planning Process. Maturation of Requirement Identification by analyzing built infrastructure data, aggregating requirements, assessing asset performance predictions from SMS, analyzing statistics and trends across portfolios, validating data, performing data management, compiling other summarizing reports for each AMP portfolio, and recommending enhancements to process. Assisting in the AFCAMP process, establishment of document templates/tools, logistic support, training, and process refinement recommendations associated with the maturation of the AFCAMP process. Qualifications + BA/BS or MA/MS degree in the field(s) of Data Engineering and Information Technology + 3 to 10 years of related experience in Web Based Application Design, Data Management, Document Management/Repository + Active Public Trust clearance Qualifications - Desired + Air Force Activity Management Experience Additional Requirements The successful candidate must not be subject to employment restrictions from a former employer (such as a non-compete) that would prevent the candidate from performing the job responsibilities as described. Disclaimer Guidehouse is an Equal Employment Opportunity / Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, national origin, ancestry, citizenship status, military status, protected veteran status, religion, creed, physical or mental disability, medical condition, marital status, sex, sexual orientation, gender, gender identity or expression, age, genetic information, or any other basis protected by law, ordinance, or regulation. Guidehouse will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of applicable law or ordinance including the Fair Chance Ordinance of Los Angeles and San Francisco. If you have visited our website for information about employment opportunities, or to apply for a position, and you require an accommodation, please contact Guidehouse Recruiting at 1-571-633-1711 or via email at RecruitingAccommodation@guidehouse.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodation. Guidehouse does not accept unsolicited resumes through or from search firms or staffing agencies. All unsolicited resumes will be considered the property of Guidehouse and Guidehouse will not be obligated to pay a placement fee. Rewards and Benefits Guidehouse offers a comprehensive, total rewards package that includes competitive compensation and a flexible benefits package that reflects our commitment to creating a diverse and supportive workplace. Benefits include: Medical, Rx, Dental & Vision Insurance Personal and Family Sick Time & Company Paid Holidays Parental Leave and Adoption Assistance 401(k) Retirement Plan Basic Life & Supplemental Life Health Savings Account, Dental/Vision & Dependent Care Flexible Spending Accounts Short-Term & Long-Term Disability Tuition Reimbursement, Personal Development & Learning Opportunities Skills Development & Certifications Employee Referral Program Corporate Sponsored Events & Community Outreach Emergency Back-Up Childcare Program"
Data Engineer,"Hi, Data Engineer Work location would be San Antonio once the Covid restrictions are lifted. Long Term Need candidates who are strong in ETL, Python Spark Scala is mandatory. Snowflake not mandatory for this role, good to have. Sincerely, HR Manager nFolks Data Solutions LLC Phone 425-999-4933 email arun(AT)nfolksdata.com"
Data Engineer,"OverviewInterested in working with talented people to help develop innovative solutions to some of society's most complex and challenging problems? We are Guidehouse, a leading consulting firm serving the public sector and commercial clients with specialized capabilities in strategy, technology, and risk management. You may not yet know our name, but we have a rich history. Guidehouse is a combination of PwC's former public sector practice and Navigant's deep expertise in energy, financial services and healthcare.We offer an exciting, fast-paced environment that fosters intellectual growth and rewards individuals based on impact, not tenure. Our firm is at the forefront of an emerging model solving complex problems that stretch across government and private companies, affording our people the opportunity to be on the cutting edge of the consulting profession. By focusing on markets facing transformational change, technology-driven innovation, and significant regulatory pressure, our employees also develop and deploy world class knowledge and problem solving that leads to breakthrough solutions.ResponsibilitiesThe Data Engineer will need to demonstrate experience and knowledge of Activity Management, Requirement Identification, and Project Programming Programs or cross-Directorate programs which are still under development and need support to meet the end-state.Develop life-cycle sustainment and risk mitigation strategy recommendations for two FYDP's for each portfolio.Analyze built infrastructure data, aggregate requirements, assesses asset performance predictions from SMS, analyze statistics and trend across portfolios, validate data, perform data management, compile other summarizing reports for each AMP portfolio, and recommend enhancements to processes.Support AMP Enterprise Managers with the development of portfolio-specific SEED's to support Installation Development Plans (IDPs) and the Enterprise Planning Process. Maturation of Requirement Identification by analyzing built infrastructure data, aggregating requirements, assessing asset performance predictions from SMS, analyzing statistics and trends across portfolios, validating data, performing data management, compiling other summarizing reports for each AMP portfolio, and recommending enhancements to process.Assisting in the AFCAMP process, establishment of document templates/tools, logistic support, training, and process refinement recommendations associated with the maturation of the AFCAMP process.Qualifications* BA/BS or MA/MS degree in the field(s) of Data Engineering and Information Technology* 3 to 10 years of related experience in Web Based Application Design, Data Management, Document Management/Repository* Active Public Trust clearanceQualifications - Desired* Air Force Activity Management ExperienceAdditional RequirementsThe successful candidate must not be subject to employment restrictions from a former employer (such as a non-compete) that would prevent the candidate from performing the job responsibilities as described.DisclaimerGuidehouse is an Equal Employment Opportunity / Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, national origin, ancestry, citizenship status, military status, protected veteran status, religion, creed, physical or mental disability, medical condition, marital status, sex, sexual orientation, gender, gender identity or expression, age, genetic information, or any other basis protected by law, ordinance, or regulation.Guidehouse will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of applicable law or ordinance including the Fair Chance Ordinance of Los Angeles and San Francisco.If you have visited our website for information about employment opportunities, or to apply for a position, and you require an accommodation, please contact Guidehouse Recruiting at 1-571-633-1711 or via email at RecruitingAccommodation@guidehouse.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodation.Guidehouse does not accept unsolicited resumes through or from search firms or staffing agencies. All unsolicited resumes will be considered the property of Guidehouse and Guidehouse will not be obligated to pay a placement fee.Rewards and BenefitsGuidehouse offers a comprehensive, total rewards package that includes competitive compensation and a flexible benefits package that reflects our commitment to creating a diverse and supportive workplace.Benefits include:* Medical, Rx, Dental & Vision Insurance* Personal and Family Sick Time & Company Paid Holidays* Parental Leave and Adoption Assistance* 401(k) Retirement Plan* Basic Life & Supplemental Life* Health Savings Account, Dental/Vision & Dependent Care Flexible Spending Accounts* Short-Term & Long-Term Disability* Tuition Reimbursement, Personal Development & Learning Opportunities* Skills Development & Certifications* Employee Referral Program* Corporate Sponsored Events & Community Outreach* Emergency Back-Up Childcare Program"
Data Engineer,"Ability to coordinate with people of many different types of skillsets including systems engineers, network defenders, network engineers, data scientists and analytics developers. Must be able to identify, analyze, normalize, ingest, and parse structured and unstructured cybersecurity and intelligence data from a wide variety of sources, to include large datasets (over 1TB). Data types include, but are not limited to, network appliance event logs, system logs, domain logs, firewall logs, Zeek logs, audit logs, vulnerability scans, packet capture, STIX formatted messages, PDF/text files, .csv files.

Statement of Work for Data Engineer

Job Responsibilities:

· Utilize various Big Data Platform technologies, to include but not limited to, Elastic/Lucene databasing, Hadoop Distributed File System (HDFS), Kafka, and Gem to prepare various datasets for use in data analytics.

· Work with 35 IS and external support engineers to develop, adapt, modify, and implement data parsers for analytic use in the Big Data Platform and other Air Force CS&D weapon systems.

· Utilize one or more of several coding languages to include Java, Python, and Scala.

· Develop documentation and comprehensive user manuals for all developed projects, to be understandable by the average analyst familiar with BDP.

· Assist in the development and implementation of a data analytics program within the 35 IS.

· Provide support to training development and instruction focusing on CS&D data analytics development.

Knowledge/Skills Ability:

Required: · Be Director of Central Intelligence Directives 6/4 eligible (Top Secret) with a current Single Scope Background Investigation (SSBI)

· Proficiency in one or more big data programming languages, such as R, Python, Scala, or Java.

· Experience working with a hybrid team of analyst, engineers, and developers to conduct research, and build and deploy complex, but easy-to-use analytical platforms.

· Previous experience performing research in Data Analytics or big data.

Minimum Experience/Education:

· Minimum 2 years of recent experience in data engineering

· Programming experience, ideally in Python, Spark, Kafka, or Java

· Experience in data cleaning, wrangling, visualization and reporting

· Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources

· Knowledge of data mining, machine learning, natural language processing, or information retrieval

Highly Desired:

· Education/ Certifications:

·Bachelor’s Degree or more in Computer Science or related field

·DoD 8570 Sec+

· 4+ years of experience in data analytics or quantitative intelligence analysis

· Four (4) years of experience in an intelligence field at a tactical or operational level

· Experience with a DoD Big Data platform is a plus

*BEAT LLC IS AN EQUAL OPPORTUNITY EMPLOYER - DISABILITY AND VETERANS*"
Data Engineer,"Data Engineer (AWS, Snowflake, Batch ETL tool) 12+ Months Phoenix, AZ San Antonio, TX Skills and Qualifications Data Engineer with Bachelorrsquos Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required 1+ year experience with Snowflake database AWS cloud experience (EC2, S3, Lambda, EMR, RDS, Redshift) Experience in ETL and ELT workflow management Familiarity with AWS Data and Analytics technologies such as Glue, Athena, Spectrum, Data Pipeline Experience building internal cloud to cloud integrations is ideal Experience with streaming related technologies ex Spark streaming or other message brokers like Kafka is a plus 3+ years of Data Management Experience 3+ years of batch ETL tool experience (DataStage Informatica Talend) 3+ yearsrsquo experience developing, deploying and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing) 2+ yearsrsquo experience with Hadoop Ecosystem (HDFSS3, Hive, Spark) 2+ yearsrsquo experience in a software engineering, leveraging Java, Python, Scala, etc. 2+ yearsrsquo advanced distributed schema and SQL development skills including partitioning for performance of ingestion and consumption patterns 2+ yearsrsquo experience with distributed NoSQL databases (Apache Cassandra, Graph databases, Document Store databases) Experience in the financial services, banking and or Insurance industries is a nice to have"
Data Engineer,"Job Description
Data Engineer

Location: San Antonio, TX

THIS COMPANY OFFERS…
Fast-pace, ground floor opportunity to make a true impact on the bottom line through Data
Upward mobility, career path that will allow this person to develop as a professional
Robust Bonus Opportunity – high performance equals reward
Stock Options
Remote work opportunity!
YOUR TYPICAL DAY…
Collaborate with business and IT to build a roadmap to extract, transform and load data from various sources
Build data models that will fuel business intelligence to increase data access
Champion opportunities to automate processes for great scalability
Foster a culture of utilizing data to influence decisions across the organization
YOU HAVE…
Demonstrated experience in building a data warehouse from scratch in a cloud environment
Ability to connect with the business and guide the internal customer to the solution they seek
Experience in executing ETL solutions, comfortable and thrives in an environment that requires heads down coding
Schema design and data modeling
Bachelor’ s Degree in Computer Engineering, Mathematics, or related field required
EXTRA CREDIT…
Financial Services experience highly preferred
For a Confidential Conversation and/or Personal Meeting regarding this outstanding career opportunity please contact:

Holly Esquivel, CPC | 210.807-5602 | hesquivel@deaconrecruiting.com"
Data Engineer,"Ability to coordinate with people of many different types of skillsets including systems engineers, network defenders, network engineers, data scientists and analytics developers. Must be able to identify, analyze, normalize, ingest, and parse structured and unstructured cybersecurity and intelligence data from a wide variety of sources, to include large datasets (over 1TB). Data types include, but are not limited to, network appliance event logs, system logs, domain logs, firewall logs, Zeek logs, audit logs, vulnerability scans, packet capture, STIX formatted messages, PDF/text files, .csv files.

Statement of Work for Data Engineer

Job Responsibilities:

· Utilize various Big Data Platform technologies, to include but not limited to, Elastic/Lucene databasing, Hadoop Distributed File System (HDFS), Kafka, and Gem to prepare various datasets for use in data analytics.

· Work with 35 IS and external support engineers to develop, adapt, modify, and implement data parsers for analytic use in the Big Data Platform and other Air Force CS&D weapon systems.

· Utilize one or more of several coding languages to include Java, Python, and Scala.

· Develop documentation and comprehensive user manuals for all developed projects, to be understandable by the average analyst familiar with BDP.

· Assist in the development and implementation of a data analytics program within the 35 IS.

· Provide support to training development and instruction focusing on CS&D data analytics development.

Knowledge/Skills Ability:

Required: · Be Director of Central Intelligence Directives 6/4 eligible (Top Secret) with a current Single Scope Background Investigation (SSBI)

· Proficiency in one or more big data programming languages, such as R, Python, Scala, or Java.

· Experience working with a hybrid team of analyst, engineers, and developers to conduct research, and build and deploy complex, but easy-to-use analytical platforms.

· Previous experience performing research in Data Analytics or big data.

Minimum Experience/Education:

· Minimum 2 years of recent experience in data engineering

· Programming experience, ideally in Python, Spark, Kafka, or Java

· Experience in data cleaning, wrangling, visualization and reporting

· Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources

· Knowledge of data mining, machine learning, natural language processing, or information retrieval

Highly Desired:

· Education/ Certifications:

· Bachelors Degree or more in Computer Science or related field

· DoD 8570 Sec+

· 4+ years of experience in data analytics or quantitative intelligence analysis

· Four (4) years of experience in an intelligence field at a tactical or operational level

· Experience with a DoD Big Data platform is a plus

*BEAT LLC IS AN EQUAL OPPORTUNITY EMPLOYER - DISABILITY AND VETERANS*"
Data Engineer,"The Business

GradTests.com is a leading provider of practice psychometric tests to graduates, students and young professionals.

The Role

The core purpose of the role is to make high quality, high availability, accurate data available for our data analysts and data scientists to do their analysis, derive their insights and build their models. You are the Scotty Pippin to the Michael Jordans. You are the Xavi to the Messis.

You'll do things like:
Ensure our data warehouse is well structured, running smoothly and efficiently for all business intelligence
Set up and maintain various data pipelines used for customer analytics, marketing analytics and product analytics
Skills and experience

Non negotiables:
SQL
Python
Strong knowledge of traditional relational databases - we don't mind which
Some experience with cloud technologies - again we don't mind if it's AWS, GCP or Azure
Experience in any streaming technology
Great experience in using third party APIs at scale
Some web scraping experience
An obsession with data quality
Strong communication skills
Nice to haves:
Experience in working with analysts
Any basic knowledge of advanced analytics techniques
Experience in a visualisation tool like Tableau
Job Types: Full-time, Contract

Salary: $100,000.00 /year

Work Remotely:
Yes"
Data Engineer,"Data Engineer

Skills and Qualifications:
Data Engineer with Bachelor's Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience with Snowflake database
AWS cloud experience (EC2, S3, Lambda, EMR, RDS, Redshift)
Experience in ETL and ELT workflow management
Familiarity with AWS Data and Analytics technologies such as Glue, Athena, Spectrum, Data Pipeline
Experience building internal cloud to cloud integrations is ideal
Experience with streaming related technologies ex Spark streaming or other message brokers like Kafka is a plus
3+ years of Data Management Experience
3+ years of batch ETL tool experience (DataStage / Informatica / Talend)
3+ years' experience developing, deploying and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
2+ years' experience with Hadoop Ecosystem (HDFS/S3, Hive, Spark)
2+ years' experience in a software engineering, leveraging Java, Python, Scala, etc.
2+ years' advanced distributed schema and SQL development skills including partitioning for performance of ingestion and consumption patterns
2+ years' experience with distributed NoSQL databases (Apache Cassandra, Graph databases, Document Store databases)
Experience in the financial services, banking and/ or Insurance industries is a nice to have
The Company is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result."
Data Engineer,"Organization: Data and AnalyticsJob DescriptionVictory Capital is a diversified global asset management firm that operates a next-generation business model combining boutique investment qualities with the benefits of a fully integrated, centralized operating and distribution platform. Victory Capital provides specialized investment strategies to institutions, intermediaries, retirement platforms and individual investors. With nine autonomous Investment Franchises and a Solutions Platform, Victory Capital offers a wide array of investment styles and investment vehicles including, actively managed mutual funds, separately managed accounts, rules-based and active ETFs, multi-asset class strategies, custom-designed solutions and a 529 College Savings Plan.We are focused on being an end-to-end data driven organization and that starts with building the data infrastructure. The Senior Data Engineer will play an integral role in this development and will be responsible for building and optimizing the infrastructure and architecture of existing and future data storage and pipelines, as well as optimizing data collection, flow, and delivery across the firm.Responsibilities* Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of internal and external data sources.* Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.* Communicate complex solutions and ideas to a variety of stakeholders (other team members, IT leadership, and business leaders) in easily understandable language* Utilize and stay current in programming languages and software technology.* Collaborates with IT and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.Minimum Requirements* If Bachelor's degree, 6 or more years' experience in a business intelligence, database management, ETL Development or related experience can be substituted in lieu of a degree in addition to the minimum years of work experience required OR If Master's Degree, 4 or more years' experience in functions articulated above.* Successful completion of a job-related assessment may be requiredPreferred Skills* Data warehousing and ETL solutions* Schema design and dimensional data modeling* Familiarity with the AWS ecosystem and implementations* Experience working with APIs* Ability to work with any level of stakeholder across the business.* Experience working with Informatica* Experience working with Tableau or other BI Visualization tools.* Experience in Financial Services Industry, preferably asset management.Preferred EducationBS or MS in Computer Science, Information Technology, Informatics or 4 years work experience in business intelligence, database management, ETL Development or related experience can be substituted in lieu of a degree in addition to the minimum years of work experience required)."
Data Engineer,"H-E-B Digital is seeking new team members (Partners)! Since our inception, we’ve been investing heavily in our customers’ digital experience, reinventing how they find inspiration from food, how they make food decisions, and how they ultimately get food into their homes. This is an exciting time to join H-E-B Digital, and we’re hiring across the stack: front-end web and mobile, full-stack, and backend engineering. We’re using the best available technologies to deliver modern, engaging, reliable, and scalable experiences to meet the needs of our growing audience. Our digital solutions are growing in popularity and adoption—like Curbside and Home Delivery—so you’ll get the opportunity to define the user experience for millions of customers and hundreds of thousands of Partners. If you’re someone who enjoys taking on new challenges, working in a rapidly changing environment, learning new skills, and applying it all to solve large and impactful business problems, we want you as part of our team.

Our Partners thrive The H-E-B Way. In the Sr. Data Engineer position, that means you have a…

HEART FOR PEOPLE… you can organize multiple engineers, negotiate solutions, and provide upward communication

HEAD FOR BUSINESS… you consistently demonstrate and uphold the standards of codding, infrastructure, and process

PASSION FOR RESULTS… you’re capable of high-velocity contributions in multiple technical domains
What you’ll do
Work with HEB Digital teams to provide data solutions for ecommerce, supply chain, store operations, finance, and marketing reporting and analytics platforms
Contribute to existing data platforms and implement new technologies
Develop a deep understanding of HEB’s data and become a domain expert
Ensure data is distributed in a timely and accurate manner
Make data discoverable and accessible to business users

Who You Are
4 years of data engineering experience
Proficient with data technologies (e.g. Spark, Kinesis, Kafka, Airflow, Oracle, PostgreSQL, Redshift, Presto, etc.)
Experienced with designing and developing ETL data pipelines using tools such as Airflow, Nifi, or Kafka.
Strong understanding of SQL and data modeling
Understanding of Linux, Amazon Web Services (or other cloud platforms), Python, Docker, and Kubernetes
Experienced with common software engineering tools (e.g., Git, JIRA, Confluence, or similar)
Bachelor's degree in computer science or comparable field or equivalent experience
A proven understanding and application of computer science fundamentals: data structures, algorithms, design patterns, and data modeling

What are the Perks?
A robust Benefits plan with coverage starting Day One
Dental, vision, life, and other insurance plans; flexible spending accounts; short term / long term disability coverage
Partner Care Team, for any time you have healthcare or coverage questions
Telehealth offers 24/7 access to board-certified doctors by phone
Partner Guidance allows free counselor visits
Funeral leave, jury duty, and military pay (subject to applicable law)
Maternal / paternal leave for new parents, including adoptions
10"" off H-E-B brand products in-store and online
Eligibility to participate in 401(k)
Opportunity to become a “Partner-Owner” after 12 months
Who We Are
H-E-B is one of the largest, independently owned food retailers in the nation, operating over 400 stores throughout Texas and Mexico, with annual sales generating over $25 billion
We hire talented people (109,000 Partners), and give them autonomy to be creative in how they impact the business
We’re a Partner-driven company with a Bold Promise – Because People Matter
We embrace Diversity and Inclusion as core values, and support them with thriving company-wide programs
We’re a truly original Texas-based company that created the Spirit of Giving to help Texas communities every day
Once eligible, our Partners become Owners in the company. “Partner-owned” means our most important resources—People—drive the innovation, growth, and success that make H-E-B The Greatest Retailing Company
04-2019
DASO3232"
Data Engineer,"Job Description


Job Summary:

Senior Data Engineer / SQL Developer / SQL Analyst. Responsible for developing and implementing data models for multiple business processes on Acelity’s data warehouse and big data analytics infrastructure. This particular role will focus on developing a semantics layer that applies the business rules and generates the business metrics required for sales reporting and analytics.

Principle Responsibilities: (essential job duties and responsibilities)
Design and implement data models, datamarts, and datasets on a Hadoop-based data warehouse and data hub
Interface directly with business and systems subject matter experts to understand analytic needs and determine logical data model requirements
Develop and implement ETL processes across Hadoop, BI systems, and databases
Work closely with data architects to identify common data requirements and develop shared solutions
Develop close collaboration with senior analysts and data owners across multiple business domains
Maintain data modeling standards and ETL best practices
Support data model and ETL solutions in production
Skills and Experiences:
Strong data warehouse and ETL background
Advanced SQL programming capabilities. Hive, Impala, or Snowflake preferred
Strong background in preparing data for analysis and reporting: creating analytical datasets and working with others to define simple to use data models (i.e. star schema)
Experience with analytical tools for data discovery & modeling, visualization, and analysis
Success in a highly dynamic technology demand driven environment with ability to shift priorities with agility
Ability to go from whiteboard discussion to code
Willingness to explore and implement new ideas and technologies
Ability to effectively communicate with technical and non-technical audiences
Ability to work independently with minimal supervision
Minimum Qualifications:
8+ years experience with SQL
6+ years experience with data modeling design and implementation
4+ years experience working directly with subject matter experts in both business and technology domains
4+ years experience with BI and analytic tools such as Tableau, Datameer, R, or similar
2+ years experience with Ab Initio, Datastage, Informatica, or Talend
Nice-to-have:
Hands-on experience with Hadoop
Familiarity with Agile methodologies
Education:
Bachelor’s in Computer Science, Information Systems, Engineering, science discipline, or similar
The information listed above is not a comprehensive list of all duties/responsibilities performed. This job description is not an employment agreement or contract. Management has the exclusive right to alter this job description at any time without notice. Any physical and mental requirements described in this job description are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.

EOE AA M/F/Vet/Disability: Acelity L.P. Inc. and its subsidiaries are an equal opportunity and affirmative action employer and give consideration for employment to qualified applicants without regard to race, ethnicity, color, religion, sex, sexual orientation, gender identity, pregnancy, national origin, age, disability, veteran status, or genetic information or any other legally protected characteristic. If you'd like more information about your EEO rights as an applicant under the law, please click here: http://www1.eeoc.gov/employers/upload/eeoc_self_print_poster.pdf VEVRAA Federal Contractor"
Data Engineer,"Job Description
Senior Data Engineer

Location: San Antonio, TX

As part of our never-ending search for the best talent in San Antonio we have been engaged to identify a Senior Data Engineer to serve as the driving force to build data architecture for this industry top-performer!

THIS COMPANY OFFERS…
Opportunity to develop professionally through mentorship and project ownership to make the leap to leadership
Highly visible role, this person will be tapped to build relationships across the enterprise to promote a data-driven culture
Lucrative discretionary bonus plan with stock options
Remote work options
YOUR TYPICAL DAY…
Spearhead initiative to build ground up data architecture – bridging data from a high volume of sources
Build data models that will fuel business intelligence to increase data access
Mentor and guide team members to foster a collaborative and open work environment
Remain hands-on in the day-to-day, take a strategic approach while still coding regularly
YOU HAVE…
Built a data warehouse from scratch in a cloud environment
Relationship builder – enjoys serving as a technical liaison to the business and seeks to improve lines of communication
Finds satisfaction performing in high demand environments
ETL/Informatica/Python/AWS/Tableau
Bachelor’ s Degree in Computer Engineering, Mathematics, or related field required
EXTRA CREDIT…
Financial Services experience highly preferred
For a Confidential Conversation and/or Personal Meeting regarding this outstanding career opportunity please contact:

Holly Esquivel, CPC | 210.807-5602 | hesquivel@deaconrecruiting.com"
Data Engineer,"Do you believe that people with compassion will support one another to create a better world? Well, we do! GoFundMe is the largest social fundraising community in the world and is just getting started. With over $9 billion raised from more than 120 million donations, GoFundMe is the largest social fundraising community in the world and is just getting started.Data is at the center of all decisions and strategy at GoFundMe. The Data Engineer will be a key part of our growing platform engineering team to help build scalable data platforms that enable business analytics, data science and data products, resulting in driving business growth toward a global culture of peer-to-peer giving. This role requires technical expertise in a wide variety of technologies to develop and own our enterprise data warehouse, sourcing data from various databases/web APIs and integrate data with external systems. If you are interested in working in a fast paced environment and like being challenged with fun data problems to solve, come join us in our Los Angeles or San Diego offices.What you'll be doing day to day...* Develop and maintain enterprise data warehouse (in Amazon Redshift)* Create and manage ETL data pipelines (sourcing data from databases, streaming data, various web APIs, etc.)* Integrate data from data warehouse into 3rd party tools to make data actionable* Develop and maintain REST API endpoints for data science products* Provide ongoing maintenance and enhancements to existing data warehouse solutions* Ensure data quality through automated testing* Collaborate with analysts, engineers and business users to design solutions* Research innovative technologies and make continuous improvementsWhat you bring to the role...* 3+ years as a data engineer designing, developing and maintaining enterprise data warehouse solutions consisting of structured and unstructured data* Proficiency with building data pipelines using ETL/data preparation tools* Experience with web APIs and data integrations across internal and external systems* Expertise in writing and optimizing SQL queries* Knowledge of Python, Java, C++ or other scripting languages* Experience with Spark and Scala* Good understanding of database architecture and best practices* Understanding of data science and machine learning technologies a plus* Experience with event tracking is a plus* Bachelor's degree in Engineering* Ping pong skills, a love for boba tea, and a sense of humorWhy you'll love it here...* Your work has real purpose and will be helping to change lives at a global scale.* Our people consistently vote GoFundMe a Great Place to Work®.* You can nominate your favorite GoFundMes to receive a donation from the company.* Great perks like lunch, snacks, wellness, company/team activities, and full benefits.* The company is strong and growing with incredible opportunities ahead.* We're a fun, close team of people who care about their work and impact.More about GoFundMe...https://www.gofundme.com/2019https://www.gofundme.com/c/heroeshttps://medium.com/gofundme-storieshttps://www.gofundme.com/why-gofundmeGoFundMe is changing the way the world gives. Every day friends, family, and members of the community come together to support one another and the causes they care about most. Our campaigners have raised over $9 billion for medical expenses, education, community projects, sports, emergencies, pets and other personal causes and life events--making us the world's largest crowdfunding platform.GoFundMe has assembled one of the best teams to go build the next leading consumer Internet company - including leaders from LinkedIn, Intuit, Groupon, YouTube, Facebook, Twitter, GoPro, Uber and several others. We are also funded by some of Silicon Valley's best venture capital firms, including Accel, Greylock, TCV, and others.GoFundMe is proud to be an equal opportunity employer that actively pursues candidates of diverse backgrounds and experiences. We are committed to providing diversity, equity, and inclusion training to all employees, and we do not discriminate on the basis of race, color, religion, ethnicity, nationality or national origin, sex, sexual orientation, gender, gender identity or expression, pregnancy status, marital status, age, medical condition, mental or physical disability, or military or veteran status."
Data Engineer,"Mercato connects independent food retailers with consumers and suppliers to streamline their business, increase sales, and lower operating costs.

Reporting to the VP of Engineering, the Data Engineer will own Mercato’s data warehouse, its design. The Data Engineer will implement and maintain custom ETLs.

You will be the first Data Engineer at Mercato and have opportunities to contribute to a variety of projects and technologies including analytics, ML modeling, tooling, services, and more.

You are focused on results, a self-starter, and have demonstrated success in developing and maintaining data infrastructure to ensure your colleagues are empowered with reliable access to data.

Responsibilities
Collaborate with Product Management and Engineering to understand data needs, solve problems, and identify trends and opportunities.
Design, build and launch new data extraction, transformation, and loading processes in production.
Manage data warehouse plans for a group of products.
Work with data infrastructure to triage infrastructure issues and drive to resolution.
Build data expertise and own data quality for allocated areas of ownership.
Support existing processes running in production.
Requirements
Experience with custom ETL design, implementation and maintenance, including schema design and dimensional data modeling.
Significant experience with workflow management engines (i.e. Airflow, AWS Step Functions, etc).
Expert proficiency in any scripting language (Python, Node.js, R, etc.) and SQL.
Experience working with cloud or on-prem Big Data/MPP analytics platform (i.e., AWS Redshift or similar).
Ability to analyze data to identify deliverables, gaps, and inconsistencies.
Communication skills. A strong ability to identify and communicate data-driven insights.
Managing and communicating data warehouse plans to internal clients.
4+ years experience working with either a Map Reduce or an MPP system.
Ability to thrive in an unstructured environment, working autonomously to find opportunities to deliver business impact.
Success Drivers And Competencies
Must be able to convey information in a clear, focused, and concise manner.
Experience in planning, coordinating, and executing multiple projects simultaneously.
Ability to think creatively and work in a team environment.
Must work well in a dynamic environment and be able to recommend and implement process improvements, work independently, and handle multiple tasks simultaneously.
Passion for helping others.
Benefits

Mercato is an equal opportunity employer. We believe passionately that employing a diverse workforce is central to our success. We make recruiting decisions based on your experience and skills. We value your passion to discover, invent, simplify, and build.

By submitting your resume and application information, you authorize Mercato to transmit and store your information in the Mercato group companies' nationwide recruitment database.

Compensation Commensurate With Experience.

Upbeat work environment at a company with a huge vision."
Data Engineer,"About Us


Life360 brings families closer with smart tools designed to protect and connect the people who matter most.

Known for first-to-market solutions for modern family challenges, Life360 recently reached #1 in Apple's US App Store's list of free social networking apps. Nearly 1 in 10 US families with kids use Life360 an average of 12 times a day, and global membership is growing exponentially, with over 28 million active users in over 140 countries as of March 31, 2020 making Life360 the largest mobile service for families in the world.

This reach gives us the opportunity to do unprecedented good for families through our valued core offerings: advanced location sharing, private messaging, driver monitoring, help alerts, 24/7 roadside assistance, and Crash Detection with emergency response. On average we respond to 1,000 roadside assists and dispatch 200+ ambulances each month to those in need.

Offering both free and paid memberships. In addition, the company has raised over $200 million in equity financing, and recently completed an IPO on the ASX exchange giving our employees the liquidity of a public company with the upside of a private growth stage business.

Life360's rapidly growing team of 150+ employees is headquartered in San Francisco, with offices in San Diego, and Las Vegas.

About the Role

At Life360, we collect a lot of data: 60 billion unique location points, 12 billion user actions, 8 billion miles driven every single month, and so much more. As a Senior Data Engineer, you will contribute to enhancing and maintaining our data processing and storage pipelines/workflows for external and internal data consumers. You should have a strong engineering background and even more importantly a desire to take ownership of our data systems to make them world class.


Responsibilities:

Primary responsibilities include, but are not limited to:
Design and develop resilient pipelines using a variety of different technologies.
Manage our data from ingestion through ETL to storage and batch processing.
Automate, test and harden all data workflows.
Architect logical and physical data models to ensure the needs of the business are met.
Participate in rotational on-call support and provide ongoing maintenance of all data infrastructure.
Be part of a fast growing data team handling massive scale with tons of automation.
Provide innovative solutions to challenging data projects and proof of concepts.
Education / Experience Requirements:
Minimum 4+ years of experience working with high volume data infrastructure.
Experience with AWS data related services.
Extensive experience programming in one of the following languages: Python / Java.
Experience in data modeling, optimizing SQL queries, and system performance tuning.
Knowledge and proficiency in the latest open source and data frameworks.
Experience evaluating industry trends and technologies.
Always be learning and staying up to speed with the fast moving data world.
Prefer candidates with AWS certifications.
Perks:


Fridays are Work From Home days at Life360
Competitive pay and benefits
Free snacks, drinks, and food in the office
Catered lunches throughout the week
Health, dental and vision insurance plans
401k plan
$200/month Quality of Life perk
An amazing office location within walking distance to the beach
Whatever makes you stronger makes us stronger. We buy you the things you need to improve yourself and get your job done.
This position is located in Encinitas, CA. It is not a remote role."
Data Engineer,"Primary Skills 4+ years working experience in data integration and pipeline development. BS degree in CS, CE or EE. 2+ years of Experience with AWS Cloud on data integration with Apache Spark, EMR, Glue, Kafka, Kinesis, and Lambda in S3, Redshift, RDS, MongoDBDynamoDB ecosystems Strong real-life experience in python development especially in pySpark in AWS Cloud environment. Design, develop test, deploy, maintain and improve data integration pipeline. Experience in Python and common python libraries. Strong analytical experience with database in writing complex queries, query optimization, debugging, user defined functions, views, indexes etc. Strong experience with source control systems such as Git, Bitbucket, and Jenkins build and continuous integration tools. Databricks or Apache Spark Experience is a plus."
Data Engineer,"SmartDrive Systems gives fleets and drivers unprecedented driving performance insight and analysis, helping save fuel, expenses and lives. Its video analysis, predictive analytics and personalized performance program help fleets improve driving skills, lower operating costs, and deliver significant ROI. With an easy-to-use managed service, fleets and drivers can access and self-manage driving performance anytime, anywhere. The Company has compiled the world's largest storehouse of nearly 200 million analyzed risky-driving events, including video and comprehensive sensor data. SmartDrive Systems is based in San Diego, CA, Shenzhen, China, and Hyderabad, India, and employs over 600 people worldwide.

We are looking for an experienced data-focused engineers to join our team, leading the design, development, and delivery of our high-performance analytics engines for solving computer vision, machine learning, sensor fusion problems running in vehicle and in the cloud at high throughput and high accuracy vehicle event analysis engines (in vehicle and in the cloud), and robust and flexible coaching workflow, reporting, and alert management engines.

Our business is nearly doubling every year, and our people and our platforms are the foundation and enabler of that growth. We are significantly expanding our team, and are looking for technologists with a passion for high performance software development, and a drive to deliver software products that make a meaningful difference in the lives of others.

Responsibilities:
Data Engineering to the core - analysis, modelling, transformation and visualization of datasets for online products and backend data platform.
Implement data engineering codebase using server-side languages Java/Scala or scripting using Python and Javascript
Design and develop apis for data pipelining frameworks on data collection, processing and storage across data stores.
Understand noSQL and stream programming apis for building data pipelines and micro-service based architecture across products
Design and development of data solutions for fast datastores, large scale data warehousing, machine learning and computer vision analytics.
Minimum Qualifications:
Bachelor's Degree in Computer Science or related discipline
5+ years of software development experience
2+ years of experience as a Data Engineer
Preferred Qualifications
Extensive knowledge of RDBMS (Microsoft SQL Server, Mysql, Postgres or similar)
Proven experience with NoSQL stores (one or more of Cassandra, MongoDB, InfluxDB, HBase/HDFS, ElasticSearch)
Experience in a distributed microservices and/or serverless (Lambda) cloud software architecture
In-Depth knowledge of ETL commercial software products (any of Informatica, Talend, Nifi or SSIS) with hands-on experience designing, implementing, and delivering solutions
Expertise with integration of complex and large data from multiple data sources, data and sensor fusion, and migration to newer methodologies
Prior experience as part of a large group working on massive data engineering pipelines and analytics for machine learning, computer vision
An aggressive problem solver who can provide creative solutions to complex situations and obtain buy-in from those affected
An independent worker who can take the initiative to define and prioritize specific goals and objectives, and to do the same for others
Strong people skills - able to communicate with colleagues while building credibility and rapport, modifying behavioral style to respond to the needs of others while maintaining objectives
An organized individual who is very detail oriented and can document and develop plans necessary for deliverables towards specific product or platform goals.
A team player that works hard, admits his/her strengths and weaknesses, and has the flexibility to improve by learning new things"
Data Engineer,"Position OverviewStepStone Portfolio Analytics and Reporting (""SPAR"") is looking for a Data Engineer to join the Analytics team. The team applies its comprehensive knowledge of private markets to deliver customized performance reports and monitoring services to meet the needs of various types of investors. The Data Engineer will ensure necessary data is being recorded, validate and maintain a large data pipeline, mine large quantities of data, and build new data infrastructures as needed.We are looking for candidates who are interested in building the team's capabilities around the extract, transform, and load processes of analytic projects. The ideal candidates will be passionate about creating better standardized processes so that the broader team is able to focus more on the analysis portion of the pipeline. Minimizing the team's efforts around ETL is the number one priority. This will be a fast-paced and dynamic environment that is ideal for those who want to continuously tackle challenging problems and learn new things.Essential Job Functions:* Develop highly efficient systems to retrieve, maintain and analyze discrete financial data* Perform data normalization and management ensuring data security and efficiency* Perform ETL tasks and maintenance* Maintain and test data integrity to ensure accuracy and timeliness* Peer review SQL queries for errors and optimization* Define and iterate on development of analytics infrastructure for interactive dashboardsQualifications:* 2+ years' experience programming large complex data sets* Experience revising and optimizing complex queries with SQL* Solid grasp of Python programming* Database management experience is a plus* Good understanding of data modeling and relational databases* Experience working in collaborative environments* Inquisitive and intellectually curious: able to independently learn new technologies, skills, and industry standards* Bachelor's degree from an accredited institution"
Data Engineer,"Job Description
Context

Working in teams (consisting of Hadoop data engineers, Hadoop data warehouse engineers, and platform engineers) that are building and managing Hadoop stacks. The teams install, configure and manage Hadoop ecosystem components.

As Hadoop data engineer, you are responsible for the functional part of provisioning data – e.g. building data ingestion pipelines and data connectors. You work closely with the data scientists and business intelligence engineers who are using this data to create analytical models.

Competence

You are well acquainted with the complete Hadoop stack. In addition, you have practical experience of being part of a DevOps team. Further requirements:
Bachelor of Science / Master’s degree in Computer Science, System Administration, or any other IT infrastructure or software related study with a passion for the automation side of IT infrastructure
Minimum 2-3 years of relevant work experience
Capable of building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets of structured, semi-structured and unstructured data
Experience in building data products incrementally and integrating and managing data sets from multiple sources
Data quality oriented
Familiar with data architecture including data ingestion pipeline design, Hadoop information architecture
Hortonworks Certified Hadoop Developer and/or Cloudera Certified Hadoop Developer and/or Certified Hadoop Administrator
Knowledge of continuous integration & delivery tooling: e.g. Jira, Git, Jenkins, Bamboo
Coding proficiency in at least one modern programming language (Python, Ruby, Java)
Strong verbal and written communication skills
Good documenting capabilities
You have a hands-on mindset, a strong customer focus, a problem-solving orientation and can show fast results
You have a clear focus on results and quality.
Willingness to travel to the Netherlands if required for training or project work
Activities
Build efficient and highly reliable data ingestion pipelines for the Hadoop stack
Own data quality and data knowledge around all data that you touch
Work side-by-side with software engineers and data scientists in designing modeled data sets to be used in many different applications, from proof-of-concept to production
Understand the entire life cycle of data that flows through any systems for which you are responsible
Pay constant attention and effort to the reliability of your pipelines
Contact

Reports to the Itility project manager, working in close harmony with team members and interfacing with the standing IT organization.
Company Description
In large organizations, project managers, architects, and business analysts often operate autonomously. Not at Itility. Here these competencies are combined into customer-targeted teams, enabling the rapid delivery of results (within weeks). This rigorous approach is an essential feature of the Itility high-end consultancy formula.

At Itility, you can advance your career as an IT professional; working on projects, Smart Factories, Smart Run, and consulting assignments onsite. An Itility team always comprises multiple competencies; ranging from project manager and scrum master to stack engineer, DevOps engineer and architect.

Our teams direct project implementation. We begin by using our own IT assessment methodology to analyze customer problems, then define the best solutions, while determining required actions.

This knowledge-intensive approach has a proven direct and positive impact on career development. With Itility, you’ll obtain a broad view of the IT profession; working with one customer in the implementation phase and another in the preparatory phase.

To support your development, we provide resources in the form of training, and our Itility toolkits (Ikits); including the Project Management Ikit, the Scrum Ikit, and the Smart Factory Ikit."
Data Engineer,"Job Description
ABOUT CUREMETRIX

Delivering CAD that Works, CureMetrix is committed to the advancement of technology that improves cancer survival rates worldwide. With research that leverages artificial intelligence (AI) and deep learning to develop the next generation of medical image analysis, CureMetrix delivers technology that radiologists, healthcare systems, and patients can confidently rely on. For more information visitwww.curemetrix.com

ROLE OVERVIEW

CureMetrix is looking for a talented, multi-disciplinary, individual to own our data ingestion, curation, and access management, as well as develop processes and tools to help derive more value and utility from the data.. At CureMetrix, curated radiology data is our lifeblood. The Data Engineer position will work with our Research and our Development teams to acquire, build, and maintain our database of clinical data and radiology images. The Data Engineer will also help build and maintain tools to assist our Research group with their Machine Learning pipeline.

Local (San Diego) candidates only, please.

ROLE SPECIFICS

Data Management
Working with a variety of public and private health institutions around the world to gather images and clinical mammography data.
Developing data validation specifications and data management plans
Cleaning and validating incoming data
Normalizing and standardizing data according to set standards
Verifying the integrity of the data
Anonymize data according to set standards
Ensuring the HIPAA compliance of the data at rest and in transit
Working to establish and maintain the security of the data
Curating the data as needed
Designing queries to extract study data as needed for both internal uses and for publications.
Working with internal and external teams to design and execute clinical studies utilizing data
Working with CTO to build dashboards providing company insight into our dataset
Ensure data adheres to company HIPAA and cybersecurity compliance policies
Performing analysis and reporting to help ensure ongoing data integrity
Performing analysis and reporting to support customer audits and consulting engagements
Python Toolset
Minimum of 4 years of Python experience working with development and research teams to build specifications, implement, and maintain data extraction and mining tools.
Cloud Storage
Minimum of 4 years of experience working in the cloud. Familiarity with AWS S3 storage in a HIPAA compliant environment. Requires knowledge of boto3 and AWS CLI tools.
Machine Learning Pipeline
Own internal medical image ground truth pipeline from ingestion to training.
Work with radiologists to determine medical image ground truth
Help manage medical image annotation vendors including quality management
Work with Development and Research teams to build specifications, implement, and maintain our internal Machine Learning training pipeline.
Assist the Research team in building datasets for Machine Learning training.
Development
Working with CTO to develop custom tools and applications for data access and management
Supporting bi-directional integrations with 3rd-parties
As needed supporting the entire team with development expertise on specific projects.


QUALIFICATIONS
Bachelor's degree in science, IT, or business-related field and/or equivalent work experience.
Ability to build and maintain a growing study database along with strong SQL skills
Strong experience using python scripting in a technical environment
A Minimum of 5 years of working with large and complex datasets, preferably in a regulated device/diagnostic, laboratory, or CRO setting.
Strong project management skills
Ability to prioritize activities and meet regular deadlines according to reasonable levels of quality.
Ability to work independently.
Have strong writing, verbal communication skills, good organizational, interpersonal and team skills
Used to working in an Agile environment
POSITION TYPE AND HOURS OF WORKFull-time position. General Hours of work are Monday through Friday, from 8:30 a.m. to 5:30 p.m. Occasional early, late or weekend work required.

POSITION COLLABORATES WITH AND IS SUPPORTED BY
Research
Software Development
Operations and IT
Business Development
Key Executives
WORK ENVIRONMENTThis job operates in a professional office environment. This role routinely uses standard office equipment such as computers, phones, photocopiers, filing cabinets, and fax machines. However, at this time all work is remote in your home office until further notice. As such candidates must be comfortable with remote work and prepared to contribute full engagement in such an environment.

PHYSICAL DEMANDS:This position requires the ability to navigate throughout a large office, 3-story complex. This is primarily a sedentary role; however, some walking, standing, bending, lifting up to 20 lbs., and hand/eye coordination for keyboard data entry and viewing data on a computer monitor may be required. Specific vision abilities required by this job include close vision, distance vision, color vision, peripheral vision, depth perception, and ability to adjust focus. The physical demands described above must be met by an employee to successfully perform the essential functions of this job."
Data Engineer,"Description:Born digital, Axos Bank has reinvented the banking model and grown to over $12 billion in assets since our founding in 2000. With a broad and ever-growing range of financial products, Axos Bank is rated among the top 5 digital banks in the country! Axos Financial is our holding company and publicly traded on the New York Stock Exchange under the symbol ""AX (NYSE: AX).We bring together human insight and digital expertise to anticipate the needs of our customers. Our team members are innovative, technologically sophisticated, and motivated to achieve.Learn more about working here!Responsibilities include:* Work with technical and business team to understand the business requirements, functional and technical specifications* Design, code and maintain new and existing complex SQL stored procedures and functions* Performance tune existing stored procedures, tables and indexes* Work with other engineers to troubleshoot, repair and performance tune databases* Review SQL code written by other developers to ensure compliance to coding standards and best practices as well as maximum performance* Create SSIS packages for data transformation, cleansing, caching, aggregation, staging, and transfer* Troubleshoot problems that may come up with database environments: performance issues; replication issues; or operational issues* Perform data analysis and data profiling tasks to provide support and recommendations for development and design decisions* Analyze and define data flow requirements and prepare applicable system documentation and operation manuals as needed* Support production data loads and ongoing refreshes of the database systems* Define, prepare, execute and implement data validation and unit testing methods to ensure data quality* Maintain re-usable development standards that help implement each solution and/or enhancements to existing systems to meet current and future needs* Perform enhancements and bug fixes as required* Perform any additional duties as assignedKey Skill Sets or Knowledge Requirements:* Expertise in SQL server design and development* Excellent verbal and written communication skills, including ability to simplify complex concepts for technical and non-technical audience* Superior problem-solving skills, self-motivation, and the capacity to work under pressure and tight deadlines* Demonstrated ability to learn, acquire and utilize new technologies, disciplines and frameworks as needed* Technical expertise in the areas of data profiling, data mining and data analytics* Technical expertise in building reliable data ETL/ELT processes, query optimization and dynamic SQL* Strong customer focus, excellent problem solving and analytical skills* Ability to work independently under minimal supervision and strong track record of setting and meeting delivery commitmentsDesired Career Experience & Education Requirements:* 3+ years' working with relational DBs in a production environment* 3+ years' experience with Microsoft SQL Server* 2+ years' experience in SSIS packages* 2+ years' experience working in an Agile/SCRUM environment* Experience delivering high quality, high traffic, scalable database objects* Bachelor's Degree in Computer Science, Information Systems, Computer Engineering or related fieldPreferred:* Experience in BigData* Banking industry experienceApply directly for consideration as we are not using any outside agencies for any of our openingsPre-Employment Drug Test:All offers are contingent upon the candidate successfully passing a credit check, criminal background check, and pre-employment drug screening, which includes screening for marijuana. Axos Bank is a federally regulated banking institution. At the federal level, marijuana is an illegal schedule 1 drug; therefore, we will not employ any person who tests positive for marijuana, regardless of state legalization.Equal Employment Opportunity:Axos Bank is an Equal Opportunity employer. We are committed to providing equal employment opportunities to all employees and applicants without regard to race, religious creed, color, sex (including pregnancy, breast feeding and related medical conditions), gender, gender identity, gender expression, sexual orientation, national origin, ancestry, citizenship status, military and veteran status, marital status, age, protected medical condition, genetic information, physical disability, mental disability, or any other protected status in accordance with all applicable federal, state and local laws.Job Functions and Work Environment:While performing the duties of this position, the employee is required to sit for extended periods of time. Manual dexterity and coordination are required while operating standard office equipment such as computer keyboard and mouse, calculator, telephone, copiers, etc.The work environment characteristics described here are representative of those an employee may encounter while performing the essential functions of this position. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions of this position.Options"
Data Engineer,"Job Description:
Bachelors or Masters in Computer Science, Engineering, Mathematics, Statistics, or related field.
3+ years of relevant experience in data engineering.
Demonstrated ability in data modeling, ETL development, and data warehousing.
Advanced SQL experience is a must.
Data Warehousing.
Experience with SQL Server.
Experience with Big Data Technologies (NoSQL databases, Hadoop, Hive, Hbase, Pig, Spark, Elasticsearch, etc.).
Experience in using DotNet, C#, and/or other data engineering languages.
Knowledge and experience of SQL Server and SSIS.
Preferred Qualifications:
Industry experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets.
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.
Experience building data products integrating and managing datasets from multiple focus."
Data Engineer,"About Us


Life360 brings families closer with smart tools designed to protect and connect the people who matter most.

Known for first-to-market solutions for modern family challenges, Life360 recently reached #1 in Apple's US App Store's list of free social networking apps. Nearly 1 in 10 US families with kids use Life360 an average of 12 times a day, and global membership is growing exponentially, with over 28 million active users in over 140 countries as of March 31, 2020 making Life360 the largest mobile service for families in the world.

This reach gives us the opportunity to do unprecedented good for families through our valued core offerings: advanced location sharing, private messaging, driver monitoring, help alerts, 24/7 roadside assistance, and Crash Detection with emergency response. On average we respond to 1,000 roadside assists and dispatch 200+ ambulances each month to those in need.

Offering both free and paid memberships. In addition, the company has raised over $200 million in equity financing, and recently completed an IPO on the ASX exchange giving our employees the liquidity of a public company with the upside of a private growth stage business.

Life360's rapidly growing team of 150+ employees is headquartered in San Francisco, with offices in San Diego, and Las Vegas.

About the Role

At Life360, we collect a lot of data: 60 billion unique location points, 12 billion user actions, 8 billion miles driven every single month, and so much more. As a Senior Data Engineer, you will contribute to enhancing and maintaining our data processing and storage pipelines/workflows for external and internal data consumers. You should have a strong engineering background and even more importantly a desire to take ownership of our data systems to make them world class.


Responsibilities:

Primary responsibilities include, but are not limited to:
Design and develop resilient pipelines using a variety of different technologies.
Manage our data from ingestion through ETL to storage and batch processing.
Automate, test and harden all data workflows.
Architect logical and physical data models to ensure the needs of the business are met.
Participate in rotational on-call support and provide ongoing maintenance of all data infrastructure.
Be part of a fast growing data team handling massive scale with tons of automation.
Provide innovative solutions to challenging data projects and proof of concepts.
Education / Experience Requirements:
Minimum 4+ years of experience working with high volume data infrastructure.
Experience with AWS data related services.
Extensive experience programming in one of the following languages: Python / Java.
Experience in data modeling, optimizing SQL queries, and system performance tuning.
Knowledge and proficiency in the latest open source and data frameworks.
Experience evaluating industry trends and technologies.
Always be learning and staying up to speed with the fast moving data world.
Prefer candidates with AWS certifications.
Perks:


Fridays are Work From Home days at Life360
Competitive pay and benefits
Free snacks, drinks, and food in the office
Catered lunches throughout the week
Health, dental and vision insurance plans
401k plan
$200/month Quality of Life perk
An amazing office location within walking distance to the beach
Whatever makes you stronger makes us stronger. We buy you the things you need to improve yourself and get your job done.
This position is located in Encinitas, CA. It is not a remote role."
Data Engineer,"????????????????????????Hi Folks ,

Greetings from Logic Planet,

We are pleased t introduce you Logic Planet as a leading recruitment and staffing company,

Regarding I have an exciting new opportunity???s that I wanted to share with you and your network. Our top client, located San Diego, CA is currently seekingData Engineer. Joins their organization. I have included a complete job description below in case you or someone you know might be interested in learning more

Job Title: Data Engineer
Location: San Diego, CA
Work Authorizations: Authorized to Work
Position Type: Contract
No: of Positions: 1
Position Start Date:
Duration: 6+ Months
Primary Skills: ETL, Date warehousing, SQL
Secondary Skills:
Description:

Roles and Responsibility:-

??? Interfacing with business customers, gathering requirements and developing new datasets in data platform

??? Building and migrating the complex ETL pipelines from on premise system to cloud

??? Identifying the data quality issues to address them immediately to provide great user experience

??? Extracting and combining data from various heterogeneous data sources

??? Designing, implementing and supporting a platform that can provide ad-hoc access to large datasets

Basic Qualifications:

??? Bachelors or Masters in Computer Science, Engineering, Mathematics, Statistics, or related field

3+ years relevant experience in data engineering.

??? Demonstrated ability in data modeling, ETL development, and data warehousing.

Advanced SQL experience is a must

??? Data Warehousing Experience with SQL Server.

??? Experience with Big Data Technologies (NoSQL databases, Hadoop, Hive, Hbase, Pig, Spark, Elastic Search etc.)

??? Experience in using .Net , C# and/or other data engineering languages

??? Knowledge and experience of SQL Sever and SSIS.

Preferred Qualifications:

??? Industry experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets.

??? Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets

??? Experience building data products incrementally and integrating and managing datasets from multiple focus.

Regards,

Vinay

Logic Planet Inc
4525 Route 27,Princeton, NJ 08540

Ph: 732 512 0009 Ext: 134 Direct : 609 256 4342
Email: vinay@logicplanet.com | www.logicplanet.com

Certified Minority Women Based Enterprise

18 years in IT. 300 employees. $30M in revenues"
Data Engineer,"Description

The Senior Software Engineer codes software applications based on business requirements. The Senior Software Engineer work assignments involve moderately complex to complex issues where the analysis of situations or data requires an in-depth evaluation of variable factors.

Responsibilities

Role: Senior Software Engineer

Assignment: Humana - Digital Health & Analytics

Location: Campbell, CA (preferred) or San Diego, CA

Summary of Duties & Job Description

We are looking for a talented Senior Software Engineer to join our development team. The ideal candidate should have an advanced grasp of the software engineering lifecycle and is particularly strong in backend development. You will join a team that work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across teams in our group.

Responsibilities

+ Lead and guide project team(s) as tech lead or key contributor

+ Participate in architectural discussion and product development

+ Evaluate and integrate tools and frameworks required to provide requested capabilities

+ Diagnose system performance issues and advise any necessary infrastructure changes

+ Interact with product and business stakeholders beyond engineering

+ Mentor and develop junior levels of engineers

Skills and Qualifications

+ Bachelor's Degree or above (Computer Science, Bio Engineering, Electronics and Electrical Engineering or any related field)

+ Proficient understanding and tech-lead level experience in one or more programming languages (Java, JavaScript, Python etc.)

+ 5+ years of experience in software development and/or data engineering

+ Experience with JSON, RESTful web services and client-server interactions

+ Knowledge of various persistence (RDBMS, noSQL, HDFS, Cassandra, Redis)

+ Understanding Data Catalog, Data Governance, Data Lineage

+ Experience with security, authentication in data platform

+ Experience with building stream-processing systems, using solutions such as Spark-Streaming or Flink

+ Experience with integration of data from multiple data sources

+ Experience with building data lakes and data warehouses by leveraging any of the major cloud providers (GCP, AWS or Azure) is highly desirable

+ Familiar with Hadoop ecosystem (HDFS, HBase etc.), especially Spark

+ Good basics of operating systems and network

+ Good knowledge of Big Data querying tools

+ Knowledge of and experience with Azure

+ Knowledge of various ETL techniques

+ Knowledge of messaging systems, such as Kafka or RabbitMQ

Role Desirables

+ Experience with Spring/Spring Boot

+ Experience with highly scalable web services

+ API design and development

+ Environment management/orchestration systems (Kubernetes, etc.)

+ Experience with testing frameworks and code quality tools

+ Experience with continuous integration environment and tools

+ Experience with Oracle, PostgresSQL, Mongo development and optimization

+ Familiarity with Healthcare (FHIR), Clinical or Financial industry

+ Experience with agile development practices

+ Familiarity with Git

+ Knowledge of JIRA and Confluence

Scheduled Weekly Hours

40

About Us

Mission: At Humana, our cultural foundation is aligned to helping members achieve their best health by delivering personalized, simplified, whole-person healthcare experiences. Recognizing healthcare needs continue to evolve for each person, for each family and for each community, Humana continuously creates innovative solutions and resources that help people live their healthiest lives on their terms -when and where they need it. Our employees are at the heart of making this happen and that's why we are dedicated to building an organization of dynamic talent whose experience and passion center on putting the customer first.

Equal Opportunity Employer

It is our policy to recruit, hire, train, and promote people without regard to race, color, religion, sex, national origin, age, sexual orientation, gender identity or expression, disability, or veteran status, except where age, sex, or physical status is a bona fide occupational qualification. View the EEO is the Law poster.

If you are an individual with a disability and require a reasonable accommodation to complete any part of the application process, or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you may contact mailbox_tas_recruit@humana.com for assistance.

Humana Safety and Security

Humana will never ask, nor require a candidate provide money for work equipment and network access during the application process. If you become aware of any instances where you as a candidate are asked to provide information and do not believe it is a legitimate request from Humana or affiliate, please contact mailbox_tas_recruit@humana.com to validate the request."
Data Engineer,"Job Title: Big Data EngineerDepartment: 470-R&D IoT EngineeringReports To: Prasanna MadabushiDirect Reports: N/APosition Overview:Staff Engineer, Data is responsible for architecture, design and implementation of our data collection, modeling, processing and storage in our cloud platform.Responsibilities:* Lead a team of globally distributed engineers to design and develop our data solutions* Work with product to understand business requirements and architect solutions to meet them* Work with onsite and remote engineers to design and implement enterprise scale data collection, modeling, processing and storage* Work with a cross functional team of HW, FW, SW, AI/ML engineers to achieve the business vision* Participate in scrum rituals and provide effort estimates for features* Publish standards and best practices and mentor junior engineers on them* Perform design and code reviews* Design and build quality, performance and reliability into our products* Champion cloud native best practices, CI/CD DevOps practices, and security and privacy guidelines* Diagnose, characterize and address performance and scale issues* Monitor and troubleshoot production issues and provide solutions to resolve them* Identify, prototype and champion new technology solutionsExperience/Requirements:* 5+ years of experience in leading application development for highly available enterprise grade software and platform applications* 3+ years of experience in Platform Engineering across various cloud computing models* Expertise in one relational db (SQL Server, Postgres, MySQL) and in one nosql store (Cassandra, MongoDB, InfluxDB, Elastic Search)* Experience with data processing systems, both batch and stream (Hadoop/MapReduce, Flink, Spark)* Expertise with integration of complex and large data from multiple data sources, device and sensor data, and telemetry* Experience in applying Data Governance, Data Privacy and Data Security regulations* Experience with Git, Jira, Confluence and similar issues tracking and collaboration tools* Must have an understanding of Kubernetes, Container Orchestrations, Docker, and Cloud Native applications* Experience in using public cloud platform services, such as Azure and AWS* Must have experience in cloud native application patterns and tools, microservice architectures, application migrations to any cloud platform* Excellent understanding of Infrastructure, Virtualization, Containers, Network, Storage, Monitoring tools, Logging analytic tools (Splunk, etc.)* Experience with Application Performance Management tools (Prometheus, Grafana)* Deep understanding of Agile principles and processes* Thrive in a fast-paced environment with minimal supervision* Experience in IoT, machine learning, computer vision, video solutions* Problem solver who can provide creative and cost effective solutionsEducation:* Bachelor's Degree in either Computer Science or a related scientific discipline or equivalent meaningful experience"
Data Engineer,"Senior Data Engineer (multiple locations)

SBG Technology Solutions, Inc. (SBG) is growing and would like you to come join our team!

SBG, a service disabled veteran owned small business specializing in engineering, information technology, cyber-security, and training, is looking for results-oriented Senior Data Engineers. We are seeking highly motivated individuals looking to join our rapidly growing company. The ideal candidates for these position would be experienced individuals who are hard-working, have the ability to excel in a fast-paced government contractor environment, and have a positive, energetic attitude. These positions are located in San Diego, CA; Norfolk/Virginia Beach, VA; Washington D.C. Metro; and Hawaii.

SBG Technology Solutions seeks Senior Data Engineers to work on an exciting Department of Defense contract to provide Systems Engineering & Technical Assistance (SETA) Support Services. This position will support the Program Executive Office Command, Control, Communications, Computers and Intelligence (PEO C4I) in their work to transform, modernize, improve, and sustain current and future operational maritime tactical Command and Control needs into effective and affordable C2 capabilities for Navy, Marine Corps, Joint, and Coalition warfighters. The overarching goal is to integrate programs into single C2 capabilities, provide configuration and scalable solutions to various platforms, and align to the FORCEnet roadmap.

Key Responsibilities:
Provides engineering and technical support services to assist in the development, upgrade, review, sustainment, production and delivery of program documentation and data.
Assists with the development of the C4ISR Data Strategy which includes Information Operations (IO), Combat Systems, Tactical Communications, C2 and Intelligence Surveillance and Reconnaissance (ISR) domains.
Assists in the preparation of point papers and support documentation to facilitate the integration of procured technologies with current and future Maritime C2, Support C2, Tactical C2, and, Defensive C2 programs as they relate Navy Tactical Cloud Reference Implementation (NTC RI) utilizing computing component (Cloudera), a Big Data analytic component (Hadoop Distributed Files System, Accumulo, MapReduce, and Storm), and a Data Storage component (Content Zone).
Assists in the establishment of program/project databases/spreadsheets to support system engineering requirements. This includes: problem resolution, use of data/statistical analysis tools, support for meetings such as Program Office Configuration Change Board (CCB), Platform Technical Review Board (PTRB) and other required Program Office specific meetings.
Supports implementation and use of M&S best practices including cost-benefit tools, standards information, data exchange techniques, authoritative data, and architectures.
Requirements:
Master's degree in technical area.
Active DoD Secret clearance with ability to obtain Top Secret Clearance with Sensitive-Compartmented Information (SCI) access. Active TS preferred.
Experience designing and developing models using large amounts of structured and unstructured data.
Desire ten (10) years of experience and knowledge in data engineering and design to include:
US Navy and/or US Air Force C2 and data systems including operational and technical architectures
Intelligence Community Information Technology
Enterprise Government and industry standards (e.g., National Information Exchange Model)
Object-Based Production
Activity-Based Intelligence
Military Force Track Data mining, machine learning, natural language processing, and information retrieval
Extending existing pedigree and provenance data items to a complete logical ontology for use across C2, IO, ISR and data domains
Practical application of Naval Tactical Cloud technologies such as Hadoop, emerging Hadoop-based standards such as Apache Spark, and data modeling architectures such as Resource Description Format
Must be U.S. citizen to obtain and maintain a DoD Security Clearance

We offer a very competitive benefits package, in a family-friendly environment.

SBG is an Equal Opportunity Employer M/F/D/V"
Data Engineer,"Functional Description/Track (Technical Individual Contributor): Establishes database management systems, standards, guidelines and quality assurance for database deliverables, such as conceptual design, logical database, capacity planning, external data interface specification, data loading plan, data maintenance plan and security policy. Documents and communicates database design. Evaluates and installs database management systems. Codes complex programs and derives logical processes on technical platforms. Builds windows, screens and reports. Assists in the design of user interface and business application prototypes. Participates in quality assurance and develops test application code in client server environment. Provides expertise in devising, negotiating and defending the tables and fields provided in the database. Adapts business requirements, developed by modeling/development staff and systems engineers, and develops the data, database specifications, and table and element attributes for an application. HandsOn development 75% of daily activities Work with our Data Architect to build robust data pipelines for our Enterprise Environment via Java (Scala) in a virtualized environment Execute on technical requirements and document new ones when needed Monitor NoSQL performance and optimize table indexes to increase query performance Participate in oncall rotation for data pipeline support in a GCP environment Support QA: diagnosing and resolving bugs Apply best practices for testing and deployment in an agile environment Understanding the technical architecture of internally developed applications Protect the confidentiality and security of client data Understand business processes and requirements from the business Review and develop dbs schema while ensuring database(s) performance is efficient and optimized Work both independently and as part of a team Manage code via Github/Bamboo and work tasks within JIRA Typically requires a Bachelor's degree in a technical discipline, and a minimum of 58 years related experience or Master's degree and 25 years equivalent industry experience or a PhD and 02 years experience. Demonstrated experience and ability to develop in Java (Scala) Primary role Demonstrated experience as a DBA supporting a Production environment Secondary role Knowledge of traditional and modern data warehouse methodologies Proven ability to learn new tools and technology Expert level SQL with the ability to create and evaluate complex SQL statements, stored procedures, and index tables Experience developing, deploying, and supporting ETL workflows Demonstrated ability to work in a fast paced and changing environment with short deadlines, interruptions, and multiple tasks/projects occurring at once Bachelor's degree in Computer Science, Information Systems, Mathematics or Engineering from an accredited academic university, or an equivalent combination of education and experience Experience with GCP infrastructure deployments application resource management in GCP environment(s) Experience developing and deploying libraries for database(s) interaction layer Experience with PostgreSQL Experience with GCP services (Dataflow/PubSub/Airflow/Spanner) Prior medical device experience Up to 10% Possesses broad understanding of technical principles and theories. Ability to synthesize external data and research findings for application that may impact technical objectives. Demonstrates successes in technical proficiency and independent thought. Works on complex problems in which analysis of situations or data requires an indepth evaluation of various factors. Exercises judgment within broadly defined practices and policies in selecting methods, techniques and evaluation criteria for obtaining results. Exercises good judgment in selecting methods and techniques for obtaining solutions. Normally receives little instruction on daytoday work, general instructions on new assignments. LICC1"
Data Engineer,"Job Requisition Number:20190821J2

Job Title: Senior Data Engineer

Salary Range or Maximum: Negotiable

Physical Address of Work Location: San Diego, CA

Relocation Assistance: None

Employment Type: Contingent upon award

Security Clearance: TOP SECRET, SCI ELIGBLE

Posted Date: August 21, 2019

Closing Date: When Filled

Desired Skill Requirements:

Desire ten (10) years of experience and knowledge in data engineering and design to include:
Navy C2 and data systems including operational and technical architectures Intelligence Community Information Technology Enterprise Government and industry standards (e.g., National Information Exchange Model)
Object Based Production Activity Based Intelligence
Military Force Track Data mining, machine learning, natural language processing, and information retrieval
Designing and developing models using large amounts of structured and unstructured data Extending existing pedigree and provenance data items to a complete logical ontology for use across C2, IO, ISR and data domains Practical application of Naval Tactical Cloud technologies such as Hadoop, emerging Hadoop-based standards such as Apache Spark, and data modeling architectures such as Resource Description Format. Incorporate UML and SysML technology in developing working MBSE for interoperable C2 systems.
Capability to obtain a TOP SECRET clearance with SCI access
Must demonstrate strong interpersonal skills including ability to communicate, both orally and in writing, and proficiency in writing reports and instructions; be able to brief senior leaders on assignments.
Job Duties:

Support the Navy Government S&T manager through all development phases of Navy C2 systems from RDT&E through delivery and execution of C2 System of Systems. Responsible to interact directly with all levels of the Government Team and their stakeholders and Customers.

Education/Equivalent: Master's Degree in Technical area.

Travel Required: Possible travel associated with role.

Comments: Apply for position online at https://industechnology.applicantpro.com/jobs; Hiring contingent upon potential work at NAVWAR, San Diego, CA."
Data Engineer,"About the Role:
The Data Engineer develops data and reporting solutions with moderate to high complexity that also have moderate to business criticality. The Engineer’s also develop data set processes; work towards improving data, efficiency and quality; use large datasets to address business issues; provide full software development lifecycle support; manage design, security, standards, data quality and compliance processes; work independently; may lead teams or projects; have in-depth domain knowledge of the relevant industry. Other responsibilities include requirement analysis, design, code, test, debug, document and maintain data and analytics solutions.
You will report to the Technology Solutions Manager who manages a Data Engineering team consisting of 12 team members in Dallas. We are a collaborative, passionate team delivering sustainable data-driven solutions on a variety of data warehousing and big data platforms to meet the needs of our customers across the Federal Reserve System.
You Will:
• Create data warehouses (very large databases, usually loaded from transaction and Enterprise Resource Planning [ERP] systems to support decision-making in an organization) and data marts (a subset of a data warehouse for a single department or function)
• Develop data mining tools and analyze to sift through large amounts of data stored in a data warehouse or data mart to find relationships and patterns
You Have:
• An Associate’s degree, a Bachelor's degree a plus
• Certified Business Intelligence Professional (CBIP) or equivalent certification a plus
• At least seven years of verifiable Data Integration (a.k.a. Extract-Transform-Load or ETL) systems design and implementation experience using Informatica PowerCenter ( v9.x or greater)
• Deployed and supported at least three such systems into production use, and have deployed at least five such systems into production use
• Verifiable integration experience with multiple concurrent data sources, relational databases and flat files
• At least 5 years of experience with Relational Database Systems, SQL, Database Architecture, Data Warehousing, Modeling and Mining, Data Analysis, Scripting Languages, Oracle
• Equivalent education and/or experience may be substituted for any of the above requirements
Why the Dallas Fed?
We are dedicated to serving the public by promoting a financial system and a healthy economy for all. These efforts take a team of dedicated individuals doing many different jobs. Together we’re creating a workplace where accomplished people can excel, and we welcome your unique background and perspective to help present the best possible solutions for our partners.
Our Benefits:
Our total rewards program offers benefits that are the best fit for you at every stage of your career:
• Comprehensive healthcare options (Medical, Dental, and Vision)
• 401K match, and a funded pension plan
• Paid vacation, holidays, and volunteer hours; flexible work environment
• Generously subsidized public transportation and free parking; annual tuition reimbursement
• Professional development programs, training and conferences
• And more…
Notes:
This position may be filled at multiple levels based on candidate's qualifications as determined by the department.
This position requires access to confidential supervisory information and/or FOMC information, which is limited to ""Protected Individuals"" as defined in the U.S. federal immigration law. Protected Individuals include, but are not limited to, U.S. citizens, U.S. nationals, and U.S. permanent residents who either are not yet eligible to apply for naturalization or who have applied for naturalization within the requisite timeframe. Candidates who are not U.S. citizens or U.S. permanent residents may be eligible for the information access required for this position and sponsorship for a work visa, and subsequently for permanent residence, if they sign a declaration of intent to become a U.S. citizen and meet other eligibility requirements.
In addition, all candidates must undergo an enhanced background check and comply with all applicable information handling rules, and all non-U.S. citizens must sign a declaration of intent to become a U.S. citizen and pursue a path to citizenship.
The Federal Reserve Bank of Dallas is proud to be an Equal Opportunity Employer that believes in the diversity
of our people, ideas and experiences, and we are committed to building an inclusive culture that represents he communities we serve.
If you need assistance or an accommodation because of a disability, please notify your Talent Acquisition Consultant."
Data Engineer,"Where good people build rewarding careers.

Think that working in the insurance field cant be exciting, rewarding and challenging? Think again. Youll help us reinvent protection and retirement to improve customers lives. Well help you make an impact with our training and mentoring offerings. Here, youll have the opportunity to expand and apply your skills in ways you never thought possible. And youll have fun doing it. Join a company of individuals with hopes, plans and passions, all using and developing our talents for good, at work and in life.
Job Description
The Big Data Engineer - is accountable for leading a technical team of developers on the Master Data Management (MDM) team to deliver innovative data quality solutions that generate business value, specifically focused on Hadoop-based, non-relational data. This role requires in-depth data experience to translate unique business requirements into technical data quality specifications that ensure data is fit-for-purpose. Engage with strategic partners to optimize the data quality function and identify strategic capabilities to keep up with data and insurance industry trends.

Data Science incorporates techniques across many disciplines including mathematics/statistics, computer programming, data engineering and ETL, software development, and high performance computing with traditional business expertise with the goal of extracting meaning from data to optimize future business decisions. Individuals in this field should be an expert/fluent in several of these disciplines and sufficiently proficient in others to effectively design, build, and deliver end to end predictive analytics products to optimize future decisions. Individual demonstrates sufficient analytic agility to quickly develop new skills across these disciplines as those disciplines evolve. The Big Data Engineer job family is accountable for end to end engineering of data solutions which includes designing and building systems for data storage and analytics that enable Allstate analysts to make better decisions to achieve Allstates goals.

This role is responsible for driving multiple complex tracks of work to deliver Big Data solutions enabling advanced data science and analytics. This includes working with the team on new Big Data systems for analyzing data; the coding & development of advanced analytics solutions to make/optimize business decisions and processes; integrating new tools to improve descriptive, predictive, and prescriptive analytics; and discovery of new technical challenges that can be solved with existing and emerging Big Data hardware and software solutions.

This role contributes to the structured and unstructured Big Data / Data Science tools of Allstate from traditional to emerging analytics technologies and methods. The role is responsible for assisting in the selection and development of other team members.
Key Responsibilities
Executes complex functional work tracks for the team.
Partners with ATSV teams on Big Data efforts.
Partners closely with team members on Big Data solutions for our data science community and analytic users.
Leverages and uses Big Data best practices / lessons learned to develop technical solutions used for descriptive analytics, ETL, predictive modeling, and prescriptive real time decisions analytics
Influence within the team on the effectiveness of Big Data systems to solve their business problems.
Participates in the development of complex technical solutions using Big Data techniques in data & analytics processes.
Supports Innovation; regularly provides new ideas to help people, process, and technology that interact with analytic ecosystem.
Participates in the development of complex prototypes and department applications that integrate Big Data and advanced analytics to make business decisions.
Uses new areas of Big Data technologies, (ingestion, processing, distribution) and research delivery methods that can solve business problems.
Understands the Big Data related problems and requirements to identify the correct technical approach.
Works with key team members to ensure efforts within owned tracks of work will meet their needs.
Drives multiple tracks of work within the research group.
Identifies and develops Big Data sources & techniques to solve business problems.
Co-mingles data sources to lead work on data and problems across departments to drive improved business & technical results through designing, building, and partnering to implement models.
Manages various Big Data analytic tool development projects with midsize teams.
Executes on Big Data requests to improve the accuracy, quality, completeness, speed of data, and decisions made from Big Data analysis.
Uses, learns, teaches, and supports a wide variety of Big Data and Data Science tools to achieve results (i.e., R, ETL Tools, Hadoop, and others).
Uses, learns, teaches, and supports a wide variety of programming languages on Big Data and Data Science work (i.e. Java, C#, Python, and Perl).
Trains more junior engineers.
Job Qualifications
Experience with various data types
Experience with development, management, and manipulation of large, complex datasets
Experience with database & ETL technologies
Demonstrated knowledge of data management competencies and implementation
Proficient at writing SQL queries and verifying the results
Familiar with organizational change management strategies
Strong communication skills, both written and verbal
Excellent time-management and organization skills
Ability to operate in a rapidly changing, high-visibility environment
Desire to learn and ability to learn quickly
Advanced analytical and problem-solving skills
Strong decision-making skills
Ability to draw meaningful conclusions and recommendations

The candidate(s) offered this position will be required to submit to a background investigation, which includes a drug screen.

Good Work. Good Life. Good Hands®.

As a Fortune 100 company and industry leader, we provide a competitive salary but thats just the beginning. Our Total Rewards package also offers benefits like tuition assistance, medical and dental insurance, as well as a robust pension and 401(k). Plus, youll have access to a wide variety of programs to help you balance your work and personal life -- including a generous paid time off policy.

Learn more about life at Allstate. Connect with us on Twitter, Facebook, Instagram and LinkedIn or watch a video.

Allstate generally does not sponsor individuals for employment-based visas for this position.

Effective July 1, 2014, under Indiana House Enrolled Act (HEA) 1242, it is against public policy of the State of Indiana and a discriminatory practice for an employer to discriminate against a prospective employee on the basis of status as a veteran by refusing to employ an applicant on the basis that they are a veteran of the armed forces of the United States, a member of the Indiana National Guard or a member of a reserve component.

For jobs in San Francisco, please click here for information regarding the San Francisco Fair Chance Ordinance.
For jobs in Los Angeles, please click here for information regarding the Los Angeles Fair Chance Initiative for Hiring Ordinance.

To view the EEO is the Law poster click here. This poster provides information concerning the laws and procedures for filing complaints of violations of the laws with the Office of Federal Contract Compliance Programs

To view the FMLA poster, click here. This poster summarizing the major provisions of the Family and Medical Leave Act (FMLA) and telling employees how to file a complaint.

It is the Companys policy to employ the best qualified individuals available for all jobs. Therefore, any discriminatory action taken on account of an employees ancestry, age, color, disability, genetic information, gender, gender identity, gender expression, sexual and reproductive health decision, marital status, medical condition, military or veteran status, national origin, race (include traits historically associated with race, including, but not limited to, hair texture and protective hairstyles), religion (including religious dress), sex, or sexual orientation that adversely affects an employee's terms or conditions of employment is prohibited. This policy applies to all aspects of the employment relationship, including, but not limited to, hiring, training, salary administration, promotion, job assignment, benefits, discipline, and separation of employment."
Data Engineer,"Data Engineer

ETL automation tools such as Informatica, Mulesoft, SSIS, Alooma, or Apache Airflow

Experience with traditional RDBMS solutions such as Microsoft SQL Server and Oracle

Experience with cloud-based platforms and tools

Familiarity with DevOps tools and practices such as Git, Jenkins, JIRA, Azure DevOps

Experience with integrating to both database systems and APIs

Experience with documenting technical requirements, designs and systems

Extensive experience building scalable and resilient data pipelines

Extensive experience writing SQL

Experience with a procedural, functional or object-oriented programming language such as Python, Java, Scala, R

Additionally, candidates should be able to perform at a high level in at least two of the following technology categories:
Big Data tools such as Apache Hadoop, Spark, and Hive including managed solutions such as Databricks, Amazon EMR, Azure HD Insight, and Google Cloud Dataproc
Cloud data storage solutions such as Amazon S3, Azure Blob Storage, or Google Cloud Storage
Data warehousing solutions such as Redshift, BigQuery, and Snowflake
Message broker solutions such as Kafka, Google Pub/Sub, Amazon Kinesis
Stream processing solutions such as Flume, Storm, Spark Streaming
NoSQL Databases such as HBase, Cassandra, Redis
Requirements

3-5+ years of experience in technology and/or consulting

Bachelor’s Degree in CS, MIS, CIS, or a comparable technical degree

US Citizen or GC Holder

Benefits

Sense Corp powers insight-driven organizations.

We turn data into actionable insights and transform organizations for the digital era.

Our people, culture, and how we engage with our clients are differentiators. Brilliant, Creative, Human, and Fun exemplify who we are. We are regularly recognized as a Best Place to Work by Austin, Houston, Dallas, and St. Louis Business Journals. With operations in Austin, Atlanta, Columbus, Dallas, Houston, San Antonio, and St. Louis we serve mid-market to Fortune 50 companies.

The Sense Corp Compass

We may be the only management consulting firm in the country where being brilliant isn’t enough to land you a job. Sense Corp people must be brilliant, creative, human, and fun all at once. In other words, we hire terrific, well-rounded people. It’s one reason clients love working with us. And it’s why we enjoy working with each other. We may not sound like typical consultants but that’s OK. We don’t think like them either.

Visit us at www.sensecorp.com."
Data Engineer,"Locations: TX - Plano, United States of America, Plano, Texas

At Capital One, were building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Data Engineer
Are you a lead technologist that thrives in a vibrant, innovative and collaborative team? Do you want to work for a tech company that writes its own code, develops its own software, and builds its own products? We experiment and innovate leveraging the latest technologies, engineer breakthrough customer experiences, and bring simplicity and humanity to banking. We make a difference for 65 million customers.

At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who love to solve real problems and meet real customer needs. We want you to be curious and ask what if? Capital One started as an information strategy company that specialized in credit cards, and we have become one of the most impactful and disruptive players in the industry. We have grown to see ourselves as a technology company in consumer finance, with great opportunities for engineers and architects who want to build innovative applications to give users smarter ways to save, transact, borrow and invest their money, as we seek to disrupt the industry again:

You will build data pipeline frameworks to automate high-volume and real-time data delivery for our Data Lake and streaming data hub
You will build data APIs and data delivery services that support critical operational and analytical applications for our internal business operations, customers and partners
You will transform complex analytical models into scalable, production-ready solutions
You will continuously integrate and ship code into our on premise and cloud Production environments
You will develop applications from ground up using a modern technology stack such as Scala, Spark, Postgres, Angular JS, and NoSQL
You will work directly with Product Managers and customers to deliver data products in a collaborative and agile environment


Responsibilities:

Develop sustainable data driven solutions with current new gen data technologies to meet the needs of our organization and business Customers
Ability to grasp new technologies rapidly as needed to progress varied initiatives
Break down data issues and resolve them
Build robust systems with an eye on the long term maintenance and support of the application
Leverage reusable code modules to solve problems across the team and organization
Utilize a working knowledge of multiple development languages


What we have:

A startup mindset with the backing of a top 10 bank
Monthly Innovation
Days dedicated to test driving cutting edge technologies
Flexible work schedules
Convenient office locations
Generous salary and merit-based pay incentives
Your choice of equipment (MacBook/PC, iPhone/Android Device)


Basic Qualifications:
Bachelors Degree
At least 2 years of experience developing software or data solutions
At least 2 years experience developing Java based software solutions or one scripting language (Python, Perl, JavaScript, Shell)
At least 2 years of experience in Spark


Preferred Qualifications:
Master's Degree
2+ years experience with Agile engineering practices
3+ years in coding in data management, data warehousing or unstructured data environments
3+ years experience working with big data technologies (Cassandra, Accumulo, HBase, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)
3+ years in coding in data management, data warehousing or unstructured data environments
3+ years experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase)
3+ years experience with Relational Database Systems and SQL
3+ years experience designing, developing, and implementing ETL

At this time, Capital One will not sponsor a new applicant for employment authorization for this position."
Data Engineer,"Founded in 2014, Vinli is an automotive technology company based in the heart of Downtown Dallas. In just four years, our team has built the most advanced vehicle data computing platform and secured global partnerships with significant players in the telecom, automotive, and asset management industries. Vinli’s fully customizable platform has enabled sophisticated brands to easily engineer connected car services, allowing them to capitalize on both their vehicle data and other related data sources. Our passion is building products that unlock the true value of connected cars by showing drivers, passengers and organizations the potential hidden within their vehicles' data.

About the Role

The Vinli Data and Analytics team is looking to add a Data Engineer to design and manage all things big data. The team covers data governance, data strategy and partnerships, reporting, machine learning and much more. You will be the cornerstone for data solutions across each of these areas. The candidate must be able to communicate, work with and deliver across each discipline as well as the end business user and executive leadership.
Responsibilities:
Lead data architect for the Vinli analytics team.
Integrate multiple data sources and software tools within the Vinli analytics ecosystem.
Collaborate with tech leaders across Vinli to ensure data strategy continuously meets all needs both internal and for Vinli customer.
Create and deliver executive presentations explaining the complex data in simple easy-to-understand terms that resonates with an executive audience.
Must haves:
BS in a STEM field.
Advanced design, coding and analytics skills in a big data ecosystem.
Expert knowledge of SQL.
Experience with other languages such as Python, R, PySpark, Java or Scala.
Strong background of data structures and big data tools (Spark, Hive, HDFS, ect.).
Data wrangling and ETL tooling experience.
Exceptional communication skills between both business and technical teams.

Preferred:
MS or higher in a STEM field.
Experience managing teams or projects.
Demonstrated experience with AWS, GCP or Azure.
Experience handling confidential and sensitive data.
Demonstrated ability to independently influence and drive outputs, meet deadlines, and set clear expectations and roadmaps.

About You:
Able to work in a fast-moving environment with high stakes for the company’s success
You take pride and responsibility seeing the product you worked on meet the real world for the first time
You love to learn and embrace the opportunity to contribute in new areas.

Vinli Core Values

Integrity. Doing what you say you will do at Vinli is our way of building trust among our team members, partners, investors and vendors. We believe that maintaining integrity requires an openness and empathy in sharing goals and challenges with others.
Drive to Innovate. People at Vinli don’t just love to learn, they feel compelled to use their knowledge to make our Company and the world a better place. We believe in learning from our mistakes and always challenging ourselves to innovate - from the biggest product decisions to the smallest processes.
Joyful Work Environment. Loving where you work isn’t about ping pong tables and free snacks. It’s the feeling that you wouldn’t want to be on a project with any other team. It’s the feeling that you can get creative energy just by showing up to work. It’s the feeling that your entire team respects your life away from the job and understands how work impacts it. At Vinli, we believe in building camaraderie and joy in our environment by supporting and encouraging each other every day.

We are an equal opportunity employer. We strictly prohibit unlawful discrimination or harassment of any kind, including discrimination or harassment on the basis of race, color, national origin, ancestry, religion, veteran status, age, pregnancy status, sex, gender identity or expression, sexual orientation, marital status, mental or physical disability, medical condition, or any other characteristics protected by law. We also make all reasonable accommodations to meet our obligations under laws protecting the rights of the disabled."
Data Engineer,"Job Description

Position: Data Engineer
Location: Dallas , TX
Total 4 candidates

VISA: USC and GC ONLY

Job Description: Detailed overview of functional and technical role expectations:
7+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
Should also have working experience using the following software/tools:
Strong Programming experience with object-oriented/object function scripting languages: Python, PySpark, Scala, etc.
Experience with big data tools: Hadoop, Apache Spark, Kafka, etc.
Experience with AWS cloud services: S3, EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with relational SQL, Snowflake and NoSQL databases, including Postgres and Cassandra.

Qualifications

null

Additional Information

All your information will be kept confidential according to EEO guidelines."
Data Engineer,"Minimum Requirements Experience developing Data Pipelines for Data Ingestion or Transformation using Java or Scala or Python Experience in the following Big Data frameworks File Format (Parquet, AVRO, ORC etc..) Developing applications with Monitoring, Build Tools, Version Control, Unit Test, TDD, Change Management to support DevOps Experience with SQL and Shell Scripting experience Experience with the Apache Spark Spark Professional work experience in Big Data development Data warehousing ETL design, development and implementation experience Python or Java development experience"
Data Engineer,"Hi,

We have an urgent requirement for Data Engineer roles in multiple locations -ÂTexas (Lewisville, Plano, Dallas) & Chicago, IL & Jersey City, NJ & Columbus, OH. Please forward suitable resumes to rakesh@athreyainc.com or call me at 732-582-4977

Role: Data Engineer

Locations:ÂTexas (Lewisville, Plano, Dallas) & Chicago, IL & Jersey City, NJ & Columbus, OH

Duration: Long Term

Mandatory Skills:

Java - 8+ Years, Spark 4+ Years, Big Data 4+ Years

Desired Skills:

Kafka - 3+ Years andÂSQL - 6+ Years.

Regards
Rakesh Sharma
Direct : (732) 582-4977
Email:Ârakesh@athreyainc.com

ATHREYA INC.

100 Jersey Avenue, Suite# B-201,
New Brunswick NJ - 08901.

""Certified Minority Business Enterprise (MBE)""
""Certified Small Business Enterprise (SBE)""
""E-Verify Enrolled Employer""
URL:Âwww.athreyainc.com

STATEMENT OF CONFIDENTIALITY

The information contained in this electronic message and any attachments hereto are intended for the sole and exclusive use of the addressee(s), and contain confidential and/or privileged information. If you are not the intended recipient of this transmission you are hereby notified that any disclosure, copying, distribution or the taking of any action in reliance on the contents of this transmission is strictly prohibited. If you have received this in error, please notify the sender of this message immediately, and destroy all copies of this message and any attachments.

""There is no power on earth that can neutralize the influence of a high, simple, and useful life.""
~ Booker T. Washington

Â"
Data Engineer,"Role Developer - Data Engineer Location Irving, TX Type of Hire Full-time Job Details Role Description Analyze and understand data sources APIs bull Design and Develop methods to connect collect data from different data sources bull Design and Develop methods to filtercleanse the data bull Design and Develop SQL , Hive queries, APIs to extract data from the store bull Work closely with data Scientists to ensure the source data is aggregated and cleansed bull Work with product managers to understand the business objectives bull Work with cloud and data architects to define robust architecture in cloud setup pipelines and work flows bull Work with DevOps to build automated data pipelines Total Experience Required bull 4 years 10 of relevant experience bull The candidate should have performed client facing roles and possess excellent communication skills Business Domain knowledge Finance banking systems, Fraud, Payments Required Technical Skills bull Big Data-Hadoop, NoSQL, Hive, Apache Spark bull Python bull Java REST bull GIT and Version Control Personal Skills bull Experienced in managing work with distributed teams bull Experience working in SCRUM methodology bull Proven sense of high accountability and self-drive to take on and see through big challenges bull Confident, takes ownership, willingness to get the job done bull Excellent verbal communications and cross group collaboration skills"
Data Engineer,"Description Could be equivalent to MgrSr. Mgr role. Job Description Minimum Requirements At least 3 years of experience developing Data Pipelines for Data Ingestion or Transformation using Java or Scala or Python At least 2 years experience in the following Big Data frameworks File Format (Parquet, AVRO, ORC etc..) At least 3 years of developing applications with Monitoring, Build Tools, Version Control, Unit Test, TDD, Change Management to support DevOps At least 3 years of experience with SQL and Shell Scripting experience At least 2 years of experience with software design and must have an understanding of cross systems usage and impact Nice to Have qualifications 2+ years of experience working with Dimensional Data Model and pipelines in relation with the same 2+ years' experience with Amazon Web Services (AWS), Microsoft Azure or another public cloud service 2+ years of experience working with Streaming using Spark or Flink or Kafka or NoSQL Intermediate level experienceknowledge in at least one scripting language (Python, Perl, JavaScript) Hands on design experience with data pipelines, joining data between structured and unstructured data Thanks Regards, Ubair Anwaar Ph (609)- 642 4135, Email ubairpalnar.com mailtoubairpalnar.com"
Data Engineer,"Your Role


The role of the Data Engineer is to work on end-to-end integration of new data sources coming in across different projects into the Innovaccer data platform.
Data Acquisition: Analyse data from varied sources like databases, CSVs, XMLs and other formats and write SQL commands/scripts/code to transform them
Data Management: Handle incoming data, validate it to ensure usability, map it to platform specifications and perform verification on the same
Platform ingestion: Get trained on and then build pipelines to import incoming data onto the platform
Issue analysis and resolution: Own the process end-to-end, identify issues & resolve the same
A Day in the Life
Design and build interfaces to facilitate workflows between client third party systems and the Data Activation Platform
Define and document best practices along with thorough message specifications
Understand Innovaccer data warehousing concepts and implement best practices
Assemble large, complex data sets that meet functional / non-functional business requirements.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs
Create data marts for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader
Work with data and analytics experts to strive for greater functionality in our data systems
What You Need
3+ years of experience in a Data Engineer role, Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Experience with relational SQL and NoSQL databases, including Postgres and MongoDB
Experience with data pipeline and workflow management tools: Azkaban
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with object-oriented/object function scripting languages: Python, Scala etc.
Experience in Healthcare Data Analytics with a focus on the understanding of healthcare data formats (CCDA, HL7, etc.)
Preferred Skills
Healthcare Data experience
Data Analytics and Visualization (PowerBI , Sisense)
Presentation and documentation skills
Value-Based Care knowledge
What We Offer
Industry-focused Certifications: We want you to be a subject matter expert in what you do. So, whether it’s our product or our domain, you will dive straight in and be certified by the best in the world.
Quarterly Rewards and Recognition Programs: We foster learning and encourage people to take moonshots. When you achieve your goals, we recognize and reward your hard work.
Health Benefits: We cover health insurance for you and your loved ones.
Sabbatical Policy: We encourage people to take time off and rejuvenate, upskill and pursue their interests so that they can generate new ideas for innovating at Innovaccer.
Pet-friendly office and open floor plan. No mundane cubicles.


Apply Here


Job Title

Data Engineer


Department

Customer Engineering


Employment Type

Full Time


Location

Dallas, TX"
Data Engineer,"Locations: TX - Plano, United States of America, Plano, Texas

At Capital One, were building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Senior Data Engineer

We are looking for driven individuals to join our team of passionate data engineers in creating Capital Ones next generation of data products and capabilities.

- You will build data pipeline frameworks to automate high-volume and real-time data delivery for our Hadoop and streaming data hub

- You will build data APIs and data delivery services that support critical operational and analytical applications for our internal business operations, customers and partners

- You will transform complex analytical models into scalable, production-ready solutions

- You will continuously integrate and ship code into our on premise and cloud Production environments

- You will develop applications from ground up using a modern technology stack such as Scala, Spark, Postgres, Angular JS, and NoSQL

- You will work directly with Product Owners and customers to deliver data products in a collaborative and agile environment

Responsibilities:

- Develop sustainable data driven solutions with current new gen data technologies to meet the needs of our organization and business Customers

- Ability to grasp new technologies rapidly as needed to progress varied initiatives

- Break down data issues and resolve them

- Build robust systems with an eye on the long term maintenance and support of the application

- Leverage reusable code modules to solve problems across the team and organization

- Utilize a working knowledge of multiple development languages

What we have:

- A startup mindset with the backing of a top 10 bank

- Monthly Innovation

- Days dedicated to test driving cutting edge technologies

- Flexible work schedules

- Convenient office locations

- Generous salary and merit-based pay incentives

- Your choice of equipment (MacBook/PC, iPhone/Android Device)

Basic Qualifications:

- Bachelors Degree

- At least 2 years in coding in data management, data warehousing or unstructured data environments

- At least 2 years experience working with leading big data technologies (Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)

Preferred Qualifications:

- Master's Degree

- 2+ years experience with Agile engineering practices

- 2+ years in-depth experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase)

- 2+ years experience with NoSQL implementation (Mongo, Cassandra)

- 2+ years experience developing Java based software solutions

- 2+ years experience in at least one scripting language (Python, Perl, JavaScript, Shell)

- 2+ years experience developing software solutions to solve complex business problems

- 2+ years experience with Relational Database Systems and SQL

- 2+ years experience designing, developing, and implementing ETL

- 2+ years experience with UNIX/Linux including basic commands and shell scripting

At this time, Capital One will not sponsor a new applicant for employment authorization for this position."
Data Engineer,"Position Role/Tile: Data Engineer
Location: Dallas, TX.

6+ months
Must Have

5+ years of work experience in a Data Engineer role

Advanced knowledge and experience in relational databases (Teradata preferred), Data Warehousing and ETL/ELT technologies

Proficiency with SQL, Hive-QL, UNIX/LINUX scripting

At least 3+ years of experience in Big Data technologies including Hadoop, Data Bricks, etc.

Hands-on experience with one or more cloud service providers (Azure preferred)

Proficient in analyzing and translating business requirements to technical requirements and modeling the Logical and Physical Data Models

Participated in performance Tuning, Data Analysis, Mapping, Loading & Validation

Should be able to work independently

Experience supporting and working with cross-functional teams in a dynamic environment.

Prior experience in working in SCALED AGILE framework

Strong Analytical skills

Nice to Have

Experience in Spark and Python is a plus

Experience with working with Teradata and Teradata utilities such as TPUMP, BTEQ, TPT

Good familiarity with the Software Development Life Cycle (SDLC) and Data Ingestion

Central Business Solutions, Inc,
37600 Central Ct.
Suite #214
Newark, CA 94560"
Data Engineer,"Where good people build rewarding careers.

Think that working in the insurance field cant be exciting, rewarding and challenging? Think again. Youll help us reinvent protection and retirement to improve customers lives. Well help you make an impact with our training and mentoring offerings. Here, youll have the opportunity to expand and apply your skills in ways you never thought possible. And youll have fun doing it. Join a company of individuals with hopes, plans and passions, all using and developing our talents for good, at work and in life.
Job Description
The Big Data Engineer - is accountable for leading a technical team of developers on the Master Data Management (MDM) team to deliver innovative data quality solutions that generate business value, specifically focused on Hadoop-based, non-relational data. This role requires in-depth data experience to translate unique business requirements into technical data quality specifications that ensure data is fit-for-purpose. Engage with strategic partners to optimize the data quality function and identify strategic capabilities to keep up with data and insurance industry trends.

Data Science incorporates techniques across many disciplines including mathematics/statistics, computer programming, data engineering and ETL, software development, and high performance computing with traditional business expertise with the goal of extracting meaning from data to optimize future business decisions. Individuals in this field should be an expert/fluent in several of these disciplines and sufficiently proficient in others to effectively design, build, and deliver end to end predictive analytics products to optimize future decisions. Individual demonstrates sufficient analytic agility to quickly develop new skills across these disciplines as those disciplines evolve. The Big Data Engineer job family is accountable for end to end engineering of data solutions which includes designing and building systems for data storage and analytics that enable Allstate analysts to make better decisions to achieve Allstates goals.

This role is responsible for driving multiple complex tracks of work to deliver Big Data solutions enabling advanced data science and analytics. This includes working with the team on new Big Data systems for analyzing data; the coding & development of advanced analytics solutions to make/optimize business decisions and processes; integrating new tools to improve descriptive, predictive, and prescriptive analytics; and discovery of new technical challenges that can be solved with existing and emerging Big Data hardware and software solutions.

This role contributes to the structured and unstructured Big Data / Data Science tools of Allstate from traditional to emerging analytics technologies and methods. The role is responsible for assisting in the selection and development of other team members.
Key Responsibilities
Executes complex functional work tracks for the team.
Partners with ATSV teams on Big Data efforts.
Partners closely with team members on Big Data solutions for our data science community and analytic users.
Leverages and uses Big Data best practices / lessons learned to develop technical solutions used for descriptive analytics, ETL, predictive modeling, and prescriptive real time decisions analytics
Influence within the team on the effectiveness of Big Data systems to solve their business problems.
Participates in the development of complex technical solutions using Big Data techniques in data & analytics processes.
Supports Innovation; regularly provides new ideas to help people, process, and technology that interact with analytic ecosystem.
Participates in the development of complex prototypes and department applications that integrate Big Data and advanced analytics to make business decisions.
Uses new areas of Big Data technologies, (ingestion, processing, distribution) and research delivery methods that can solve business problems.
Understands the Big Data related problems and requirements to identify the correct technical approach.
Works with key team members to ensure efforts within owned tracks of work will meet their needs.
Drives multiple tracks of work within the research group.
Identifies and develops Big Data sources & techniques to solve business problems.
Co-mingles data sources to lead work on data and problems across departments to drive improved business & technical results through designing, building, and partnering to implement models.
Manages various Big Data analytic tool development projects with midsize teams.
Executes on Big Data requests to improve the accuracy, quality, completeness, speed of data, and decisions made from Big Data analysis.
Uses, learns, teaches, and supports a wide variety of Big Data and Data Science tools to achieve results (i.e., R, ETL Tools, Hadoop, and others).
Uses, learns, teaches, and supports a wide variety of programming languages on Big Data and Data Science work (i.e. Java, C#, Python, and Perl).
Trains more junior engineers.
Job Qualifications
Experience with various data types
Experience with development, management, and manipulation of large, complex datasets
Experience with database & ETL technologies
Demonstrated knowledge of data management competencies and implementation
Proficient at writing SQL queries and verifying the results
Familiar with organizational change management strategies
Strong communication skills, both written and verbal
Excellent time-management and organization skills
Ability to operate in a rapidly changing, high-visibility environment
Desire to learn and ability to learn quickly
Advanced analytical and problem-solving skills
Strong decision-making skills
Ability to draw meaningful conclusions and recommendations

The candidate(s) offered this position will be required to submit to a background investigation, which includes a drug screen.

Good Work. Good Life. Good Hands®.

As a Fortune 100 company and industry leader, we provide a competitive salary but thats just the beginning. Our Total Rewards package also offers benefits like tuition assistance, medical and dental insurance, as well as a robust pension and 401(k). Plus, youll have access to a wide variety of programs to help you balance your work and personal life -- including a generous paid time off policy.

Learn more about life at Allstate. Connect with us on Twitter, Facebook, Instagram and LinkedIn or watch a video.

Allstate generally does not sponsor individuals for employment-based visas for this position.

Effective July 1, 2014, under Indiana House Enrolled Act (HEA) 1242, it is against public policy of the State of Indiana and a discriminatory practice for an employer to discriminate against a prospective employee on the basis of status as a veteran by refusing to employ an applicant on the basis that they are a veteran of the armed forces of the United States, a member of the Indiana National Guard or a member of a reserve component.

For jobs in San Francisco, please click here for information regarding the San Francisco Fair Chance Ordinance.
For jobs in Los Angeles, please click here for information regarding the Los Angeles Fair Chance Initiative for Hiring Ordinance.

To view the EEO is the Law poster click here. This poster provides information concerning the laws and procedures for filing complaints of violations of the laws with the Office of Federal Contract Compliance Programs

To view the FMLA poster, click here. This poster summarizing the major provisions of the Family and Medical Leave Act (FMLA) and telling employees how to file a complaint.

It is the Companys policy to employ the best qualified individuals available for all jobs. Therefore, any discriminatory action taken on account of an employees ancestry, age, color, disability, genetic information, gender, gender identity, gender expression, sexual and reproductive health decision, marital status, medical condition, military or veteran status, national origin, race (include traits historically associated with race, including, but not limited to, hair texture and protective hairstyles), religion (including religious dress), sex, or sexual orientation that adversely affects an employee's terms or conditions of employment is prohibited. This policy applies to all aspects of the employment relationship, including, but not limited to, hiring, training, salary administration, promotion, job assignment, benefits, discipline, and separation of employment."
Data Engineer,"Locations: TX - Plano, United States of America, Plano, Texas

At Capital One, were building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Data Engineer
Are you a lead technologist that thrives in a vibrant, innovative and collaborative team? Do you want to work for a tech company that writes its own code, develops its own software, and builds its own products? We experiment and innovate leveraging the latest technologies, engineer breakthrough customer experiences, and bring simplicity and humanity to banking. We make a difference for 65 million customers.

At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who love to solve real problems and meet real customer needs. We want you to be curious and ask what if? Capital One started as an information strategy company that specialized in credit cards, and we have become one of the most impactful and disruptive players in the industry. We have grown to see ourselves as a technology company in consumer finance, with great opportunities for engineers and architects who want to build innovative applications to give users smarter ways to save, transact, borrow and invest their money, as we seek to disrupt the industry again:

You will build data pipeline frameworks to automate high-volume and real-time data delivery for our Data Lake and streaming data hub
You will build data APIs and data delivery services that support critical operational and analytical applications for our internal business operations, customers and partners
You will transform complex analytical models into scalable, production-ready solutions
You will continuously integrate and ship code into our on premise and cloud Production environments
You will develop applications from ground up using a modern technology stack such as Scala, Spark, Postgres, Angular JS, and NoSQL
You will work directly with Product Managers and customers to deliver data products in a collaborative and agile environment


Responsibilities:

Develop sustainable data driven solutions with current new gen data technologies to meet the needs of our organization and business Customers
Ability to grasp new technologies rapidly as needed to progress varied initiatives
Break down data issues and resolve them
Build robust systems with an eye on the long term maintenance and support of the application
Leverage reusable code modules to solve problems across the team and organization
Utilize a working knowledge of multiple development languages


What we have:

A startup mindset with the backing of a top 10 bank
Monthly Innovation
Days dedicated to test driving cutting edge technologies
Flexible work schedules
Convenient office locations
Generous salary and merit-based pay incentives
Your choice of equipment (MacBook/PC, iPhone/Android Device)


Basic Qualifications:
Bachelors Degree
At least 2 years of experience developing software or data solutions
At least 2 years experience developing Java based software solutions or one scripting language (Python, Perl, JavaScript, Shell)
At least 2 years of experience in Spark


Preferred Qualifications:
Master's Degree
2+ years experience with Agile engineering practices
3+ years in coding in data management, data warehousing or unstructured data environments
3+ years experience working with big data technologies (Cassandra, Accumulo, HBase, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)
3+ years in coding in data management, data warehousing or unstructured data environments
3+ years experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase)
3+ years experience with Relational Database Systems and SQL
3+ years experience designing, developing, and implementing ETL

At this time, Capital One will not sponsor a new applicant for employment authorization for this position."
Data Engineer,"Job Description: Detailed overview of functional and technical role expectations:
 7+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
 Should also have working experience using the following software/tools:
 Strong Programming experience with object-oriented/object function scripting languages: Python, PySpark, Scala, etc.
 Experience with big data tools: Hadoop, Apache Spark, Kafka, etc.
 Experience with AWS cloud services: S3, EC2, EMR, RDS, Redshift
 Experience with stream-processing systems: Storm, Spark-Streaming, etc.
 Experience with relational SQL, Snowflake and NoSQL databases, including Postgres and Cassandra.

Job Type: Full-time

Pay: $45.00 per hour

Schedule:
Monday to Friday
Work authorization:
United States (Preferred)"
Data Engineer,"Key Responsibilities:End to end ownership of ETL data pipelines, from ingestion of data to consumption by business intelligence and advanced analytics teams
Experience in Cloud Data Warehouse Platform Snowflake, Spark processing and AWS foundational services.
Understanding of complete data analytics stack and workflow, from ETL to data platform design to BI and analytics tools.
Strong skills in data integration, data warehouses, and data processing
Design and build an automated, self-service data platform, freeing teams to focus on customer features and analysis.
Evolve existing tools and framework to support new scalability requirements as well new functionality as needed.
Identify and drive new solutions to enhance the development cycle to increase development productivity.
Work with product owners to identify and mature upcoming business needs and develop technical backlog to answer those needs in a timely manner.
Work with team to identify and resolve technical debt to improve the teams throughput.

Skills:

Strong communication skills.
Deep experience designing and implementing highly scalable, distributed application systems.
5+ years experience building data pipelines.
5+ years experience programming in Python
Extensive knowledge in fine tuning SQL, understanding optimizers, and execution plans.
Extensive experience architecting complex data models to handle millions of transactions.
Experience in application design and Implementation using agile practices & TDD.
Experience leveraging open source data infrastructure projects, such as Apache Spark, Kafka, Flink.
Strong understanding of software development life cycle and release management
Self-motivated, independent, team-player

Krishna | IT Minds LLC |

Phone: 949.534.3939 x 406 | Email: krishna@itminds.net |
: 23172 Plaza Pointe Dr, # 265 | Laguna Hills, CA 92653 |
www.itminds.net"
Data Engineer,"Position Data Engineer Location Plano, TX (Remote for now) Duration Long Term Need to have strong Pyspark, Python, AWS, Scalahellip building pipelines, working in a data lake environment"
Data Engineer,"Plano, Texas
Skills : Data Scientist, Python, Ansible, Algorithms, Data Structures, Machine Learning, Elasticsearch
Description : Looking for a Sr. Data Engineer
Experience with the following:
Data Scientist, Python, Ansible, Algorithms, Data Structures, Machine Learning, ElasticsearchData Scientist, Python, Ansible, Algorithms, Data Structures, Machine Learning, Elasticsearch"
Data Engineer,"SUMMARYThe SQL Database Engineer will provide will have an enterprise background using MS SQL Server and be able to understand complex SQL design concepts, as well as practices and procedures. This position involves identifying business requirements, developing data models, performing data analysis, writing advanced SQL queries, designing, and coding complex stored procedures and performance tuning existing database processes.

ESSENTIAL FUNCTIONS AND ACCOUNTABILITIES

• Design and develop database objects, stored procedures, views, functions, tables, triggers and SSIS packages; ensure their stability, reliability, and performance
• Troubleshoots escalated, complex problems; recommends, and reviews the implementation of associated fixes
• Design, develop, and maintain SSRS reports based on user requirements
• Write and optimize SQL statements for data access and retention
• Test databases for performance, fine-tune when necessary
• Collaborate with developers on database design, query tuning, and schema refinement
• Independently analyze, solve, and correct database related issues in real time while providing efficient resolutions
• Respond to feedback from users regarding performance and work with developers to improve the performance of queries and indexes
• Tune stored procedures and T-SQL queries to improve performance and sustainability
• Provide support for the deployment of database scripts in development, test, pre-production and production environments
• Study the technical requirements provided by product team and design databases to fulfill the requirements
• Migrate on-premises SQL databases and related workloads to Azure
• Ensures that the environment is suitable for any proposed business applications, or propose scaling the environment to meet requirements
• Utilize techniques to minimize operational costs
• Ensures that the operational team is actively monitoring the backup environment for all applicable instances
• Ensures that the operational team maintains compliance with established service level agreements

WORK EXPERIENCE

• In-depth knowledge of standard concepts, practices and procedures related to database management.
• 7+ years of experience in software development using Microsoft SQL Server 2008 R2, T-SQL and T-SQL language (queries, views, procedures), SQL Server database design, stored procedure design and implementation.
• Experience coding complex stored procedures (using T-SQL).
• Experience developing SSIS packages
• Experience with migrating on-premises databases to IaaS-based and PaaS-based SQL Databases
• Experience with Azure cloud environment strongly preferred including:
Azure Databricks
Azure Databox
Azure Synapse Analytics/SQL Data Warehouse
Azure Event Hubs
Azure Data Factory
Azure Data Lake Storage
Migrating On-Premises Databases to Azure IaaS and PaaS
• Proven analytical problem solving and debugging skills
• Experience in troubleshooting and resolving database integrity issues, performance issues, blocking and deadlocking issues, etc.
• Experience with performance tuning, query optimization, using Performance Monitor, SQL Profiler and other related monitoring and troubleshooting tools.
• Familiarity with BI technologies (e.g. Microsoft Power BI/Tableau)
• Excellent communications skills (written and verbal)

Job Requirements:"
Data Engineer,"JD:

ETL/ELT, Informatica, DBT(Data Build Tool), Snowflake, AWS S3 and Glue

Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company."
Data Engineer,"Position Role/Tile: Data Engineer
Location: Dallas, TX.

6+ months
Must Have
5+ years of work experience in a Data Engineer role
Advanced knowledge and experience in relational databases (Teradata preferred), Data Warehousing and ETL/ELT technologies
Proficiency with SQL, Hive-QL, UNIX/LINUX scripting
At least 3+ years of experience in Big Data technologies including Hadoop, Data Bricks, etc.
Hands-on experience with one or more cloud service providers (Azure preferred)
Proficient in analyzing and translating business requirements to technical requirements and modeling the Logical and Physical Data Models
Participated in performance Tuning, Data Analysis, Mapping, Loading & Validation
Should be able to work independently
Experience supporting and working with cross-functional teams in a dynamic environment.
Prior experience in working in SCALED AGILE framework
Strong Analytical skills
Nice to Have
Experience in Spark and Python is a plus
Experience with working with Teradata and Teradata utilities such as TPUMP, BTEQ, TPT
Good familiarity with the Software Development Life Cycle (SDLC) and Data Ingestion

Central Business Solutions, Inc,
37600 Central Ct.
Suite #214
Newark, CA 94560"
Data Engineer,"Need Only OPT-EAD consultant, who can able to work on W2 Job Title Data Engineer Primary Skill - JavaPython and AWS Secondary Skill Scala Duration 12+months Location Plano TX Job Description Overall 2+ years of IT Experience with expertise in delivering application leveraging agile methodologies of scrumKanban for all the SDLC phases starting from Advanced analysis to providing necessary production support for built applications. At least 1-2 years of hands on experience in building distributed data processing application leveraging spark with Scalajava or python. At least 1-2 years of hands on experience in building application on AWS (certified preferred) Experience around building data processing platformsapplications on Cloud (AWS) Good knowledge around data warehousing concept Experience around Devops leveraging Jenkins, Git and automated test-driven development approach Quick adoption to the emerging open source Technologies and perform proof of ConceptPilots to evaluate the tech stacks middot Excellent communication skills."
Data Engineer,"Innovators Wanted!!!As an experienced member of our Software Engineering Group we look first and foremost for people who are passionate around solving business problems through innovation & engineering practices. You will be required to apply your depth of knowledge and expertise to all aspects of the software development lifecycle, as well as partner continuously with your many stakeholders on a daily basis to stay focused on common goals. We embrace a culture of experimentation and constantly strive for improvement and learning. You'll work in a collaborative, trusting, thought-provoking environment-one that encourages diversity of thought and creative solutions that are in the best interests of our customers globallyThis role requires a wide variety of strengths and capabilities, including:* BS/BA degree or equivalent experience* Expertise in application, data and infrastructure architecture disciplines* Advanced knowledge of architecture, design across all systems* Proficiency in multiple modern programming languages* Knowledge of industry wide technology trends and best practices* Keen understanding of financial control and budget management* Ability to work in large, collaborative teams to achieve organizational goals, and passionate about building an innovative culture4+ years' experience with building large scale big data applications* Provide technical leadership with solutions architecture and building frameworks* Experience building Data Lake using Cloudera or Hortonworks distributions* Hands-on experience in AWS Bigdata ecosystem including AWS Glue, EMR, Athena, Redshift, QuickSight and Lake Formation* Extensive experience in Spark leveraging Python, Scala or R.* In depth knowledge of Java 8* Experience working on 1 or more NoSQL Databases such as Cassandra, HBase, MongoDB, DynamoDB, Elastic Search* Hands on experience with building CI/CD* Experience in developing software solutions leveraging Test Driven Development (TDD)* Expertise in Data governance and Data Quality* Experience working with PCI Data is a plus* Experience working with Data Scientists* In depth knowledge of OO and SOLID design principles* Demonstrable experience of successfully delivering big data projects using Kafka, Spark, Cassandra and related stack on premise or cloud* Able to tune big data solutions to improve performance* Excellent understand of Spring frameworkJPMorgan Chase & Co. is an equal opportunity employer and affirmative action employer Disability/Veteran."
Data Engineer,"Req ID: 89867At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company's growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here.NTT DATA Services currently seeks a Data Engineer to join our team in Irving, Texas (US-TX), United States (US).Role Responsibilities:* Analyze and understand data sources & APIs* Design and Develop methods to connect & collect data from different data sources* Design and Develop methods to filter/cleanse the data* Design and Develop SQL, Hive queries, APIs to extract data from the store* Work closely with data Scientists to ensure the source data is aggregated and cleansed* Work with product managers to understand the business objectives* Work with cloud and data architects to define robust architecture in cloud setup pipelines and workflows* Work with DevOps to build automated data pipelinesBasic Qualifications:* 3+ years of Advanced knowledge of Hadoop ecosystem and Big Data technologies* 3+ years of Hadoop (Cloudera) or Cloud Technologies building pipelines using Spark /Pyspark* 3+ years of experience in programming in Scala and Python* 2+ years Hadoop eco-system (HDFS, MapReduce, Yarn, Hive, Pig, Impala, Spark, Kafka,)* 2+ years ETL tools* 1 year of HTTP and invoking web-APIs* 1 year of NLP and text processingPreferences:* Machine learning engineering* Ab Initio* Work with distributed teams* SCRUM methodologyINDFSThis position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment.About NTT DATA ServicesNTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services.NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more.NTT DATA, Inc. (the ""Company"") is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result."
Data Engineer,"Data Engineer Requirements:

Â8 + years of professional experience

Â 3+ years of experience with Big data technology and analytics

Â

Â
Experience working with traditional warehouse and correlation into hive warehouse on big data technologies
Experience setting data modeling standards in Hive
Â

Â

Â

Â

Â Proficiency in using query languages such as SQL, Hive

Â Understanding of data preparation and manipulation using Datameer tool

Â Knowledge of SOA, IaaS, and Cloud Computing technologies, particularly in the AWS environment

Â Knowledge of setting standards around data dictionary and tagging data assets within DataLake for business consumption.

Â

Â Experience with data visualization tools like Tableau

Â Identifying technical implementation options and issues

Â Partners wand communicates cross-functionally across the enterprise

Â Ability to explain technical issues to senior leaders in non-technical and understandable terms

Â Foster the continuous evolution of best practices within the development team to ensure data standardization and consistency

Â Experience in agile software development paradigm (e.g., Scrum, Kanban)

Â Strong written and verbal communication

Â"
Data Engineer,"Data Engineer Ideal candidates should have experience with Data Ingestion and Consumption. That is transforming from source raw data, cleansing missing data and outliers and preparing the data ready for analytics processing. Key requirements Understanding of Kafka-Yarn-Spark-HDFS ecosystem for ingestion IS A MUST. In depth knowledge of preparing large scale data analytics for consumption's. Key knowledge on HIVE and query optimization in HIVE Banking experience related to risk management and analysis on Fraud is a plus Career progression must show initial work with Hadoop and moving on to include Kafka and Spark in the latter career Hands on experience with Spark implementation using either Spark in Scala or PySpark"
Data Engineer,"• Minimum 8 years of experience working with Hadoop, Hive,Sqoop, Spark, Scala, Python, Kafka and Sentry* Proven Experience in handling Terabyte and Petabyte of data on Cloudera Cluster* Proven Experience in handling variety of data formats* Experience in building large scale Data Lake Environment* Troubleshooting Hive Performance issues and developing HQL queries* Experience with Spark and PySpark* Experience in implementing CI/CD Process and Job Automation through Autosys* Experience in Hadoop Cluster Administration is a big plus* Experience with integration of data from multiple data sources* Assist Analytics and Data Scientist team and Business Users* Exceptional communication skills and the ability to communicate appropriately at all levels of the organization; this includes written and verbal communications as well as visualizations* The ability to act as liaison conveying information needs of the business to IT and data constraints to the business; applies equal conveyance regarding business strategy and IT strategy, business processes and work flow* Team player able to work effectively at all levels of an organization with the ability to influence others to move toward consensus* Strong situational analysis and decision making abilitiesLI-AG1"
Data Engineer,"Title: Data EngineerLocation: Plano, TX

Duration: 12 Months

Job Description:

Responsibilities for Data Engineer:

Job Description from Beeline:

Minimum Requirements:

At least 3 years of experience developing Data Pipelines for Data Ingestion or Transformation using Java or Scala or Python
At least 2 years experience in the following Big Data frameworks: File Format (Parquet, AVRO, ORC etc..)
At least 3 years of developing applications with Monitoring, Build Tools, Version Control, Unit Test, TDD, Change Management to support DevOps
At least 3 years of experience with SQL and Shell Scripting experience
At least 2 years of experience with software design and must have an understanding of cross systems usage and impact

Nice to Have qualifications:

2+ years of experience working with Dimensional Data Model and pipelines in relation with the same
2+ years experience with Amazon Web Services (AWS), Microsoft Azure or another public cloud service
2+ years of experience working with Streaming using Spark or Flink or Kafka or NoSQL
Intermediate level experience/knowledge in at least one scripting language (Python, Perl, JavaScript)
Hands on design experience with data pipelines, joining data between structured and unstructured data

Regards,

Karthik (KP)

Resource Development Manager

Direct: 469-533-7270

Cell: 469-717-0141

Email: karthik@infovision.com"
Data Engineer,"About the Role:The Data Engineer develops data and reporting solutions with moderate to high complexity that also have moderate to business criticality. The Engineer's also develop data set processes; work towards improving data, efficiency and quality; use large datasets to address business issues; provide full software development lifecycle support; manage design, security, standards, data quality and compliance processes; work independently; may lead teams or projects; have in-depth domain knowledge of the relevant industry. Other responsibilities include requirement analysis, design, code, test, debug, document and maintain data and analytics solutions.You will report to the Technology Solutions Manager who manages a Data Engineering team consisting of 12 team members in Dallas. We are a collaborative, passionate team delivering sustainable data-driven solutions on a variety of data warehousing and big data platforms to meet the needs of our customers across the Federal Reserve System.You Will:* Create data warehouses (very large databases, usually loaded from transaction and Enterprise Resource Planning [ERP] systems to support decision-making in an organization) and data marts (a subset of a data warehouse for a single department or function)* Develop data mining tools and analyze to sift through large amounts of data stored in a data warehouse or data mart to find relationships and patternsYou Have:* An Associate's degree, a Bachelor's degree a plus* Certified Business Intelligence Professional (CBIP) or equivalent certification a plus* At least seven years of verifiable Data Integration (a.k.a. Extract-Transform-Load or ETL) systems design and implementation experience using Informatica PowerCenter ( v9.x or greater)* Deployed and supported at least three such systems into production use, and have deployed at least five such systems into production use* Verifiable integration experience with multiple concurrent data sources, relational databases and flat files* At least 5 years of experience with Relational Database Systems, SQL, Database Architecture, Data Warehousing, Modeling and Mining, Data Analysis, Scripting Languages, Oracle* Equivalent education and/or experience may be substituted for any of the above requirementsWhy the Dallas Fed?We are dedicated to serving the public by promoting a financial system and a healthy economy for all. These efforts take a team of dedicated individuals doing many different jobs. Together we're creating a workplace where accomplished people can excel, and we welcome your unique background and perspective to help present the best possible solutions for our partners.Our Benefits:Our total rewards program offers benefits that are the best fit for you at every stage of your career:* Comprehensive healthcare options (Medical, Dental, and Vision)* 401K match, and a funded pension plan* Paid vacation, holidays, and volunteer hours; flexible work environment* Generously subsidized public transportation and free parking; annual tuition reimbursement* Professional development programs, training and conferences* And moreNotes:This position may be filled at multiple levels based on candidate's qualifications as determined by the department.This position requires access to confidential supervisory information and/or FOMC information, which is limited to ""Protected Individuals"" as defined in the U.S. federal immigration law. Protected Individuals include, but are not limited to, U.S. citizens, U.S. nationals, and U.S. permanent residents who either are not yet eligible to apply for naturalization or who have applied for naturalization within the requisite timeframe. Candidates who are not U.S. citizens or U.S. permanent residents may be eligible for the information access required for this position and sponsorship for a work visa, and subsequently for permanent residence, if they sign a declaration of intent to become a U.S. citizen and meet other eligibility requirements.In addition, all candidates must undergo an enhanced background check and comply with all applicable information handling rules, and all non-U.S. citizens must sign a declaration of intent to become a U.S. citizen and pursue a path to citizenship.The Federal Reserve Bank of Dallas is proud to be an Equal Opportunity Employer that believes in the diversityof our people, ideas and experiences, and we are committed to building an inclusive culture that represents he communities we serve.If you need assistance or an accommodation because of a disability, please notify your Talent Acquisition Consultant."
Data Engineer,"Onica is one of the fastest growing AWS Premier Partners in North America. As a full spectrum AWS integrator, we assist hundreds of companies to realize the value, efficiency, and productivity of the cloud. We take customers on their journey to enable, operate, and innovate using cloud technologies from migration strategy to operational excellence and immersive transformation.

If you like a challenge, you'll love it here, because we're solving complex business problems every day, building and promoting great technology solutions that impact our customers' success. The best part is, we're committed to you and your growth, both professionally and personally.

Location: Dallas, TX

Overview

Our Data Engineers are experienced technologists with technical depth and breadth, along with strong interpersonal skills. In this role, you will work directly with customers and our team to help enable innovation through continuous, hands-on, deployment across technology stacks. You will work to build data pipelines and by developing data engineering code ( as well as writing complex data queries and algorithms.

What You'll Be Doing
Build complex ETL code
Build complex SQL queries using MongoDB, Oracle, SQL Server, MariaDB, MySQL
Work on Data and Analytics Tools in the Cloud
Develop code using Python, Scala, R languages
Work with technologies such as Spark, Hadoop, Kafka, etc.
Build complex Data Engineering workflows
Create complex data solutions and build data pipelines
Establish credibility and build impactful relationships with our customers to enable them to be cloud advocates
Capture and share industry best practices amongst the Onica community
Attend and present valuable information at Industry Events
Traveling up to 50% of the time
Qualifications & Experience
3+ years design & implementation experience with distributed applications
2+ years of experience in database architectures and data pipeline development
Demonstrated knowledge of software development tools and methodologies
Presentation skills with a high degree of comfort speaking with executives, IT management, and developers
Excellent communication skills with an ability to right level conversations
Technical degree required; Computer Science or Math background desired
Demonstrated ability to adapt to new technologies and learn quickly
If you get a thrill working with cutting-edge technology and love to help solve customers' problems, we'd love to hear from you. It's time to rethink the possible. Are you ready?"
Data Engineer,"Locations: TX - Plano, United States of America, Plano, Texas

At Capital One, were building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Senior Data Engineer

We are looking for driven individuals to join our team of passionate data engineers in creating Capital Ones next generation of data products and capabilities.

- You will build data pipeline frameworks to automate high-volume and real-time data delivery for our Hadoop and streaming data hub

- You will build data APIs and data delivery services that support critical operational and analytical applications for our internal business operations, customers and partners

- You will transform complex analytical models into scalable, production-ready solutions

- You will continuously integrate and ship code into our on premise and cloud Production environments

- You will develop applications from ground up using a modern technology stack such as Scala, Spark, Postgres, Angular JS, and NoSQL

- You will work directly with Product Owners and customers to deliver data products in a collaborative and agile environment

Responsibilities:

- Develop sustainable data driven solutions with current new gen data technologies to meet the needs of our organization and business Customers

- Ability to grasp new technologies rapidly as needed to progress varied initiatives

- Break down data issues and resolve them

- Build robust systems with an eye on the long term maintenance and support of the application

- Leverage reusable code modules to solve problems across the team and organization

- Utilize a working knowledge of multiple development languages

What we have:

- A startup mindset with the backing of a top 10 bank

- Monthly Innovation

- Days dedicated to test driving cutting edge technologies

- Flexible work schedules

- Convenient office locations

- Generous salary and merit-based pay incentives

- Your choice of equipment (MacBook/PC, iPhone/Android Device)

Basic Qualifications:

- Bachelors Degree

- At least 2 years in coding in data management, data warehousing or unstructured data environments

- At least 2 years experience working with leading big data technologies (Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)

Preferred Qualifications:

- Master's Degree

- 2+ years experience with Agile engineering practices

- 2+ years in-depth experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase)

- 2+ years experience with NoSQL implementation (Mongo, Cassandra)

- 2+ years experience developing Java based software solutions

- 2+ years experience in at least one scripting language (Python, Perl, JavaScript, Shell)

- 2+ years experience developing software solutions to solve complex business problems

- 2+ years experience with Relational Database Systems and SQL

- 2+ years experience designing, developing, and implementing ETL

- 2+ years experience with UNIX/Linux including basic commands and shell scripting

At this time, Capital One will not sponsor a new applicant for employment authorization for this position."
Data Engineer,"Title Data Engineer Location Plano, TX Translates technical specifications, andor design models into code for new or enhancement projects (for internal or external clients). Develops code that reuses objects, is well structured, includes sufficient comments, and is easy to maintain. Writes programs and reportsElevates code into the development, test, and Production environments on schedule. Participates in design, code, and test inspections throughout life cycle to identify issues and ensure methodology compliance. Participates in systems analysis activities, including system requirements analysis and definition, e.g. prototyping. Participates in other meetings, such as those for use case creation and analysis.Skills RequiredMust have expertise in spark, kafka, aws, and sql Regards, Chase Melton JNIT Technologies Inc Tel 903-292-4270 Email chasejnitinc.com mailtochasejnitinc.com"
Data Engineer,"Title Big Data Engineer Summary The Big Data Engineer provides data and reporting development services andor technical support daily. Our Data Engineers also maintain data or reporting solutions (data warehousemartstoresdata lakeanalytics) using tools and programming languages. Develop data set processes, assist with design and identify ways to improve data, efficiency and quality. Responsibilities include requirement analysis, code, test, debug, document and maintain data and analytics solutions. Responsibilities bull Create data warehouses (very large databases, usually loaded from transaction and Enterprise Resource Planning ERP systems to support decision-making in an organization) and data marts (a subset of a data warehouse for a single department or function) bull Develop data mining tools and analyze to sift through large amounts of data stored in a data warehouse or data mart to find relationships and patterns Requirements bull 4+ years of experience in Data Engineering with SQL, Python experience with relational SQL and NoSQL databases experience in the following Big Data frameworks File Format (Parquet, AVRO, ORC etc.) bull 3+ years working with large data sets, streaming, experience working with distributed computing (Map Reduce, Hadoop, Hive, Apache Spark, Apache Kafka etc.) bull Working knowledge of data structures, SQL, XML, JSON, Data visualization tools, Version Control Systems, Programming, and UnixLinux shell scripting experience with AWS, GCP or Azure bull Experience with RESTful API development, Familiarity with HTTP and invoking web-APIs bull An Associatersquos degree andor Bachelor's degree a plus bull Certified Business Intelligence Professional (CBIP) or equivalent certification bull Equivalent education andor experience may be substituted for any of the above requirements Benefits Competitive Overall Benefit Plan, 401K Match, Pension, flexible work environment, free parking and more!"
Data Engineer,"Job Description: Detailed overview of functional and technical role expectations:
 7+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
 Should also have working experience using the following software/tools:
 Strong Programming experience with object-oriented/object function scripting languages: Python, PySpark, Scala, etc.
 Experience with big data tools: Hadoop, Apache Spark, Kafka, Hive etc.
 Experience with AWS cloud services: S3, EC2, EMR, RDS, Redshift
 Experience with stream-processing systems: Storm, Spark-Streaming, etc.
 Experience with relational SQL, Snowflake and NoSQL databases, including Postgres and Cassandra.
Job Type: Full-time
Pay: $40.00 per hour
Schedule:
Monday to Friday
Work authorization:
United States (Preferred)
Work Location:
One location
Visa Sponsorship Potentially Available:
Yes: H-1B work authorization
Work Remotely:
Temporarily due to COVID-19"
Data Engineer,"Job Description
Our firm has been retained by a SaaS & Data company in Allen, TX to help find a Data Engineer to join their team. The Data Engineer will be an integral part of the development team designing strategies for database systems and setting standards for operations, programming and security. This role comes with 100% of both individual and family health premiums covered. We are searching for an engineer with creative problem solving- the type of engineer who not only knows what code to write but when and how to write that code. Looking for a team member with a hunger to constantly improve and suggest innovative solutions. This person will be a team player open to collaborating and sharing resources with other team members. We are looking for developers with strong job tenure.

Roles
• Design databases and ensure their stability, reliability, and performance
• Design, create, and implement database systems based on end user requirements
• Design and create large relational databases and database tables to store applicant data
• Investigate exceptions with application, refine system performance and functionality
• Prepare documentation for database applications
• Develop database schemas, tables and dictionaries
• Ensure the data quality and integrity in databases
• Fix any issues related to database performance and provide corrective measures
• Create complex functions, scripts, stored procedures and triggers to support application development
• Review and analyze existing SQL queries for performance improvement, optimize code and suggest new queries
• Write queries used by applications and work with application developers to create optimized queries
• Perform data modeling to visualize database structure
• Integrate new systems with existing in-house structure
• Program views, stored procedures, and functions
• Develop, implement and optimize stored procedures and functions using T-SQL
• Review and interpret ongoing business report requirements, build appropriate and useful reporting deliverables and provide scheduled reports to management
• Develop procedures and scripts for data migration
• Perform security assessment for potential risks
• Provide architecture guidance and support to technical leads
• Develop best practices for database design and development activities
• Research new technologies and develop proofs of concept

Requirements
• BS/MS degree in Computer Science, Engineering or a closely related subject
• At least 5 years of experience as a Data Engineer, SQL Developer, or similar role
• At least 5 years of experience working with SQL Server Reporting Services and SQL Server Integration Services
• Excellent understanding of T-SQL programming and Microsoft SQL Server
• Knowledge of HTML and C#, .Net, Visual Studio
• Sense of ownership and pride in your performance and its impact on company’s success
• Critical thinking and problem-solving skills
• Team player with strong interpersonal and communication skills
• Effective time-management skills and deadline-oriented"
Data Engineer,"Multiple Bigdata Job Positions across Dallas, TX and Tampa, FL

Position 1: Big Data Engineer

Location: Dallas TX

Experience Required - 6+ years

Job Description
Hadoop/HDFS.
Spark is a must. Scala preferred but Java is ok too.

Experience Spark core, Spark SQL, Spark Streaming ( these 3 are a must) and Spark ML is good to have.

Position 2: Sr. Big Data Engineer

Experience Required - 8-15 years

Location: Tampa, FL

Job Description
The client is looking for a senior engineer who can drive things rather than being managed by the client.
Hadoop/Hive and Kafka.
Spark streaming using Java is a must.

Position 3: Lead Big Data Engineer

Location: Dallas TX

Experience Required - 8-15 years

Job Description
Primary requirement - Spark, Scala developer with knowledge of Kafka.
Good to have exposure to other NoSQL databases like Casandra.
AWS experience will be a definite plus."
Data Engineer,"Tiger Analytics is a fast-growing advanced analytics consulting firm. Our consultants bring deep expertise in Data Science, Machine Learning and AI. We are the trusted analytics partner for several Fortune 100 companies, enabling them to generate business value from data. Our business value and leadership has been recognized by various market research firms, including Forrester and Gartner. We are looking for top-notch talent as we continue to build the best analytics global consulting team in the world.This role will be responsible for Architecture, Designing and implementing Advanced Analytics capabilities . These capabilities include Batch and Streaming Analytics, Machine learning models, Natural Language processing and Natural language generation and other emerging technologies in the field of Advanced Analytics.* Experience in developing, deploying and operating on large scale distributed systems on a commercial scale* Experience working in Cloud-based Big Data Infrastructure eg: Azure, AWS* Good working experience on Cloud, Delta Lake, ETL processing.* Experience in Big Data technologies like HDFS, Hadoop, Hive, Pig, Sqoop, Kafka, Spark, etc.* Working knowledge on Python and PySpark Programming.* Working with a wide range of data sources like (DB2, SAP HANA etc) and intermediate expertise in SQL and PL/SQL(optional)* Ability to work with a global team, playing a key role in communicating problem context to the remote teams, stake holders and product owners.* Work in a highly agile environment* Excellent communication and teamwork skills.* Knowledge on Data Governance & Security Principles* Bachelor's degree in Computer Science or closely relatedSignificant career development opportunities exist as the company grows. The position offers a unique opportunity to be part of a small, challenging, and entrepreneurial environment, with a high degree of individual responsibility."
Data Engineer,"We are at the forefront of change in this rapidly evolving lending market. mello™, the Greek word for “future,” was the product of a recent $80+ million dollar investment in research & development to transform & streamline the home buying process into a digital experience like no other competitor offers. But mello™ is just the beginning… loanDepot will continue to invest in developing our own advanced technology ecosystem built around serving our customers & enabling our valued employees to provide exceptional service. We have funding, we have opportunities, you have ideas—it’s a perfect match. Come join us!

loanDepot — We are America’s Lender.

Position Summary:

Responsible for delivering senior-level innovative, compelling, coherent software solutions for our consumer, internal operations and value chain constituents across a wide variety of enterprise applications through the creation of discrete business services and their supporting components. The job duties and requirements are defined for the backend. The senior role provides technical leadership and mentorship to junior team members. This position ensures the performance of all duties in accordance with the company’s policies and procedures, all U.S. state and federal laws and regulations, wherein the company operates.

Responsibilities:
Designs, develops and delivers solutions that meet business line and enterprise requirements.
Creates enterprise-grade application services.
Participates in rapid prototyping and POC development efforts.
Advances overall enterprise technical architecture and implementation best practices.
Assists in efforts to develop and refine functional and non-functional requirements.
Participates in iteration and release planning.
Performs functional and non-functional testing.
Contributes to overall enterprise technical architecture and implementation best practices.
Informs efforts to develop and refine functional and non-functional requirements.
Demonstrates knowledge of, adherence to, monitoring and responsibility for compliance with state and federal regulations and laws as they pertain to this position.
Strong ability to produce high-quality, properly functioning deliverables the first time.
Delivers work product according to established deadlines.
Estimates tasks with a level of granularity and accuracy commensurate with the information provided.
Works collaboratively in a small team.
Excels in a rapid iteration environment with short turnaround times.
Deals positively with high levels of uncertainty, ambiguity, and shifting priorities.
Accepts a wide variety of tasks and pitches in wherever needed.
Constructively presents, discusses and debates alternatives.
Takes shared ownership of the product.
Communicates effectively both verbally and in writing.
Takes direction from team leads and upper management.
Ability to work with little to no supervision while performing duties.
Experience designing enterprise database systems using Microsoft SQL Server preferred.
Experience with advanced queries, stored procedures, views, triggers, etc.
Experience with indexing and normalization.
Experience of performance tuning queries.
Experience of both DDL and DML.
Experience of database administration
Experience in Visual Studio 2013/2015 and SSIS to develop enterprise ETL processes.
Understanding of data mart and data warehousing concepts including variant schemas (Star, Snowflake).
Deep understanding of one or more source/version control systems. Develops branching and merging strategies.
Working understanding of Web API, REST, JSON.
Working understanding of unit testing creation.
Knowledge of cubes and SSAS is a plus.
Requirements:
Experience in the Mortgage industry preferred.
Bachelor’s Degree preferred, and/or a minimum of four (4) + related work experience.
The Perks:
Competitive compensation reliant on ability & experience.
Excellent benefits package including multiple health, dental & vision options.
Company paid life and AD&D Insurance, as well as additional voluntary benefit possibilities.
401K with robust company match.
DTO in addition to 8 paid company holidays.
The opportunity to work for America’s Lender under the vision of industry legend, Anthony Hsieh.
We are an equal opportunity employer and value diversity in our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
Data Engineer,"About the Role:
The Data Engineer develops data and reporting solutions with moderate to high complexity that also have moderate to business criticality. The Engineer's also develop data set processes; work towards improving data, efficiency and quality; use large datasets to address business issues; provide full software development lifecycle support; manage design, security, standards, data quality and compliance processes; work independently; may lead teams or projects; have in-depth domain knowledge of the relevant industry. Other responsibilities include requirement analysis, design, code, test, debug, document and maintain data and analytics solutions.

You will report to the Technology Solutions Manager who manages a Data Engineering team consisting of 12 team members in Dallas. We are a collaborative, passionate team delivering sustainable data-driven solutions on a variety of data warehousing and big data platforms to meet the needs of our customers across the Federal Reserve System.

You Will:
Create data warehouses (very large databases, usually loaded from transaction and Enterprise Resource Planning (ERP) systems to support decision-making in an organization) and data marts (a subset of a data warehouse for a single department or function)
Develop data mining tools and analyze to sift through large amounts of data stored in a data warehouse or data mart to find relationships and patterns

You Have:
An Associate's degree, a Bachelor's degree a plus
Certified Business Intelligence Professional (CBIP) or equivalent certification a plus
At least seven years of verifiable Data Integration (a.k.a. Extract-Transform-Load or ETL) systems design and implementation experience using Informatica PowerCenter ( v9.x or greater)
Deployed and supported at least three such systems into production use, and have deployed at least five such systems into production use
Verifiable integration experience with multiple concurrent data sources, relational databases and flat files
At least 5 years of experience with Relational Database Systems, SQL, Database Architecture, Data Warehousing, Modeling and Mining, Data Analysis, Scripting Languages, Oracle
Equivalent education and/or experience may be substituted for any of the above requirements

Why the Dallas Fed?
We are dedicated to serving the public by promoting a financial system and a healthy economy for all. These efforts take a team of dedicated individuals doing many different jobs. Together we're creating a workplace where accomplished people can excel, and we welcome your unique background and perspective to help present the best possible solutions for our partners.

Our Benefits:
Our total rewards program offers benefits that are the best fit for you at every stage of your career:
Comprehensive healthcare options (Medical, Dental, and Vision)
401K match, and a funded pension plan
Paid vacation, holidays, and volunteer hours; flexible work environment
Generously subsidized public transportation and free parking; annual tuition reimbursement
Professional development programs, training and conferences
And more

Notes:
This position may be filled at multiple levels based on candidate's qualifications as determined by the department.

This position requires access to confidential supervisory information and/or FOMC information, which is limited to ""Protected Individuals"" as defined in the U.S. federal immigration law. Protected Individuals include, but are not limited to, U.S. citizens, U.S. nationals, and U.S. permanent residents who either are not yet eligible to apply for naturalization or who have applied for naturalization within the requisite timeframe. Candidates who are not U.S. citizens or U.S. permanent residents may be eligible for the information access required for this position and sponsorship for a work visa, and subsequently for permanent residence, if they sign a declaration of intent to become a U.S. citizen and meet other eligibility requirements.

In addition, all candidates must undergo an enhanced background check and comply with all applicable information handling rules, and all non-U.S. citizens must sign a declaration of intent to become a U.S. citizen and pursue a path to citizenship.

The Federal Reserve Bank of Dallas is proud to be an Equal Opportunity Employer that believes in the diversity
of our people, ideas and experiences, and we are committed to building an inclusive culture that represents he communities we serve.

If you need assistance or an accommodation because of a disability, please notify your Talent Acquisition Consultant."
Data Engineer,"Job Description
Job Summary:

Fairway is looking for a technology professional specializing in data engineering, with an emphasis on SQL Server database modeling and scripting. Responsible for expanding, optimizing, and modeling our existing data, as well as optimizing our current data flows utilizing both on premise and cloud based architecture. Support the data architect, data analysts, and mobile development team, and other cross functional teams on their data initiatives.

Essential Functions:

· Create, Improve, and modernize existing SSIS packages

· Working understanding of big data and big data modeling concepts

· Work closely with the Principal Data Architect to Develop, construct, test and maintain various data models for OLTP and OLAP Systems

· Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Azure ‘big data’ technologies

· Provide SQL tuning suggestions when necessary

· Build data pipelines from multiple 3rd part sources utilizing cloud based technology

Non-essential Job Functions:

· Participates in and leads proactive team efforts to achieve departmental and company goals.

· Adopts Fairway values in personal work behaviors, decision making, contributions and interpersonal interactions.

· Contributes to a positive work environment by demonstrating cultural expectations and influencing others to reward performance and value ""can do"" people, accountability, diversity and inclusion, flexibility, continuous improvement, collaboration, creativity and fun.

· Performs other duties as assigned.

Required Knowledge, Skills and Abilities:

· Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.

· Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.

· Ability to communicate effectively

· Ability to follow tasks assigned in Agile environment

· Strong analytic skills related to working with unstructured datasets.

· Experience supporting and working with cross-functional teams in a dynamic environment.

· Demonstrates high personal integrity and ability to earn trust from others

Required Education/Experience:

· Bachelor’s degree in computer science or a related field or equivalent work experience.

· 7+ years’ experience in Data Engineering or Database Development

· 2+ years’ experience in Cloud Services such as Azure/AWS/Google

Physical Environment:

· This position is primarily an in office position

Normal office environment
Company Description
Whether you’re looking to advance your career as a mortgage professional in our branch network or in a corporate support role, we offer a variety of opportunities to help you grow personally and professionally. Here at Fairway, we strongly believe the way we do things is just as important as what we do. Our Core Values define how we work together as a team, support individual growth, and guide us in determining how we can best serve our customers, team members and communities.

We believe that success is determined by the depth of our commitment to perform as a unified team, which is why we could not succeed without every person at Fairway. We also understand that loan originators, also known as “The Street,” are the true driving force for our company.

Our horizontal management structure provides our branch network with some of the fastest turn times in the industry giving a clear advantage in each market. We are a team-oriented, sales-minded organization that truly respects The Street’s needs by providing:
•Entrepreneurial branch platforms
•Extensive loan products and programs
•Competitive and flexible pricing
•Fast Underwriting turn times
•Exceptional Rush Team
•Knowledgeable Support Teams
•Employee Stock Ownership Plan (ESOP)
•Cutting-edge marketing resources, including free access to Home Buyers Marketing (HBM)"
Data Engineer,"Role: Sr. Application Data Engineer

Duration: 12+ Months onsite contract

Job Location: Dallas, TX

Job Description:
8-10 years of Information Technology experience with data management
5+ years of software engineering experience in Python, Scala, Java, or .NET
5+ years of experience with schema design, dimensional data modeling, and data storage technology
5+ years of multiple kinds of database experience (SQL and No-SQL)
Proven ability in managing and communicating data warehouse plans to internal clients
Experience designing, building, and maintaining secure, reliable batch and real time data pipelines
Experience with cloud data ingestion, data lake, and modern warehouse solutions (Azure is a plus)
Experience with event streaming platforms like Kafka or Azure Event Hubs
Ability to provide data architecture and engineering thought leadership across business and technical dimensions solving complex business cases
Possesses a deep understanding of enterprise software patterns and how they may be leveraged in modern data management
Knowledge of best practices and IT operations in an always-up, always-available service
Experience with or knowledge of Agile Development methodologies (SAFe is a plus)
Excellent analytical problem solving and troubleshooting skills
Excellent oral and written communication skills with a keen sense of customer service
Excellent team player with proven ability to influence
Highly adaptable to a continuously changing environment
Able to give and receive open, honest feedback and to foster a feedback environment
Outstanding communication, interpersonal, relationship building skills for team development
Possible Travel (10%)
Experience within financial services is a plus
Job Types: Full-time, Contract

Pay: $70.00 - $75.00 per hour

Experience:
Data Management: 10 years (Required)
Hive: 6 years (Required)
Kafka: 6 years (Required)
Java: 5 years (Required)
C#: 5 years (Required)
SQL: 8 years (Required)
Azure: 6 years (Required)
Python: 5 years (Required)
Work Remotely:
No"
Data Engineer,"Position-Data Engineer
Client: USAA (IBM)
Location: Plano, TX

Duration: 04 May 2020 - 27 Nov 2020

Experience in working Data Integration teams on Data Analytics and Data Warehousing engagements.
Minimum of 5 years Data Integration experience working in medium to large sized projects
Strong understanding of data warehousing, data integration, reporting and advanced analytics technologies
Strong communication skills
Experience managing client relationship and expectations
Specific knowledge of Datastage 9.X/11.x and UNIX
Prior Data integration experience with the following IBM software products:
DB2
SQL Server
UNIX scripting
Scheduling tool Control-M
Experience in Unix/Linux/Redhat
Experience leading Technical teams
Experience with ETL tool Datastage"
Data Engineer,"Net2Source is a Global Workforce Solutions Company headquartered at NJ, USA with its branch offices in Asia Pacific Region.We are one of the fastest growing IT Consulting company across the USA and We offer a wide gamut of consulting solutions customized to our 450+ clients ranging from Fortune 500/1000 to Start-ups across various verticals like Technology, Financial Services, Healthcare, Life Sciences, Oil & Gas, Energy, Retail, Telecom, Utilities, Technology, Manufacturing, the Internet, and Engineering.

Job Title: Data Engineer
Location: Richardson TX/ Chicago IL
Duration: 6+ Months Contract

Job Description:
Data Engineer- Big data (Hadoop, map reduce Spark, HIVE) with data warehousing background

About Net2Source, Inc.
Net2Source is an employer-of-choice for over 2200+ consultants across the globe. We recruit top-notch talent for over 40 Fortune and Government clients coast-to-coast across the U.S. We are one of the fastest-growing companies in the U.S. and this may be your opportunity to join us!
Want to read more about Net2Source?, Visit us at www.net2source.com

Equal Employment Opportunity Commission
The United States Government does not discriminate in employment on the basis of race, color, religion, sex (including pregnancy and gender identity), national origin, political affiliation, sexual orientation, marital status, disability, genetic information, age, membership in an employee organization, retaliation, parental status, military service, or other non-merit factor.

Net2Source Inc. is one of the fastest growing Global Workforce Solutions company with a growth of 100% YoY for last consecutive 3 years with over 2200+ employees globally and 30 locations in US and operations in 20 countries. With an experience of over a decade we offer unmatched workforce solutions to our clients by developing an in-depth understanding of their business needs. We specialize in Contingent hiring, Direct Hires, Statement of Work, Payroll Management, IC Compliance, VMS, RPO and Managed IT Services.

Fast Facts about Net2Source:
Inception in 2007, privately held, Debt free
2200+ employees globally
375+ In- house Team of Sales, Account Management and Recruitment with coast to coast COE.
30 offices in US and 50+ Offices globally
Operations in 20 countries (US, Canada, Mexico, APAC, UK, UAE, Europe, , Europe, Latin America, Japan, Australia)
Awards and Accolades:
2018 Fastest Growing IT Staffing Firm in North America by Staffing Industry Analysts
2018 Fastest-Growing Private Companies in America as a 5 times consecutive honoree Inc. 5000
2018 Fastest 50 by NJBiz
2018 Techserve Excellence Award (IT and Engineering Staffing)
2018 Best of the Best Platinum Award by Agile1
2018 40 Under 40 Award Winner by Staffing Industry Analysts
2018 CEO World Gold Award by SVUS
2017 Best of the Best Gold Award by Agile1
Regards,
Kapil Sharma
Account Manager: Client Delivery Services
Net2Source Inc.
Corp. HQ'S: 270 Davidson Ave., Suite 704, Somerset, NJ 08873, USA
T: (201) 340.8700 Ext. 440| D: (201) 479 2141| F: (201) 221.8131
Email: Kapil.sharma@net2source.com | Web: www.net2source.com

LinkedIn: http:// www.linkedin.com/in/kapil-sharma-2b1519b2/

To unsubscribe from Net2Source mailing list, click here"
Data Engineer,"Job Description

•
Minimum 8 years of experience working with Hadoop, Hive,Sqoop, Spark, Scala,
Python, Kafka and Sentry
• Proven Experience in handling Terabyte and Petabyte of data on Cloudera
Cluster
• Proven Experience in handling variety of data formats
• Experience in building large scale Data Lake Environment
• Troubleshooting Hive Performance issues and developing HQL queries
• Experience with Spark and PySpark
• Experience in implementing CI/CD Process and Job Automation through
Autosys
• Experience in Hadoop Cluster Administration is a big plus
• Experience with integration of data from multiple data sources
• Assist Analytics and Data
Scientist team and Business Users

• Exceptional communication skills and the ability to communicate
appropriately at all levels of the organization; this includes written and
verbal communications as well as visualizations
• The ability to act as liaison conveying information needs of the business
to IT and data constraints to the business; applies equal conveyance
regarding business strategy and IT strategy, business processes and work flow
• Team player able to work effectively at all levels of an organization
with the ability to influence others to move toward consensus
• Strong situational analysis and decision making abilities
#LI-AG1

Job Function

TECHNOLOGY

Role

Developer

Job Id

158552

Desired Skills

Big Data

Desired Candidate Profile

Qualifications :
BACHELOR OF ENGINEERING"
Data Engineer,"Position: Big Data Engineer
Location: Dallas , TX
Total 4 candidates

Job Description: Detailed overview of functional and technical role expectations:
 7+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
 Should also have working experience using the following software/tools:
 Strong Programming experience with object-oriented/object function scripting languages: Python, PySpark, Scala, etc.
 Experience with big data tools: Hadoop, Apache Spark, Kafka, Hive etc.
 Experience with AWS cloud services: S3, EC2, EMR, RDS, Redshift
 Experience with stream-processing systems: Storm, Spark-Streaming, etc.
 Experience with relational SQL, Snowflake and NoSQL databases, including Postgres and Cassandra.

Mandatory Technology to look for :

Python, Hadoop, Scala, Hive

Job Type: Full-time

Schedule:
Monday to Friday
Experience:
S3, EC2, EMR, RDS, Redshift: 3 years (Required)
Hadoop, Apache Spark, Kafka, Hive: 3 years (Required)
relational SQL, Snowflake and NoSQL databases: 3 years (Required)
Storm, Spark-Streaming: 3 years (Required)
Postgres and Cassandra.: 3 years (Required)
Data Engineer: 7 years (Required)
Python, PySpark, Scala: 3 years (Required)"
Data Engineer,"Position: Data Engineer
Location: Dallas , TX
Total 4 candidates

Must Have : Python, Hadoop, Scala, Hive

Job Description:

Detailed overview of functional and technical role expectations:

 7+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
 Should also have working experience using the following software/tools:
 Strong Programming experience with object-oriented/object function scripting languages: Python, PySpark, Scala, etc.
 Experience with big data tools: Hadoop, Apache Spark, Kafka, Hive ,etc.
 Experience with AWS cloud services: S3, EC2, EMR, RDS, Redshift
 Experience with stream-processing systems: Storm, Spark-Streaming, etc.
 Experience with relational SQL, Snowflake and NoSQL databases, including Postgres and Cassandra.

Job Type: Contract

Pay: $45.00 per hour

Schedule:
Monday to Friday
Experience:
AWS: 6 years (Required)
Hive: 6 years (Required)
Scala: 6 years (Required)
Hadoop: 6 years (Required)
Python: 7 years (Required)
Location:
Dallas, TX (Required)
Work authorization:
United States (Required)
Work Remotely:
Temporarily due to COVID-19"
Data Engineer,"Should have working experience in Python and Spark(Pyspark)
Experience in AWS preferably in Lambda, S3, EMR and EC2 is must.
Experience in SQL and should have data warehousing concepts.
Experience in building data pipeline and framework development using above technology is preferred."
Data Engineer,"Senior Data Engineer

Federal Reserve Bank of Dallas

TX-Dallas

Full-time

Regular

Exempt

Experienced

Yes, 10 % of the Time

Day Job

M-F, 8 am to 5 pm, with flexibility

Tier II - Credit Check

About the Role:
The Data Engineer develops data and reporting solutions with moderate to high complexity that also have moderate to business criticality. The Engineer's also develop data set processes; work towards improving data, efficiency and quality; use large datasets to address business issues; provide full software development lifecycle support; manage design, security, standards, data quality and compliance processes; work independently; may lead teams or projects; have in-depth domain knowledge of the relevant industry. Other responsibilities include requirement analysis, design, code, test, debug, document and maintain data and analytics solutions.

You will report to the Technology Solutions Manager who manages a Data Engineering team consisting of 12 team members in Dallas. We are a collaborative, passionate team delivering sustainable data-driven solutions on a variety of data warehousing and big data platforms to meet the needs of our customers across the Federal Reserve System.

You Will:
Create data warehouses (very large databases, usually loaded from transaction and Enterprise Resource Planning [ERP] systems to support decision-making in an organization) and data marts (a subset of a data warehouse for a single department or function)
Develop data mining tools and analyze to sift through large amounts of data stored in a data warehouse or data mart to find relationships and patterns
You Have:
An Associate's degree, a Bachelor's degree a plus
Certified Business Intelligence Professional (CBIP) or equivalent certification a plus
At least seven years of verifiable Data Integration (a.k.a. Extract-Transform-Load or ETL) systems design and implementation experience using Informatica PowerCenter ( v9.x or greater)
Deployed and supported at least three such systems into production use, and have deployed at least five such systems into production use
Verifiable integration experience with multiple concurrent data sources, relational databases and flat files
At least 5 years of experience with Relational Database Systems, SQL, Database Architecture, Data Warehousing, Modeling and Mining, Data Analysis, Scripting Languages, Oracle
Equivalent education and/or experience may be substituted for any of the above requirements
Why the Dallas Fed?
We are dedicated to serving the public by promoting a financial system and a healthy economy for all. These efforts take a team of dedicated individuals doing many different jobs. Together we're creating a workplace where accomplished people can excel, and we welcome your unique background and perspective to help present the best possible solutions for our partners.

Our Benefits:
Our total rewards program offers benefits that are the best fit for you at every stage of your career:
Comprehensive healthcare options (Medical, Dental, and Vision)
401K match, and a funded pension plan
Paid vacation, holidays, and volunteer hours; flexible work environment
Generously subsidized public transportation and free parking; annual tuition reimbursement
Professional development programs, training and conferences
And more...
Notes:
This position may be filled at multiple levels based on candidate's qualifications as determined by the department.

This position requires access to confidential supervisory information and/or FOMC information, which is limited to ""Protected Individuals"" as defined in the U.S. federal immigration law. Protected Individuals include, but are not limited to, U.S. citizens, U.S. nationals, and U.S. permanent residents who either are not yet eligible to apply for naturalization or who have applied for naturalization within the requisite timeframe. Candidates who are not U.S. citizens or U.S. permanent residents may be eligible for the information access required for this position and sponsorship for a work visa, and subsequently for permanent residence, if they sign a declaration of intent to become a U.S. citizen and meet other eligibility requirements.

In addition, all candidates must undergo an enhanced background check and comply with all applicable information handling rules, and all non-U.S. citizens must sign a declaration of intent to become a U.S. citizen and pursue a path to citizenship.

The Federal Reserve Bank of Dallas is proud to be an Equal Opportunity Employer that believes in the diversity
of our people, ideas and experiences, and we are committed to building an inclusive culture that represents he communities we serve.

If you need assistance or an accommodation because of a disability, please notify your Talent Acquisition Consultant."
Data Engineer,"Tachyon Technologies is a Digital Transformation consulting firm that partners with businesses to implement customer-focused business transformation. Aligned with SAP's digital core, Tachyon Technologies collaborates with its clients to transform their business by leveraging existing IT investments and leading-edge digital solutions to positively impact their customers' experience. From initiation through realization, Tachyon Technologies understands what it takes for a consulting partner to be effective and strives to deliver a meaningful solution that exceeds its clients' expectations

JD: Python Lambda, building API integration on AWS platform (with Python as the language) and hands-on developer.

Disclaimer: The above statements are not intended to be a complete statement of job content, rather act as a guide to the essential functions performed by the employee assigned to this classification. Management retains the discretion to add or change the duties of the position at any time"
Data Engineer,"We are at the forefront of change in this rapidly evolving lending market. mello, the Greek word for ""future,"" was the product of a recent $80+ million dollar investment in research & development to transform & streamline the home buying process into a digital experience like no other competitor offers. But mello is just the beginning loanDepot will continue to invest in developing our own advanced technology ecosystem built around serving our customers & enabling our valued employees to provide exceptional service. We have funding, we have opportunities, you have ideas-it's a perfect match. Come join us!loanDepot - We are America's Lender.Position Summary:Responsible for delivering senior-level innovative, compelling, coherent software solutions for our consumer, internal operations and value chain constituents across a wide variety of enterprise applications through the creation of discrete business services and their supporting components. The job duties and requirements are defined for the backend. The senior role provides technical leadership and mentorship to junior team members. This position ensures the performance of all duties in accordance with the company's policies and procedures, all U.S. state and federal laws and regulations, wherein the company operates.Responsibilities:* Designs, develops and delivers solutions that meet business line and enterprise requirements.* Creates enterprise-grade application services.* Participates in rapid prototyping and POC development efforts.* Advances overall enterprise technical architecture and implementation best practices.* Assists in efforts to develop and refine functional and non-functional requirements.* Participates in iteration and release planning.* Performs functional and non-functional testing.* Contributes to overall enterprise technical architecture and implementation best practices.* Informs efforts to develop and refine functional and non-functional requirements.* Demonstrates knowledge of, adherence to, monitoring and responsibility for compliance with state and federal regulations and laws as they pertain to this position.* Strong ability to produce high-quality, properly functioning deliverables the first time.* Delivers work product according to established deadlines.* Estimates tasks with a level of granularity and accuracy commensurate with the information provided.* Works collaboratively in a small team.* Excels in a rapid iteration environment with short turnaround times.* Deals positively with high levels of uncertainty, ambiguity, and shifting priorities.* Accepts a wide variety of tasks and pitches in wherever needed.* Constructively presents, discusses and debates alternatives.* Takes shared ownership of the product.* Communicates effectively both verbally and in writing.* Takes direction from team leads and upper management.* Ability to work with little to no supervision while performing duties.* Experience designing enterprise database systems using Microsoft SQL Server preferred.* Experience with advanced queries, stored procedures, views, triggers, etc.* Experience with indexing and normalization.* Experience of performance tuning queries.* Experience of both DDL and DML.* Experience of database administration* Experience in Visual Studio 2013/2015 and SSIS to develop enterprise ETL processes.* Understanding of data mart and data warehousing concepts including variant schemas (Star, Snowflake).* Deep understanding of one or more source/version control systems. Develops branching and merging strategies.* Working understanding of Web API, REST, JSON.* Working understanding of unit testing creation.* Knowledge of cubes and SSAS is a plus.Requirements:* Experience in the Mortgage industry preferred.* Bachelor's Degree preferred, and/or a minimum of four (4) + related work experience.The Perks:* Competitive compensation reliant on ability & experience.* Excellent benefits package including multiple health, dental & vision options.* Company paid life and AD&D Insurance, as well as additional voluntary benefit possibilities.* 401K with robust company match.* DTO in addition to 8 paid company holidays.* The opportunity to work for America's Lender under the vision of industry legend, Anthony Hsieh.We are an equal opportunity employer and value diversity in our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
Data Engineer,"About Frontier Communications

Frontier Communications provides communications services to urban, suburban,
and rural communities in 25 states. Frontier offers a variety of services to
residential customers over its FiOS and Vantage fiber-optic and copper
networks, including video, high-speed internet, advanced voice, and Frontier
Secure digital protection solutions. Frontier Business offers
communications solutions to small, medium, and enterprise businesses.

Big_Data_Engineer_-_Job_Description

Frontier communications network generates a large sum of data each day form of
communications data, network device data, log files, customer interaction data,
etc. You will be architecting a ""big data"" platform to store, process these
data to be used by data scientists and machine learning engineers. You will
have the opportunity to work with a small team of data scientists and machine
learning engineers to build products and services to improve the state of the
Frontier communications network and elevate the customer experience.
You will design, develop, test, and maintain big data infrastructure in
the cloud and on-prem locations.
You will develop ETL pipelines to collect data from various sources,
transform and store them, and enable stakeholders to consume it.
You will be developing pipelines to support machine learning application
development processes.
You will be working with different parts of a large organization to
locate, understand, and extract data from a diverse variety of systems
and transform them into a big data platform.
Monitoring data performance and modifying infrastructure as
Define data retention policies
Qualifications
Minimum:

o Computer Science degree or relevant experience.
o 5+ years of industry experience in Data Engineering
o Strong experience with MapReduce development for large datasets
(Hadoop, HDFS, YARN).
o Strong background in Linux
o 5+ years of experience in python
o Industry experience in developing ETL pipelines to manage large
datasets.
o Working knowledge of machine learning development process
Preferred:

o Master s degree in computer science
o Experience in terabyte scale data manipulation.
o Experience in ingestion tools such as sqoop, and flume is a plus.
o Experience in Kafka and airflow is a plus.
o Processing framework such as Spark and Hive is a plus
o Experience in Apache HBase is a plus
o AWS big data certification is a plus

Frontier Communications is an Affirmative Action and Equal Opportunity
Employer. All qualified applicants will receive consideration for employment
without regard to race, color, religion, sex, sexual orientation, gender
identity, national origin, or protected veteran status and will not be
discriminated against on the basis of disability."
Data Engineer,"Sr. Big Data Engineer with 7+ years of programming experience with 1 or more programming language such as: Java (preferred), Python or Scala is workable
Encourage innovation, implementation of cutting-edge technologies, inclusion, outside-of-the-box thinking, teamwork, self-organization, and diversity
Bring a passion to stay on top of tech trends, experiment with and learn new technologies, participate in internal & external technology communities, and mentor other members of the engineering community
Job Type: Contract
Schedule:
Monday to Friday
Experience:
Programming: 7 years (Required)
Big Data: 5 years (Required)
snowflake: 1 year (Preferred)
Location:
Plano, TX 75024 (Preferred)
Contract Length:
1 year
Work Remotely:
Temporarily due to COVID-19"
Data Engineer,"Job Title: Big Data Engineer

Location: Irving, TX

Duration: Full Time

Technical & Functional Skills :

Minimum 8 years of experience working with Hadoop, Hive,Sqoop, Spark, Scala, Python, Kafka and Sentry
Proven Experience in handling Terabyte and Petabyte of data on Cloudera Cluster
Proven Experience in handling variety of data formats
Experience in building large scale Data Lake Environment
Troubleshooting Hive Performance issues and developing HQL queries
Experience with Spark and PySpark
Experience in implementing CI/CD Process and Job Automation through Autosys
Experience in Hadoop Cluster Administration is a big plus
Experience with integration of data from multiple data sources
Assist Analytics and Data Scientist team and Business Users
Exceptional communication skills and the ability to communicate appropriately at all levels of the organization; this includes written and verbal communications as well as visualizations
The ability to act as liaison conveying information needs of the business to IT and data constraints to the business; applies equal conveyance regarding business strategy and IT strategy, business processes and work flow
Team player able to work effectively at all levels of an organization with the ability to influence others to move toward consensus
Strong situational analysis and decision making abilities"
Data Engineer,"Application Apply on Employer's Site The application opened in a new tab. By using this feature you agree to our Terms and Conditions and Privacy Policy.DetailsPosted: July 7, 2020

Location:Sunnyvale, California Show Map

Salary:Salary dependent on qualificatType:Full Time - ExperiencedYears of Experience:5 - 10Category:Operations

Required Education:4 Year Degree

Silicon Valley Clean Energy is seeking a Senior Data Engineer who works under the direction of the Director for Decarbonization & Grid Innovation to lead an organization-wide data strategy, and the design, implementation and management of an optimized and reliable overarching data architecture for SVCE's business functions to support achieving our decarbonization mission.

The position will be responsible to:
Design, implement and manage an optimized and reliable overarching data architecture to support SVCE's business and mission.
Develop and maintain the infrastructure required for extraction, transformation, and loading of data from a wide variety of data sources (ETL).
Identify, design, and implement internal process improvements, including automating manual processes, optimizing data delivery, and re-designing infrastructure for greater scalability.
Design and implement best practices for data storage, versioning, querying, as well as managing relevant
Build processes supporting data transformation, data structures, metadata, dependency, and workload
Collaborate across the organization and with external partners to aggregate additional relevant data sets to support business functions.
Design and develop data analytics dashboards and visualization tools. Collaborate with internal and external stakeholders to understand business and policy challenges, goals and objectives and translate them into data analytics use
Support data requests with key Silicon Valley stakeholders such as research institutions, start-ups, high-tech, students and the community

Qualifications for Senior Data Engineer

Any combination of experience and training that would likely provide the required knowledge and abilities is qualifying.
Education:
A Bachelor's Degree from an accredited university or college in science, technology, engineering, mathematics, or a related, quantitative field. A Master's Degree in the aforementioned fields can substitute for up to one year of the required experience. A Doctoral Degree in the aforementioned fields can substitute for up to three years of the required experience.
Experience:
Six (6) years of progressively responsible experience as a data engineer at an electric utility, cleantech company, software company, or similar organization
Applicants must submit a cover letter outlining key qualifications and reasons for your interest, a current resume including education and 5 years of previous employment history, and a completed copy of the SVCE job application, available at Please email these documents and any questions to: by 5 PM on July 24, 2020.

NOTES: Telecommuting is allowed.

Additional Salary Information: Competitive benefits
Create a Job Alert for Similar Jobs Connections working at Silicon Valley Clean EnergyMore Jobs from This EmployerMore Jobs Like This
Faculty Positions in Solid Mechanics and Aerospace Engineering Shenzhen, China

Southern University of Science and Technology (SUSTech) 2 Weeks Ago

Traffic Engineer Technician Overland Park, Kansas

City of Overland Park, KS Today

Senior Construction Materials Testing Technician El Paso, Texas

Wood Yesterday
BACK TO TOP"
Data Engineer,"We are looking for a passionate Data Engineer to add to Credera's Data & Analytics practice. Our ideal candidate is excited about being on project-based teams in a client facing roles to architect and implement enterprise data solutions. The data engineer will be part of our team of strategy and technology consultants, contributing to projects focused on data strategy, data science, modern data architecture, data visualization and insights. They are an experienced data pipeline developer who enjoys data wrangling, optimizing data flows, and building solutions for data collection and analysis. They are experienced with developing and deploying cloud-based data solutions. They are team player who is passionate about building team culture. They enjoy a fast-paced environment, solving business problems with the use of data. The data engineer will be comfortable applying their expertise across many use cases in various industries.

QUALIFICATIONS:
Candidate with 3+ years of experience in a Data Engineer role
Degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field
Experience building and optimizing data pipelines and data architecture
Experience with wrangling, exploring, and analyzing data to answer specific business questions and identify opportunities for improvement
Strong project management and organizational skills
Consulting experience is preferred
The ideal candidate will have working knowledge of the following:
Big data tools (e.g. Hadoop, Spark, Kafka, etc.)
Relational SQL and NoSQL databases (e.g. Postgres, MySQL, SQL Server, Cassandra, MongoDB, etc.)
Data pipeline and workflow management tools (e.g. Azkaban, Oozie, Luigi, Airflow, etc.)
Stream-processing systems (e.g. Storm, Spark-Streaming, etc.)
Scripting languages (e.g. Python, Java, C++, Scala, etc.)
Container Orchestration (e.g. Kubernetes, Docker, etc.)
Experience with one or more of the following cloud service providers:
AWS cloud services
Google Cloud Platform
Azure cloud services
Travel: Up to 25%

LEARN MORE:

We do not currently commence ""sponsor"" immigration cases in order to employ candidates.

Credera is a management consulting, user experience, and technology solutions firm with offices in Dallas, Denver, Houston, Chicago, and New York. We work with clients ranging from Fortune 500 companies to emerging industry leaders, and provide expert, objective advice to help solve complex business and technology challenges. Our deep capabilities in strategy, organization, process, analytics, technology and user experience help our clients improve their performance. Clients depend on our ability to anticipate, recognize, and address their specific needs. Credera's consultants work with some of the world's best known brands in a variety of industries, including one of the top five fast food chains, leading energy organizations, retailers, airlines. More information is available at www.credera.com. We are part of the OPMG Group of Companies, a division of Omnicom Group Inc.

Along with a great company culture, Credera provides an outstanding compensation package including a competitive salary and a comprehensive benefit plan (e.g., medical, dental, disability, matching 401k, PTO, etc.). This position is an exempt position.

U.S. Equal Opportunity Employment Information (Completion is voluntary)

Individuals seeking employment at Credera are considered without regards to race, color, religion, national origin, age, sex, marital status, ancestry, physical or mental disability, veteran status, gender identity, or sexual orientation. Completion of the last four questions in our application is entirely voluntary. Whatever your decision, it will not be considered in the hiring process or thereafter. Any information that you do provide will be recorded and maintained in a confidential file."
Data Engineer,"Innovators Wanted!!!As an experienced member of our Software Engineering Group we look first and foremost for people who are passionate around solving business problems through innovation & engineering practices. You will be required to apply your depth of knowledge and expertise to all aspects of the software development lifecycle, as well as partner continuously with your many stakeholders on a daily basis to stay focused on common goals. We embrace a culture of experimentation and constantly strive for improvement and learning. You'll work in a collaborative, trusting, thought-provoking environment-one that encourages diversity of thought and creative solutions that are in the best interests of our customers globallyThis role requires a wide variety of strengths and capabilities, including:* BS/BA degree or equivalent experience* Expertise in application, data and infrastructure architecture disciplines* Advanced knowledge of architecture, design across all systems* Proficiency in multiple modern programming languages* Knowledge of industry wide technology trends and best practices* Keen understanding of financial control and budget management* Ability to work in large, collaborative teams to achieve organizational goals, and passionate about building an innovative culture* 7+ years' experience with building large scale big data applications* Provide technical leadership with solutions architecture and building frameworks* Experience building Data Lake using Cloudera or Hortonworks distributions* Hands-on experience in AWS Bigdata ecosystem including AWS Glue, EMR, Athena, Redshift, QuickSight and Lake Formation* Extensive experience in Spark leveraging Python, Scala or R.* In depth knowledge of Java 8* Experience working on 1 or more NoSQL Databases such as Cassandra, HBase, MongoDB, DynamoDB, Elastic Search* Hands on experience with building CI/CD* Experience in developing software solutions leveraging Test Driven Development (TDD)* Expertise in Data governance and Data Quality* Experience working with PCI Data is a plus* Experience working with Data Scientists* In depth knowledge of OO and SOLID design principles* Demonstrable experience of successfully delivering big data projects using Kafka, Spark, Cassandra and related stack on premise or cloud* Able to tune big data solutions to improve performance* Excellent understand of Spring frameworkJPMorgan Chase & Co. is an equal opportunity employer and affirmative action employer Disability/Veteran."
Data Engineer,"Client- City Bank
Location- Irving, TX
FullTime Role

• Minimum 8 years of experience working with Hadoop, Hive,Sqoop, Spark, Scala, Python, Kafka and Sentry
• Proven Experience in handling Terabyte and Petabyte of data on Cloudera Cluster
• Proven Experience in handling variety of data formats
• Experience in building large scale Data Lake Environment
• Troubleshooting Hive Performance issues and developing HQL queries
• Experience with Spark and PySpark
• Experience in implementing CI/CD Process and Job Automation through Autosys
• Experience in Hadoop Cluster Administration is a big plus
• Experience with integration of data from multiple data sources
• Assist Analytics and Data Scientist team and Business Users

• Exceptional communication skills and the ability to communicate appropriately at all levels of the organization; this includes written and verbal communications as well as visualizations
• The ability to act as liaison conveying information needs of the business to IT and data constraints to the business; applies equal conveyance regarding business strategy and IT strategy, business processes and work flow
• Team player able to work effectively at all levels of an organization with the ability to influence others to move toward consensus
• Strong situational analysis and decision making abilities

Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company."
Data Engineer,"JD Skills : Senior Data Engineer

Â
At least 8 years of experience in software application development
At least 3 years' experience with Big Data / Hadoop architecture and related technologies
Hands-on experience with Spark â RDDs, Datasets, Dataframes, Spark SQL,
Hands-on experience with streaming technologies such Spark Streaming and Kafka
Hands-on experience using SQL, Spark SQL, HiveQL and performance tuning for big data operations
Hands-on experience with Java 8 and use of IDEs for the same
Hands-on experience using technologies such as Hive, Pig, Sqoop,
Experience building micro-services based application
Experience dealing with SQL and NoSQL databases such as Oracle, DB2, Teradata, Cassandra
Experience using CI/CD processes for application software integration and deployment using Maven, Git, Jenkins, Jules
Experience building scalable and resilient applications in private or public cloud environments and cloud technologies
Experience using SDLC and Agile software development practices
Experience building enterprise applications enabled for logging, monitoring, alerting and operational control
Experience enabling scheduling for big data jobs
Hands-on experience working in unix environment
Good written, verbal, presentation and interpersonal communication skills, given an opportunity willing to work in a challenging and cross platform environment.
Strong Analytical and problem-solving skills. Ability to quickly master new concepts and applications
Preferable â experience in Financial industry
Preferable â experience in Data Science, Machine Learning, Deep learning, Business Intelligence and Visualization.
Â

Â"
Data Engineer,"Title: Big Data EngineerLocation: Plano, TX

Duration: 12+ Months contract

Minimum 10 Years of IT experience required

Description:

At least 3 years of experience developing Data Pipelines for Data Ingestion or Transformation using Java or Scala or Python
At least 2 years experience in the following Big Data frameworks: File Format (Parquet, AVRO, ORC etc..)
At least 3 years of developing applications with Monitoring, Build Tools, Version Control, Unit Test, TDD, Change Management to support DevOps
At least 3 years of experience with SQL and Shell Scripting experience
At least 2 years of experience with software design and must have an understanding of cross systems usage and impact
2+ years of experience working with Dimensional Data Model and pipelines in relation with the same
2+ years experience with Amazon Web Services (AWS), Microsoft Azure or another public cloud service
2+ years of experience working with Streaming using Spark or Flink or Kafka or NoSQL
Intermediate level experience/knowledge in at least one scripting language (Python, Perl, JavaScript)
Hands on design experience with data pipelines, joining data between structured and unstructured data

Thanks & Regards

Kumar Beeram

Manager- Resource Development

D: 469- 533- 7235

T: 972-234-0058 X 7235

E: kumar.beeram@infovision.com"
Data Engineer,"Hi, I am rehman from SLIQ IT, SLiQ Information Technologies is a software development services firm, based in Edison, NJ. At SliQ IT, we pride ourselves on using the best technologies to solve our clients toughest challenges. We are currently hiring for one of our clients for below job role NOTE NEED GC AND CITIZENS( NO OPTCPTH1) Job Description Senior Data Engineer with excellent Programming Skills (JavaPython) from Data Background, with AWS DevOps Exposure Sr Data Engineer - Python, Scala and Java is preferred. - AWS - Big Data tools - Spark and Hadoop - Ec2, S3, ECS, and other related services within AWS - Drive initiatives and guide juniors with tech challenges - Highly skilled in programming (Java preferred) - SQL is required - Snowflake data warehousing used but not mandatory - 6 years minimum of programming skills - 3 years in big data and AWS environment - ETL processes and knowledge as a foundation but no expectation to use ETL tools If you are interested and possess the suitable skillset required for this job role Kindly share your updated resume and below details to rehmansliqit.com Full Name Current Location Visa Status Total IT Experience Expected Hourly RateAnnual Salary Phone Number Email ID Date of Birth Year of Gradution Last 4 Digits SSN Skype ID Linkedin ID 2 References"
Data Engineer,"Title: Software Engineering - Big Data
Location: Lewisville, TX
Duration: 12+ months

Expert in Big Data and design technique as well as experience working across large environments with multiple operating systems/infrastructure for large-scale programs (e.g., Expert Engineers) starting to be firm-wide resources working on projects across Client.
§ Is multi-skilled with expertise across software development lifecycle and toolset
§ May be recognized as a leader in Agile and cultivating teams working in Agile frameworks
§ Sought out as coach for at least one technical skill
§ Strong understanding of techniques such as Continuous Integration, Continuous Delivery, Test Driven Development, Cloud Development, resiliency, security
§ Stays abreast of cutting edge technologies/trends and uses experience to influence application of those technologies/trends to support the business; may give speeches and outside the firm, writes articles

Job Type: Contract

Schedule:
Monday to Friday
Work Remotely:
Temporarily due to COVID-19"
Data Engineer,"Title Senior Data Engineer City Plano, TX Duration 12+ Months Must have skills AWS certified SQL Spark Nice to have Tableau reporting or any reporting tools Data Engineer Create and maintain optimal data pipeline architecture, Assemble large, complex data sets that meet functional non-functional business requirements. Identify, design, and implement internal process improvements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS lsquobig datarsquo technologies. Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Keep our data separated and secure across national boundaries through multiple data centers and AWS regions. Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader. Work with data and analytics experts to strive for greater functionality in our data systems. Qualifications for Data Engineer Advanced working SQL knowledge and experience working with relational databases,query authoring (SQL) as well as working familiarity with a variety of databases. Experience building and optimizing lsquobig datarsquo data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. Strong analytic skills related to working with unstructured datasets. Build processes supporting data transformation, data structures, metadata, dependency and workload management. A successful history of manipulating, processing and extracting value from large disconnected datasets. Working knowledge of message queuing, stream processing, and highly scalable lsquobig datarsquo data stores. Strong project management and organizational skills. Experience supporting and working with cross-functional teams in a dynamic environment. Best regards httpslh5.googleusercontent.comb-g1iUrSj7YML7Un0xN0QD622MGpMiHBQIi4haC--CddN3uyfsUcIw3g httpswww.infovision.com httpslh5.googleusercontent.comynd3Hsuw2UVGJVZ6mf0CAuN1X8y2jHP0JA- httpswww.ivlglobal.com cidimage003.png01D5B023.1A61D220 httpsinfovisionsocial.com httpslh6.googleusercontent.comwNRdyop1uFlrgojoRrfmKVVmpChOM4sPrI-hEkPf2sq2eT4PCushelGGXu65Ws7GFEE--Cgxw8FJ httpswww.vcollab.com Charan Varanasi Resource Development Manager c 972-792-9969 e charaninfovision.com mailtocharaninfovision.com cidimage002.png01D54227.DDA0D340 httpswa.me19724271954 httpswa.me19727929969 httpswa.me19727929969 cidimage003.png01D54227.DDA0D340 httpwww.linkedin.cominvvdcharan www.linkedin.cominvvdcharan httpwww.linkedin.cominvvdcharan"
Data Engineer,"Our Data Warehouse team is looking to add a Senior Data Engineer to their team. A qualified applicant will have 5-7 years of experience in a Data Engineer position; as well as a background in supporting data transformation, data structures, metadata, dependency, and workload management. Expert experience with Talend and SQL is highly desired.

Here's what you can expect from the job and what you need to be successful:

Job Duties
Create and maintain optimal data pipeline architecture
Assemble large and complex data sets to meet functional/non-functional business requirements
Build a data pipeline infrastructure for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, cloud based relational, and/or non-relational databases employing Talend and/or scripting languages like Perl/Python
Identify, design, and implement internal process improvements, including automating manual processes, optimizing data delivery, and re-designing infrastructure for greater scalability
Work with key stakeholders, including members of the executive, product, data and design teams to assist with data-related technical issues and support data infrastructure needs
Create data tools for analytics and data scientist team members that assist in building and optimizing data integration platform to serve credit union business data needs
Partner with data and analytics experts to ensure greater functionality of data integration platform
Follow established configuration/change control processes
Identify options for ETL, data, and data warehouse potential solutions and assess for technical and business suitability
Essential Skills
5-7 years of experience in a Data Engineer or similar position
Minimum 3 years of developing ETL processes using Talend, informatica cloud including the processing of NoSQL and JSON formats
Talend experience is a requirement
Advanced working knowledge in employing Talend tool to build robust data pipelines. Talend certification is preferred
Advanced working knowledge in scripting languages, including Perl/Shell& Python
Advanced working SQL knowledge and experience working with relational/non-relational databases, query authoring (SQL), and excellent SQL troubleshooting skills
Experience performing root cause analysis on internal and external data and processes to respond to specific business questions and identify opportunities for improvement
Experience in building processes to support data transformation, data structures, metadata, dependency, and workload management
Experience in working with an agile project management environment
Exposure to basic issues in working within a Cloud (Azure/AWS ) environment
Experience in building data pipelines in a Big Data environment
Excellent problem solving and critical thinking skills with the ability to carry out assigned tasks with limited oversight
Superb verbal and written communication skills
Bachelor’s degree in Business Administration, Computer Science or other related fields of study or equivalent work experience. Experience in financial services industry is preferred
Location: San Jose, CA 95134

First Tech may consider Visa sponsorship for this position,but only for existing H1B (no new).

HP123"
Data Engineer,"About the Position


Mist, a Juniper Company, is the first vendor to bring enterprise-grade Wi-Fi, BLE and IoT together through a highly scalable cloud architecture. Our mission is to deliver personalized location-based wireless services by making Wi-Fi predictable, reliable and measurable. At Mist, we built the first AI-empowered platform to provide an unprecedented visibility into the user experience. Mist is the new global standard for many fortune 500 companies.

The mission of the Data Science team at Mist is to deliver AI driven self-driving network solution. We build analytics infrastructure, insights, models and tools, to empower our AI platform. Data Engineers on the team will be the enabler and amplifiers. This position offers limitless opportunities for an ambitious data science engineer to make an immediate and meaningful impact within a hyper growth team.

We are looking for a talented and driven individual to partner closely with data scientists, build and scale our self-driving network solution. He/she is encouraged to think out of the box and play with the latest technologies while exploring their limits. Successful candidates will have strong technical capabilities, a can-do attitude, and are highly collaborative.

Minimum Qualifications:
Fluent with Python, Java, or Scala
5+ years of experience of developing and managing streaming or batch data pipelines. Hands on experience with either one or all, Storm, Spark, Kafka and Flink .
Familiarity with cloud based platforms - AWS or GCP
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Highly motivated to build a great product and great codebase in a fast-paced environment
Strong communication skills with a growth and learning mindset
Preferred Qualifications:

MS in Computer Science, Electrical Engineering or other Engineering majors with 10+ years of total experience.
Knowledge and experiences of using machine learning tools such as Numpy, ScikitLearn, MLlib, Tensorflow
Juniper Networks is enrolled in E-Verify® and will be participating in E-Verify in addition to our Form I-9 process. www.dhs.gov/E-Verify
Juniper Networks is an Equal Opportunity/Affirmative Action Employer."
Data Engineer,"Grow your career at AppLovin.

AppLovin is a global leader in mobile entertainment. Its studios create popular, immersive mobile games and its technology brings games to more players around the world. Since 2012, the company's platform has been instrumental in driving the explosive growth of mobile games, resulting in a richer ecosystem and more games played by millions of people every day. AppLovin is headquartered in Palo Alto, California with several offices globally. Learn more at applovin.com.

AppLovin is one of Inc.'s Best Workplaces and a recipient of the 2019 Glassdoor Top CEO employee's choice award. The San Francisco Business Times awarded AppLovin one of the Bay Area's Best Places to Work in 2019 and 2020, and the Workplace Wellness Award in 2019 which recognizes businesses that are leaders in improving worker well-being.

About You:
Have 1-3 years of experience and a minimum of a BS and/or MS in Computer Science
Have excellent knowledge of computer science fundamentals including data structures, algorithms, and coding
Experience independently creating and maintaining projects
Product focused mindset
Have experience working with big data systems (Spark, Hadoop, Pig, Impala, Kafka)
Experience designing, building, and maintaining data processing systems
Experience with a backend language such as Java or Scala
About the Role:
Collaborate with various engineering teams to meet a wide range of technological challenges
Work closely with product and business teams to improve data models that feed business intelligence tools
Define company data models using Spark
Perks:
Free medical, dental, and vision insurance
Daily lunches and fully stocked kitchen
Free public transit
Free laundry service (wash/dry clean)
Free gym membership
401k matching
Fun company parties and events
Autonomy to make decisions in a rapidly growing company
Flexible Time Off - work hard and take time when you need it
Interested? Send us your resume and let's talk!

#LI-JZ1"
Data Engineer,"Plume develops and deploys cloud based control planes with scale to manage tens of millions of customer homes through some of the world's largest Internet Service Providers. Our cloud applications include WiFi network management and optimization, device access control, network provisioning, IoT security, and end customer user interaction through mobile apps.

We are growing our team and looking for talented individuals to help us define and drive the success of our cloud based service offering. Our focus is on the home market and we support B2B and B2C product offerings.

The Opportunity:

As a Senior Data Engineer at Plume, you will focus on providing actionable insights and build highly available resilient systems that will impact and help make business decisions. You will ensure that the platform service we are delivering will meet both our and our customers' reliability expectations.

What you will do:
Build infrastructure and abstractions that can enable anyone (engineer or data scientist) to craft a scalable ETL pipeline for whatever the purpose is: metrics, analysis, machine learning, dashboard visualizations
Work closely with ops team to monitor and tune existing infrastructure.
Make intuitive decisions about what services, frameworks, and capabilities need to be in place before they are needed.
Build and maintain a data collection system that robustly extracts meaningful data from multiple sources and data stores.
Build analytics and Machine learning platforms to collect, store, process, and analyze huge sets of data
Who you are:
BA/BS in Computer Science, Information Systems or related technical field.
3+ years of experience in Data Infrastructure, with Cloud SW experience a plus.
Experience in crafting and scaling data infrastructure, models, and pipelines
Hands-on experience with a variety of data infrastructures, such as:
Processing: Spark, Flink, Hadoop, Lambda
Messaging: Kafka, Zookeeper, Pulsar
Storage: Hive, Mongo DB, Athena, Phoenix, Splice, Redshift, DynamoDB
Machine Learning: Sagemaker, H2O, Keras, NumPy
Open and active in sharing knowledge as well as excellent communication skills
Programming experience in one or more application or systems languages including Scala, Java, or Python
Have an ability to own a project from inception to completion"
Data Engineer,"Get your career started at eHealth


eHealthInsurance has many exciting career opportunities in a number of locations, across various functions. Come join us today!

Data Engineer

At eHealth, we are passionate about solving our nation's toughest problems to bring more suitable, accessible, and affordable health insurance to Americans. We are seeking a talented data engineer to join our growing data team, which is already making a valuable impact on the entire company. This person will help us develop cutting-edge data tools and pipelines to drive better and faster decision making within our company and to better serve our customers. This is a fast-paced, collaborative, and iterative environment requiring quick learning, agility, and flexibility.

Responsibilities:
Become the subject matter expert on our data and its capabilities. Your scope of knowledge will need to include various data systems that are specialized to internal departments and 3rd party data platforms.
Design and build highly scalable data integration / ETL pipelines to improve data accessibility and consumption.
Automate data processing using workflows tools to schedule and manage dependency of various data pipelines.
Work directly with data scientists to develop scalable implementation of statistical and machine learning models in production, and work with software engineers to design, build, and maintain APIs to interact with those models.
Recommend ways to improve data reliability, efficiency, and quality.
Assist eHealth’s data architect with logical and physical data model designs and documentation.
Work with data infrastructure team to triage issues and support issue resolution.
Minimum Qualifications:
Bachelors or Masters in Computer Science, Engineering, or a related quantitative field.
2+ years of experience with designing, implementing and maintaining scalable and reliable data pipelines
Mastery of SQL in writing complex and high performance queries
Working experience with MPP systems (Snowflake, Spark SQL, Hive) and NoSQL systems (MongoDB, etc).
Production coding experience with Python, Scala or Java and scripting languages (Unix shell) as well as solid experience with git.
Expertise with relational databases and experience with schema design and dimensional data modeling.
Working experience with various ETL technologies and frameworks (Pentaho, Informatica, Matillion, etc.)
Knowledge of AWS data tools.
Excellent communication skills.
Highly motivated problem-solver who enjoys working in a fast-paced environment and can also be patient with the pace of highly regulated industries like healthcare.
Nice to Have:
Experience collaborating with data science team.
Strong experience in designing and implementing data APIs.
Product familiarity with Adobe Analytics, Cisco systems, Snowflake.
Familiarity with workflow management tools (Airflow).
Working experience with data warehousing.
Ability to create beautiful data visualizations using D3, Tableau, or similar tools.
Working experience with large healthcare related datasets, including EHRs, medical claims data, and health population surveys. Experience in building healthcare data pipelines would be a big plus.
Knowledge of healthcare insurance industry, products, systems, business strategies, and products.
Experience working with call center operations.
eHealth is an Equal Employment Opportunity employer. It is our policy to provide equal opportunity to all employees and applicants and to prohibit any discrimination because of race, color, religion, sex, national origin, age, marital status, sexual orientation, genetic information, disability, protected veteran status, or any other consideration made unlawful by applicable federal, state or local laws. The foundation of these policies is our commitment to treat everyone fairly and equally and to have a bias-free work environment.

If you are interested in applying for employment with eHealth and need special assistance or an accommodation to apply for a posted position contact us at: accommodations@ehealthinsurance.com."
Data Engineer,"At SpringML, we are all about empowering the doers in companies to make smarter decisions with their data. Our predictive analytics products and solutions apply machine learning to todays most pressing business problems so customers get insights they can trust to drive business growth. We are a tight knit, friendly team of passionate and driven people who are dedicated to learning, get excited to solve tough problems and like seeing results, fast.

Your primary role will be to design and build data pipelines. You will be focused on designing and implementing solutions on Hadoop, Spark, Pig, Hive. In this role you will be exposed to Google Cloud Platform including Dataflow, BigQuery and Kubernetes so the ideal candidate will have a strong big data technology foundation and bring a passion to learn new technologies. If you believe you have these skills please email your resume to info@springml.com.

Required Skills:
4-7 years Python and Java programming
3-5 years knowledge of Java/J2EE
3-5 years Hadoop, Big Data ecosystem experience
3-5 years of Unix experience
Bachelors in Computer Science (or equivalent)
Duties and Responsibilities:
Design and develop applications utilizing the Spark and Hadoop Frameworks or GCP components.
Read, extract, transform, stage and load data to multiple targets, including Hadoop, Hive, BigQuery.
Migrate existing data processing from standalone or legacy technology scripts to Hadoop framework processing.
Should have experience working with gigabytes/terabytes of data and must understand the challenges of transforming and enriching such large datasets.
Additional Skills that are a plus:
C, Perl, Javascript or other programming skills and experience a plus
Production support/troubleshooting experience
Data cleaning/wrangling
Data visualization and reporting
Devops, Kubernetes, Docker containers
Powered by JazzHR"
Data Engineer,"When you come across a sea of data, is your first instinct to put on your software diving suit and go deep? We’re looking for highly-skilled Data Engineers to design and automate large scale data solutions that power our state-of-the-art artificial intelligence platform. If “changing the world” is on your to-do list, Entefy is your chance to make a career of it.

We’re redefining digital interaction, and our next Data Engineer will play a key role in the growing agile team that’s making it all happen.
Requirements
6+ years relevant experience developing and integrating frameworks and database technologies that support highly scalable data processing
Advanced knowledge of system architecture and database design
Proficiency in Big Data tools: Spark, Hadoop, Kafka, etc.
Advanced experience with SQL and NoSQL database architecture and implementation (hands-on experience with PostgreSQL, Elasticsearch, and Cassandra a plus)
Demonstrable experience designing, developing, and implementing ETL processes
Experience working with private cloud infrastructure
Demonstrable experience building and optimizing Big Data pipelines and architecture
Proficiency in Python, Java, C++
Demonstrable experience with Stream Processing and workload management for data transformation, augmentation, analysis, etc.
Ability to collaborate well with others
Strong communication skills
Visit www.entefy.com and www.blog.entefy.com"
Data Engineer,"On April 1, 2020, Rubicon Project and Telaria, Inc. merged to create one company. The combined company will rebrand as one in the coming months. In the interim, each company will operate under its existing brand name. The ticker symbol for the combined public entity will remain NYSE: RUBI.

We are looking for Data Engineers to help us build tools, enhance our platform, and leverage our vast amounts of advertising data to make informed decisions around business optimizations and efficiencies. We're close to the customers and have the reward of seeing our work being used immediately. We take pride in the reliability and scalability of our platform, as well as our pace of implementation. We are a small and efficient team building out a solution in a new space with lots of green field ahead of it.

Why You'll Be Excited
Having a large stake and impact on the product and business direction and bottom-line
Collaborating with innovative and goal-focused engineering and business teams
Working with data scientists, data analysts, and product managers to identify and use the data that is most relevant to the problem at hand
Building systems that can effectively stream, store, and crunch vast amounts of data to help inform customers and power business analytics
Solving complex problems revolving around real-time strategic decision-making and large data systems
Developing, deploying, and maintaining robust and high-performance systems and features
Why We'll Be Excited About You
You have strong verbal and written communication skills that help you express your work in meaningful ways to cross functional teams
You are passionate about learning different technologies, exploring engineering challenges, and working in a dynamic and collaborative environment
You have working experience and skills designing and coding in Java/Scala and/or Python
You are proficient in writing efficient and well-structured SQL queries and have experience with database schemas and design
You have experience with big data technologies (Spark, Presto, Druid, etc.)
You have knowledge of UNIX/Linux and scripting with Perl, Shell, etc.
Degree in Computer Science or a related field
Bonus: Experience working in a data science / machine learning environment
Bonus: Experience working with AWS Services (Redshift, Kinesis, Glue, etc.)
Why We (and You'll) Love It Here
We are a technology and data-driven business
We embrace analytical thinking, kind, and results driven people
We have a plethora of challenging and interesting problems to solve
We help and support each other in creating a productive work/life balance
Perks and Benefits:

At Telaria we place an emphasis and importance on ensuring our total rewards are competitive, aligned with industry and to help you create a productive work/life balance. Benefits are highly subsidized and include medical, company paid dental, vision, employer contributed Health Savings Account, 401k matching, corporate gym discounts, pre-tax health and commuter savings, life insurance, 5 and 10-year Sabbatical programs, Discretionary Time Off (a.k.a. open vacation policy!), Paid Parental Leave, an Employee Referral Program, Employee Stock Purchase Plan (ESPP), and much more! All this is within a collaborative work environment you can personalize and topped with engaging programs like Micro-Mentorship, and Team Sports.

Telaria values diversity and is proud to be an equal opportunity employer. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
Data Engineer,"Team Overview


Luminar Technologies is seeking a Senior Data Engineer to contribute to the development efforts of our end-to-end data pipeline and corresponding web applications. Our vision is to make autonomous transportation safe and ubiquitous. Far too many lives are lost in vehicle accidents each year. Because when real people’s lives are at stake, driving safely 99% of the time isn't good enough. We just launched Hydra, and it’s capabilities are unmatched: road tracking out to 80m, lanes to 150m, and objects to 250m.

Responsibilities
Contribute to the development of automotive-grade software for perception and self-driving applications based on Luminar’s industry leading LiDAR platform
Implementation of BigData infrastructure and services
Quality control of data infrastructure and data service development
Automation of data infrastructure
Implementation of data transformation and streaming services
24/7 operation and site reliability of data services
Minimum Qualifications
MS in Computer Science or a related field
4+ years of relevant industry experience
Already shipped and operated Big Data systems in production environments
Experience in scalability of data systems
Hands-on Big Data software and infrastructure development skills
Expert knowledge in technologies like Kafka, Kibana/Elastic, Spark, Airflow
Hands-on experience of data storage and schemes like Avro, Parquet
Preferred Qualifications
Expert knowledge in rational, no-sql and distributed data stores
Operational experience with on-prem and cloud data systems
Very strong coding skills in Java, Python, Scala
Experience with large-scale ingestion architectures
Hands-on experience in data streaming technologies
Experience in cloud and on-premises data systems
Benefits & Perks
Location: HQ near Stanford University in the beautiful city of Palo Alto, California
Timing: A start-up backed by industry leaders, at a critical stage of growth
Compensation: Competitive salaries and meaningful equity
Benefits: Comprehensive package (medical, dental, vision, and more)
PTO: Take it when you need it, we are a results-oriented team (not just 9-5 job)
Other: Paid lunch and dinner, team fitness, and fun team off-sites
Luminar is an equal opportunity employer. All applicants will be considered for employment without regard to race, color, ancestry, national origin, sex, gender, sexual orientation, marital status, religion, age, disability, gender identity, results of genetic testing, service in the military, or any other characteristic protected by applicable federal, state or local laws. We will make a reasonable accommodation for any qualified applicant with a disability, provided that the individual is otherwise qualified to safely perform the essential functions of the job with or without accommodation and that the accommodation would not impose an undue hardship on the operation of our business. Please let us know if you believe you require reasonable accommodation, or if you would like assistance to complete an application or to participate in an interview at the company."
Data Engineer,"We are looking for a Data Engineer who is excited about building products that wrangle AV data to supercharge our customers. You will drive the design and development of data infrastructure across our products and internal tools. At Applied, we encourage all engineers to take ownership over technical and product decisions, closely interact with users to collect feedback, and contribute to a thoughtful, dynamic team culture.At Applied, you will:* Design powerful data pipelines that process fast sensor streams, leverage appropriate data stores, and offer easy-to-use APIs* Develop and deploy high-quality software using modern tooling and frameworks* Work with products and teams across Applied Intuition* Work with customers across the AV ecosystem to understand their needs and the innards of their data systemsWe're looking for someone who:* Has 1.5+ years experience building scalable big data pipelines* Has experience with open source data processing frameworks (Spark, Kafka, etc.)* Has experience with different data storages (e.g., relational and NoSQL)* Has experience with containerization and other modern software development workflows* Takes initiative and ownership in a fast-paced environmentNice to have:* Expertise with multiple modern programming languages (Python, C++, Go, etc.)* Prior work in enterprise software, including on-prem and/or cloud deployments* Prior work in either autonomy or simulation productsAutonomy is one of the leading technological advances of this century that will come to impact our lives. The work you'll do at Applied will meaningfully accelerate the efforts of the top autonomy teams in the world. At Applied, you will have a unique perspective on the development of cutting edge technology while working with major players across the industry and the globe."
Data Engineer,"Who we are


Fueled by a fundamental belief that having access to financial services creates opportunity, PayPal (NASDAQ: PYPL) is committed to democratizing financial services and empowering people and businesses to join and thrive in the global economy. Our open digital payments platform gives PayPal’s 305 million active account holders the confidence to connect and transact in new and powerful ways, whether they are online, on a mobile device, in an app, or in person. Through a combination of technological innovation and strategic partnerships, PayPal creates better ways to manage and move money, and offers choice and flexibility when sending payments, paying or getting paid. Available in more than 200 markets around the world, the PayPal platform, including Braintree, Venmo and Xoom, enables consumers and merchants to receive money in more than 100 currencies, withdraw funds in 56 currencies and hold balances in their PayPal accounts in 25 currencies.

When applying for a job you are required to create an account, if you have already created an account - click Sign In.

Creating an account will allow you to follow the progress of your applications. Our system does have some requirements that will help us process your application, below are some guidelines for creation of your account:
Provide full legal First Name/Family Name – this is important for us to ensure our future hires have the right system set up.
Please Capitalize first letter of your First and Last Name.
Please avoid using fully capitalized text for your First and/or Last Name.
NOTE: If your name is hyphenated or has multiple capitalization, please use the same format as your government ID.
Job Description Summary:

Global Product Data Services is a newly formed team in PayPal’s product and engineering organization under Customer Experiences and Technology. Its vision is to “Provide Enterprise Product Data at lower cost and better quality”. To achieve this vision, we are looking for people with a passion and curiosity to solve customer problems with data. The internal stakeholders for the Program include but are not limited to PayPal Inc.’s Product Teams, Finance, Risk, Compliance and Strategy Teams. Our External stakeholders include Customers and Regulators amongst others.

This position is focused on delivering Core Data solutions using modern technology to serve the various needs of the business. The scope of the organization is global, and its data platforms serve a wide array of functions including Merchant, Partner, Operations, and Compliance business operations. At GPDS, we are committed to bringing innovation, passion and customer focus to the business of enterprise solutions.

One of the charters of GPDS is to deliver on data driven company wide transformational initiatives to integrate PayPal Inc. data seamlessly using Big Data for both operational and analytical needs. In this position you will also have the opportunity to work with stakeholders and users to understand their needs and partner with engineering to deliver the solution. This position requires an individual who is comfortable working in cross-functional teams with a very high degree of analytical and technical skills.

Job Description:

In this role, the individual will be part of the engineering team in Global Product Data Services Organization and will be responsible for.
Participating and collaborating with Product Owner/ Cross functional teams in the organization to understand the business requirements and to deliver solutions that can scale.
Creativity and out of the box thinking is required.
Proactively anticipating problems and keeping the team and management informed in a timely manner.
Being flexible and being able to support all functions of product life cycle when required.
Ability to use data to draw insights or drive decisions. Be able to explain complex technical concepts to management, product managers and other engineers.
Motivated and interested in delivering results, especially in the area of writing high-performance, reliable and maintainable code.
Ability to adapt to new development environments, changing business requirements and learning new systems highly desired.
Good team player, able to effectively work across multiple teams on solutions that have complex dependencies and requirements in a fast-paced environment
Excellent verbal and written communication skills.
Skills and Experience
8+ years of experience in the IT industry, experience in data engg is Mandatory.
Shell/ Perl scripting experience or proficiency in any programming language like Java/C/ C++
Hands on in Java programming
Proficient in Frameworks – Spring, Maven, Hibernate
Knowledge Of Real time Analytics
Strong fundamentals of object-oriented design, data structures, algorithms and design patterns
Expert in software engineering tools and best practices
Expert in design/implementation for reliability, availability, scalability and performance
Should have strong SQL programming skills
Knowledge of data warehousing concepts
Proficient in Big data Environments – Pig,Hive,MR
Excellent written and oral communication skills
Working experience in an Agile methodology is highly preferred.
Experience with Tableau or other visualization tools is a plus.
Knowledge of Scheduling Tools is a plus
Intermediate level knowledge on following technologies, with expertise on few of them:
Knowledge in MPP Databases/ Distributed systems
Knowledge on Data Encryption Standards is a huge plus
Exposure to Data Quality and Profiling tools is a plus.
Exposure BI tools desired, but not required (Micro strategy, Business Objects)
Basic level knowledge on following business domains is a plus:
Payments and banking
Subsidiary:

PayPal

Travel Percent:

0

Primary Location:

San Jose, California, United States of America

Additional Locations:

We're a purpose-driven company whose beliefs are the foundation for how we conduct business every day. We hold ourselves to our One Team Behaviors which demand that we hold the highest ethical standards, to empower an open and diverse workplace, and strive to treat everyone who is touched by our business with dignity and respect. Our employees challenge the status quo, ask questions, and find solutions. We want to break down barriers to financial empowerment. Join us as we change the way the world defines financial freedom.

PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com."
Data Engineer,"Responsibilities:
Responsible for building and maintaining the machine learning data and development platform.
Build, integrate and deploy machine learning solutions into the BlackLine application in collaboration with product management, cloud, engineering and data science teams.
Create and maintain scalable data pipeline in the cloud (AWS and GCP).
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement processes automation and data delivery.
Build infrastructure for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Execute extract, transform and load (ETL) operations on large datasets including data identification, mapping, aggregation, conditioning, cleansing, and analyzing.
Build analytics tools to provide actionable insights into business and product performance.
Keep data separated, isolated and secured.
Assist data scientists in implementing achine learning algorithms and contribute to building and optimizing our product into an innovative industry leader.
Participate in establishing best practices while team is transitioning to new technologies, tools and infrastructure. Maintain specifications and metadata; follow the best practices.
Recommend and implement process improvements.
Maintain specifications and metadata; follow and develop best practices.
Coach and technically train data analysts, if needed.
Qualifications:
5+ years as a data engineer.
Experience with SQL, Python, R languages.
ETL experience using Python.
Experience with Hadoop, Spark, Hive. Presto is a plus.
Practical experience with GIT version control.
Strong familiarity with GCP, AWS, SQL Server.
Comfortable working with open source tools in Unix/Linux environments.
Data warehousing experience, data modeling and database design.
Experience with machine learning packages and various ML algorithms.
Experience with predictive and prescriptive analytics, modeling, and segmentation.
Experience with data analytics, big data, and analytics architectures.
Comfortable handling large amounts of data.
Experience ensuring data and modeling accuracy, cleanliness, reliability.
Works independently without the need for supervision.
Experience translating business requirements into functional, and non-functional requirements.
Strong sense systems and data ownership."
Data Engineer,"Job TITLE: Data Engineer

Location: Santa Clara, CA

Term: Contract

Skill: Client is looking for a Data engineer who has passion and can drive for customer success. Your primary focus will be on improving customer business outcomes through optimization and automation of data operations and data engineering. Strong verbal and written communications as well as troubleshooting skills are critical for success in this role.

• Contribute in data engineering efforts
• Develop ETL and data integration processes
• Ensure that critical customer operational issues are addressed quickly and effectively
• Understand existing data environment, variations of implementation and develop effective triage mechanisms and tools
• Coordinate and collaborate with development team to remain current on code and technology
• Analyze individual issues in the context of overall platform to proactively identify larger problems
• Differentiate between issues that arise in operations, user code, third party libraries or product
• Plan and implement upgrades, changes for various implementations
• Maintain and manage SLA
• Work closely with off-shore team
• Drive for success across teams and customers
• Documentation of issues, resolutions and processes
• Develop Splunk Queries, Splunk custom inputs

Experience: • 5+ years of strong SQL and data pipeline development experience
• 3+ years of data pipeline development experience using any programming language like Python, C++, Java, Ruby
• 3+ years of experience developing backend platform services using RoR
• 2+ years of experience working with AWS and various services
• 1+ years of experience with Splunk
• Desire and ability to learn new technologies
• Strong written and verbal communications skills
• Experience working with off-shore teams

Education: • Prior data operations experience
• Splunk administration experience
• 1+ years development experience with map-reduce paradigm with hadoop-hive environment
• Any type of contribution to open source community
• Experience with any of the reporting tools
• Experience working with Elastic Search, Redis or other noSQL data source.
• Prior experience of Infrastructure Systems Engineer Jobs"
Data Engineer,"Posted: Apr 22, 2019
Role Number:
200054902
Do you thrive on working at analyzing state-of-the-art deep learning algorithms? Do you have a passion for constructing automation pipelines? Are you highly organized and detail-oriented?

As part of our Video Engineering group, you’ll help deliver creative solutions to various problems that could impact the people all over the world.

This Data Engineer will work closely with other members of the Video Engineering group to mine data, implement model evaluation pipeline, analyze large scale data, visualize data, and ensure the delivery is of the highest quality. This position will also require strong coding skills, presentation skills, and collaborating with multiple teams (ex: machine learning, cloud infrastructure support).
Key Qualifications
A curious mind
An obsession for quality
Background in Data science, Data mining, Multivariate statistics, Computer vision, Machine learning
Experience working with large scale data sets
Solid programming skills including:
Python
C/C++
Experience with data visualization and presentation, familiar with data analysis tools such as Tableau
Excellent problem solving and communication skills
Description
The responsibilities of this position includes the following for current and future products:
Implement algorithm evaluation methods
Analyze data and build data analysis tools
Deep-dive failure analysis
Discover new perspectives for old data
Produce / Present meaningful data visualization to higher-ups and across various involved teams
Education & Experience
PhD or Masters in Computer Science"
Data Engineer,"Slalom is a purpose-driven consulting firm that helps companies solve business problems and build for the future, with solutions spanning business advisory, customer experience, technology, and analytics. We partner with companies to push the boundaries of what’s possible—together.

Founded in 2001 and headquartered in Seattle, WA, Slalom has organically grown to over 8,000 employees. We were named one of Fortune’s 100 Best Companies to Work For five consecutive years from 2016 - 2020 and are regularly recognized by our employees as a best place to work. You can find us in 35 cities across the U.S., U.K., Australia, and Canada.

The Data & Analytics teams across Slalom Northern California are all hiring! Come make an impact with our East Bay, Sacramento, San Francisco, or Silicon Valley markets.

Data Engineer Consultant

As a Data Engineer for Slalom Consulting, you'll work in small teams to deliver data pipelines and data models for our clients. You will design and build highly scalable and reliable modern data platforms including data lakes and data warehouse using Amazon Web Services, Azure, Google Cloud. Your work will include a variety of core data warehousing tools, Hadoop, Spark, event stream platforms, and ETL tools such as Airflow. In addition to building the next generation of data platforms, you'll be working with some of the most forward-thinking organizations in data and analytics.

Who are you?
You have passion for data!
You’re a smart, collaborative person who is excited about technology and driven to get things done.
You’re not afraid to be bring your authentic self to work.
You embrace a continuous learner mentality.
Who are we?
We are engineers, makers, planners, architects, and designers.
We choose to imagine things made better, and then set out on a journey to realize what’s possible.
We’ll never trade the upside of wonder for the comfort of the familiar or the safety of convention.
What technologies will you be using?

Every element of a modern data & analytics stack. It’s about using the right technologies to solve problems and playing with new technologies to figure out how to apply them intelligently. We work with technologies across the board.

Why do we work here?

Each of us came to Slalom because we wanted something different. We wanted to make a difference, we wanted autonomy to own and drive our future while working with some of the best companies in Silicon Valley leveraging the coolest technologies. At Slalom, we found our people.

Qualifications:
Bachelor’s degree in Computer Engineering, Computer Science, Information Systems or related discipline
3+ years relevant experience
Experience in capturing end users requirements and align technical solutions to the business objectives
Understanding of different types of storage (filesystem, relation, MPP, NoSQL) and working with various kinds of data (structured, unstructured, metrics, logs, etc.)
Understanding of data architecture concepts such as data modeling, metadata, workflow management, ETL/ELT, real-time streaming), data quality
3+ years of experience working with SQL
Experience with setting up and operating data pipelines using Python or SQL
1+ years of experience working on AWS, GCP or Azure
Experience working with data warehouses such as Redshift, BigQuery and Snowflake
Exposure to open source and proprietary cloud data pipeline tools such as Airflow, Glue and Dataflow
Experience working with relational databases
Experience with data serialization languages such as JSON, XML, YAML
Experience with code management tools (e.g. Git, SVN) and DevOps tools (e.g. Docker, Bamboo, Jenkins)
Strong analytical problem-solving ability
Great presentation skills, written and verbal communication skills
Self-starter with the ability to work independently or as part of a project team
Capability to conduct performance analysis, troubleshooting and remediation
Slalom is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law."
Data Engineer,"Dubbed an ""open-source unicorn"" by Forbes, Confluent is the fastest-growing enterprise subscription company our investors have ever seen. And how are we growing so fast? By pioneering a new technology category with an event streaming platform, which enables companies to leverage their data as a continually updating stream of events, not as static snapshots. This innovation has led Coatue Management, Altimeter Capital and Franklin Templeton to join earlier investors Sequoia Capital, Benchmark, and Index Ventures in the recent Series E financing of a combined $250 million at a $4.5B valuation. Our product has been adopted by Fortune 100 customers across all industries, and we’re being led by the best in the space—our founders were the original creators of Apache Kafka®. We’re looking for talented and amazing team players who want to accelerate our growth, while doing some of the best work of their careers. Join us as we build the next transformative technology platform!

About the Team:

The mission of the Data Science team at Confluent is to serve as the central nervous system of all things data for the company: we build analytics infrastructure, insights, models and tools, to empower data-driven thinking, and optimize every part of the business. Data Engineers on the team will be the enabler and amplifiers. This position offers limitless opportunities for an ambitious data science engineer to make an immediate and meaningful impact within a hyper growth start-up, and contribute to a highly engaged open source community.

About the Role:

We are looking for a talented and driven individual to build and scale our data analytics infrastructure and tooling. This person will build state of art data warehousing, ETL, and BI platforms, to make data accessible to the entire company. They will also partner closely with Data Scientists and cross functional leaders to develop internal data products. Data Engineers are encouraged to think out of the box and play with the latest technologies while exploring their limits. Successful candidates will have strong technical capabilities, a can-do attitude, and are highly collaborative.
Responsibilities:
Collaboration with Data Scientists, Engineers, and business partners to understand data needs to drive key decision making throughout the company
Implementing a solid, robust, extensible data warehousing design that supports key business flows
Performing all of the necessary data transformations to populate data into a warehouse table structure that is optimized for reporting and analysis; deploy inclusive data quality checks to ensure high quality of data
Developing strong subject matter expertise and manage the SLAs for those data pipelines
Set up and improve BI tooling and platforms to help the team create dynamic tools and reporting
Partnering with Data Scientists and business partners to develop internal data products to improve operational efficiencies organizationally
Building and growing partnership with cross functional teams, and evangelize data-driven culture
Contributing to innovations that fuel Confluent’s vision and mission

What We're Looking For:

4+ years of experience in a Data Engineering role, with a focus on data warehouse technologies, data pipelines, BI tooling and/or data apps development
Bachelor or advanced degree in Computer Science, Mathematics, Statistics, Engineering, or related technical discipline
Highly proficient in Python and SQL coding
Highly proficient with tuning and optimizing data models and pipelines
Experience in developing data apps with Python, Javascript, high charts etc.
The ability to communicate cross-functionally, derive requirements and architect shared datasets; ability to synthesize, simplify and explain complex problems to different types of audiences, including executives

What Gives You An Edge:

Experience with Apache Kafka
Experience with B2B enterprise apps data: Salesforce, Marketo, Zendesk, etc.
Experience in developing data apps with Python, Javascript, high charts, etc.
#LI-MT1

Come As You Are

At Confluent, equality is a core tenet of our culture. We are committed to building an inclusive global team that represents a variety of backgrounds, perspectives, beliefs, and experiences. The more diverse we are, the richer our community and the broader our impact.

Click here to review our California Candidate Privacy Notice, which describes how and when Confluent, Inc., and its group companies, collects, uses, and shares certain personal information of California job applicants and prospective employees."
Data Engineer,"Minimum qualifications:
Bachelor’s degree in Engineering, Computer Science, Statistics, Economics, Mathematics, Finance, a related quantitative field, or equivalent practical experience.
6 years of experience in consulting, business intelligence, analytics, or an equivalent analyst position with experience in SQL and an additional object-oriented programming language (e.g., Python, Java).
Preferred qualifications:
Experience designing and building scalable and robust data pipelines to enable data-driven decisions for the business.
Effective problem solving and analytical skills. Ability to manage multiple projects and report simultaneously across different stakeholders.
Structured thinking with ability to easily break down ambiguous problems and propose impactful data modeling designs.
Attention to detail and effective verbal/written communication skills.
About the job


The Business Strategy & Operations organization provides business critical insights using analytics, ensures cross functional alignment of goals and execution, and helps teams drive strategic partnerships and new initiatives forward. We stay focused on aligning the highest-level company priorities with effective day-to-day operations, and help evolve early stage ideas into future-growth initiatives.

At Google Customer Solutions (GCS), data and insights drive our growth. The GCS Insights team powers Google’s high growth organization. We deliver actionable business intelligence and analytics at scale utilizing a customer centric approach.

As a Data Engineer, you will take on big data challenges in an agile way. In this role, you will use an analytical, data-driven approach to drive a deep understanding of our fast changing business. You will build data pipelines that enable engineers, analysts and other stakeholders across the organization. You will also build data models to deliver insightful analytics while ensuring the highest standard in data integrity.

When our millions of advertisers and publishers are happy, so are we! Our Google Customer Solutions (GCS) team of entrepreneurial, enthusiastic and client-focused members are the ""human face"" of Google, helping entrepreneurs both individually and broadly build their online presence and grow their businesses. We are dedicated to growing the unique needs of advertising companies. Our teams of strategists, analysts, advisers and support specialists collaborate closely to spot and analyze customer needs and trends. In collaboration, we create and implement business plans broadly for all types of businesses.
Responsibilities
Use an analytical, data-driven approach to drive a deep understanding of our fast changing business.
Build data pipelines and reports that enable analysts and other stakeholders across the organization.
Build data models to deliver insightful analytics while ensuring the highest standard in data integrity.

Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing this form."
Data Engineer,"Title: Data Engineer

Location: Mountain View,CA

Duration: 6+ Months Contract

Interview Process: Phone & Skype

Must-Have:

2+ yearsexperience in design, implementation, and support of solutions big data solution in Hadoop using Hive, Spark, Drill, Impala, HBase

Job Description

Designs develop, and implement Hadoop eco-system-based applications to support business requirements. Follows approved life cycle methodologies, creates design documents, and performs program coding and testing. Resolves technical issues through debugging, research, and investigation.

Experience/Skills Required:

Bachelors degree in computer science, Information Technology, or related field and 5 years experience in computer programming, software development or related

2+ years experience in design, implementation, and support of solutions big data solution in Hadoop using Hive, Spark, Drill, Impala, HBase

Hands-on experience with Unix, Teradata and other relational databases.

Thanks & Regards!

Andy

Sr. Technical Recruiter

D: (848) 200 2354

andy(at)eateam.com"
Data Engineer,"At CrowdStrike we’re on a mission - to stop breaches. Our groundbreaking technology, services delivery, and intelligence gathering together with our innovations in machine learning and behavioral-based detection, allow our customers to not only defend themselves, but do so in a future-proof manner. We’ve earned numerous honors and top rankings for our technology, organization and people – clearly confirming our industry leadership and our special culture driving it. We also offer flexible work arrangements to help our people manage their personal and professional lives in a way that works for them. So if you’re ready to work on unrivaled technology where your desire to be part of a collaborative team is met with a laser-focused mission to stop breaches and protect people globally, let’s talk.

About the Team:

The data engineering team is on a mission to create a hyper scale data lake, which helps finding bad actors and stop breaches. The team builds and operates systems to centralize all of the data the falcon platform collects, making it easy for internal and external customers to transform and access the data for analytics, machine learning, and threat hunting. As an engineer on the team you will contribute to the full spectrum of our systems, from foundational processing and data storage, through scalable pipelines, to frameworks, tools and applications that make that data available to other teams and systems.

Job Responsibilities :
Design, develop, and maintain a data platform that processes petabytes of data
Participate in technical reviews of our products and help us develop new features and enhance stability
Continually help us improve the efficiency of our services so that we can delight our customers
Help us research and implement new ways for both internal stakeholders as well as customers to query their data efficiently and extract results in the format they desire
Qualifications for Data Engineer :

We are looking for a candidate with a BS and 5+ years or MS and 3+ years in Computer Science or related field. They should also have experience with the following software/tools -
A solid understanding of algorithms, distributed systems design and the software development lifecycle
Solid background in Java/Scala and a scripting language like Python
Experience building large scale data pipelines
Strong familiarity with the Apache Hadoop ecosystem including : Spark, Kafka, Hive, Apache Presto, etc.
Experience with relational SQL and NoSQL databases, including Postgres/MySQL, Cassandra, DynamoDB
Good test driven development discipline
Reasonable proficiency with Linux administration tools
Proven ability to work effectively with remote teams
Experience with the following tools is desirable :
Go
Kubernetes
Jenkins
Parquet
Protocol Buffers/GRPC
#LI-JF1

Benefits of Working at CrowdStrike:
Market leader in compensation and equity awards
Competitive vacation policy
Comprehensive health benefits + 401k plan
Paid parental leave, including adoption
Flexible work environment
Wellness programs
Stocked fridges, coffee, soda, and lots of treats
We are committed to building an inclusive culture of belonging that not only embraces the diversity of our people but also reflects the diversity of the communities in which we work and the customers we serve. We know that the happiest and highest performing teams include people with diverse perspectives and ways of solving problems so we strive to attract and retain talent from all backgrounds and create workplaces where everyone feels empowered to bring their full, authentic selves to work.
CrowdStrike is an Equal Opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex including sexual orientation and gender identity, national origin, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law.
CrowdStrike participates in the E-Verify program.

Notice of E-Verify Participation

Right to Work"
Data Engineer,"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started.How would Facebook scale to the next billion users? The Infrastructure Strategy group is responsible for the strategic analysis to support and enable the continued growth critical to Facebooks infrastructure organization.

We are looking for a Data Engineer to not only build data pipelines but also extend the next generation of our data tools. As a Data Engineer, you will develop a clear sense of connection with our organization and leadership - as Data Engineering is the eyes through which they see the product.

This is a partnership-heavy role. As a member of Infrastructure Strategy Data Engineering, you will belong to a centralized Data Science/Data Engineering team who partners closely with teams in Facebooks Infrastructure organization. Through the consulting-nature of our team, you will contribute to a variety of projects and technologies, depending on partner needs. Projects include analytics, ML modeling, tooling, services, and more. The broad range of partners equates to a broad range of projects and deliverables: ML Models, datasets, measurements, services, tools and process.

Responsibilities:

Partner with leadership, engineers, program managers and data scientists to understand data needs.
Design, build and launch extremely efficient and reliable data pipelines to move data across a number of platforms including Data Warehouse, online caches and real-time systems.
Communicate, at scale, through multiple mediums: Presentations, dashboards, company-wide datasets, bots and more.
Educate your partners: Use your data and analytics experience to see whats missing, identifying and addressing gaps in their existing logging and processes.
Leverage data and business principles to solve large scale web, mobile and data infrastructure problems.
Build data expertise and own data quality for your areas.
Mininum Qualifications:

5+ years of Python development experience.
5+ years of SQL experience.
3+ years of experience with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, digdag.io, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M).
3+ years experience with Data Modeling.
Experience analyzing data to discover opportunities and address gaps.
5+ years experience in custom ETL design, implementation and maintenance.
Experience working with cloud or on-prem Big Data/MPP analytics platform(i.e. Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar).
Preferred Qualifications:

Experience with more than one coding language.
Experience with designing and implementing real-time pipelines.
Experience with data quality and validation.
Experience with SQL performance tuning and E2E process optimization.
Experience with anomaly/outlier detection.
Experience with notebook-based Data Science workflow.
Experience with Airflow.
Experience querying massive datasets using Spark, Presto, Hive, Impala, etc.
Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.

Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com."
Data Engineer,"Job Description
FocusKPI, Inc. is looking for a Data Engineer to work for our client in Mountain View, CA. This is a full-time contract with negotiable pay rate.

Job responsibilities :
Construct and refine AI algorithms to optimize our marketing and on-boarding funnels.
Craft machine learning and predictive models to drive intelligent product features.
Work with our client seriously large volume of analytics data.
Define and build out scalable distributed infrastructure to support our client vision of optimized real-time personalization across the portfolio of their products.
Extend existing ML libraries and frameworks.
Research and implement appropriate ML algorithms and tools.
Provide technical leadership and influence data-driven optimization efforts.
Challenge and enrich yourself in an environment of like-minded engineers and data scientists, and most importantly have fun!
Requirements:
MS or PhD in Computer Science or a related quantitive field.
5+ years of related industry experience in a data science or engineering domain.
Development experience in a Python/ Java/ Scala.
Experience working in an Agile environment.
Extensive data modeling and data architecture skills.
Knowledge of Spark or other distributed cloud computing systems.
Experience developing, building and scaling machine learning models in business applications using large amounts of data Strong written and verbal communication skills
Thank you for applying!

FocusKPI Team

About FocusKPI:

FocusKPI is a data science and technology firm specializing in predictive analytics practice and methodologies."
Data Engineer,"Job Title: Data EngineerLocation: Santa Clara, CACompanyWork matters. It's where we spend a third of our lives. And the workplace of the future is going to be a great place. We're dedicated to bringing that to life for people everywhere. That's why we put people at the heart of everything we do.People matter. Our people have a passion for learning, building, and innovating. Whether you're an engineer, a sales professional, a finance professional, or anything in-between, our roles aim to provide each person with meaningful impact and plenty of space to grow.TeamThis is a new team within the Data and Analytics organization. The team is fast-paced while managing highly accurate detailed information. We collaborate and support every business unit within ServiceNow.RoleWe are looking for a dynamic, perennially curious, self-motivated, and data-centric individual to drive Business Intelligence endeavors for the Business. A candidate to nurture, execute, and deploy BI projects to direct our investments, manage business levers, track progress against guidance, predict, and prescribe targeted opportunities. The role entails collaborative engagements with Field & Product Line Sales Teams, Global Services, Alliances & Channels, and FP&A to deploy insightful analytic products, establish alignment on processes teams and deliver strategic metrics for current and future business initiatives.What you get to do in this role:* You will provide insights and deep analysis being sought by users/business stakeholders* Work with Cross-Functional Analytics team members to curate and assimilate insights* Grow into being SME on business functions* Gather business requirements from stakeholders on various analytics initiatives* Analyze requirements, determine optimal solutions and determine gap from the current state, dependencies and ways to mitigate risks* Develop business requirements documentation, process workflow diagrams, functional specifications, user acceptance test scripts and other supporting documentation for Business Intelligence and Analytics initiatives* Assist stakeholders with data analysis, design data models & develop DB Views, procs, models in SAP HANA to meet the business need* Develop dashboard and report prototypes and mockups with respect to the UX/UI Best Practices and have impactful UI Design* Communicate status regularly with stakeholders* Define required data integration requirements between various systems and work with extended team to get them created* Collaborate with India Development Center BI team to translate business requirements and get appropriate data solutions developed to meet the business need* Partner with Global BI team to help implement solutions for end-user adoptionIn order to be successful in this role, we need someone who has:* Bachelor's Degree in Information System, Analytics, Business Intelligence or related field required* 0 to 3+ years of documented experience in writing strong SQL, PLSQL in data warehouse technologies (Hana, Snowflake, or any modern database).* Ability to analyze data coming from myriad data sources, mine and analyze and derive value from it to improve business SQL and other computer programs (Python, R is preferred)* Ability to visualize the results in the previous step by putting together simple and easily consumable dashboards using reporting tools* Working knowledge Tableau, Power BI is a plus* Strong analytical and problem-solving ability and be able to dive into technical details and design analytics solutions* Expertise in database design & development, writing optimized queries, handling Facts, dimension data effectively* Must have good communication, presentation, and documentation skills* Capable of using Microsoft Project, Excel, Word, PowerPoint, and Visio or similar products* Business process design, project management, and/or Agile SDLC experience a plus* 1-2 years of SAP HANA experience is a plus"
Data Engineer,"Remote for now until covid restrictions are lifted Job Description Design and build data models to conform to our existing EDW architecture. Change control documentation Design and build data pipelines using tools - SAP SLT, SAP Data Services, Python and Microsoft SSIS. Design and development of data warehouse using T-SQL, SQL, and python Work with teams to deliver effective, high-value reporting solutions by leveraging an established delivery methodology. Implement data structures using best practices in data modeling, processes, and technologies. Design and development of data warehouse using Microsoft SQL Server, SAP HANA and Snowflake databases. Writing analytics programs (transformationscalculations) in T-SQL,R, Python or comparable Perform data mining and analysis to uncover trends and correlations to develop insights that can materially improve our decisions. Development with one or more data visualizationreporting tools (Tableau, Business Objects, Hana Analytics, Microsoft PowerBI) Work with various product owners to ensure applications are instrumented with proper tracking mechanisms to enable analytics. Continually recommend, develop, and implement process improvements and tools to collect and analyze data, and visualizepresent insights Skills Qualifications Bachelorrsquos degree in Business, MIS or related area. Masterrsquos degree a plus. 8+ years Data engineering experience Has worked on 5-6 data source projects at a time Python experience preferred 3+ years of experience in ETL development tools, preferably with knowledge of Microsoft Integration Services 2005 or greater (SSIS), SAP Data Services, SAP SLT and Python. SQL developer background is also accepted Documentation skills and can to handle heavy documentation Able to be versatile Able to read and understand codes and can make adjustment to parameters 5+ years of experience in design and development using Microsoft SQL Server, SAP HANA and Snowflake databases. Experience in full life cycle development, implementation, management and performance tuning of the Enterprise Data Warehouse Experience in database development (T-SQL, PLSQL, andor SQL scripts) Experience in building data pipelines using python, C and JSON Experience in Microsoft BI development in Integration Services (SSIS), Analysis Services (SSAS) or Reporting Services (SSRS) Experience building and managing data flows to and from cloud applications Demonstrated experience in utilizing R, Python, SPSS or comparable to develop analyses Experience visualizing data in business intelligence tools such as Tableau, Business Objects or Hana Analytics"
Data Engineer,middot Strong SQL skills middot Can build pipelines and do project managementprogram management. (but more technical than a program manager. middot Good Python skills middot Having a strong technical background middot Excellent Comm skills
Data Engineer,"Grow your career at AppLovin.

AppLovin is a global leader in mobile entertainment. Its studios create popular, immersive mobile games and its technology brings games to more players around the world. Since 2012, the company's platform has been instrumental in driving the explosive growth of mobile games, resulting in a richer ecosystem and more games played by millions of people every day. AppLovin is headquartered in Palo Alto, California with several offices globally. Learn more at applovin.com.

AppLovin is one of Inc.'s Best Workplaces and a recipient of the 2019 Glassdoor Top CEO employee's choice award. The San Francisco Business Times awarded AppLovin one of the Bay Area's Best Places to Work in 2019 and 2020, and the Workplace Wellness Award in 2019 which recognizes businesses that are leaders in improving worker well-being.

About You:
Have 1-3 years of experience and a minimum of a BS and/or MS in Computer Science
Have excellent knowledge of computer science fundamentals including data structures, algorithms, and coding
Experience independently creating and maintaining projects
Product focused mindset
Have experience working with big data systems (Spark, Hadoop, Pig, Impala, Kafka)
Experience designing, building, and maintaining data processing systems
Experience with a backend language such as Java or Scala
About the Role:
Collaborate with various engineering teams to meet a wide range of technological challenges
Work closely with product and business teams to improve data models that feed business intelligence tools
Define company data models using Spark
Perks:
Free medical, dental, and vision insurance
Daily lunches and fully stocked kitchen
Free public transit
Free laundry service (wash/dry clean)
Free gym membership
401k matching
Fun company parties and events
Autonomy to make decisions in a rapidly growing company
Flexible Time Off - work hard and take time when you need it
Interested? Send us your resume and let's talk!

#LI-JZ1"
Data Engineer,middot Strong SQL skills middot Can build pipelines and do project managementprogram management. (but more technical than a program manager. middot Good Python skills middot Having a strong technical background middot Excellent Comm skills
Data Engineer,"""United States Citizens and those authorized to work in the US for any employer are encouraged to apply. We are unable to sponsor visas at this time""

Job Description:

Role and Responsibilities:
5-10 years of engineering experience as a Developer, Tester, DE-Manager.
Deep Understanding of Product Lifecycle, tools, and processes.
Deep understanding of product development in regards to the engineering aspects.
Good understanding of Machine Learning and Predictive Analytics fundamentals.
Set long term goals and drive a team to deliver on them.
Willing to tackle high-level objectives and translate them into meaningful outcomes in the direction of the team's charter.
Ability to lead and architect data modeling, to help predict areas for Software improvement.
Proven experience with Modeling and Simulation tools.
Proficient in R or Similar Statistical Package is a plus.
Understanding of high scale SW development models.
Good understanding of Machine Learning and Predictive Analytics fundamentals.
Python/R, Data Science, AI/ML.
Analytics, visualization, and dashboards (Grafana/Kibana, InfluxDB, Bokeh, D3.js, etc.).
Streaming data pipelines (e.g., Kafka, Apache Spark, Apache Beam).
Big-data stores (e.g., Apache Cassandra).
Data processing workflows (Luigi, Apache Airflow.
Education:
Bachelor's degree."
Data Engineer,"DeepMap is a start-up providing HD mapping and localization as a service to autonomous vehicle fleets. Our team is made up of the highest caliber engineers from the most prominent universities and software companies in the world. Additionally, we have received backing from top-tier VCs and investors. We are looking to grow our team with talented software/data engineers who are passionate about the self-driving car industry, maps and big data. If you are an independent problem solver, have a strong drive to excel, and are looking for opportunities to make a big impact, this is the right place for you.

Job Responsibilities:
Work closely across various development teams to support data processing pipelines
Write python, shell scripts and tools to facilitate customer data processing and product delivery
Proactively identify and fix sensor data and operational errors to minimize workflow latency
Monitor, troubleshoot and fix jobs running in cloud environment
Perform log analysis and present insightful conclusions/suggestions
Requirements:
BS, MS degree or higher in Computer Science, GIS or related field
More than 1 year working experience with Python and Shell scripts
A self-starter with the ability to work independently
Experience with AWS cloud platform and virtualization technologies like Docker is a plus
Experience with GIS or sensor data is plus
At DeepMap, we cherish and celebrate every individual's difference. We are proud to be an equal opportunity employer and we are committed to equal employment opportunity regardless of race, color, religion, national origin, sex, sexual orientation, age, marital status, gender identity, veteran status, and disability, or another legally protected status. DeepMap will provide reasonable accommodation to employees who have protected disabilities consistent with local law. We will also consider qualified candidates regardless of criminal histories consistent with legal requirements. If you are unable to submit an application because of incompatible assistive technology or a disability, please let us know and we will make every effort to respond to your request for assistance as soon as possible."
Data Engineer,"Data Engineer

Team: Data Engineering

Location: Sunnyvale, CA

What We Do:

Company: We create AI software that allows enterprises to design, build, experiment, customize, operate and own vertical AI solutions in a wide range of industries and areas, such as healthcare, industrial manufacturing and utilities, financial services, telecommunications, autonomous driving, and beyond. Petuum lets enterprises easily understand and apply AI to gain deep insight for better decision-making and improved productivity and efficiency. Our mission is to enable organizations to own, build and become informed users of their AI solutions, without relying on expensive talents.

Team: The Data Engineering team is the ""hub of the wheel"" and responsible for everything related to data - Collection of raw data, Storing the data, Analyzing the data, and creating tool to make business decision that fuels the AI/ML models developed on Petuum's platform product. We are extending our system with more data sources and in more formats as well as with more data transformations. Particularly, we distinguish our offering with AI/Model based transformations to enable fast and comprehensive data understanding and automatic data processing, and eventually to power user friendly experience.

The Role:

The Data Engineer should be an expert familiar with all of the data warehousing technical components (e.g. ETL, Reporting, Data Model), infrastructure (e.g. hardware and software) and their integration. The ideal candidate will be responsible for developing overall architecture, high-level design, building the data pipeline and implementing data preprocessing components. The candidate must have extensive experience with Star Schemas, Dimensional Models, Data Marts, and Data Lake infrastructure. Excellent written and verbal communication skills are required as the candidate will work very closely with diverse teams.

What You Will Do:
Drive for key results as an individual contributor.
Work with team members to implement features.
Follow team engineering process and contribute to the development of sound engineering infrastructure.
Maintain high-quality code base which is well factored, performant and robust.
Be able to learn and grow in a dynamic and fast growing start up environment.
Be familiar with the mainstream data processing and AI/ML technologies.
Other duties as assigned.
What You've Already Done:
You have a Bachelor's Degree in Computer Science or related quantitative field. An advanced degree or equivalent practical work experience is a plus.
You have 4 + years of work experience.
2-4 years of software development experience in one or more object-oriented languages, including Python, Java, C#, or C++.
Demonstrated strength in data modeling, ETL development, and Data warehousing.
Experience with Relational Database like Oracle, MS SQL, MySQL, PostgreSQL
Experience with NoSQL and Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.)
Experience with AWS services including S3, Glue, Redshift, Athena, QuickSight, EMR, Kinesis and RDS.
Experience in working and delivering end-to-end projects independently.
Knowledge of distributed systems as it pertains to data storage and computing
Experience with data pipeline, server architectures, and distributed systems is a plus.
Startup experience is a plus.
What You Already Know:
Languages: Python and SQL. Experience with Bash/Shell, Java, C#/C++, R or Scala is a plus.
Frameworks/Libraries: Spark, Hadoop. Experience with TensorFlow, Torch/PyTorch, Caffe, CNTK, Scikit-Learn, Samza, or Storm is a plus.
Databases: Experience with SQL Server, MongoDB, AWS Cloud Storage, Microsoft Azure Storage, Google Cloud Storage, Cassandra, Amazon Redshift, Apache Hive, or Google BigQuery is a plus.
Platforms: Experience with Windows Desktop/Server, Openstack/GCS/AWS, Firebase, Azure, Google Cloud Platform/App Engine.
Tools: Experience with Docker, Kubernetes, GitHub, Flink, Pulsar, or Kafka is a plus.
What We Offer for your Valuable Work:

Petuum offers Medical, Dental, Vision, Life/Disability, Paid Time Off, Parental Leave, and more.

Petuum is a welcoming workplace that considers applicants for employment without regard to, and does not discriminate on the basis of, gender, race, protected veteran status, disability, or any other legally protected status. Petuum is an at-will employer."
Data Engineer,"Position Senior Data Engineer Location Menlo Park, CA Duration 6 Months to Long Term Job Description 6+ yearsrsquo experience in data engineering and data warehouse Technologies 4+ years of experience Python experience Must be extremely strong with SQL Must Have experience with PrestoHiveHadoop Clear understanding of testing methodologies and Best Practices Experience with any of Elastic Search, Aurora, MySQL, Postgres, Redshift, Snowflake, DynamoDB, or Redis Experience with one or more MPP databases (Redshift, Bigquery, Snowflake, etc) Experienced in working collaboratively across different teams and departments Strong technical and business communication collaboratively across different teams and departments Strong technical and business communication"
Data Engineer,"As a data engineer, you will build a solid data foundation that powers the entire spectrum from Business Intelligence to Artificial Intelligence. Youll be critical to helping us in our transition from batch to real-time, one-to-one to many-to-many connections, centrally managed infrastructure to self-service tools that allow easy experimentation, and from manual to automated processes.

This role will work closely with the Data Science and AI team and will focus on the enablement and acceleration of new and existing workflows. We need someone who will bring a thoughtful perspective, empathy, creativity, and a positive attitude to solve problems at scale. This role is ideal for someone looking to extend software engineering skills into the field of Machine Learning and Artificial Intelligence.

Responsibilities:

• Establish scalable, efficient, automated processes for data analyses, model development, validation and implementation

• Work closely with data scientists and analysts to create and deploy new features

• Write efficient and well-organized software to ship products in an iterative, continual-release environment

• Monitor and plan out core infrastructure enhancements

• Contribute to and promote good software engineering practices across the team

• Mentor and educate team members to adopt best practices in writing and maintaining production code

• Communicate clearly and effectively to technical and non-technical audiences

• Actively contribute to and re-use community best practices

Requirements:

• University or advanced degree in engineering, computer science, mathematics, or a related field

• Strong experience working with a variety of relational SQL and NoSQL databases

• Strong experience working with big data tools: Hadoop, Spark, Kafka, etc.

• Experience with at least one cloud provider solution (AWS, GCP, Azure)

• Strong experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.

• Ability to work in Linux environment

• Experience working with APIs

• Strong knowledge of data pipeline and workflow management tools

• Expertise in standard software engineering methodology, e.g. unit testing, code reviews, design documentation

• Experience creating ETL processes that prepare data for consumption appropriately

• Experience in setting up, maintaining and optimizing databases for production usage in reporting, analysis and ML applications

• Working in a collaborative environment and interacting effectively with technical and non-technical team members equally well

• Relevant working experience with Docker and Kubernetes preferred

• Ability to work with ML frameworks preferred

Will be a plus:
Knowledge of CI/CI processes and components
Experience with OKTA and Optimizely

NB:

Placement and Staffing Agencies need not apply. We do not work with C2C at this time. At this moment, we are not able to process H1B transfers. Applicants with CPT and OPT visas are welcome to apply.

About us:
Grid Dynamics is the engineering services company known for transformative, mission-critical cloud solutions for retail, finance and technology sectors. We architected some of the busiest e-commerce services on the Internet and have never had an outage during the peak season. Founded in 2006 and headquartered in San Ramon, California with offices throughout the US and Eastern Europe, we focus on big data analytics, omnichannel services, DevOps, and cloud enablement."
Data Engineer,"XMotors.ai provides autonomous self-driving features for the Xiaopeng EV vehicles in China market. We are looking for people passionate about self-driving vehicles to join in and help us build top class autonomous self-driving vehicles. We are building Data/Machine Learning pipeline and AI Algorithm Models to help us navigate all the challenges presented by the complex problems of autonomous driving. You will be working with a team of data platform for the big data analysis, AI model validation, report visualization and deployment of various systems.
Responsibilities:
Collaborate with algorithm designers to specify required data sets that include both training and testing of ML/DL/CV algorithms
Analyze the data/results from AI algorithms and generate report
Request specific data sets with various driving scenarios by adding requests to recording plan
Responsible for quality of available data and maintenance of ground truth
Work with algorithms team to test algorithms based on the collected data
Experience and Skills:
2+ years of working experience in big data / deep learning analysis and validation.
Should have good Python programming skills, experience of vision libraries such as OpenCV will be a plus.
Should have worked with data science and computer vision, machine learning/deep learning teams, especially in the areas of data acquisition, data pipeline and data management.
Should have extensive experience with one or more of the following big data frameworks: Spark, Hadoop, HDFS, Kafka, DataBase, etc.
Should have experience with DevOps tools (GitHub, JIRA, Jenkins)
Experience in developing CV/ML algorithms is a plus.
Should be willing to work as part of teams.
Good communication skills
Good written and spoken English is a requirement
What do we provide:
A fun, supportive and engaging environment
Opportunities to pursue and work on cutting edge technologies.
Snacks, lunches and fun activities.
Competitive salary

We are an Equal Opportunity Employer. It is our policy to provide equal employment opportunities to all qualified persons without regard to race, age, color, sex, sexual orientation, religion, national origin, disability, veteran status or marital status or any other prescribed category set forth in federal or state regulations."
Data Engineer,"Data Engineer
If you are a Data Engineer with experience, please read on!

+ Biotech/Pharma/Life Science experience strongly preferred +

We are a life sciences company working on providing liquid biopsies, tests that look for key molecular signatures of disease in blood, and other patient samples, without the need to take a tissue sample. We want to develop tests that diagnose or screen patients before they show any symptoms. A huge goal that other academic groups and companies are also pursuing, with large clinical trials enrolling many thousands of patients already underway. We do it for a fraction of the cost. Our main focus is early disease detection because thats what we see as the biggest need in patient care. If you want to be part of a trailblazing company read on.
Top Reasons to Work with Us
+ We are well funded. Our series D back on December 19' was $55 million.
+ We are a team of industry-leading experts in our field!
+ Be part of a 2-year-old start-up that is growing exponentially over the next few years.
+ Competitive pay and full benefits package
What You Will Be Doing
- Design, develop, and maintain performance measure visualizations and reports using data visualization tools.
- Work with the Data Science team to understand the business needs to design an effective and dynamic visualization.
- Work the Product Management team to understand and gather the user experience data.
What You Need for this Position
3-5 years of experience in the following required:

- Python, Django, AWS, SQL
- Data Visualization tools such as Qlik Sense, Power BI, Tableau.
What's In It for You
Competitive Pay
Full Benefits Package
So, if you are a Data Engineer with experience, please apply today!

1. Apply directly to this job opening here!

Or

2. E-mail directly for more information to Marcus.Quigley@cybercoders.com
-
Applicants must be authorized to work in the U.S.


CyberCoders, Inc is proud to be an Equal Opportunity Employer

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.

Your Right to Work In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire."
Data Engineer,"Location: San Jose (CA)

Zoom is an award-winning workplace. We have been recognized by Comparably as #1 CEO, Company Happiness, Benefits, Compensation, Diversity, and more! Not to mention we've been awarded by Glassdoor as the 2nd Best US workplace & Best Large Company US CEO in 2018, Wealthfront, and Business Insider. Our culture focuses on delivering happiness, our commitment to transparency, and the tangible benefits we provide our employees and our customers.

Big Data Engineer

Job Summary:

We're looking for a Big Data Engineer who can find out the solution to support the requirement on service operation and product development. As a Big Data Engineer, you'll understand and manage our data, work with the engineering team and operation team to figure out the tough problem about service operation and product design.

Job Responsibilities:
Gather and process raw data at scale.
Process unstructured data into structured data, manage schema of new data.
Manage data access to protect data in a safe way.
Read, extract, transform, stage and load data to selected tools and frameworks as required and requested.
Perform tasks such as writing scripts, write SQL queries, etc.
Work closely with the operation team to advise the solution about service scalability, healthy monitoring and refining optimization by data analysis.
Work closely with the engineering team to monitor product performance and track product quality trends.
Analyze processed data.
Monitoring data performance and modifying infrastructure as needed.
Define data retention policies.
Job Requirements:
3 + years of recent experience in data engineering.
Bachelor's Degree or more in Computer Science or a related field.
Experiences on Cloudear CDH platform,Spark programmingImpala SQL Lauguage, Analyze data via Hive,etc.
A solid track record of data management showing your flawless execution and attention to detail.
Strong knowledge of and experience with statistics.
Programming experience, ideally in Python, Java or Scala, and a willingness to learn new programming languages to meet goals and objectives.
Experience in C, Linux Shell, JavaScript or other programming languages is a plus.
Knowledge of ETL, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks.
Deep knowledge of data features engineering, data mining, machine learning, or information retrieval.
Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources.
Experience in production support and troubleshooting.
You find satisfaction in a job well done and thrive on solving head-scratching problems.
Language requirement: English, Mandarin is plus"
Data Engineer,"Data Engineer

Location: Bay Area , California

And who better than you to join the Trianz family?

At Trianz, we offer you an open and learning-oriented culture essential to emerge as a leader. Completely focused on the Digital Evolution philosophy and phenomenon, we view delivering our value proposition consistently as a non-negotiable commitment. Our enablers include Intelligent Team Formations, a Client-Centric Approach, Predictability in Execution, and establishing a Unique Relationship Experience. A culture of innovation, encouraging our people to create, and belief in the importance of training and development set us apart.

We are looking forward to see you bring the following to the table to craft your, and our, success story:

Location: San Jose, San Francisco, Santa Clara, Austin, TX

Terms: Full-time

Data Engineer

About the Role

Trianz is looking for passionate Data Engineers who are looking to tackle challenges and build solutions

Job Description

Strong data engineer able to:
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements,
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources,
Work with stakeholders including Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs,
Experience building and optimizing data pipelines, architectures and data sets,
A successful history of manipulating, processing and extracting value from large disconnected datasets.
A plus:
Experience in creating reports and dashboards in Tableau.
Technologies we use:

Dataswarm (data pipeline framework in Python), Hive, Presto, Python, Scuba (in-memory database), SQL, Oracle, Tableau

Your passion for execution and zeal to become a leader capable of taking on anything is enough reason for us to talk immediately!

Need more details?

Trianz is growing at a faster pace than the industry for the last five years. Read through some of the key industry recognitions we have received for our innovative execution and strategic client initiatives here.

About Trianz

Trianz simplifies digital evolutions through effective strategies and excellence in execution. Collaborating with business and technology leaders, we help formulate and execute operational strategies to achieve intended business outcomes by bringing the best of consulting, technology experiences and execution models. Powered by knowledge, research, and perspectives, we enable clients to transition to a digital enterprise by leveraging Cloud, Analytics, Digital, Infrastructure and Security paradigms. With offices in Silicon Valley, Washington DC Metro, Rosemont, Chicago, Austin, Boston, Denver, Irvine, Raleigh, San Francisco, Seattle, New York, Dubai, Bengaluru, Hyderabad and Chennai, we serve Fortune 1000 and emerging organizations across industries globally. For more information, visit www.trianz.com.

Trianz is an Equal Opportunity Employer and does not discriminate on the basis of race, color, creed, national or ethnic origin, gender, religion, disability, age, political affiliation or belief, disabled veteran, veteran of the Vietnam Era, or citizenship status (except in those special circumstances permitted or mandated by law).

Job Type: Full-time

Experience:
relevant: 3 years (Preferred)"
Data Engineer,"Responsibilities Develop and maintain scalable ETL pipelines, build new pipelines and facilitate API integrations to support new requirements. Writing complex SQL queries to serve new requirements for ETL, data analysis and debugging. Writing SQL functions, procedures as required based on the requirements Finetune or optimize queries to support the increasing volume of data. Debug Python code, modify and enhance Python ETL applications based on the requirements on Linux environment. Writing reusable and efficient code in Python and SQL. Write unit, functional, regression tests for enhanced feature, maintain engineering documentation. Communicate closely with all product owners, Business and engineering teams to develop approaches for data platform architecture. Skills Basics of Computer Science - OOPS, Data Structures and Algorithms. Basic understand of regular Linux commands and usage. 5+ years of experience having hands on experience in writing, debugging and optimizing SQL queries, function and stored procedures. 3+ years of experience with hands on experience in writing, debugging Python code on Linux. Experience writing python applications that interact with ORM (Object Relational Mapper) libraries. Knowledge of XML and JSON parsing with unit test and debugging skills. Willingness and ability to learn new toolslanguages as needed. Process oriented with excellent oral and written communication skill with a desire for customer service. An excellent team player and communicator who can work effectively with cross functional teams and ability to navigate ambiguity."
Data Engineer,"NG - 012
Data Engineer

San Jose, California, USA

JOB TITLE


Data Engineer

Job Duties


- Work with different data platforms to provide the most applicable storage and access protocol.

- Use ETL tools or workflow libraries to extract and transform data for ingestion into data platforms.

- Integration of data from multiple data sources.

- Design the solution.

- Enable analytics on the data that was extracted and transformed.

- Data Exploration. Reporting.

- Development of the data set as per business userâ€™s expectations.

- Align stakeholders with their objectives, assist in data literacy and provide data access respective to their needs.

- Automate mundane work.

- Train users and establish agreements on the requirements

- Learn the latest tools and technologies.

- Resolve bugs and/issues.

- Document all relevant artifacts.

Job Requirements


Required Bachelors or foreign equivalent in CS, CA, CIS, IT, MIS, Engineering (Any), or any related field. Must be able to travel/relocate to various client sites throughout the U.S.

Location of Work


San Jose, CA.

To apply please send resumes to NextGen Technologies Inc., 1735 N 1st ST. Ste#308, San Jose, CA 95112 or email resumes to kushal@nextgentechnologiesinc.com."
Data Engineer,"Greetings from Trovetechs!!!

We have an immediate position for Data Engineer Role @ Mountain View, CA.

Â

Duration: Long Term Contract

Relevant Experience: 8+ Years

$DOE/hr.

Â

Â

Job Description:
We are looking for a talented and highly motivated senior engineer for a technical domain that comprises of customer data analytics on spark/MPP platform and ML models development to support different business use cases.
Experience with Bigdata ecosystem EMR, Hive, Spark/PySpark is a must.
Exposure to Cloud environment is required preferably AWS Discovering, analyzing, structuring and mining data Developing models supporting business use cases Deep understanding of data mining algorithms and statistical methods
Experience in successfully applying machine learning to real-world problems Proficient in python and packages for data analysis ( numpy, pandas, matplotlib)Â
Strong knowledge of extracting and processing data with RDBMS/MPP's Good communication skills and team player attitude
If you are interested, please send us your updated resume along with best Time & Number to reach ASAP.

Thanks and look forward to working with you,"
Data Engineer,"What your role and responsibilities will be
Collaborate with Integration team to build ETL processes to ingest data into BI stores.
Work with the internal teams in understanding the client requirements and convert
them into technical solutions.
Be a team player in performing development work during the production life cycle.
Experience with building stream and batch data processing systems.
Gather and process complex raw data at scale (including writing scripts, calling APIs,
write SQL queries, etc.).
Design and develop data processing solutions that support high performing and scalable
analytic solutions.

What you'll need to succeed

3+ years in a data engineering role.

Advanced knowledge of SQL and SQL queries performance tuning.

Good experience with RDBMS (Potgres, MS SQL Server, Oracle, DB2 ... etc)

Good experience with REST APIs

Good Experience with NOSQL databases (HBase, Mongo DB etc)

Experience with Impala, Hive & Presto is an asset.

Good knowledge of the Spark/Hadoop ecosystem.

Good knowledge of Scala/Java is an asset

Good knowledge of Python and Shell scripting.

Familiarity with micro-services and lambda architecture is an asset."
Data Engineer,"Duties:

Partner with internal stakeholders to understand business requirements, work with cross-functional data and products teams and build efficient and scalable data solutions. Build data expertise and own data quality for allocated areas of ownership. Design, build, optimize, launch and support new and existing data models and ETL processes in production. Monitor and manage the SLA for all data sets and systems in allocated areas of ownership. Work with data infrastructure and data engineering teams to triage infra issues and drive to resolution.

Skills:

5+ years hands-on experience with Linux and shell scripting 2+ years hands-on experience in MySQL database administration, implementation and maintenance 5+ years hands-on experience in SQL or similar languages and development experience in at least one scripting language (Python preferred) Data architecture, data modeling and schema design skills Excellent communication skills and proven experience in leading data driven projects from definition through interpretation and execution Ability to initiate and drive projects with all stakeholders Proactive and preventative solutions mindset a large plus Need to be able to travel to China periodically Mandarin language a plus"
Data Engineer,"Job Description
Company Overview

An American multinational company headquartered in Redwood City, California, that specializes in internet connection and data centers. The company leads in global colocation data center market share, with 200 data centers in 24 countries on five continents.

Job Responsibilities:

We are looking for a Senior Data Engineer with advanced knowledge of SQL and intermediate knowledge of Python. Nice to have (but not required) beginner or intermediate level java experience. Your primary focus will be the writing complex SQL queries, optimizing them and development of all server-side backend data processing logic, ensuring high performance using Python and SQL.
Develop and maintain scalable ETL pipelines, build new pipelines and facilitate API integrations to support new requirements.
Writing complex SQL queries to serve new requirements for ETL, data analysis and debugging.
Writing SQL functions, procedures as required based on the requirements
Finetune or optimize queries to support the increasing volume of data.
Debug Python code, modify and enhance Python ETL applications based on the requirements on Linux environment.
Writing reusable and efficient code in Python and SQL.
Write unit, functional, regression tests for enhanced feature, maintain engineering documentation.
Communicate closely with all product owners, Business and engineering teams to develop approaches for data platform architecture.
Skills:
Basics of Computer Science - OOPS, Data Structures and Algorithms.
Basic understand of regular Linux commands and usage.
5+ years of experience having hands on experience in writing, debugging and optimizing SQL queries, function and stored procedures.
3+ years of experience with hands on experience in writing, debugging Python code on Linux.
Experience writing python applications that interact with ORM (Object Relational Mapper) libraries.
Knowledge of XML and JSON parsing with unit test and debugging skills.
Willingness and ability to learn new tools/languages as needed.
Process oriented with excellent oral and written communication skill with a desire for customer service.
An excellent team player and communicator who can work effectively with cross functional teams and ability to navigate ambiguity.
Powered by JazzHR

N0WQZGog2K"
Data Engineer,"Hello,
Â
Position:ÂData EngineerÂ
Location:ÂBay Area, CA
Duration:Â12+ months
Â
Â
Job Descrption:

Must-Have strong experienceÂonÂSQLÂ,Python,ÂETL.

RequiredÂSkills:

Â

Experience with building scalable and reliable data pipelines using Data engine technologies like Matillion, Python, and SQL based programming.
8+ years of experience with and detailed knowledge of AWS based data warehouse technical architectures, data modeling, infrastructure components, ETL/ ELT and reporting/analytic tools and environments, data structures, and hands-on SQL coding.
4+ years of experience designing and developing complex ETL/ELT programs with the following Matillion, Python, etc
8+ year's experience developing complex SQL
Experience using Cloud database technologies such as RedShift, Snowflake
3+ years' experience programming in Python, and/or Java
2+ year's experience in data quality testing; adept at writing test cases and scripts, presenting and resolving data issues
2+ years implementing and programming data ingestion and ETL programs with large datasets (Terabytes sized analytical environment)
Experience with integration of data from multiple data sources
Experience developing and implementing streaming data ingestion solutions
Experience in Agile methodology (2+ years)"
Data Engineer,"Plume develops and deploys cloud based control planes with scale to manage tens of millions of customer homes through some of the world's largest Internet Service Providers. Our cloud applications include WiFi network management and optimization, device access control, network provisioning, IoT security, and end customer user interaction through mobile apps.

We are growing our team and looking for talented individuals to help us define and drive the success of our cloud based service offering. Our focus is on the home market and we support B2B and B2C product offerings.

The Opportunity:

As a Senior Data Engineer at Plume, you will focus on providing actionable insights and build highly available resilient systems that will impact and help make business decisions. You will ensure that the platform service we are delivering will meet both our and our customers' reliability expectations.

What you will do:
Build infrastructure and abstractions that can enable anyone (engineer or data scientist) to craft a scalable ETL pipeline for whatever the purpose is: metrics, analysis, machine learning, dashboard visualizations
Work closely with ops team to monitor and tune existing infrastructure.
Make intuitive decisions about what services, frameworks, and capabilities need to be in place before they are needed.
Build and maintain a data collection system that robustly extracts meaningful data from multiple sources and data stores.
Build analytics and Machine learning platforms to collect, store, process, and analyze huge sets of data
Who you are:
BA/BS in Computer Science, Information Systems or related technical field.
3+ years of experience in Data Infrastructure, with Cloud SW experience a plus.
Experience in crafting and scaling data infrastructure, models, and pipelines
Hands-on experience with a variety of data infrastructures, such as:
Processing: Spark, Flink, Hadoop, Lambda
Messaging: Kafka, Zookeeper, Pulsar
Storage: Hive, Mongo DB, Athena, Phoenix, Splice, Redshift, DynamoDB
Machine Learning: Sagemaker, H2O, Keras, NumPy
Open and active in sharing knowledge as well as excellent communication skills
Programming experience in one or more application or systems languages including Scala, Java, or Python
Have an ability to own a project from inception to completion"
Data Engineer,"About Shape Security

We are security and web experts, pioneers, evangelists, and elite researchers. We believe in the power of the Internet to be a positive force; our mission is to protect every website and mobile app from cybercriminals. Shape’s founders fought cybercrime at the Pentagon, Google, and other leading security companies. We are backed by some of the most prominent leaders and investors in the technology industry including Kleiner Perkins, Google Ventures, and more. Come be a part of our unparalleled team that is responsible for making the Internet a safer place for everyone.

Position Summary

We are looking for a Big Data Engineer that will work on the storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company.
Responsibilities
Selecting and integrating big data frameworks required to provide requested capabilities
Build analysis pipelines as well as visualization dashboards
Monitoring performance and iterate the solution fast
Skills and Qualifications
Experience with Cloud-based service and development environment, such as AWS or GCP
Proficiency with programming languages such as Python and Java
Proficient understanding of distributed computing principles
Good knowledge of Big Data querying databases, such as BigQuery, BigTable and MongoDB"
Data Engineer,"Hiretual is an AI-powered sourcing platform, and has been recognized as one of the best recruiting tools on the market (https://goo.gl/ERiQgY). During 2018, Hiretual achieves 500% growth with minimal focus on sales to date (https://goo.gl/ZTA1D9).

As a data engineer engineer, you will join the core engineering team to build scalable and robust data engine towards an AI and data-driven recruiting SaaS. You will be working with top AI researchers and infrastructure gurus to explore unlimited career space.

The core technical skills you should have:
• Strong computer science fundamentals: algorithms, data structures, and object-oriented programming
• Strong coding capability with Python
• must have 2+ years of experience in data processing pipeline, including data crawling, cleaning, processing, ETL
• Proficient in working variant databases: MySQL, Redis, Cassandra, Elasticsearch, graph databases.
• Proficient with big data processing frameworks: Spark, Hadoop, Hive, Kafka, EMR
• Writing scalable REST APIs for web services

Benefits
• Unlimited growth/promotion space
• Competitive salary and options
• 401k matching program
• Free and nice food
• Comprehensive medical, dental, and life insurance
• PTO policy
• Commuter benefits
• Fun, collaborative, and energetic team environment with nice office environment
• Fun events for family!"
Data Engineer,"Company Industry: Technology
Opportunity: The Mom Project is helping to source candidates for the above client who is looking for a Senior Data Engineer

Commitment Level: 40 hours per week, on-site, contract through December 2020

Our Customer is one of the leading global interconnection platforms and the world’s largest data center provider. They connect the world's leading businesses to their customers, employees and partners inside the world's most connected data centers in 52 markets across five continents. Their mission is to protect, connect and power the digital economy.
The candidate must be able to pass I-9 verification.

Overview:
We are seeking a Senior Data Engineer on a contract basis with advanced knowledge of SQL, Java and intermediate experience with API and python skills to help build and evolve Customer’s data services.
In this role your primary focus will be writing complex SQL queries, fine-tuning and development of all server-side backend data processing logic, ensuring high performance and responsiveness to requests from the front-end / API requests on Linux environment. The project has three modules:
- Real-time Data processing using Kafka, XML, Python and SQLAlchemy
- ETL Data processing module developed in Python and SQL – Postgres
- Rest API module developed in Java

What You’ll Do:
● Develop and maintain scalable ETL pipelines, build new pipelines and facilitate API integrations to support new requirements
● Writing complex SQL queries to serve new requirements for ETL, data analysis and debugging
● Writing SQL functions, procedures as required based on the requirements
● Fine-tune or optimize queries to support the increasing volume of data
● Debug Java, Python code, modify and enhance ETL applications based on the requirements on Linux environment
● Writing reusable and efficient code in Java, Python and SQL
● Develop Rest APIs using java libraries
● Write unit, functional, regression tests for enhanced feature, maintain engineering documentation
● Communicate closely with all product owners, Business and engineering teams to develop approaches for data platform architecture

Must Haves:
● 5+ years of SQL experience having hands-on experience in writing, debugging and optimizing SQL queries, functions and stored procedures
● 3+ years in writing Java-based code, debugging and knowledge of REST API
● 2+ years of experience with hands-on experience in writing, debugging Python code on Linux
● Basic understanding of regular Linux commands and usage
● Strong knowledge of Computer Science fundamentals - OOPS, Data Structures and Algorithms.
● Experience writing Java and python applications that interact with ORM (Object Relational Mapper) libraries
● Able to integrate multiple data sources and databases
● Strong knowledge in XML and JSON parsing with unit test and debugging skills
● Knowledge in Kafka, EMS queues or any messaging platform gateway application
● Willingness and ability to learn new tools/languages as needed
● Process-oriented with excellent verbal and written communication skill with a desire for customer service
● Experience and knowledge of ETL’s and basics of Data warehousing

Education: Bachelor's Degree required

Hours & Location:
M-F, 40 hours/week. This position is located onsite at our Customer’s Sunnyvale, CA location.

Now for the Perks!
Health Benefits: Medical, Dental, Vision, Life (including spouse & child), 401k, STD/LTD, AD&D, and Commuter Benefits program."
Data Engineer,"· Expert Data Engineering skills

· Expert Python skills

· Experience with web development technologies such as HTML and CSS + JavaScript a plus

· Python scripting skills for performance and efficiency

· Knowledge of SQL, setup and tuning of databases

· Linux knowledge and shell scripting.

Job Types: Full-time, Temporary, Contract

Experience:
Data Engineering: 5 years (Required)
sql: 5 years (Required)
Python Scripting: 3 years (Required)
Work Remotely:
Temporarily due to COVID-19"
Data Engineer,"Â

JD:

10+ years of experience

Experience in Advance SQL, Python, ETL, Data Modelling, Tableau or any BI tool

Should be well versed in creating data pipelines using Python. Should be very strong in writing advance SQL queries. Should be in BI Engineer

Python (Nice to have)

Please share resumes to bpolice@esharpedge.com"
Data Engineer,"Company:
Safeway Inc-001

Job Title:
Data Engineer

Address:
5918 STONERIDGE MALL RD, PLEASANTON, California 94588

Job Description:
The Data Engineering team at Albertsons Companies is looking for an experienced Data Engineer to work for the most transformational food and drug retailers in the United States. Albertsons operates over 2,300 stores under 19 well-known banners including Albertsons, Safeway, Vons, Jewel-Osco, Shaw's, Acme, Tom Thumb, Randalls, United Supermarkets, Pavilions, Star Market, Haggen and Carrs. The company reported revenue of over $60billion from over 34 million weekly shoppers and is the third largest private company in the country.

The Information Technology Department has an opening for a Data Engineer. This position is located in Pleasanton, California.

Position Purpose

Data Engineering at Albertsons is inspired to build best in class customer experience and revolutionize the food and drug retail industry. We are looking for people who are excited in re-imagining the grocery experience by harnessing the power of data and digital technologies. The Data Engineering team uses the Big Data paradigm to make data available for data scientist and multitude of other business users to draw insights to delight our customers, to improve store operations, to optimize supply chain and to proactively improve product lifecycle. You will enjoy working with one of the richest data sets in the world, cutting edge technology, and the ability to see your insights turned into business impacts on regular basis.

The candidate will have a background in computer science or a related technical field with experiences working with large data sets, analytical platforms, and a passion towards enabling data-driven decision making. A successful candidate will be both technically strong and business savvy, with a passion to make an impact through creative storytelling and timely actions. You are a self-starter, smart yet humble, with a bias for action.

Key Responsibilities include, but are not limited to:
Design and build highly scalable data pipelines and analytical platforms using new generation tools and technologies like Spark, Kafka, and other cloud technologies to ingest and process real time and near real time data from various systems
Build and optimize data pipelines using SQL and other programming languages to process and store data following complex business requirements
Show strong understanding of analytics needs and proactive-ness to build generic solutions to improve the efficiency
Implement, communicate, and enforce best practices and standards
Work with cross functional teams across the organization in varying roles
Provide production support as needed
Qualifications:
BS in Computer Science, Information Systems, Engineering or Mathematics
5+ years of experience in developing, tuning, and supporting data pipelines in a big data environment, preferably in the Cloud; Azure and Snowflake experience is a plus
5+ years of experience in creating scalable pipelines for both real time and near real time data for analytical and operational use cases
5+ years of experience with Spark, Spark Streaming, Kafka, or related technologies
5+ years of experience with at least one programming language like Python or Java
Experience with Azure Databricks, Datafactory or related technologies
Experience in working with - flat files, XML, JSON, Avro files and databases
Experience with workflow management and scheduling tools like Airflow
Experience in handling exceptions and automated re-processing and reconciling
Passion for Data Quality with an ability to integrate these capabilities
Knowledge of Jenkins for continuous integration and End-to-End automation
Proven capabilities for strong written and oral communication skills
Ability to integrate into a project team and contribute to project planning activities
Experience in developing implementation plans and schedules and preparing documentation for the jobs according to the business requirements
How to Apply: Interested candidates are encouraged to submit a resume by visiting https://www.albertsonscompanies.com/careers.html

Diversity is fundamental at Albertsons Companies. We foster an inclusive working environment where the different strengths and perspectives of each employee is both recognized and valued. We believe that building successful relationships with our customers and our communities is only possible through the diversity of our people. A diverse workforce leads to better teamwork and creative thinking, as well as mutual understanding and respect.

The Albertsons Companies policy is to provide employment, training, compensation, promotion and other conditions of employment without regard to race, color, religion, sexual orientation, gender identity, national origin, sex, age, disability, veteran status, medical condition, marital status or any other legally protected status.

We support a drug-free workplace -- all applicants offered a position are required to pass a pre-employment drug test before they are hired.

AN EQUAL OPPORTUNITY EMPLOYER"
Data Engineer,"Nyansa is a fast-growing innovator of advanced IT infrastructure analytics software based in Palo Alto, California. Founded in September 2013 by technology professionals from MIT, Meraki, Aruba Networks and Google, Nyansa is credited with developing the first cloud sourced, vendor-agnostic network analytics and IoT security platform, called Voyance.


We embrace simplicity and take following to heart on everything we do:
""Any intelligent fool can make things bigger, more complex, and more violent. It takes a touch of genius -- and a lot of courage -- to move in the opposite direction."" - Einstein

Nyansa is looking for a data engineer to join the team that is building a new, vendor-agnostic IT network analytics service purpose built for CIOs, network operations and helpdesk personnel managing heterogeneous enterprise environments. Our product is focused on the end user experience by helping IT staff gain new insights into client access conditions, network service behavior and enterprise applications issues that impact user performance.

Our current big data analytics system analyzes billions of streaming events per day using advanced algorithms. Going forward, we aim to scale the system extensively and are looking for radical ideas to achieve this.
The company is well funded and provides competitive compensation package, stock options, benefits, catered lunch, and a fun work environment.

We’re located within a 1 min walk from the Palo Alto Caltrain station.

Responsibilities:
• Design and develop highly scalable and available real time analytics platform using Spark, Kafka, Cassandra, and Elasticsearch for large data input streams
• Work closely with data science and UI teams to define and implement various analytics features related to product
• Configure, monitor, and optimize Spark and related infrastructure

Requirements:
• Strong desire to work for an early stage startup and be a part of its success
• Strong in Map-Reduce, parallelizing computations, and identifying bottleneck computations
• Strong in Scala and Python
• Experience in configuring and tuning Spark and Kafka systems
• Good understanding on Spark UI to extract useful information on application stages, and identify bottlenecks
• B.S. or higher degree in Computer Science or equivalent

Pluses:
• Experience with Cassandra, Elasticsearch, Mongo
• Experience with Ganglia and able to correlate information from various UIs to diagnose efficiency issues
• Experience working with AWS
• Experience with Spray to build REST endpoints"
Data Engineer,"Job Description
Experience in Advance SQL, Python, Tableau or any BI tool
Should be well versed in creating data pipelines using Python.
Should be very strong in writing advance SQL queries."
Data Engineer,"Job Details:

Job Title: Data Engineer

Location: San Jose, CA

Duration: 9-12 Months Contract

• 6-8 years of overall experience

• Excellent SQL and data querying skills.

• Experience with SQL Tuning using good sql coding practices

• Experience with Data Warehousing development, preferably Teradata is a plus

• Experience with Cloud Data Warehouse, preferably Snowflake is a plus.

• Background to ANSI SQL

• Sound knowledge of Database concepts and database architecture

Thanks & Regards

Ram Kishor

Phone: 732-452-1006*238

ram.kishor@diverselynx.com

Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company."
Data Engineer,"We are looking for a Senior Data Engineer who is excited about building products that wrangle AV data to supercharge our customers. You will lead the design and development of data infrastructure across our products and internal tools. At Applied, we encourage all engineers to take ownership over technical and product decisions, closely interact with users to collect feedback, and contribute to a thoughtful, dynamic team culture.At Applied, you will:* Design powerful data pipelines that process fast sensor streams, leverage appropriate data stores, and offer easy-to-use APIs* Develop and deploy high-quality software using modern tooling and frameworks* Work with products and teams across Applied Intuition* Work with customers across the AV ecosystem to understand their needs and the innards of their data systemsWe're looking for someone who:* Has 4+ years experience building scalable big data pipelines* Has experience with open source data processing frameworks (Spark, Kafka, etc.)* Has experience with different data storages (e.g., relational and NoSQL)* Has experience with containerization and other modern software development workflows* Takes initiative and ownership in a fast-paced environmentNice to have:* Expertise with multiple modern programming languages (Python, C++, Go, etc.)* Prior work in enterprise software, including on-prem and/or cloud deployments* Prior work in either autonomy or simulation productsAutonomy is one of the leading technological advances of this century that will come to impact our lives. The work you'll do at Applied will meaningfully accelerate the efforts of the top autonomy teams in the world. At Applied, you will have a unique perspective on the development of cutting edge technology while working with major players across the industry and the globe."
Data Engineer,"Job Title: Data Engineer
Location: Menlo Park, CA
Duration: 12 Months

RESPONSIBILITIES
Apply your expertise in quantitative analysis, data mining, and the presentation of data to see beyond the numbers and understand how our users interact with both our consumer and business products
Partner with Product and Engineering teams to solve problems and identify trends and opportunities
Inform, influence, support, and execute our product decisions and product launches

MINIMUM QUALIFICATIONS
3+ years' experience doing quantitative analysis within a large-scale company or fast-paced environment
BA/BS in Computer Science, Math, Physics, Engineering, Statistics or other technical field
Experience in SQL or other programming languages
Development experience in any scripting language (PHP, Python, Perl, etc.)
Experience communicating the results of analyses with product and leadership teams to influence the strategy of the product
Knowledge of statistics (e.g. hypothesis testing, regressions)
Experience manipulating data sets through statistical software (ex. R, SAS) or other methods"
Data Engineer,"We have following urgent role with our DIRECT client Title Sr. Data Engineer (Sql, Hive, TeraData) Location Sunnyvale, CA Duration 6+ Months Rate Market Position Summary Very Strong engineering skills. Should have an analytical approach and have good programming skills. Provide business insights, while leveraging internal tools and systems, databases and industry data Minimum of 5+ yearsrsquo experience. Experience in retail business will be a plus. Excellent written and verbal communication skills for varied audiences on engineering subject matter Ability to document requirements, data lineage, subject matter in both business and technical terminology. Guide and learn from other team members. Demonstrated ability to transform business requirements to code, specific analytical reports and tools This role will involve coding, analytical modeling, root cause analysis, investigation, debugging, testing and collaboration with the business partners, product managers other engineering team. Must Have Strong analytical background Self-starter Must be able to reach out to others and thrive in a fast-paced environment. Strong background in transforming big data into business insights Technical Requirements Knowledgeexperience on Teradata Physical Design and Implementation, Teradata SQL Performance Optimization Experience with Teradata Tools and Utilities (FastLoad, MultiLoad, BTEQ, FastExport) Advanced SQL (preferably Teradata) Experience working with large data sets, experience working with distributed computing (MapReduce, Hadoop, Hive, Pig, Apache Spark, etc.). Strong Hadoop scripting skills to process petabytes of data Experience in UnixLinux shell scripting or similar programmingscripting knowledge Experience in ETL processes Real time data ingestion (Kafka) Nice to Have Development experience with Java, Scala, Flume, Python Cassandra Automic scheduler RR studio, SAS experience a plus Presto Hbase Tableau or similar reportingdash boarding tool Modeling and Data Science background Retail industry background Education BS degree in specific technical fields like computer science, math, statistics preferred"
Data Engineer,"Position Role/Tile: Data Engineer
Location: SF/San Jose, CA.

Python programming skills. Must have used python in ETL process. Pull data from API and move data to Snowflake
Good in SQL


Central Business Solutions, Inc,
37600 Central Ct.
Suite #214
Newark, CA 94560"
Data Engineer,"middot Expert Data Engineering skills middot Expert Python skills middot Experience with web development technologies such as HTML and CSS + JavaScript a plus middot Python scripting skills for performance and efficiency middot Knowledge of SQL, setup and tuning of databases middot Linux knowledge and shell scripting."
Data Engineer,"Jobs Itility-US

Data Engineer

Working in teams (consisting of Hadoop data engineers, Hadoop data warehouse engineers, and platform engineers) that are building and managing Hadoop stacks. The teams install, configure and manage Hadoop ecosystem components.

As Hadoop data engineer, you are responsible for the functional part of provisioning data – e.g. building data ingestion pipelines and data connectors. You work closely with the data scientists and business intelligence engineers who are using this data to create analytical models.

After taking inventory of an application, all found servers, storage, network and database configurations will be transferred to the new data center. To efficiently execute this migration, we use a strict step-by-step plan, as one might find in a factory.

We need your expertise


You are well acquainted with the complete Hadoop stack. In addition, you have practical experience of being part of a DevOps team. Further requirements:
Bachelor of Science / master’s degree in Computer Science, System Administration, or any other IT infrastructure or software related study with a passion for the automation side of IT infrastructure
Minimum two to three years of relevant work experience
Capable of building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets of structured, semi-structured and unstructured data
Experience in building data products incrementally and integrating and managing data sets from multiple sources
Data quality oriented
Familiar with data architecture including data ingestion pipeline design, Hadoop information architecture
Hortonworks Certified Hadoop Developer and/or Cloudera Certified Hadoop Developer and/or Certified Hadoop Administrator
Knowledge of continuous integration & delivery tooling: e.g. Jira, Git, Jenkins, Bamboo
Coding proficiency in at least one modern programming language (Python, Ruby, Java)
Strong verbal and written communication skills
Good documenting capabilities
You have a hands-on mindset, a strong customer focus, a problem-solving orientation and can show fast results
You have a clear focus on results and quality.
Willingness to travel to the Netherlands if required for training or project work
Bachelor of Science / master’s degree
Minimum 3-5 years of relevant work experience within an enterprise environment
Advanced knowledge of RHEL 6 & 7
Advanced knowledge of VMWare 5 & 6; VCAP5-DCD, VCDX5-DCV preferred
Experience with Cisco UCS manager and NetApp FAS / ScaleIO storage solutions
Experience with databases (MSSQL, Oracle) preferred
Strong verbal and written communication skills
Good documenting capabilities
Willingness to travel to the Netherlands if required for training or project work

Meet Itility


At Itility we believe in merging technology and data to drive our customers one step beyond. Itility digital consultants are experts in data, cloud, software, and IT infrastructure.

Our culture can be described as ‘no-nonsense, with passion’. Working at Itility is about working with people, staying close to our customers.

We work for large enterprises and innovative startups. Acting as the ‘digital twin’ of customers, we work shoulder-to-shoulder to exceed business goals and push the boundaries of what you thought was possible.

Do you like to go above and beyond? Do you want to work with passion for what you do, in a team of people fueled by the same passion?

Then we would love to meet you!

You believe in
Build efficient and highly reliable data ingestion pipelines for the Hadoop stack
Own data quality and data knowledge around all data that you touch
Work side-by-side with software engineers and data scientists in designing modeled data sets to be used in many different applications, from proof-of-concept to production
Understand the entire life cycle of data that flows through any systems for which you are responsible
Pay constant attention and effort to the reliability of your pipelines
Reports to the Itility project manager, working in close harmony with team members and interfacing with the standing IT organization.

Location


San Jose, CA.


Contact person


Apply now

Share:


Share on linkedin

Share on twitter

Share on facebook

Share on whatsapp

Share on email"
Data Engineer,"Get your career started at eHealth


eHealthInsurance has many exciting career opportunities in a number of locations, across various functions. Come join us today!

Data Engineer

At eHealth, we are passionate about solving our nation's toughest problems to bring more suitable, accessible, and affordable health insurance to Americans. We are seeking a talented data engineer to join our growing data team, which is already making a valuable impact on the entire company. This person will help us develop cutting-edge data tools and pipelines to drive better and faster decision making within our company and to better serve our customers. This is a fast-paced, collaborative, and iterative environment requiring quick learning, agility, and flexibility.

Responsibilities:
Become the subject matter expert on our data and its capabilities. Your scope of knowledge will need to include various data systems that are specialized to internal departments and 3rd party data platforms.
Design and build highly scalable data integration / ETL pipelines to improve data accessibility and consumption.
Automate data processing using workflows tools to schedule and manage dependency of various data pipelines.
Work directly with data scientists to develop scalable implementation of statistical and machine learning models in production, and work with software engineers to design, build, and maintain APIs to interact with those models.
Recommend ways to improve data reliability, efficiency, and quality.
Assist eHealth’s data architect with logical and physical data model designs and documentation.
Work with data infrastructure team to triage issues and support issue resolution.
Minimum Qualifications:
Bachelors or Masters in Computer Science, Engineering, or a related quantitative field.
2+ years of experience with designing, implementing and maintaining scalable and reliable data pipelines
Mastery of SQL in writing complex and high performance queries
Working experience with MPP systems (Snowflake, Spark SQL, Hive) and NoSQL systems (MongoDB, etc).
Production coding experience with Python, Scala or Java and scripting languages (Unix shell) as well as solid experience with git.
Expertise with relational databases and experience with schema design and dimensional data modeling.
Working experience with various ETL technologies and frameworks (Pentaho, Informatica, Matillion, etc.)
Knowledge of AWS data tools.
Excellent communication skills.
Highly motivated problem-solver who enjoys working in a fast-paced environment and can also be patient with the pace of highly regulated industries like healthcare.
Nice to Have:
Experience collaborating with data science team.
Strong experience in designing and implementing data APIs.
Product familiarity with Adobe Analytics, Cisco systems, Snowflake.
Familiarity with workflow management tools (Airflow).
Working experience with data warehousing.
Ability to create beautiful data visualizations using D3, Tableau, or similar tools.
Working experience with large healthcare related datasets, including EHRs, medical claims data, and health population surveys. Experience in building healthcare data pipelines would be a big plus.
Knowledge of healthcare insurance industry, products, systems, business strategies, and products.
Experience working with call center operations.
eHealth is an Equal Employment Opportunity employer. It is our policy to provide equal opportunity to all employees and applicants and to prohibit any discrimination because of race, color, religion, sex, national origin, age, marital status, sexual orientation, genetic information, disability, protected veteran status, or any other consideration made unlawful by applicable federal, state or local laws. The foundation of these policies is our commitment to treat everyone fairly and equally and to have a bias-free work environment.

If you are interested in applying for employment with eHealth and need special assistance or an accommodation to apply for a posted position contact us at: accommodations@ehealthinsurance.com."
Data Engineer,"At SpringML, we are all about empowering the doers in companies to make smarter decisions with their data. Our predictive analytics products and solutions apply machine learning to todays most pressing business problems so customers get insights they can trust to drive business growth. We are a tight knit, friendly team of passionate and driven people who are dedicated to learning, get excited to solve tough problems and like seeing results, fast.

Your primary role will be to design and build data pipelines. You will be focused on designing and implementing solutions on Hadoop, Spark, Pig, Hive. In this role you will be exposed to Google Cloud Platform including Dataflow, BigQuery and Kubernetes so the ideal candidate will have a strong big data technology foundation and bring a passion to learn new technologies. If you believe you have these skills please email your resume to info@springml.com.

Required Skills:
4-7 years Python and Java programming
3-5 years knowledge of Java/J2EE
3-5 years Hadoop, Big Data ecosystem experience
3-5 years of Unix experience
Bachelors in Computer Science (or equivalent)
Duties and Responsibilities:
Design and develop applications utilizing the Spark and Hadoop Frameworks or GCP components.
Read, extract, transform, stage and load data to multiple targets, including Hadoop, Hive, BigQuery.
Migrate existing data processing from standalone or legacy technology scripts to Hadoop framework processing.
Should have experience working with gigabytes/terabytes of data and must understand the challenges of transforming and enriching such large datasets.
Additional Skills that are a plus:
C, Perl, Javascript or other programming skills and experience a plus
Production support/troubleshooting experience
Data cleaning/wrangling
Data visualization and reporting
Devops, Kubernetes, Docker containers
Powered by JazzHR"
Data Engineer,"The Data Engineering team at Albertsons Companies is looking for an experienced Data Engineer to work for the most transformational food and drug retailers in the United States. Albertsons operates over 2,300 stores under 19 well-known banners including Albertsons, Safeway, Vons, Jewel-Osco, Shaw's, Acme, Tom Thumb, Randalls, United Supermarkets, Pavilions, Star Market, Haggen and Carrs. The company reported revenue of over $60billion from over 34 million weekly shoppers and is the third largest private company in the country.

The Information Technology Department has an opening for a Data Engineer. This position is located in Pleasanton, California.

Position Purpose

Data Engineering at Albertsons is inspired to build best in class customer experience and revolutionize the food and drug retail industry. We are looking for people who are excited in re-imagining the grocery experience by harnessing the power of data and digital technologies. The Data Engineering team uses the Big Data paradigm to make data available for data scientist and multitude of other business users to draw insights to delight our customers, to improve store operations, to optimize supply chain and to proactively improve product lifecycle. You will enjoy working with one of the richest data sets in the world, cutting edge technology, and the ability to see your insights turned into business impacts on regular basis.

The candidate will have a background in computer science or a related technical field with experiences working with large data sets, analytical platforms, and a passion towards enabling data-driven decision making. A successful candidate will be both technically strong and business savvy, with a passion to make an impact through creative storytelling and timely actions. You are a self-starter, smart yet humble, with a bias for action.

Key Responsibilities include, but are not limited to:
Design and build highly scalable data pipelines and analytical platforms using new generation tools and technologies like Spark, Kafka, and other cloud technologies to ingest and process real time and near real time data from various systems
Build and optimize data pipelines using SQL and other programming languages to process and store data following complex business requirements
Show strong understanding of analytics needs and proactive-ness to build generic solutions to improve the efficiency
Implement, communicate, and enforce best practices and standards
Work with cross functional teams across the organization in varying roles
Provide production support as needed
Qualifications:
BS in Computer Science, Information Systems, Engineering or Mathematics
5+ years of experience in developing, tuning, and supporting data pipelines in a big data environment, preferably in the Cloud; Azure and Snowflake experience is a plus
5+ years of experience in creating scalable pipelines for both real time and near real time data for analytical and operational use cases
5+ years of experience with Spark, Spark Streaming, Kafka, or related technologies
5+ years of experience with at least one programming language like Python or Java
Experience with Azure Databricks, Datafactory or related technologies
Experience in working with - flat files, XML, JSON, Avro files and databases
Experience with workflow management and scheduling tools like Airflow
Experience in handling exceptions and automated re-processing and reconciling
Passion for Data Quality with an ability to integrate these capabilities
Knowledge of Jenkins for continuous integration and End-to-End automation
Proven capabilities for strong written and oral communication skills
Ability to integrate into a project team and contribute to project planning activities
Experience in developing implementation plans and schedules and preparing documentation for the jobs according to the business requirements
How to Apply: Interested candidates are encouraged to submit a resume by visiting https://www.albertsonscompanies.com/careers.html

Diversity is fundamental at Albertsons Companies. We foster an inclusive working environment where the different strengths and perspectives of each employee is both recognized and valued. We believe that building successful relationships with our customers and our communities is only possible through the diversity of our people. A diverse workforce leads to better teamwork and creative thinking, as well as mutual understanding and respect.

The Albertsons Companies policy is to provide employment, training, compensation, promotion and other conditions of employment without regard to race, color, religion, sexual orientation, gender identity, national origin, sex, age, disability, veteran status, medical condition, marital status or any other legally protected status.

We support a drug-free workplace -- all applicants offered a position are required to pass a pre-employment drug test before they are hired.

AN EQUAL OPPORTUNITY EMPLOYER"
Data Engineer,"10+ years of experience Experience in Advance SQL, Python, Tableau or any BI tool Should be well versed in creating data pipelines using Python. Should be very strong in writing advance SQL queries."
Data Engineer,"Location:ÂSan Jose, CA

Start: Immediate

Duration: 12 Months+

Â

This is a highly technical requirement. Must be senior level.

Candidate must have:
Very Strong SQL, Python + Hive
Familiarity with Hadoop
Know how to transform Data & put in Hadoop
Must be able to translate the business req into technical specs
Should have strong Data Analyst skills
Work closely with IT Team.
Â"
Data Engineer,"Job Description
2+ years of experience in developing production quality code in Python/C/C++
1+ years of experience in data warehousing and SQL
Experience in running applications in the cloud (AWS preferred) is a plus
Track record of taking on open ended problems and implementing robust solutions
Experience in SSD/NAND Flash development and validation a plus
Experience in USB, Serial port, PCIe knowledge is a plus
BS/MS in Engineering or related technical discipline"
Data Engineer,"The Business

GradTests.com is a leading provider of practice psychometric tests to graduates, students and young professionals.

The Role

The core purpose of the role is to make high quality, high availability, accurate data available for our data analysts and data scientists to do their analysis, derive their insights and build their models. You are the Scotty Pippin to the Michael Jordans. You are the Xavi to the Messis.

You'll do things like:
Ensure our data warehouse is well structured, running smoothly and efficiently for all business intelligence
Set up and maintain various data pipelines used for customer analytics, marketing analytics and product analytics
Skills and experience

Non negotiables:
SQL
Python
Strong knowledge of traditional relational databases - we don't mind which
Some experience with cloud technologies - again we don't mind if it's AWS, GCP or Azure
Experience in any streaming technology
Great experience in using third party APIs at scale
Some web scraping experience
An obsession with data quality
Strong communication skills
Nice to haves:
Experience in working with analysts
Any basic knowledge of advanced analytics techniques
Experience in a visualisation tool like Tableau
Job Types: Full-time, Contract

Salary: $100,000.00 /year

Work Remotely:
Yes"
Data Engineer,"If you thrive on working with big data in high performance teams then this is the place for you. You would work on data and build some of the tools that are critical to moving & transforming this data into valuable and insightful information. Creating reliable, scalable, and high performance products requires exceptional technical expertise and practical experience working with large-scale distributed systems. Finally, you will tackle challenging issues of scale, reliability and security while delivering a delightful, simple user experience to a global user base.

RESPONSIBILITIES

You will manage data warehouse plans for a product or a group of products. You will interface with engineers, product managers and product analysts to understand data needs. In addition, you will design, build and launch new data extraction, transformation and loading processes in production. You will work with data infrastructure to triage infra issues and drive to resolution. Be prepared to build and launch new data models that provide intuitive analytics to your customers as well as design and extremely efficient & reliable data pipelines to move data to our Data Warehouse. You will use your expert coding skills across a number of languages from Python, Scala, Java and PHP and work across multiple teams in high visibility roles.

REQUIREMENTS
2+ years of Scala and/or Python development experience is necessary
2+ years of SQL (Oracle, Vertica, Hive, etc) experience is required
2+ years of experience in custom or structured (ie. Informatica/Talend/Pentaho) ETL design, implementation and maintenance
2+ years or experience applying statistical data analysis to real-life problems
Experience working with either a Map Reduce or a MPP system on any size/scale
BS or MS degree in Computer Science or a related technical field
Previous experience with Data ingestion and IR (information retrieval) is highly desirable
Industry experience as a Data Engineer or related specialty
Powered by JazzHR"
Data Engineer,Senior Data Engineer with 5+ years of data engineering experience. Key skills required SQL Python Pyspark AWS
Data Engineer,"Job Description Position Data Engineer Location Menlo Park, CA Skills Required Must have 10+ Years of in BI Technologies Experience in Advanced SQL, Python, ETL, Data Modelling, Tableau or any BI tool Should be well versed in creating data pipelines using Python. Should be very strong in writing advanced SQL queries. Should have experience with complex data types shorting manipulationclubbingordering. Thanks and Regards, Sumit Kumar skumarbraintreeus.com mailtoskumarbraintreeus.com httpwww.braintreeus.com httpwww.braintreeus.com"
Data Engineer,"RESPONSIBILITIES Kforce has a client in search of a Data Engineer in Mountain View, CA. Duties Interacting with business users and analysts to understand business processes and creating analytic requirements and technical specs Designing data mart dimensions and facts to satisfy the business needs Implementing transformation logic to populate dimensions and facts Designingdeveloping ETL jobs across multiple platforms and tools including S3, Hadoop, Vertica in order to pull in source data needed for the data marts Roughly 60-70 hands-on coding Act in a technical leadership capacity Mentoring junior engineers, new team members, and applying technical expertise to challenging data and design problems Resolve defectsbugs during QA testing, pre-production, production, and post-release patches Work cross-functionally with various teams Product Management, Project Management, Data Architects, Data Scientists, Data Analysts, Software Engineers, and other Data Engineers Contribute to the design and architecture of project across the data landscape REQUIREMENTS BSMS in Computer Science or equivalent work experience 4-6 years of experience designing and implementing data marts using star schema design Strong expertise in data warehousedata mart architecture Advanced experience in writing complex SQL, SQL tuning is a must have Strong experience in one of the Columnar and MPP database (Vertica, Teradata, Netezza, etc.) is a must have Advanced experience with scripting language Python or Shell is a must have Excellent communication skills, both spoken and written Ability to communicate effectively with engineers, analysts, business users and adjusting as needed Experience with Agile Development, SCRUM, or Extreme Programming methodologies Experience with ETL batch and streaming processes Experience with Amazon analytics services including EMR Redshift Experience working with large data volumes The ability to get code into production SQL star schema design required Not looking for someone who know transactional systems SQL tuning Python or Shell a must MPP - Vertica experience AWS is a nice to have Kforce is an Equal OpportunityAffirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status."
Data Engineer,"Job Title Data Engineer III Duration 6+months Location Sunnyvale, CA Responsibilities Develop and maintain scalable ETL pipelines, build new pipelines and facilitate API integrations to support new requirements. Writing complex SQL queries to serve new requirements for ETL, data analysis and debugging. Writing SQL functions, procedures as required based on the requirements Finetune or optimize queries to support the increasing volume of data. Debug Python code, modify and enhance Python ETL applications based on the requirements on Linux environment. Writing reusable and efficient code in Python and SQL. Write unit, functional, regression tests for enhanced feature, maintain engineering documentation. Communicate closely with all product owners, Business and engineering teams to develop approaches for data platform architecture Skills Basics of Computer Science - OOPS, Data Structures and Algorithms. Basic understand of regular Linux commands and usage. 5+ years of experience having hands on experience in writing, debugging and optimizing SQL queries, function and stored procedures. 3+ years of experience with hands on experience in writing, debugging Python code on Linux. Experience writing python applications that interact with ORM (Object Relational Mapper) libraries. Knowledge of XML and JSON parsing with unit test and debugging skills. Willingness and ability to learn new toolslanguages as needed. Process oriented with excellent oral and written communication skill with a desire for customer service. An excellent team player and communicator who can work effectively with cross functional teams and ability to navigate ambiguity."
Data Engineer,"Please find the below job description and let me know your thoughts on this. Location Fremont or Menlo Park CA Role- Data Engineer Duration 1.5 years. Should be comfortable working with complex SQL queries , fine tuning them , using functions like maps and arrays Should have good knowledge in Python and Hives Experience working on Data pipelines Must have exposure working on SFDC and familiar with table structures (deals, quotes,service orders, subscription billing etc etc) Should be comfortable in creating tables ( preferably in dimensional modeling ) , should also know normalization Good to know Tableau or any other reporting tool. Requirement gatherings and co-coordinating with multiple clients and cross functional teams"
Data Engineer,"What your role and responsibilities will be
• Collaborate with Integration team to build ETL processes to ingest data into BI stores.
• Work with the internal teams in understanding the client requirements and convert
them into technical solutions.
• Be a team player in performing development work during the production life cycle.
• Experience with building stream and batch data processing systems.
• Gather and process complex raw data at scale (including writing scripts, calling APIs,
write SQL queries, etc.).
• Design and develop data processing solutions that support high performing and scalable
analytic solutions.
What youll need to succeed
3+ years in a data engineering role.
Advanced knowledge of SQL and SQL queries performance tuning.
Good experience with RDBMS (Potgres, MS SQL Server, Oracle, DB2 ... etc)
Good experience with REST APIs
Good Experience with NOSQL databases (HBase, Mongo DB etc)
Experience with Impala, Hive & Presto is an asset.
Good knowledge of the Spark/Hadoop ecosystem.
Good knowledge of Scala/Java is an asset
Good knowledge of Python and Shell scripting.
Familiarity with micro-services and lambda architecture is an asset."
Data Engineer,"Job Description
5+ years of hands-on server-side programming
Experience in JVM languages (Java, Scala, Groovy)
Experience in designing and developing scalable RESTful
services
Experience with concurrency and data structures
Experience in data-modelling for RDBMS/NoSQL databases
Experience in developing scalable event driven data pipelines
using Kafka, Spark
Experience with containers such as Apache/Tomcat, Jboss
Passion to excel in fast-paced, dynamic environment
What we also love to see -
Solid understanding of GitHub and open source development culture
Exposure to A/B testing
Open source/Stack Overflow contributions!
Ecommerce!"
Data Engineer,"Work Authorization Those authorized to work in the United states are encouraged to apply.We are able to sponsor H1-B at this time. Job Description PETADATA is looking for a Data Engineer. Design, develop, maintain and support of Enterprise Data Warehouse amp BI platform within using various data amp BI tools, this position offers a unique opportunity to make significant impact to the entire organization in developing data tools and driving data driven culture. Responsibilities Work in a time constrained environment to analyse, design, develop and deliver Enterprise Data Warehouse solutions for Sales, Delivery and Logistics Teams Create ETL pipelines using Python, Airflow Create real time data streaming and processing using Open source technologies like Kafka , Spark etc Required Skills 1. Strong in Python 2. Good hold on Data Modelling and Data Warehousing 3. Self-Starter If you are interested and meet the above job requirements, please submit your resume. After carefully reviewing your experience and skills, one of our Hiring team members will contact you on the next steps."
Data Engineer,"(6 - 12) Months Contract.

Requirements:
5+ years of data engineering experience
Proficiency with REST APIs, Cassandra, Python, MongoDB, Postgres, and querying.
Proficiency with both relational and non-relational databases and how to combine them
Proficiency with distributed computing engines,frameworks like Hadoop v2, Spark
Responsibilities:
Experience with building stream-processing systems, (e.g., using solutions such as Spark-
Streaming or Storm)
Ability to manage Hadoop/Spark cluster or other ingestion tools, with all included services
Previous Experience with various messaging systems, such as Kafka or Amazon Kinesis(
for example)
Coding Experience with any of the programming languages - Python,Java, Scala
Powered by JazzHR"
Data Engineer,"Amick Brown is seeking an experienced Data Engineer for our direct client.

Location: Sunnyvale, CA
Duration: 6 Months +

Roles and Responsibilities
Design and build data models to conform to our existing EDW architecture.
Design and build data pipelines using tools - SAP SLT, SAP Data Services, Python and Microsoft SSIS.
Design and development of data warehouse using T-SQL, SQL, and python
Work with teams to deliver effective, high-value reporting solutions by leveraging an established delivery methodology.
Implement data structures using best practices in data modelling, processes, and technologies.
Design and development of data warehouse using Microsoft SQL Server, SAP HANA and Snowflake databases.
Writing analytics programs (transformations/calculations) in T-SQL, R, Python or comparable
Knowledge and understanding of Enterprise applications like SAP ECC and SAP CRM, Salesforce etc.
Knowledge and functional understanding of Finance, Global Supply Chain business processes
Perform data mining and analysis to uncover trends and correlations to develop insights that can materially improve our decisions.
Development with one or more data visualization/reporting tools (Tableau, Business Objects, Hana Analytics, Microsoft PowerBI)
Work with various product owners to ensure applications are instrumented with proper tracking mechanisms to enable analytics.
Continually recommend, develop, and implement process improvements and tools to collect and analyse data, and visualize/present insights.
Skill/Job Requirements:
Bachelors degree in Business, MIS or related area. Masters degree a plus.
8+ years Business Intelligence / Data Warehouse development experience
3+ years of experience in ETL development tools, preferably with knowledge of Microsoft Integration Services 2005 or greater (SSIS), SAP Data Services, SAP SLT and Python.
5+ years of experience in design and development using Microsoft SQL Server, SAP HANA and Snowflake databases.
Strong experience in full life cycle development, implementation, management and performance tuning of the Enterprise Data Warehouse
Experience in database development (T-SQL, PLSQL, and/or SQL scripts)
Experience in building data pipelines using python, C# and JSON
Experience in Microsoft BI development in Integration Services (SSIS), Analysis Services (SSAS) or Reporting Services (SSRS)
Experience building and managing data flows to and from cloud applications
Demonstrated experience in utilizing R, Python, SPSS or comparable to develop analyses
Experience visualizing data in business intelligence tools such as Tableau, Business Objects or Hana Analytics
Experience and functional understanding with Enterprise applications like SAP ECC and SAP CRM, Salesforce etc.
Strong experience with performance and scalability design and testing
Experience creating test plans, testing and resolving data discrepancies
Must be a self-motivated, energetic, detail-oriented team player passionate about producing high quality BI & Analytics deliverables
Strong sense of customer service for internal customers
Medical robotics has unique characteristics that will require immersion in clinical and technical training and he or she must come up to speed quickly an interest and desire to learn are critical
Amick Brown, LLC is an Information Technology consulting company providing IT staffing and managed solutions. Founded in 2010, we are a certified woman-owned small business headquartered in San Ramon, CA with an additional office in Sacramento, CA. Amick Browns experienced IT professionals support customers nationwide in both the commercial and public sectors. We are SBA 8(a) and IS0 9001:2015 certified and are an SAP Services Silver Partner.

Regular full-time employees are eligible for the following Amick Brown provided benefits:
Health
Vision
Dental
401k with company match
Paid time off
Sick Leave
Short-Term Disability
Life Insurance
Wellness & Discount Programs"
Data Engineer,"Ref ID: 00420-9502602834Classification: Data Engineer

Compensation: DOE

Ability to Clean/transform data from raw data inputs
Ability to deep dive into the data to meet stakeholders requirements Outstanding communication skills with the ability to influence decision makers and build consensus with teams
Development and execution of data movement tools using scripts in languages such as SQL, HQL, SAS

Please send resumes to Trupti Deshpande.

Job Requirements:
5+ years relevant experience Advanced SQL skills to get the data you need from a warehouse (Vertica, Hive, SparkSQL, etc) Advanced SAS skills Experience with AWS EMR Strong ETL experience Experience using Github and Tidal

Robert Half Technology matches IT professionals with some of the best companies on a temporary, project or full-time basis. From roles in software and applications to IT infrastructure and operations, we provide you unparalleled access to exciting career opportunities. Our personalized approach, innovative matching technology and global network with local market expertise help you find the technology jobs that match your skills and priorities fast. By working with us, you have access to challenging opportunities, competitive compensation and benefits, and training to enhance your skill sets.

From philanthropy to environmental stewardship to employee programs, Robert Half is proud to have an active role in the communities in which we live and work. Our company has appeared on FORTUNEs Most Admired Companies list every year since 1998.

Download our mobile app to take your job search on the go!

Contact your local Robert Half Technology office at 888.490.4429 or visit www.roberthalf.com/jobs/technology to apply for this job now or find out more about other job opportunities.

All applicants applying for U.S. job openings must be authorized to work in the United States. All applicants applying for Canadian job openings must be authorized to work in Canada.

© 2020 Robert Half Technology. An Equal Opportunity Employer M/F/Disability/Veterans.

By clicking 'Apply Now' you are agreeing to Robert Half Terms of Use."
Data Engineer,"Greetings from Trovetechs !!!

We have an immediate need for a Data Engineer @ Mountain View, CA. Our Client is a Global Implementation Partner and they are implementing this for their Customer in Finance Industry.

Duration: 12+ Months Contact/Long Term
Rate: $DOE

Responsibilities:
â Responsible for developing and translating computer algorithms into prototype code and maintaining, organizing, and identifying trends in large data sets.
â Proficiency in SQL database design, proficiency in creating process documentation, strong written and verbal communication skills, and the ability to work independently and on teams.Â
â Familiarity with the computer coding languages python, java, Kafka, hive or storm may be required in order to oversee real-time business metric aggregation, data warehousing and querying, schema and data management, and related duties.Â
â Should have knowledge of algorithms, data structures, and performance optimism and experience with processing and interpreting data sets.Â
â Develop technical solutions to improve access to data and data usage. Understand data needs and advise company on technological resources.Â
â Aggregate and analyze various data sets to provide actionable insight.Â
â Develop reports, dashboards, and tools for business-users

Required Skills:
â Experience designing and deploying data systems on AWS.
â Hands-on experience with AWS technologies like S3, Redshift, Dynamo DB, EMR/EC2, HiveÂ
â Hands-on experience building scalable and reliable data pipelines based on Big Data processing technologies like Hadoop, MapReduce, Spark, Python.Â
â Hands-on experience in ETL tools and working with large data sets in the cloud Capability of building data marts and data solutions

If you are interested, please send us your updated resume along with best Time & Number to reach you ASAP.

Thanks and look forward to working with you,"
Data Engineer,"Design and build data models to conform to our existing EDW architecture.
Design and build data pipelines using tools - SAP SLT, SAP Data Services, Python and Microsoft SSIS.
Design and development of data warehouse using T-SQL, SQL, and python
Work with teams to deliver effective, high-value reporting solutions by leveraging an established delivery methodology.
Implement data structures using best practices in data modeling, processes, and technologies.
Design and development of data warehouse using Microsoft SQL Server, SAP HANA and Snowflake databases.
Writing analytics programs (transformations/calculations) in T-SQL,R, Python or comparable
Knowledge and understanding of Enterprise applications like SAP ECC and SAP CRM, Salesforce etc.
Knowledge and functional understanding of Finance, Global Supply Chain business processes
Perform data mining and analysis to uncover trends and correlations to develop insights that can materially improve our decisions.
Development with one or more data visualization/reporting tools (Tableau, Business Objects, Hana Analytics, Microsoft PowerBI)
Work with various product owners to ensure applications are instrumented with proper tracking mechanisms to enable analytics.
Continually recommend, develop, and implement process improvements and tools to collect and analyze data, and visualize/present insights.
Job Posting Type

Agency Recruited Worker Required
Worker Legal Name (For Manager Sourced Only)

(No Value)"
Data Engineer,"Data Engineer

Share3

Job ID: FA-0100-560

Open Since: 2019-12-17

City: Sunnyvale
State: California
Country: United States of America

Job Description:


Frontend Arts brings together the brightest minds to create breakthrough technology solutions, helping our customers gain a competitive advantage. We are continuously evolving how we work and how we look at business challenges, so we can continue to deliver measurable, sustainable solutions to our clients.

We are looking for a self-motivated ""Data Engineer"" with excellent communication and customer service skills.

Job Skills:
Perform data quality analytics on streaming data with business rules setup and modified during streaming to assess the quality.
He would need Big data lead at onsite and team at offshore to supplement.
Must Skills - Spark, Spark streaming, neo4j, Java / Scala, Kafka
Minimum Experience: 8 Yrs

Education:

Must have a Bachelor's degree, preferably in Computer Science or Engineering"
Data Engineer,"Must have skills SQL, Python, Tableau 10+ years of experience Experience in Advance SQL, Python, Tableau or any BI tool Should be well versed in creating data pipelines using Python. Should be very strong in writing advance SQL queries. Hacker rank test is must Please apply with LinkedIn profile for faster processing"
Data Engineer,"When you come across a sea of data, is your first instinct to put on your software diving suit and go deep? We’re looking for highly-skilled Data Engineers to design and automate large scale data solutions that power our state-of-the-art artificial intelligence platform. If “changing the world” is on your to-do list, Entefy is your chance to make a career of it.

We’re redefining digital interaction, and our next Data Engineer will play a key role in the growing agile team that’s making it all happen.
Requirements
6+ years relevant experience developing and integrating frameworks and database technologies that support highly scalable data processing
Advanced knowledge of system architecture and database design
Proficiency in Big Data tools: Spark, Hadoop, Kafka, etc.
Advanced experience with SQL and NoSQL database architecture and implementation (hands-on experience with PostgreSQL, Elasticsearch, and Cassandra a plus)
Demonstrable experience designing, developing, and implementing ETL processes
Experience working with private cloud infrastructure
Demonstrable experience building and optimizing Big Data pipelines and architecture
Proficiency in Python, Java, C++
Demonstrable experience with Stream Processing and workload management for data transformation, augmentation, analysis, etc.
Ability to collaborate well with others
Strong communication skills
Visit www.entefy.com and www.blog.entefy.com"
Data Engineer,"On April 1, 2020, Rubicon Project and Telaria, Inc. merged to create one company. The combined company will rebrand as one in the coming months. In the interim, each company will operate under its existing brand name. The ticker symbol for the combined public entity will remain NYSE: RUBI.

We are looking for Data Engineers to help us build tools, enhance our platform, and leverage our vast amounts of advertising data to make informed decisions around business optimizations and efficiencies. We're close to the customers and have the reward of seeing our work being used immediately. We take pride in the reliability and scalability of our platform, as well as our pace of implementation. We are a small and efficient team building out a solution in a new space with lots of green field ahead of it.

Why You'll Be Excited
Having a large stake and impact on the product and business direction and bottom-line
Collaborating with innovative and goal-focused engineering and business teams
Working with data scientists, data analysts, and product managers to identify and use the data that is most relevant to the problem at hand
Building systems that can effectively stream, store, and crunch vast amounts of data to help inform customers and power business analytics
Solving complex problems revolving around real-time strategic decision-making and large data systems
Developing, deploying, and maintaining robust and high-performance systems and features
Why We'll Be Excited About You
You have strong verbal and written communication skills that help you express your work in meaningful ways to cross functional teams
You are passionate about learning different technologies, exploring engineering challenges, and working in a dynamic and collaborative environment
You have working experience and skills designing and coding in Java/Scala and/or Python
You are proficient in writing efficient and well-structured SQL queries and have experience with database schemas and design
You have experience with big data technologies (Spark, Presto, Druid, etc.)
You have knowledge of UNIX/Linux and scripting with Perl, Shell, etc.
Degree in Computer Science or a related field
Bonus: Experience working in a data science / machine learning environment
Bonus: Experience working with AWS Services (Redshift, Kinesis, Glue, etc.)
Why We (and You'll) Love It Here
We are a technology and data-driven business
We embrace analytical thinking, kind, and results driven people
We have a plethora of challenging and interesting problems to solve
We help and support each other in creating a productive work/life balance
Perks and Benefits:

At Telaria we place an emphasis and importance on ensuring our total rewards are competitive, aligned with industry and to help you create a productive work/life balance. Benefits are highly subsidized and include medical, company paid dental, vision, employer contributed Health Savings Account, 401k matching, corporate gym discounts, pre-tax health and commuter savings, life insurance, 5 and 10-year Sabbatical programs, Discretionary Time Off (a.k.a. open vacation policy!), Paid Parental Leave, an Employee Referral Program, Employee Stock Purchase Plan (ESPP), and much more! All this is within a collaborative work environment you can personalize and topped with engaging programs like Micro-Mentorship, and Team Sports.

Telaria values diversity and is proud to be an equal opportunity employer. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
Data Engineer,"8+Years of overall IT Experience
Can program using Spark with Python and Scala
Understands Hive and Hadoop configuration
Understands different file formats in the Hadoop environment and how to organize data for query performance.
Supporting number of tools: Data Lake sync, SMFDB population, Metadata sync
Tuning of the cluster for performance optimization with Spark and Presto.
Understand the different formats parquet, Avro and snappy"
Data Engineer,"As a senior data engineer at Moveworks, you will be in one of the most visible cross-functional roles at Moveworks. As Moveworks' data multiplies during a critical time of hypergrowth, your goal is to design and integrate systems that power processing of unstructured datasets to ultimately fine tune our product and achieve fully autonomous resolution of enterprise IT. You will be responsible to build out the data engineering platform on top of our data infrastructure that lets our internal product and engineering teams as well as customer teams derive value from this data. Given the nature of our data, this is a non-trivial but highly valuable endeavor.

You are not only a strong software engineer, but you bring a passion for data and analytics to the table, and a track record of robust design experience. You thrive working in cross-functional teams between product, engineering, ML, and analytics, and you are looking to make substantial impact in the next stage of your career.

What will you do?
Work with various internal teams and customers to gather requirements and deliver data solutions to address those requirements
Model data and metadata to support analytics and reporting for different use cases
Design and implement a data platform to process large, complex data sets
Implement best practices around data integrity, validation, and documentation for data processing, reporting, and analysis
Optimize data processing pipelines and storage performance
Build the team of talented data engineers and coach them as the company grows
What do you bring to the table?
You have 4+ years of experience as a data engineer, ideally with a cloud-based SaaS company
Strong coding and design expertise
Familiarity with latest data processing and warehousing technologies is required
Hands on experience working with different teams for their data requirements
Experience with some data stores, such as Postgres, MySQL, HBase, etc.
BS or higher in Computer Science or a related field
Nice-to-haves:
Experience with large-scale machine learning pipelines is a plus
Who we are:
Moveworks is an AI-first company with a singular focus: building a platform with advanced NLU to fully automate enterprise IT resolution. Founded by serial entrepreneurs and raised over $105M funding from top VCs, we're well poised for hyper-growth, having tripled the revenue and doubled the team last year. With a stellar team of leading experts in AI/NLU, Big Data, enterprise SaaS, we've built a platform that currently autonomously resolves 30-40% of all IT issues at many Fortune 500 companies and are on a mission to revolutionize IT for hundreds of millions of knowledge workers.

Our engineering team consists of world-class talent who have built out question answering systems at Google, led ML/data infrastructure teams at Airbnb/AWS/LinkedIn/Facebook/etc., scaled many successful enterprise startups from the ground up, and above all, are motivated, result-driven, 10x engineers."
Data Engineer,"Hi, I tried reaching you ndash Howrsquos everything? My name is Payal Dey and I am actively looking for Data Engineer at Menlo Park, CA Location. If you are interested in the opportunity listed below, please share your updated resume along with contact information to payal.deyvlinkinfo.com mailtopayal.deyvlinkinfo.com or you can reach me at 860.247.1400 x 160 Role Data Engineer Location Menlo Park, CA Job duration Long term MOI WebExskype Job Description - NOTE Source only experts from TOP CompaniesCustomer (Client is looking for only Uber, Lyft, Airbnb, Google, Facebook, Amazon, LinkedIn, Walmart, Adobe, Microsoft, Yahoo, Gap, Macys, Kohlrsquos, Apple as recent or ex customer. Kindly look for same only. 10+ years of experience Experience in Advance SQL, Python, Tableau or any BI tool Should be well versed in creating data pipelines using Python. Should be very strong in writing advance SQL queries. Regards, Payal Dey Technical Recruiter VLink Inc. Phone 860.247.1400 x 160 Fax 860 256 8484 payal.deyvlinkinfo.com mailtopayal.deyvlinkinfo.com www.vlinkinfo.com httpwww.vlinkinfo.com"
Data Engineer,"Job Description
Role: Data Engineer

Location: Menlo Park, CA

Duration: 12 to 18 Months

6 Positions
10+ years of experience
Experience in Advance SQL, Python, Tableau or any BI tool
Should be well versed in creating data pipelines using Python.
Should be very strong in writing advance SQL queries.
As hacker rank test is must"
Data Engineer,"Job Description
8+Years of overall IT Experience
Can program using Spark with Python and Scala
Understands Hive and Hadoop configuration
Understands different file formats in the Hadoop environment and how to organize data for query performance.
Supporting number of tools: Data Lake sync, SMFDB population, Metadata sync
Tuning of the cluster for performance optimization with Spark and Presto.
Understand the different formats parquet, Avro and snappy"
Data Engineer,"10+ years of experience
Experience in Advance SQL, Python, ETL, Data Modelling, Tableau or any BI tool
Should be well versed in creating data pipelines using Python. Should be very strong in writing advance SQL queries. Should be in BI Engineer
Python (Nice to have)"
Data Engineer,"**Please Read**

Local candidates only. This opportunity does not provide Visa sponsorship. No corp to corp applicants please. Candidate must be available to work on our W2.

Data Engineer

Data applications are critical to our success, powering many aspects of our marketplace and supporting products. We are looking for data engineers who will build, migrate and maintain data pipelines. In this role, you’ll expand and refactor the data sets that generate and transform data into applications, insights, and experiences for our users.

The work includes:
? Refactoring existing and build new data pipelines
? Migrating existing data sets into next-gen reporting frameworks and tools
? Using existing data tools and frameworks to configure reports and metrics
? Developing and automating large scale, high-performance data processing systems to drive our business growth and improve the product experience
? Building and refactoring scalable data pipelines on top of Hive and Spark leveraging Airflow scheduler/executor framework

We are looking for engineers with:
? Demonstrated ability to analyze large data sets to identify gaps and inconsistencies, provide data insights, and drive effective product solutions
? Experience designing and deploying high performance systems with robust monitoring and logging practices
? Experience building high performance data pipelines
? Nice to have: proven ability to think critically about team direction and use analysis to inform that
? Experience using machine learning is a plus, but not required.
? Excellent communication skills, both written and verbal"
Data Engineer,"Hi,

Â

This is Bharat from Yochana IT Solutions Inc.

Â

We are Yochana is an Award-winning staffing and recruiting agency that provides the top qualified candidates to our clients.Â

Â

Yochana is headquartered in Farmington Hills, Michigan, We are approachable perfectionists who believe in the power of people. We believe strongly in our Commitment to Quality, Competitive Rates, Professionalism, a Rapid Response time, and above all Integrity.

Â

We are looking for a Sr. Data Engineer for our client at Sunnyvale, CA. Please find the below Job Description and let me know your interest in the position.

Â

Please share the resumes to bharat@yochana.com

Â

For more details, please reach me on 248-599-1102.

Â

Position Details

Job Title Sr. Data Engineer (Data Analyst, SQL)

Job Location Sunnyvale CA

Project Duration 6 months+

No of openings 1

Client Interview Needed for Selection (yes / No) Yes

Overall Responsibilities
Understand end-to-end Legal entity process and drive opportunities to increase efficiencies, scale and transparency
Assess current manual processes and systems/tools used to track and report Legal entity data/approvals; develop & recommend frameworks and solutions
Drive improvements in data integrity by creating and maintaining standardized documentation and processes to store Legal entity data
Own detailed day-to-day tasks around end to end LE tracking & data management while working as part of a projects with multiple moving, related parts
Work collaboratively cross-functionally with other teams across Product Areas, Tax, Treasury, Legal, business intelligence, product operations, vendor management, and finance
Find insights from our data and use them to create data visualization elements; use this information to tell effective stories in business presentations to internal stakeholders
Â

Minimum Qualifications:
BA/BS degree or equivalent practical experience.
5+ years of financial or consulting experience; 3-4 years operational experience preferred.
Excellent communication skills, both written and in person, with a strong attention to detail
Excellent spreadsheet and data management skills; working knowledge of SQL and relational databases
Â

Preferred skills:
Comfort working in a geographically distributed, cross-functional environment.
Proven self-starter who sets priorities, understands the broader business context of the work, and works efficiently in a high-paced environment
Ability to plan, execute, and deliver on projects in a timely manner and the ability to multitask on varying projects and initiatives, all while dealing with ambiguity in an unstructured, ever-changing environment.
Experience with (i) Google PLX dashboard suite (ii) DataStudio or other database visualization tools (iii) building and maintaining compelling websites and other stakeholder engagement outreach outlets
Â

Â

Regards,

23000 Commerce Dr, Farmington hills, MI-48335

bharat@yochana.com|| www.yochana.com

Note: This is not an unsolicited mail. If you are not interested in receiving our e-mails then please reply with subject line Remove

Â"
Data Engineer,"RESPONSIBILITIES:

Kforce has a client in search of a Data Engineer in Mountain View, CA.

Duties:
Interacting with business users and analysts to understand business processes and creating analytic requirements and technical specs
Designing data mart dimensions and facts to satisfy the business needs
Implementing transformation logic to populate dimensions and facts
Designing/developing ETL jobs across multiple platforms and tools including S3, Hadoop, Vertica in order to pull in source data needed for the data marts
Roughly 60-70% hands-on coding
Act in a technical leadership capacity: Mentoring junior engineers, new team members, and applying technical expertise to challenging data and design problems
Resolve defects/bugs during QA testing, pre-production, production, and post-release patches
Work cross-functionally with various teams: Product Management, Project Management, Data Architects, Data Scientists, Data Analysts, Software Engineers, and other Data Engineers
Contribute to the design and architecture of project across the data landscape
REQUIREMENTS:
BS/MS in Computer Science or equivalent work experience
4-6 years of experience designing and implementing data marts using star schema design
Strong expertise in data warehouse/data mart architecture
Advanced experience in writing complex SQL, SQL tuning is a must have
Strong experience in one of the Columnar and MPP database (Vertica, Teradata, Netezza, etc.) is a must have
Advanced experience with scripting language
Python or Shell is a must have
Excellent communication skills, both spoken and written; Ability to communicate effectively with engineers, analysts, business users and adjusting as needed
Experience with Agile Development, SCRUM, or Extreme Programming methodologies
Experience with ETL batch and streaming processes
Experience with Amazon analytics services including EMR & Redshift; Experience working with large data volumes; The ability to get code into production
SQL star schema design required; Not looking for someone who know transactional systems; SQL tuning
Python or Shell a must
MPP - Vertica experience
AWS is a nice to have
Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status."
Data Engineer,"Menlo Park, CA Data Engineer 6 Positon 10+ years of experience Experience in Advance SQL, Python, Tableau or any BI tool Should be well versed in creating data pipelines using Python. Should be very strong in writing advance SQL queries. Skill Matrix Sl.NO Skill Years of Exp 1 SQL Must Have 2 Python Must Have 3 Tableau Must Have"
Data Engineer,"Hippo is a hyper-growth Unicorn startup that is taking the market by storm. Our technology-first approach is modernizing the homeownership and insurance industry, from IoT monitoring devices to proactive home protection and our industry-leading cloud software. Led by a strong team of passionate business and technology veterans, we're rapidly expanding by inviting great engineers to join our talented, close knit team here in Palo Alto.We're looking for a talented Senior Data Engineer to help expand our data warehouse and supercharge our data analysis capabilities. An ideal candidate brings curiosity, a passion for data, and a deep understanding of the technologies behind data pipelines, warehousing, big data and analytics. Prior startup experience and ability to thrive in a fast-paced environment is a big plus.Responsibilities:* Integrate new data sources into our data warehouse (RDS and BigQuery)* Maintain and expand the data warehouse offerings using a combination of SQL, code, and scheduled jobs* Connect our data with data analysis tools like Tableau, Looker, Data Studio, and others* Work with business partners in finance and insurance underwriting to build executive dashboards that help monitor business health* Proactively identify places where we can improve our data pipelines* Plan and develop machine learning models that directly affect our business and softwareRequired Qualifications:* 3+ years of work experience experience with advanced knowledge of SQL databases or cloud SQL platforms* Bachelor's degree in Computer Science or related field* Fluent coding in Python or JavaScript to analyze, format, and move data around* Experience with a dashboard/BI tools like Tableau or Looker* Experience with configuring and using at least one big data platform (e.g. Google BigQuery, Amazon RedShift or Athena) for data analysis* Understanding of using data to reconcile financial models* Collaborative, empathetic, cares about the team and the development and enablement of othersPreferred Qualifications:* Prior startup experience* Experience with running cloud-scheduled tasks using a platform like Amazon Lambda, Kubernetes (cron)* Experience with cloud storage like Google Cloud Storage or Amazon s3. Understanding of FTP and file transfer* Fluency with command line tools and/or Linux shell scripting* Understanding of data security and user data privacy* Developing and training machine learning models - a big plusCompensation & Perks:* Great equity package, competitive salaries, unlimited PTO policy* Generous medical, dental, vision, 401K, FSA account, commuter benefits, parking, and more* Fully stocked kitchen & AMAZINGLY GOOD in-house BBQ on Fridays* Work and play with super smart, friendly people in our high energy Palo Alto office!Hippo is an equal opportunity employer, and we are committed to building a team culture that celebrates diversity and inclusion.Hippo's applicants are considered solely based on their qualifications, without regard to an applicant's disability or need for accommodation. Any Hippo applicant who requires reasonable accommodations during the application process should contact the Hippo's People Team to make the need for an accommodation known"
Data Engineer,"Role : Data Engineer
Location : Sunnyvale, CA
Duration : Long Term Contract
Job Description
6+ of Experience
1. Hadoop, Teradata, Google Cloud platform, Programming(Java/Python), SQL Skills, Kafka, API Development"
Data Engineer,"NG - 017
Big Data Engineer

San Jose, California, USA

JOB TITLE


Big Data Engineer

Job Duties


• Complete spark and hive based ETL framework with Python/Java/Scala along with AWS services and Cloud.

• Write the Python Pyspark code for data analysis and data management and spark development for reports.

• Design AWS migration Python framework application which helps the Data engineering team data pipelines migration from On-prem to AWS cloud.

• Translate business requirements into flow charts and user stories as a baseline for development for Data pipelines with automation tools to eliminate human error and speed up production processes.

• Write SQL and NoSQL queries for different data requirements.

• Participate in designing and develop pipelines from different type resources.

• Deploy AWS EMR cluster and create data for reports using Spark and Hive.

• Write CloudFormation scripts to provision different AWS resources.

• Manage the AWS EMR clusters to complete data generation.

• Analyze data and create data pipeline for data modeling and data quality.

• Contribute and implement the continuous integration and continuous delivery pipeline

• Ensure data availability, data quality and data modeling smooth process.

• Act to find out the root cause of any issues, while generating the data using spark and Hadoop eco system components with AWS services and cloud services.

• Implement data management using Redshift, HBase, Kafka, spark, hive and sqoop.

• Create reports using AWS Glue, AWS Athena, Hadoop, Spark, AWS EMR, Hive and Databricks Platform.

• Ability to communicate with data analysts, Data scientists and Clients requirements for Data, to deliver the data from data pipelines. Taking proactive identification & resolve the incidents in data delivery process.

• Review Design, code changes, test scenarios and test results to onboard the migration changes.

Job Requirements


Required Bachelors or foreign equivalent in CS, CA, CIS, IT, MIS, Engineering (Any), or any related field. Must be able to travel/relocate to various client sites throughout the U.S.

Location of Work


San Jose, CA.

To apply please send resumes to NextGen Technologies Inc., 1735 N 1st ST. Ste#308, San Jose, CA 95112 or email resumes to kushal@nextgentechnologiesinc.com."
Data Engineer,"Job Description:
We are looking for a Senior Data Engineer with advanced knowledge of SQL and intermediate knowledge of Python.
Nice to have (but not required) beginner or intermediate level java experience.

Your primary focus will be the writing complex SQL queries, optimizing them and development of all server-side backend data processing logic, ensuring high performance using Python and SQL.

Responsibilities:

Develop and maintain scalable ETL pipelines, build new pipelines and facilitate API integrations to support new requirements.
Writing complex SQL queries to serve new requirements for ETL, data analysis and debugging.
Writing SQL functions, procedures as required based on the requirements
Finetune or optimize queries to support the increasing volume of data.
Debug Python code, modify and enhance Python ETL applications based on the requirements on Linux environment.
Writing reusable and efficient code in Python and SQL.
Write unit, functional, regression tests for enhanced feature, maintain engineering documentation.
Communicate closely with all product owners, Business and engineering teams to develop approaches for data platform architecture.
Skills:
Basics of Computer Science - OOPS, Data Structures and Algorithms.
Basic understand of regular Linux commands and usage.
5+ years of experience having hands on experience in writing, debugging and optimizing SQL queries, function and stored procedures.
3+ years of experience with hands on experience in writing, debugging Python code on Linux.
Experience writing python applications that interact with ORM (Object Relational Mapper) libraries.
Knowledge of XML and JSON parsing with unit test and debugging skills.
Willingness and ability to learn new tools/languages as needed.
Process oriented with excellent oral and written communication skill with a desire for customer service.
An excellent team player and communicator who can work effectively with cross functional teams and ability to navigate ambiguity."
Data Engineer,"THE COMPANYB-Stock is the world's largest online marketplace for returned, excess, and other liquidation merchandise. Our customers range from SMB to the world's largest brands and retailers (including nine of the top 10 U.S. retailers). Led by eBay veterans, B-Stock completes over 175,000 transactions per year, selling 90 million items annually, making us a clear leader in the space.The amount of inventory that is returned or unsold each year is growing very rapidly; in 2018, the value of this merchandise was estimated at $500 billion. Much of it ends up being liquidated for pennies on the dollar; some of it is even destroyed or landfilled. We believe there is tremendous value in and demand for this inventory - no matter the category, condition, or location. The B-Stock platform gives buyers a simple and direct way to buy valuable products, and offers sellers a trusted replacement for traditional liquidation and a critical boost in operational efficiency.Backed by top investors including Spectrum Equity, True Ventures, and Susquehanna Growth Equity, B-Stock runs lean, fast, and shows no signs of slowing down. Our core values (teamwork, honesty, humor, and the passion to build something great) have shaped the company we are today and will certainly drive our success for many years to come.For more information, visit www.bstock.com/careers/JOB SUMMARYB-Stock is looking for a Data Engineer to help design, build, scale and maintain the next generation of the company's SaaS infrastructure. You will partner closely with cross-functional teams, including Data Science, Engineering, and Product / Business Technology, to build data infrastructure, processes, and tooling.ESSENTIAL JOB DUTIES AND RESPONSIBILITIES* Manage and optimize core data infrastructure* Build monitoring infrastructure to give visibility into the pipeline's status* Monitor all jobs for impact on cluster performance* Run maintenance routines regularly* Tune table schemas (i.e. partitions, compression, distribution) to minimize costs and maximize performance* Develop custom data infrastructure not available off-the-shelf* Build and maintain custom ingestion pipelines* Support data team resources with design and performance optimization* Build non-SQL transformation pipelinesMINIMUM QUALIFICATIONS, JOB SKILLS, AND ABILITIESEDUCATION:* Bachelor's degree in a technical and/or quantitative field of study-e.g., computer science, math, physics or statistics, or equivalent and/or appropriate experienceEXPERIENCE:* 3+ years of experience working with distributed data technologies* Experience working with server-side concepts such as containers, micro-services, caching, performance monitoring, and API design* Experience with cloud technologies such as AWS, Azure, and Google Cloud* Experience using Python, preferred* Experience with databases such as MySQL, and PostgreSQL, preferred* Experience with highly scalable ETL/ELT/Data Lake technologies, nice to haveOUR VALUESBe honest. We do the right thing because it's right.Have passion for building something great. We empower employees to ""think like an owner"" so we dare to try. Let's find new ways to grow B-Stock together.Humor. Take whatever you are doing very seriously but do not take yourself too seriously.Teamwork. Our successes are achieved because we work in stride, leveraging each other's strengths as a unified effort.Respect. We show consideration for each other and recognize the power in our diversity.EMPLOYEE BENEFITS* Competitive compensation packages, including bonus and options* Medical, dental, and vision benefits* Paid Time Off & matching 401(k)* Support for continuing education* Team off-sites, social events and extracurricular activities are a staple* Snacks, drinks, and the occasional box of donutsNo applicant will face discrimination/harassment based on: race, color, ancestry, national origin, religion, age, gender, marital domestic partner status, sexual orientation, gender identity, disability status, or veteran status. Above and beyond discrimination/harassment based on ""protected categories,"" B-Stock also strives to prevent other, subtler forms of inappropriate behavior (e.g., stereotyping) from ever gaining a foothold in our office. Whether blatant or hidden, barriers to success have no place at B-Stock.US Work Authorization required."
Data Engineer,"Strategy and Analytics- Data Engineer with Map Reduce- Project Delivery Specialist

Are you an experienced, passionate pioneer in technology – a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with Project Delivery Practice.

Work you’ll do

• Support the implementation of data integration requirements and develop the pipeline of data from raw to curation layers including the cleansing, transformation, derivation and aggregation of data.
• Communicate effectively (written and spoken) and work with the multi-location development teams and self-manage own work
• Support in the development of technical solutions to business problems

The team

Analytics & Cognitive

In this age of disruption, organizations need to navigate the future with confidence, embracing decision making with clear, data-driven choices that deliver enterprise value in a dynamic business environment.

The Analytics & Cognitive team leverages the power of data, analytics, robotics, science and cognitive technologies to uncover hidden relationships from vast troves of data, generate insights, and inform decision-making. Together with the Strategy practice, our Strategy & Analytics portfolio helps clients transform their business by architecting organizational intelligence programs and differentiated strategies to win in their chosen markets.

Analytics & Cognitive will work with our clients to:

• Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms

• Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions

• Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements

Qualifications

Required
• Bachelor-level degree in engineering, information technology, data communications, telecommunications, computer science, or equivalent professional experience and/or qualifications
• 5+ years of hands-on experience as a Data Engineer or Big Data developer role
• 3+ years of experience in building scalable and high-performance data pipelines using Apache Hadoop, Map Reduce, Pig & Hive
• 5+ years of experience in Core JAVA and SQL
• 3+ years or experience in Python & Unix Shell Scripting
• Experience with bigdata cross platform compatible file formats like Apache Avro & Apache Parquet
• Hands on big data performance tuning and optimization experience in Map Reduce and Pig
• Strong SQL knowledge with ability to work with the latest database technologies.
• Strong data & logical analysis skills
• Limited immigration sponsorship may be available

Preferred
• Experience in Apache Spark & Scala is a plus

Additional Requirements
• Must be willing to live and work in the Greater San Jose, CA area. Relocation assistance provided to qualifying candidates

How you’ll grow

At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there’s always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs at Deloitte University, our professionals have a variety of opportunities to continue to grow throughout their career.

Benefits

At Deloitte, we know that great people make a great organization. We value our people and offer employees a broad range of benefits.

Deloitte’s culture

Our positive and supportive culture encourages our people to do their best work every day. We celebrate individuals by recognizing their uniqueness and offering them the flexibility to make daily choices that can help them to be healthy, centered, confident, and aware. We offer well-being programs and are continuously looking for new ways to maintain a culture where our people excel and lead healthy, happy lives. Learn more about Life at Deloitte.

Corporate citizenship

Deloitte is led by a purpose: to make an impact that matters. This purpose defines who we are and extends to relationships with our clients, our people and our communities. We believe that business has the power to inspire and transform. We focus on education, giving, skill-based volunteerism, and leadership to help drive positive social impact in our communities. Learn more about Deloitte’s impact on the world.

Recruiter tips

We want job seekers exploring opportunities at Deloitte to feel prepared and confident. To help you with your interview, we suggest that you do your research: know some background about the organization and the business area you’re applying to. Check out recruiting tips from Deloitte professionals.
As used in this posting, “Deloitte” means Deloitte Consulting LLP, a subsidiary of Deloitte LLP. Please see www.deloitte.com/us/about for a detailed description of the legal structure of Deloitte LLP and its subsidiaries. Certain services may not be available to attest clients under the rules and regulations of public accounting.

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability or protected veteran status, or any other legally protected basis, in accordance with applicable law.

Requisition code: E21SJOCSRCKR300-PDM"
Data Engineer,"Qualifications Skills and Experience At least 5+ years of experience as a Data Engineer in analyzing product, customer, marketing related data and building data pipelines. Strong Communication skills Strong background in quantitative data analysis as well as prediction and regression techniques. Experience performing AB testing on very large, multi-dimension datasets Proficient in SQL and Visualization (TablueauQlikview) Strong Proficiency in Microsoft Excel and standard analytic programs Strong business acumen and ability to manage conversations at multiple levels of the organization Ability to present data insights concisely to various stakeholders, especially business teams Entrepreneurial spirit and a passion for data, customer focused mindset, team player Responsibilities Demonstrate up-to-date expertise in handling data extractions from multiple data sources and capable of analyzing large volumes of data to address key business questions to enable growth Manage stakeholder relationships and expectations by developing a communication process to keep others up-to-date on project results and timeline expectations Lead or participate in multiple projects by completing and updating project documentation managing project scope adjusting schedules when necessary determining daily priorities ensuring efficient and on-time delivery of project tasks and milestones following proper escalation paths and managing customer and supplier relationships Provide decision makers and influencers with logical data-driven insights translating into hypotheses for lean testing, rapid prototyping and effective implementation Construction of critical business application tools to be used by managers and consultants to be tuned to the pulse of the business Identify new technical requirements, and testtroubleshoot data capture to ensure reporting and analytics systems conform to data quality standards"
Data Engineer,"Job Description
--------------
We

are looking for a data engineer responsible for the development
and maintenance

of critical data processing and classification pipelines. Your
primary focus

will be developing new systems and services while leveraging
cutting edge data

science and machine learning technologies. You will be working
alongside other

engineers and developers working on different layers of the
infrastructure.

Therefore, a commitment to collaborative problem solving,
sophisticated design,

and the creation of quality products is essential.

Responsibilities
---------------
Design and build data processing systems and APIs
Ensure the performance, quality, and responsiveness of
applications
Collaborate with a team to define, design, and ship new
features
Identify and correct bottlenecks and fix bugs
Help maintain code quality, organization, and
automation

Skills
-----
Works with Python on a daily basis
Experience working with distributed systems
Working knowledge of database technologies like SQL or
MongoDB
Experience developing and scaling RESTful APIs
Familiarity with best practices for functional and unit
testing
Knack for benchmarking and optimization
Experience with source control using Git
Ability to review and identify issues in other developers
code
Familiarity with continuous integration
Knowledge of queueing and messaging systems such as Celery or
Kafka
Experience building web applications with Flask, Django, or
other

Python web server frameworks"
Data Engineer,"Skills Required Experience range 8-14 years Java Development Experience Data Engineer. Python Hadoop Stack Data Pipeline Using ETL or other tool spark - spark streaming, RDD SQL, noSQL, Cassandra Handling High Volume Data AWS Expeience Experience On Big Data Migration Experience is PLUS"
Data Engineer,"Big Data Engineer SAN JOSE, CA Skills Google Big Query JIRA, Jenkins, Power BI, Tableau Six Sigma Green Belt TSQL, My SQL, Python Extract, Transformation, Load (ETL) Dash boarding Visualization Data Modelling Data Mining Data Analysis Supply Chain Management Business Process Automation Business Analysis Project Management Business Intelligence Analysis Full Legal Name Email Address Skype ID Contact Number Address-Current Location Total Experience Relevant Experience Work Authorization Passport Num LinkedIn Profile Link Interview Availability 4 slots Expected SalaryRate Notice Period Current Organization Highest Qualification Willing to relocate References 1) Full Name RoleDesignation Client Name Contact NoEmail ID 2) Full Name RoleDesignation Client Name Contact NoEmail ID"
Data Engineer,"Job Description


Job #: 1075497

Apex Systems is searching for an experienced Data Engineer to fill a contract role with our Fortune 500 client in Menlo Park, CA!

Contract Length: 5 months

Location: Menlo Park, CA

Pay Rate: Competitive

Duties:

Partner with internal stakeholders to understand business requirements, work with cross-functional data and products teams and build efficient and scalable data solutions.

Build data expertise and own data quality for allocated areas of ownership. Design, build, optimize, launch and support new and existing data models and ETL processes in production.

Monitor and manage the SLA for all data sets and systems in allocated areas of ownership. Work with data infrastructure and data engineering teams to triage infra issues and drive to resolution.

Skills:

5+ years hands-on experience with Linux and shell scripting

2+ years hands-on experience in MySQL database administration, implementation and maintenance

5+ years hands-on experience in SQL or similar languages and development experience in at least one scripting language (Python preferred)

Data architecture, data modeling and schema design skills

Excellent communication skills and proven experience in leading data driven projects from definition through interpretation and execution

Ability to initiate and drive projects with all stakeholders Proactive and preventative solutions mindset a large plus

Need to be able to travel to China periodically Mandarin language a plus

Education:

BE/BTECH/MCA degree (preferred) with a strong academic record
Partner with internal stakeholders to understand business requirements, work with cross-functional data and products teams and build efficient and scalable data solutions. Build data expertise and own data quality for allocated areas of ownership. Design, build, optimize, launch and support new and existing data models and ETL processes in production. Monitor and manage the SLA for all data sets and systems in allocated areas of ownership. Work with data infrastructure and data engineering teams to triage infra issues and drive to resolution.
Skills:

5+ years hands-on experience with Linux and shell scripting 2+ years hands-on experience in MySQL database administration, implementation and maintenance 5+ years hands-on experience in SQL or similar languages and development experience in at least one scripting language (Python preferred) Data architecture, data modeling and schema design skills Excellent communication skills and proven experience in leading data driven projects from definition through interpretation and execution Ability to initiate and drive projects with all stakeholders Proactive and preventative solutions mindset a large plus Need to be able to travel to China periodically Mandarin language a plus
Education:

BE/BTECH/MCA degree (preferred) with a strong academic record

EEO Employer

Apex Systems is an equal opportunity employer. We do not discriminate or allow discrimination on the basis of race, color, religion, creed, sex (including pregnancy, childbirth, breastfeeding, or related medical conditions), age, sexual orientation, gender identity, national origin, ancestry, citizenship, genetic information, registered domestic partner status, marital status, disability, status as a crime victim, protected veteran status, political affiliation, union membership, or any other characteristic protected by law. Apex will consider qualified applicants with criminal histories in a manner consistent with the requirements of applicable law. If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation in using our website for a search or application, please contact our Employee Services Department at employeeservices@apexsystemsinc.com or 844-463-6178."
Data Engineer,"The Job Details are as follows:

OVERVIEW

We are looking for creative and enthusiastic Data Engineers to join our team in building the best Data Platform on the street and enabling our investment teams to monetize data assets. We treat our data systems as software systems and engineer them accordingly. In your role, you will:
Develop cloud-first data ingestion processes using Python, SQL, and Spark
Engineer data models and infrastructure for a wide variety of market and alternative datasets
Design and build services and plugins to enhance our Data Acquisition Platform
Maintain alerting systems to ensure smooth day-to-day operations for hundreds of datasets
Author tests to validate data quality and the stability of the platform
Investigate and defuse time-sensitive data incidents
Communicate with data providers to onboard new datasets and troubleshoot technical issues
Evangelize best practices to our partners throughout the firm
Work directly with Analysts, Quants, and Portfolio Managers to understand requirements and provide end-to-end data solutions
WHAT YOU’LL BRING
Bachelors/Masters degree in Computer Science or a related field
3+ years experience with at least one of Spark, Airflow, data warehousing
Strong analytical, data and programming skills (Python/SQL/NoSQL/Spark)
Ability to understand and contribute to our existing data system software
Experience containerizing workloads with Docker (Kubernetes a plus)
Aptitude for designing infrastructure, data products, and tools for Data Scientists a plus
Strong oral and written communication skills, most importantly, must be a team player"
Data Engineer,"At SpringML, we are all about empowering the doers in companies to make smarter decisions with their data. Our predictive analytics products and solutions apply machine learning to todays most pressing business problems so customers get insights they can trust to drive business growth. We are a tight knit, friendly team of passionate and driven people who are dedicated to learning, get excited to solve tough problems and like seeing results, fast.

Your primary role will be to design and build data pipelines. You will be focused on designing and implementing solutions on Hadoop, Spark, Pig, Hive. In this role you will be exposed to Google Cloud Platform including Dataflow, BigQuery and Kubernetes so the ideal candidate will have a strong big data technology foundation and bring a passion to learn new technologies. If you believe you have these skills please email your resume to info@springml.com.

Required Skills:
4-7 years Python and Java programming
3-5 years knowledge of Java/J2EE
3-5 years Hadoop, Big Data ecosystem experience
3-5 years of Unix experience
Bachelors in Computer Science (or equivalent)
Duties and Responsibilities:
Design and develop applications utilizing the Spark and Hadoop Frameworks or GCP components.
Read, extract, transform, stage and load data to multiple targets, including Hadoop, Hive, BigQuery.
Migrate existing data processing from standalone or legacy technology scripts to Hadoop framework processing.
Should have experience working with gigabytes/terabytes of data and must understand the challenges of transforming and enriching such large datasets.
Additional Skills that are a plus:
C, Perl, Javascript or other programming skills and experience a plus
Production support/troubleshooting experience
Data cleaning/wrangling
Data visualization and reporting
Devops, Kubernetes, Docker containers
Powered by JazzHR"
Data Engineer,"Join SADA as a Data Engineer!

Your Mission

As a Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data issues facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring a combination of batch or streaming data pipelines, data lakes and data warehouses.

You will be recognized as an established contributor by your team. You will contribute design and implementation components for multiple projects. You will work mostly independently with limited oversight. You will also participate in client-facing discussions in areas of expertise.
Pathway to Success

#BeOneStepAhead: At SADA we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage adaptability. This means that not only do our engineers understand that change is inevitable, but they embrace this change to continuously broaden their skills, preparing for future customer needs.

Your success comes from your enthusiasm, insight, and positive impact. You will be given direct feedback quarterly with respect to the scope and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, your collaboration with your peers, and the consultative skill you demonstrate in customer interactions.

As you continue to execute successfully, we will build a personalized development plan together that leads you through the engineering or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with a first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:
Google Professional Data Engineer Certified or able to complete within the first 45 days of employment

Required Qualifications:
Expertise in at least one of the following domain areas:
Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or other customer-facing role

Useful Qualifications:
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Applied experience operationalizing machine learning models on large datasets
Knowledge and understanding of industry trends and new technologies and ability to apply trends to architectural needs
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem

About SADA

Values: We built our core values on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing

Work with the best: SADA has been the largest partner in North America for GCP since 2016 and has been named the 2019 and 2018 Google Cloud Global Partner of the Year. SADA has also been awarded Best Place to Work by Inc. as well as LA Business Journal!

Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, RRSP, professional development reimbursement program as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."
Data Engineer,"Be a part of what sets Amazon apart from everyone else! Join the power behind Prime.


Supply Chain Optimization Technologies (SCOT) creates the science and technology to drive Amazon's supply chain. SCOT builds software systems to make the most products available to the most people for delivery as quickly as possible. The SCOT Austin teams focus on improving the promises we make to Amazon customers, defining what is available with 2-day Prime shipping, optimizing fulfillment costs, consolidating multiple orders into a single shipment, predicting future supply and demand, and creating the execution plan for our global fulfillment network. To accomplish these goals we build simulation and experimentation systems at scale and leverage cutting-edge technologies across Operations Research, Software Engineering, Machine Learning, Forecasting, and Linear Programming.
Watch this short video for more on SCOT: http://bit.ly/amazon-scot

Amazon is seeking a truly innovative Data Engineer to join the FastTrack Data Engineering Team.

As an Amazon Data Engineer you will be working in one of the world's largest and most complex data warehouse environments. We maintain one of the largest data marts in Amazon as well as work on Business Intelligence reporting and dashboarding solutions that are used by thousands of users world-wide.


Our team is responsible for mission critical analytic reports and metrics that are viewed at the highest levels in the organization. We are also working on newer tools that help users discover data using visualization and Big Data technologies. You should have deep expertise in the design, creation, management, and business use of significantly large datasets. You should have excellent business and communication skills to be able to work with business owners to develop and define key business questions, and to build data sets that answer those questions. You should be expert at designing, implementing, and operating stable, scalable, low cost solutions to flow data from production systems into the data warehouse and into end-user facing applications. You should be able to work with business customers in a fast paced environment understanding the business requirements and implementing reporting solutions. Above all you should be passionate about working with huge data sets and someone who loves to bring datasets together to answer business questions and drive change.
This opportunity is perfect for highly motivated and talented data engineers who want to apply and grow their technical depth and breadth while defining and driving key aspects of the customer experience on Amazon.com.

Amazon is an Equal Opportunity-Affirmative Action Employer Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation.

Basic Qualifications


· Degree in Computer Science or related field
· 4+ years professional experience in database development, handling large data sets using SQL and databases in a business environment
· Must be proficient with Oracle/SQL Server/ Redshift/Tera data
· Familiar with ETL and DW processes
· Prior experience with Scala, Python or Java
· Strong troubleshooting and problem solving skills



Preferred Qualifications

· Previous experience with Linux
· Experience with multiple database platforms
· Familiar with computer science fundamentals including object-oriented design, data structures, algorithm design, problem solving, and complex analysis

Amazon is an Equal Opportunity-Affirmative Action Employer Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation.

We believe passionately that employing a diverse workforce is central to our success and we make recruiting decisions based on your experience and skills. We welcome applications from all members of society irrespective of age, gender, disability, sexual orientation, race, religion or belief."
Data Engineer,"Posted: Feb 6, 2020
Role Number:
200148400
At Apple, excellent ideas have a way of becoming extraordinary products, services, and customer experiences very quickly. Bring passion and dedication to your job and there's no telling what you could accomplish. Would you like to work in a fast-paced environment where your technical abilities will be challenged on a day-to-day basis? If so, Apple's Global Business Intelligence (GBI) team is seeking a hardworking Data Engineer to build high quality, scalable and resilient distributed systems that power apple's analytics platform and data pipelines.

Apple's Enterprise Data warehouse system cater to a wide variety of real-time, near real-time and batch analytical solutions. These solutions are integral part of business functions like Sales, Operations, Finance, AppleCare, Marketing and Internet services enabling business drivers to make critical decisions. We use a diverse technology stack such as Teradata, HANA, Vertica, Hadoop, Kafka, Spark, and Cassandra and beyond. Designing, Developing and scaling these Big Data technologies are a core part of our daily job. The team member will be able think outside of the box and should have passion for building analytics solutions to enable business in making time sensitive and critical decisions.
Key Qualifications
We would like for you to have In-depth understanding of data structures and algorithms
We are looking for experience in designing and building dimensional data models to improve accessibility, efficiency, and quality of data
Database development experience with Relational or MPP/distributed systems such as Oracle/Teradata/Vertica/Hadoop
We are seeking programming experience in building high quality software in Java, Python or Scala preferred
Experience in designing and developing ETL data pipelines. Should be proficient in writing Advanced SQLs, Expertise in performance tuning of SQLs
You will demonstrate excellent understanding of development processes and agile methodologies
Strong analytical and interpersonal skills
Enthusiastic, highly motivated and ability to learn quick
Experience with or advance courses on data science and machine learning is ideal
Work/project experience with Big Data and advanced programming languages is a plus
Experience developing Big Data/Hadoop applications using java, Spark, Hive, Oozie, Kafka, and Map Reduce is a huge plus
Description
You will build and design data structures on MPP platform like Teradata, Hadoop to provide efficient reporting and analytics capability.

Design and build highly scalable data pipelines using new generation tools and technologies like Spark, Kafka to induct data from various systems.

Translate complex business requirements into scalable technical solutions meeting data warehousing design standards.

Strong understanding of analytics needs and proactive-ness to build generic solutions to improve the efficiency.

Build dashboards using Self-Service tools like Tableau and perform data analysis to support business.

Collaborate with multiple multi-functional teams and work on solutions which has larger impact on Apple business.

We seek a self starter, forward-thinking person with strong leadership capabilities.

Ability to communicate effectively, both written and verbal, with technical and non-technical multi-functional teams.

You will interact with many other group’s internal team to lead and deliver elite products in an exciting rapidly changing environment.
Education & Experience
Bachelors Degree"
Data Engineer,"We are looking for a teammate that is excited to take ownership of our data infrastructure and put us on a path where we are iterating quickly and using data to solve some of the biggest problems facing our industry. You will have a seat at the table in making technology decisions and in helping determine what and how we build things as opposed to just getting handed specifications to implement.

RunTitle is an Austin-based venture-backed software company innovating within the 100 year-old oil & gas title industry. We’re looking for someone who is eager to disrupt an extremely archaic industry, but also enjoys startup perks, happy hours, and afternoon ping pong showdowns. We’re a small but quickly growing organization, so every single person here has a mission critical impact on our business.

Responsibilities:

Take ownership and lead product development for our internal data processing pipeline
Perform analysis and generate models to improve our automated data extraction capabilities
Prototype, test and build models that will be adapted for use in our production data pipeline
Work closely with the engineering team to advise on system architecture and help guide engineering priorities
Build and lead a team of analysts and data scientists
Deepen our culture of data driven decision making

A little more about you:

18+ months of NLP, Data Science, ML professional work
Experience specifying and building clean and functional data pipelines
Comfort manipulating and analyzing complex, unstructured, data from various sources
Ability to communicate complex quantitative analysis and approaches clearly

Nice-to-have experience:

Named Entity Recognition
Dependency Parsing
Semantic Role Labeling
Probabilistic String Matching
Elasticsearch, Apache Spark

Perks:

Small team = opportunity for big impact
Compensation includes equity and competitive salary
Excellent company-sponsored benefits
Stocked kitchen including all of the Topo Chico your heart desires

An Equal Opportunity Employer"
Data Engineer,"Tachyon Technologies is a Digital Transformation consulting firm that partners with businesses to implement customer-focused business transformation. Tachyon Technologies understand what it takes for a consulting partner to be effective and strives to deliver a meaningful solution that exceeds its clients' expectations

Â

Title: Data Engineer/Software Engineer

Location: Austin, TX

Â

Key Qualifications:

Â

Â Strong programming skills in Java, knowledge of Scala is a plus

Â Experience with Big Data applications that use Spark, Hive, Kafka, Hadoop, and Oozie

Â Knowledge of build and test tools such as Maven, Gradle, SBT, and JUnit

Â Good understanding of relational and NoSQL databases

Â Experience writing and optimizing SQL queries

Â Experience in developing ETL data pipelines

Â Strong communication skills

Â Passion for excellence and commitment to continuous learning

Â

Tachyon's full-time employees receive complete and competitive benefits. We offer a collaborative work environment, competitive compensation, generous work/life opportunities, and an outstanding benefits package that includes paid time off plus holidays. In addition, all colleagues are eligible for a number of rewards and recognition programs. Encouraging a healthy work/life balance and providing our colleagues great benefits are just part of what makes Tachyon a great place to work.

A Disclaimer: The above statement is not a complete job description. The Client retains the discretion to add or change the duties of the position at any time"
Data Engineer,"Data Engineer

ETL automation tools such as Informatica, Mulesoft, SSIS, Alooma, or Apache Airflow

Experience with traditional RDBMS solutions such as Microsoft SQL Server and Oracle

Experience with cloud-based platforms and tools

Familiarity with DevOps tools and practices such as Git, Jenkins, JIRA, Azure DevOps

Experience with integrating to both database systems and APIs

Experience with documenting technical requirements, designs and systems

Extensive experience building scalable and resilient data pipelines

Extensive experience writing SQL

Experience with a procedural, functional or object-oriented programming language such as Python, Java, Scala, R

Additionally, candidates should be able to perform at a high level in at least two of the following technology categories:
Big Data tools such as Apache Hadoop, Spark, and Hive including managed solutions such as Databricks, Amazon EMR, Azure HD Insight, and Google Cloud Dataproc
Cloud data storage solutions such as Amazon S3, Azure Blob Storage, or Google Cloud Storage
Data warehousing solutions such as Redshift, BigQuery, and Snowflake
Message broker solutions such as Kafka, Google Pub/Sub, Amazon Kinesis
Stream processing solutions such as Flume, Storm, Spark Streaming
NoSQL Databases such as HBase, Cassandra, Redis
3-5+ years of experience in technology and/or consulting

Bachelor's Degree in CS, MIS, CIS, or a comparable technical degree

US Citizen or GC Holder

Sense Corp powers insight-driven organizations.

We turn data into actionable insights and transform organizations for the digital era.

Our people, culture, and how we engage with our clients are differentiators. Brilliant, Creative, Human, and Fun exemplify who we are. We are regularly recognized as a Best Place to Work by Austin, Houston, Dallas, and St. Louis Business Journals. With operations in Austin, Atlanta, Columbus, Dallas, Houston, San Antonio, and St. Louis we serve mid-market to Fortune 50 companies.

The Sense Corp Compass

We may be the only management consulting firm in the country where being brilliant isn't enough to land you a job. Sense Corp people must be brilliant, creative, human, and fun all at once. In other words, we hire terrific, well-rounded people. It's one reason clients love working with us. And it's why we enjoy working with each other. We may not sound like typical consultants but that's OK. We don't think like them either.

Visit us at www.sensecorp.com."
Data Engineer,"Join SADA as a Sr. Data Engineer!

Your Mission

As a Sr. Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring batch or streaming data pipelines, data lakes and data warehouses.

You will be expected to run point on whole projects, end-to-end, and to mentor less experienced Data Engineers. You will be recognized as an expert within the team and will build a reputation with Google and our customers. You will demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise. You will also participate in early-stage opportunity qualification calls, as well as lead client-facing technical discussions for established projects.
Pathway to Success

#BeOneStepAhead: At SADA we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our engineers know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.

Your success starts by positively impacting the direction of a fast-growing practice with vision and passion. You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions.

As you continue to execute successfully, we will build a customized development plan together that leads you through the engineering or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:
Google Professional Data Engineer Certified or able to complete within the first 45 days of employment

Required Qualifications:
Mastery in at least one of the following domain areas:
Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or customer-facing role

Useful Qualifications:
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Experience operationalizing machine learning models on large datasets
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem

About SADA

Values: We built our core values on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing

Work with the best: SADA has been the largest partner in North America for GCP since 2016 and has been named the 2019 and 2018 Google Cloud Global Partner of the Year. SADA has also been awarded Best Place to Work by Inc. as well as LA Business Journal!

Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match, professional development reimbursement program as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."
Data Engineer,"Job Description
Treehouse Technology Group is changing the way individuals interact with the data they generate on a daily basis. We are looking for hard-working, game-changing people who are able to take big technical ideas and market them to non-technical people. We are looking for big thinkers and doers that constantly push to the next level. Joining Treehouse Technology Group means being part of a team that is redefining data analytics and business intelligence software. Redefining an industry is hard work and it takes focused, dedicated team players. Come get in the game.

As a Data Engineer, you will work with stakeholders and the internal development team to guide technical development of enterprise data solutions. The Data Engineer is a combination of a business and technical customer facing role that will be accountable for the end-to-end customer data architecture, development, deployment and support. The Data Engineer will be responsible for working with stakeholders to identify opportunities to leverage data to drive business value. In addition, you will be responsible for mining and analyzing data to drive efficiency and optimization, develop custom data models and algorithms, and develop processes and tools to monitor production systems and data accuracy.

The ideal candidate will have experience in customer facing and development management roles and have led successful technical and economic value discussions with senior customer executives, driving decisions and implementation.

Requirements:
Austin, TX
Minimum 5 years related experience
AWS and\or Azure experience architecting solutions in Cloud Environments
Agile software development experience
Data background either in Analytics, Warehousing, Data Integration\API Dev, Visualization, etc
Technical Background
Can be one of following or multiple:
Data Engineer
Integration Engineer using SSIS, Talend, Pentaho
Data Warehousing
Data Modeling
Big Data technologies
Map/Reduce, Hadoop, Hive, Spark, Elasticsearch, etc
SQL Server, MySQL, Aurora primary experience (Oracle, Postgres, MongoDB secondary experience)
Data Scientist
Experience creating and using advanced machine learning algorithms and statistics: regression, simulation, scenario analysis, modeling, clustering, decision trees, neural networks, etc.
Knowledge and experience in statistical and data mining techniques: GLM/Regression, Random Forest, Boosting, Trees, text mining, social network analysis, etc.
TensorFlow, Kubernetes
R, SQL, Python – Pandas, Scikit, Numpy
Data Visualization
SQL Server, MySQL, Aurora primary experience (Oracle, Postgres, MongoDB secondary experience)
Map/Reduce, Hadoop, Hive, Spark, etc
InsightOut™ by Treehouse Technology Group is a data visualization platform that turns disparate, raw data into actionable business intelligence. It features a powerful method to integrate internal, proprietary data and third-party data into a common platform that will transform analytics into insights. Our user-friendly and visual-first focus enables all users and abilities to work with and create value from data and enable complex client reporting. Not just for internal analysis anymore, InsightOut™ provides industry leading capabilities to our clients by providing them a platform to share raw data, visualizations and insights discovery with external clients and stakeholders as necessary. Additionally, no other business intelligence tool uses real-time animation to show time-series data trends with the ability to customize that data on the fly.

We offer competitive compensation, company-sponsored premium benefits, full medical premiums covered, optional dental, vacation/holidays, etc. This can be a fully remote or work-from-home position, but those based in Austin have the option to work in an office. Treehouse Technology Group is an equal opportunity employer. We consider all qualified applicants of diverse background and hire without regard to race, color, gender identity, religion, national origin, ancestry, citizenship, physical abilities, age, sexual orientation, veteran status, or any other characteristic protected by law.
Company Description
InsightOutTM by Treehouse Technology Group is a data visualization platform that turns disparate, raw data into actionable business intelligence. It features a powerful method to integrate internal, proprietary data and third-party data into a common platform that will transform analytics into insights. Our user-friendly and visual-first focus enables all users and abilities to work with and create value from data and enable complex client reporting. Not just for internal analysis anymore, InsightOutTM provides industry-leading capabilities to our clients by providing them a platform to share raw data, visualizations and insights discovery with external clients and stakeholders as necessary. Additionally, no other business intelligence tool uses real-time animation to show time-series data trends with the ability to customize that data on the fly.
Accordingly, we perform vendor analysis and technical development based on industry best practices. When an off-the-shelf solution does not meet the needs of our partners, custom application development is necessary to bridge the gap between best-in-breed (specialized) and ERP solutions (general).



TTG prides itself on our ability to solve complex business problems with custom-tailored solutions that support the ever-changing business environment of our partners. We focus on business value first, aligning the strategy and direction of our partners with their specific technical needs, before providing a technology roadmap to implement the mutually established vision. By working closely with our partners and obtaining feedback along the way, we guarantee a positive experience that yields fruitful results."
Data Engineer,"Pilytix brings Explainable Artificial Intelligence (XAI) to sales teams so they can be more effective and close more deals, faster. We use best-in-class tools to quickly deliver data-driven insights to all of our clients. Data Engineers will assist in the development of cutting-edge data pipelines to ingest, transform and archive data for our clients and to support our team of Data Scientists.

Responsibilities:
Author and monitor directed acyclic graphs (DAGs) in Apache Airflow to ingest and transform data
Build and maintain internal Python packages to streamline ingest processes and add connections for new types of data
Manage Kubernetes infrastructure and PostgreSQL databases on Google Cloud Platform
Work collaboratively with the development and data science teams to add new data-driven features to our software-as-a-service product
Participate in Agile / Kanban processes on a daily basis
Comply with change management policies and code reviews to ensure data integrity and system stability
Requirements
BS/MS in a STEM field and 2+ years of industry experience programming and working with data
Exceptional understanding of data architecture and software engineering best practices
2+ years experience with Python (Python 3 preferred)
2+ years experience with SQL (PostgreSQL preferred)
2+ years of experience with Docker
1+ years experience with cloud infrastructure (GCP preferred)
1+ years of server orchestration (Kubernetes preferred)
Experience using Apache Airflow or similar data pipeline systems
Experience using Git or other DVCS
Knowledge of Agile / Kanban processes
Entrepreneurial spirit and highly self-motivated
Job is based in Austin TX, but extraordinarily qualified remote candidates (willing to travel to Austin semi-regularly) may apply.

Benefits
Competitive base salary with ability to earn bonuses
Professional development and entrepreneurial opportunities
Paid time off
401(k)
Medical and dental plans"
Data Engineer,"At SpringML, we are all about empowering the doers in companies to make smarter decisions with their data. Our predictive analytics products and solutions apply machine learning to todays most pressing business problems so customers get insights they can trust to drive business growth. We are a tight knit, friendly team of passionate and driven people who are dedicated to learning, get excited to solve tough problems and like seeing results, fast.

Your primary role will be to design and build data pipelines. You will be focused on designing and implementing solutions on Hadoop, Spark, Pig, Hive. In this role you will be exposed to Google Cloud Platform including Dataflow, BigQuery and Kubernetes so the ideal candidate will have a strong big data technology foundation and bring a passion to learn new technologies. If you believe you have these skills please email your resume to info@springml.com.

Required Skills:
4-7 years Python and Java programming
3-5 years knowledge of Java/J2EE
3-5 years Hadoop, Big Data ecosystem experience
3-5 years of Unix experience
Bachelors in Computer Science (or equivalent)
Duties and Responsibilities:
Design and develop applications utilizing the Spark and Hadoop Frameworks or GCP components.
Read, extract, transform, stage and load data to multiple targets, including Hadoop, Hive, BigQuery.
Migrate existing data processing from standalone or legacy technology scripts to Hadoop framework processing.
Should have experience working with gigabytes/terabytes of data and must understand the challenges of transforming and enriching such large datasets.
Additional Skills that are a plus:
C, Perl, Javascript or other programming skills and experience a plus
Production support/troubleshooting experience
Data cleaning/wrangling
Data visualization and reporting
Devops, Kubernetes, Docker containers
Powered by JazzHR"
Data Engineer,"Since our founding in 2009, weve relentlessly worked toward a vision of a future powered by Phunware. We spend our days obsessing over how best to design, build, launch, promote and support branded apps that engage, compel and delight the worlds most discerning audiences. For over a decade, weve helped Fortune 5000 businesses throughout the mobile app lifecycle with data-backed decisions at every step.

Everything You Need to Succeed on Mobile Transforming Digital Human Experience

Phunware, Inc. (NASDAQ: PHUN), is the pioneer of Multiscreen-as-a-Service (MaaS), an award-winning, fully integrated enterprise cloud platform for mobile that provides companies the products, solutions, data, and services necessary to engage, manage and monetize their mobile application portfolios and audiences globally at scale. Phunwares Software Development Kits (SDKs) include location-based services, mobile engagement, content management, messaging, advertising, loyalty (PhunCoin & Phun) and analytics, as well as a mobile application framework of pre-integrated iOS and Android software modules for building in-house or channel-based mobile application and vertical solutions. Phunware helps the worlds most respected brands create category-defining mobile experiences, with more than one billion active devices touching its platform each month.

If you share our passion for innovative mobile app experiences and dream of a world empowered by seamless, one-to-one interactions, we want to hear from you. Get in touch with us todayour Phamily always has room for one more!

Job Summary:

Phunware is seeking a Data Engineer with hands-on experience creating, deploying and optimizing large-scale data systems.

The ideal candidate will bring strong technical skills and be proactive, responsive and very comfortable dealing with ambiguity. He or she will also bring good experience with Big Data systems/technologies and have a strong track record of deployment, maintenance, and optimization of production code.

The ideal candidate is someone who combines an understanding of business processes with knowledge of both client and server-side technical requirements in mobile software projects. They will put the customer first, quickly build strong relationships, learn rapidly, and enjoy autonomy and problem-solving. They must be a gifted leader with a genuine passion for working with high-performance teams, extraordinarily organized., and have a strong work ethic. Additionally, the position may require travel both domestically and internationally.

What Youll Do:
Create robust, high-volume production systems/architectures, and develop prototypes quickly
Work with development teams to design maintenance and support strategies
Create optimized workflows using relevant technologies (Spark, Elastic Search, Kafka, Oozie, Hadoop)
Create architectural workflows, diagrams, and specification documents to help define platform features/functionality
Perform experiments and analyze results to improve the performance and quality of algorithms
Work with product management and executive stakeholders to take detailed requirements and implement them using Agile Test Driven techniques
Work in an organized team-oriented environment with shared responsibilities
What Youll Bring:
Bachelors Degree or higher in Computer Science or Computer Engineering; Masters Degree preferred
Have previously worked in Big Data technologies and deployed in production environment
Strong experience in building highly scalable, available and responsive systems using open-source software tools and technologies
5-10 years of professional software development
5-8 years strong Java development experience
Good experience with REST API frameworks
Strong SQL skills
1+ years of professional software development experience with some of the big data technologies including: Spark, Map Reduce, Hive, HBase, Hadoop, Kafka, Impala, Cassandra
Experience in Elastic Search is highly desirable
Some experience with one or more of the following will be an added advantage: statistical analysis, machine learning, natural language processing, predictive modeling
Domain experience in one or more of the following:
Outstanding skills for interacting with people
Responsible, organized and hardworking with excellent communication skills
Must be living in the Irvine, CA or Austin, TX area or be able to immediately relocate
Desirable:
NoSQL or similar DB design/implementation experience with large number of records (i.e. 1 Billion+)
Experience with information retrieval, network programming and/or developing large software systems
Experience with cloud delivery platforms, ideally Amazon
Experience doing Test Driven Development (TDD), Continuous Integration (CI) and test automation
Open-source software contributions
Track record of success in a start-up or high-growth environment
Compensation and Benefits:
Fun, casual, fast-paced work environment filled with talented colleagues
Flexible paid time off
Competitive salary
Restricted Stock Units
Full range of benefits, including 401(k), medical, dental and vision coverage
Candidates for this position must be authorized to work in the United States and not require work authorization sponsorship by our company for this position now or in the future."
Data Engineer,"Position Data Engineer We are seeking a Data Engineer with strong data analysis skills performing data profiling, quality checks, writing SQL and Python scripts for data movement ie ETL. This position is a consulting position with our client. The best candidate will be someone that can understand data issues, perform profiling and analysis using SQL and stored procedures.This is mostly a back end data position but does require some interaction with business users. Most of the work will be performed remotely. Qualifications Minimum six (6) years experience writing SQL code preferable SQL Server or MySQL. Knowledge with Python is required. Ability to work with business users and management to understand the business need for data exploration, data quality and profiling. Experience working with relational databases, data modeling and writing ETL scripts to move data from source systems to an Enterprise Datawarehouse. Understanding of Data Warehousing concepts and experience in creating data warehouse schemas with Kimball methodology. Knowledge working with data in the cloud (AWS) desired. Solid understanding of Software Development Lifecycle (SDLC) and versionsource control disciplines. Be able to work effectively in an Agile Project Development team Must be able to define functional and technical docs based on data availability, quality and profiling on source data systems. The ability to work effectively within a team environment Understand complex logic and solve data issues by coming up with sound technical solutions. Soft Skills The ability to work effectively with minimal direction and supervision. To think creativity and come up with solutions that improve process and efficiencies. Ability to understand existing process and requirements. Understand complex logic and solve data issues by coming up with sound technical solutions. Must have solid written and oral communication skills. Can prepare and maintain technical documentation. Education Bachelors Degree in Computer Science, Finance or Data Analytics preferred. SQL andor Python related certifications preferred Work references will be requested. Must be authorized to work in the U.S."
Data Engineer,"LMI is currently seeking a data engineer within LMI’s Advanced Analytics service line to support the design and implementation of business critical data management & engineering solutions.
This position is located in Austin, TX

The ideal candidate will have direct, applied experience with one or more of the following areas:
- Develop data structures and systems to support the generation of business insights
- Knowledge and experience in overall ETL processes
- Maintain data infrastructure and develop scripts for regular processes
- Define, design, and develop data flow diagrams, data dictionaries, and logical and physical models
- Define data requirements, document data elements, and capture and maintain metadeta
- Identify and clean incomplete, incorrect, inaccurate or irrelevant data
- Identify new opportunities to use data to improve business performance
- Communicate and present data by developing reports using Tableau or Business Intelligence tools
- Adhere to compliance and audit requirements for data storage, architecture, cybersecurity, etc.

Bachelor’s degree in a quantitative field (e.g., engineering, statistics, mathematics, information technology, etc.) is preferred.
Master's degree is desired.
Must have at least 3 years of experience, preferably with a federal government customer.
Experience with big data tools: Hadoop, Spark, Kafka
Experience with relational SQL and NoSQL databases: Postgres, Cassandra, MongoDB
Experience with data governance tools: Collibra, Immuta
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala
Must possess strong written and verbal communication skills.
Secret or Top Secret clearance is preferred.

LMI is an Equal Opportunity Employer-all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, or national origin."
Data Engineer,"We're looking for a Data Engineer to join Procore's Information Technology Engineering team to help evolve our data-driven culture and become a world-class data organization. In this role, you'll help us gain a data advantage by leveraging our data assets and designing the foundation for which our advantage is constructed.

As a successful Data Engineer, you have a strong background in cloud infrastructure, particularly AWS and Google Cloud Platform. You strive to excel at everything you do while being able to prioritize between the must-haves and nice-to-haves. If you're intrinsically motivated and ready to roll up your sleeves and dive in—we'd love to hear from you!

This position will report into our Director, IT Engineering and has the option to be based in our Austin, TX offices located at the heart of downtown. We're looking for candidates to join us immediately.

What you'll do:
Create ETL (Extract, Transform & Load) pipelines to deliver sanctioned data to stakeholders, while maintaining high accuracy and reliability
Tune and monitor data infrastructure Performance to support a growing organization
Brainstorm data product ideas and partner closely with Data Scientists, Product Management and Operations teams to develop, test, deploy, and operate high-quality software
Develop data infrastructure that ingests and transform data from different sources and customers at scale.
Partner end-to-end with Business Managers, Product Managers, and Data Scientists to understand customer requirements and design prototypes and bring ideas to production
Work with internal business leaders to ingest data to enrich their data modeling and work products.
Participate in conversations with teams about business-impacting topics and brainstorm innovative ways to transform data into information and knowledge that drives revenue and reduces cost
What we are looking for:
BS or MS in Computer Science or equivalent
5+ years of data warehousing or data engineering experience with a distinguished track record on technically demanding projects
Deep knowledge of SQL databases (preferably PostgreSQL)
Comfort working with cloud-managed data warehouse technologies (Amazon Redshift, Google BigQuery, Snowflake)
Strong experience working with Python, particularly for ETL or Data Science related tasks
Experience working in a data lake architecture, separating compute from storage
Passion for creating new products and services, including being comfortable with the ambiguity associated with designing new products
Experience working with REST APIs to ingest and enrich data sets
Experience with Apache Airflow for workflow management is preferred
Comfort using Hadoop related technologies(Spark, Hive, Presto, etc.) is preferred
Data Science/Machine Learning background is preferred
Familiarity with the construction industry is preferred
About Us
Procore Technologies is building the software that builds the world. We provide cloud-based construction management software that helps clients more efficiently build skyscrapers, hospitals, housing complexes, and more. Our headquarters is located on the bluffs above the Pacific Ocean in Carpinteria, CA, with growing offices worldwide. Check us out on Glassdoor to see what others are saying about working at Procore!

We are an equal opportunity employer and welcome builders of all backgrounds. We thrive in a diverse, dynamic and inclusive environment. We do not tolerate discrimination against employees on the basis of age, color, disability, gender, gender identity or expression, marital status, national origin, political affiliation, race, religion, sexual orientation, veteran status, or any other classification protected by law.

Perks & Benefits
You are a person with dreams, goals, and ambitions—both personally and professionally. That's why we believe in providing benefits that not only match our Procore values (Openness, Optimism, and Ownership) but enhance the lives of our team members. Here are just a few of our benefit offerings: competitive health care plans, unlimited paid time off (Procore Values Time), employee enrichment and development programs, and volunteer days."
Data Engineer,"Position Title Data Engineer Location Austin, TX Position Type Full-time Permanent Role Job Description Open-source data warehousing(DWH), Python, java, google cloud, Apache Airflow design and architecture of data pipelines Should be able to articulate design challenges, performance considerations CICD SQL abilities. Complex SQL is used in BigData or DWH"
Data Engineer,"You…

As a Data Engineer you will work with the application and data science teams to support the development of custom data solutions.

Us…

We were founded as a fresh alternative in the Government Consulting Community and are dedicated to the belief that results are a product of analytical thinking, agile design principles and that solutions are built in collaboration with, not for, our customers. This mantra drives us to succeed and act as true partners in advancing our client’s missions.

Program Mission…

The project you will be working on is an existing cloud-based information technology infrastructure to host mission systems, applications, services, and data. While the current environment exists as an enterprise enabling platform, the end state of the environment is to enable Army leaders at every echelon to make fully informed, data driven decisions, based on authoritative and/or production data sources. Our approach is to transform legacy applications to be cloud native and reside on a Platform as a Service (PaaS). Additionally, modernize current applications by breaking them down into loosely coupled micro-services, and leveraging a continuous integration / continuous delivery pipeline to enable an agile DevOps Strategy.

What we'd like to see...
Capable of supporting rapid iterations of feature engineering with the development of coding solutions to enrich existing data sets.
Experience with traditional, modern, and cloud native database solutions.
Support the database design, development, implementation, information storage and retrieval, data flow and analysis activities.
Translate a set of requirements and data into a usable database schema by creating or recreating ad hoc queries, scripts and macros, updates existing queries, creates new ones to manipulate data into a master file.
Support development of databases, database parser software, database loading software, and database structures that fit into the overall architecture of the system under development.
Current IAT I Certification (A+ CE, Network+, CCNA, SSCP).
Desired Skills:
Works with considerable freedom to make decisions on the techniques and approaches to be used.
Prepares recommendations for system improvement for management and user consideration.
Years of Experience: 3 years of experience or more

Education: Bachelor’s Degree in Computer Science, Information Systems, or other closely related discipline

Clearance: Active Secret Security Clearance Required"
Data Engineer,"WorldQuant develops and deploys systematic financial strategies across a variety of asset classes and global markets. We seek to produce high-quality trading signals (Alphas) through our proprietary research platform to employ trading strategies focused on exploiting market inefficiencies. Our teams work collaboratively to drive the production of Alphas and financial strategies the foundation of a sustainable, global trading platform.

Technologists at WorldQuant research, design, code, test and deploy projects while working collaboratively with researchers and portfolio managers. Our environment is relaxed yet intellectually intense. Our teams are lean and agile, which means rapid prototyping of products with immediate user feedback. We seek people who think in code, aspire to solve undiscovered computer science challenges and are motivated by being around like-minded people. In fact, of the 600 employees globally, approximately 500 of them code on a daily basis.

WorldQuants success is built on a culture that pairs academic sensibility with accountability for results. Employees are encouraged to think openly about problems, balancing intellectualism and practicality. Great ideas come from anyone, anywhere. Employees are encouraged to challenge conventional thinking and possess a mindset of continuous improvement. Thats a key ingredient in remaining a leader in any industry. Our goal is to hire the best and the brightest engineers. We value intellectual horsepower first and foremost, and people who demonstrate an exceptional talent. There is no roadmap to future success, so we need people who can help us create it. Our collective intelligence will drive us there.

The Role:

Design and implement software to facilitate data integration with trading and simulating systems
Adopt new technologies to improve existing frameworks of data flow and monitoring
Implement and maintain software that interface with external vendors to bring in new data sets
Implement the rules and procedures that ensure integrity in data sets
Provide second level support to production support team regarding market data issues
Collect and analyze statistics on market data applications and devise approaches to improve the relevant processes
Design and implement systems that track and manage data availability, access and usage
What Youll Bring:

Degree in a quantitative or technical discipline from a top university and strong academic scores
Interest in applying technology to real situations, comfortable working in a fast-paced environment, detail-oriented and capable of performing tasks under pressure
Demonstrated experience with C++ or other object oriented languages
Experience with scripting languages such as Perl, Python, and shell scripting; Interface with database (such as MySQL)
Possess strong trouble shooting and problem solving skills
Ability to work independently and as member of a team
Strong verbal and written communication skills
Have experience working under a Linux environment, familiar with Vim or Emacs for editing files under the command line
Copyright © 2020 WorldQuant, LLC. All Rights Reserved. WorldQuant is an equal opportunity employer and does not discriminate in hiring on the basis of race, color, creed, religion, sex, sexual orientation or preference, age, marital status, citizenship, national origin, disability, military status, genetic predisposition or carrier status, or any other protected characteristic as established by applicable law."
Data Engineer,"Onica is one of the fastest growing AWS Premier Partners in North America. As a full spectrum AWS integrator, we assist hundreds of companies to realize the value, efficiency, and productivity of the cloud. We take customers on their journey to enable, operate, and innovate using cloud technologies from migration strategy to operational excellence and immersive transformation.

If you like a challenge, you'll love it here, because we're solving complex business problems every day, building and promoting great technology solutions that impact our customers' success. The best part is, we're committed to you and your growth, both professionally and personally.

Location: Texas

Overview

Our Data Engineers are experienced technologists with technical depth and breadth, along with strong interpersonal skills. In this role, you will work directly with customers and our team to help enable innovation through continuous, hands-on, deployment across technology stacks. You will work to build data pipelines and by developing data engineering code ( as well as writing complex data queries and algorithms.

What You'll Be Doing
Build complex ETL code
Build complex SQL queries using MongoDB, Oracle, SQL Server, MariaDB, MySQL
Work on Data and Analytics Tools in the Cloud
Develop code using Python, Scala, R languages
Work with technologies such as Spark, Hadoop, Kafka, etc.
Build complex Data Engineering workflows
Create complex data solutions and build data pipelines
Establish credibility and build impactful relationships with our customers to enable them to be cloud advocates
Capture and share industry best practices amongst the Onica community
Attend and present valuable information at Industry Events
Traveling up to 50% of the time
Qualifications & Experience
3+ years design & implementation experience with distributed applications
2+ years of experience in database architectures and data pipeline development
Demonstrated knowledge of software development tools and methodologies
Presentation skills with a high degree of comfort speaking with executives, IT management, and developers
Excellent communication skills with an ability to right level conversations
Technical degree required; Computer Science or Math background desired
Demonstrated ability to adapt to new technologies and learn quickly
If you get a thrill working with cutting-edge technology and love to help solve customers' problems, we'd love to hear from you. It's time to rethink the possible. Are you ready?"
Data Engineer,"Job Description
Senior Data Engineer

Position Overview: From software hacking to hardware hacking, we help secure everything from cryptocurrency exchanges and space telescopes to autonomous vehicles and the electric grid. Today, our client is making significant investments in terms of financial and engineering resources to develop a radically new customer experience we call “Security-as-a-Service” to provide customers with a unified, efficient, and data-driven security platform. We they're looking to add the right individual to their growing team supporting the next wave of cybersecurity products and solutions.

As part of that investment, our client is seeking a seasoned Data Engineer with a successful track record in data engineering in a hyper growth company setting. You will have the opportunity to work with some of the best security engineers in the world who hail from organizations such as Amazon, CIA, Facebook, Google, Microsoft, NSA, Redhat, Sun Microsystems, and US Air Force. As an Inc. Best Places to Work, Inc. 500 | 5000, Cybersecurity 500, and Austin Fast 50 Award recipient, we are seeking an individual that understands the professional and personal growth attached to this opportunity and who has the corresponding internal drive to maximize it.

Career opportunity:
Join an industry with massive socio, economic, and political importance in the 21st century
Work alongside some of the best and the brightest minds in the security industry
Leave an indelible mark on a company where individual input has real impact
Be recognized, internally and publicly, for your contributions in a high profile position
Align your career trajectory with a hyper growth company that is on the move
Core responsibilities:
Create pipelines to ingest and maintain complex data sets into our client's data stores for use in machine learning models
Create tools to scour the internet to find important security information and ingest it into their infrastructure
Work with data scientists to create and maintain data ontologies for security
Create the roadmap of how to continually evolve the data engineering infrastructure and techniques to improve our client's ability to find security information
Mentor junior data engineers and teach them how to use data engineering techniques to solve real world problems
Communicate complex concepts to team members
Accountable for:
Creation of data engineering pipelines to find and ingest security vulnerabilities
Creation of data engineering tools to help label and validate data
Required qualifications:
At least 8 years experience designing and building data processing/ETL pipelines
At least 8 years experience in Python and Spark or similar technologies
At least 8 years experience with SQL and relational databases
At least 8 years experience parsing flat files
8+ years development experience
Prior track record in a hyper-growth, high-tech company
Bachelor's degree or equivalent practical experience
Desired qualifications:
Experience working with Google Tensorflow
Experience with modern technology stacks
Experience with micro-services architectures
Experience with cloud platforms and SaaS solutions
Experience with agile/scrum development practices
Experience with test driven development, continuous integration, continuous deployment
Experience with Git, JIRA, Confluence
Experience with Google Compute, Firebase, and GKE
Experience with Docker
Desired behaviors:
Relentless restlessness to turn theory into practice and develop production worthy code that solves real-world customer problems
Determination to always learn and get better and never rest on ones laurels
Personable individual who enjoys working in a team-oriented environment
Comfort dealing with ambiguity in an environment where we build the plane as we fly it
Ability to work within constraints and to challenge the status quo
Ability to self-direct work and truly own the position in a hyper-growth environment
Compensation package:
Competitive compensation
Ownership opportunity through employee stock option plan
Health, dental, and vision insurance
4% company 401K matching vested immediately"
Data Engineer,"OverviewOBXtek Inc. is an established, award-winning business providing information technology and professional management services to the federal government. Our corporate growth has coincided with our investment in our employees as well as in outreach to our civilian and military community.ResponsibilitiesOBXtek is recruiting for a Data Engineer for the Army Forces Command Data & Decision Sciences Division (AFC DDSD) in Austin, TX. In this AFC, uses the Modernization Application & Data Environment (MADE) in a managed service provider style model to enable subordinate elements throughout AFC to access commercial cloud services. AFC DDSD has primary responsibility for the provisioning and operating the shared common services, maintaining approved desired state configurations for Infrastructure as a Service (IaaS) deployments, operating and maintaining a central data warehouse capability, and maintaining the Risk Management Framework (RMF) documentation for the commons services and environmental level accreditation in a manner that maximizes inheritance.The Data Engineer will:* Work with application and data science teams to support development of custom data solutions.* Support rapid iterations of feature engineering with the development of coding solutions to enrich existing data sets.* Support the database design, development, implementation, information storage and retrieval, data flow and analysis activities.* Translate a set of requirements and data into a usable database schema by creating or recreating ad hoc queries, scripts and macros, updates existing queries, creates new ones to manipulate data into a master file.* Support development of databases, database parser software, database loading software, and database structures that fit into the overall architecture of the system under development.Potential CONUS Travel of 25%QualificationsActive Secret ClearanceRequires a Bachelor's degree in Computer Science, Information Systems, Engineering, or other Scientific/Technical discipline* Experience with traditional, modern, and cloud native database solutions.* 3 years of related data engineering work experience.* Must meet IAT-I requirements as specified in DoD 8570.01-M; which includes A+ CE, CCNA-Security, Network + CE, SSCP.Company InformationHeadquartered in McLean, Virginia and founded in 2009, OBXtek is a fast-growing leader in the government contracting field. Our mission is Our PeopleOur Reputation. Our people are trained professionals who enhance our customers' knowledge and innovation using technology, collaboration, and education.We offer a robust suite of benefits including comprehensive medical, dental and vision plans, Flexible Spending Accounts, matching 401K, paid time off, tuition reimbursement plans and much more.As a prime contractor for 93% of our current work, OBXtek pairs lessons learned across disciplines with industry standard quality practices such as CMMI-Dev Level III, ITIL, 6Sigma, PMI, and ISO. Our rapid growth has been recognized by INC500, the Washington Business Journal, and Washington Technology magazine.OBXtek is an Equal Opportunity Employer and does not discriminate based on race, color, religion, sex, age, national origin, gender identity, disability, veteran status, sexual orientation or any other classification protected by federal, state or local law."
Data Engineer,"Our Story

The National Research Center for College and University Admissions™ (NRCCUA®), now a part of ACT®, is a membership organization that links colleges and universities to the nation’s largest college and career planning program for students seeking post-secondary guidance. In addition, members can receive exclusive access to Encoura™ Data Lab—an educational data science, analytics, and research platform. Since 1972, ACT | NRCCUA has been a leading provider of data, technology, and programs servicing public and private colleges and universities to enhance their marketing and recruiting efforts.

Over the years, ACT | NRCCUA has continued to evolve its offering to represent the link between students and higher education institutions. As part of that evolution, ACT | NRCCUA acquired Eduventures—the leading research and advisory firm focused exclusively on higher education. We are now able to provide forward-looking and actionable research based on proprietary market data, as well as advisory services that support both strategic and operational decision-making. All Eduventures Research is now available in the Encoura Data Lab platform.

The Senior Database Engineer is a hands-on technical position for a senior-level professional. The role involves an advanced, experienced skill set to design, develop and implement database objects, procedures and processes using the SQL Server platform to support business objectives throughout the organization.
You'll be working as part of an Agile team, developing the systems that power our business. This is a hands-on role, working with other engineers, writing code, testing, and deploying the finished apps and libraries.
Responsibilities and Deliverables
In partnership with your co-workers, design and develop database infrastructure (tables and views) to support a complex and rapidly changing data environment.
Create working, maintainable, and fast Python scripts and stored procedures using best practices and current organizational standards to support data-driven applications, both internal and client-facing.
Use indexing and other techniques to optimize new and existing objects and processes.
Develop processes for the ETL of data throughout the entire organization.
Generate data to support reporting (ad-hoc and standardized).
Follow and help develop database team standards and methodologies; use source control and build management procedures to ensure stable development, staging and production database environments.
Enhance, refactor, and continuously improve the database schemas and related code.
Communicate effectively with technical and non-technical people.
Solve business needs with short-term deliverables, while constantly improving and moving towards long-term architectural goals.
Generate new ideas, never say or think ""that's not my job.""
Be proactive in keeping your skills fresh
Qualifications and Experience
5+ years of T-SQL development experience in SQL Server.
3+ years of experience developing ETL processes with SQL Server Integration Services, Pentaho, or other tools. Bonus points for having developed ETL processes with Python and AWS.
2+ years of experience in development with Python 3.
1+ years of experience developing on AWS and Linux.
Mastery of advanced database design methodologies and experience with database modeling tools, dimensional modeling and statistical analysis
Clear understanding of SQL Server best practices for development of stored procedures, views, tables, security objects, indexes, etc.
Experience with any of the following is a plus: Postgres, SQL Server Reporting Services, MongoDB, Redis, Exasol
An appreciation for pragmatism and simplicity in code.
A strong code and architecture design sensibility.
Customized mathematical skills as determined by the requirements of the job
NRCCUA is an Equal Employment Opportunity (EEO) employer. It is the policy of the Company to provide equal employment opportunities to all qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information. All applicants must be eligible to work in the U.S.

NRCCUA endeavors to make reasonable accommodations for applicants with disabilities and disabled veterans pursuant to applicable federal and state law. If you are an individual with a disability and require a reasonable accommodation to complete any part of the application process or are limited in the ability and need an alternative method for applying, please contact the People Team.

To review our privacy policy, please click this link: https://encoura.org/privacy-policy/"
Data Engineer,"Integrated Technology Strategies is a provider of information technology consulting. Digital transformation, improving business performance, developing strategies and enhancing value is at the core of what we are.

Integrated Technology Strategies is looking to hire a Data Engineer to be a part of our Consulting team. You will participate and effectively contribute to the design, development, and implementation of complex applications, often using new technologies. You will provide technical expertise and systems design for individual initiatives.

Education & Experience:
Bachelor’s degree in engineering or a bachelor’s degree in technology from a recognized university
Candidate should have recent work experience with US-based customers
Minimum 5 years of relevant experience is required
Technical Skill Requirements:
4+ years of data engineer experience with large data lake platforms, ideally google or hadoop.
Familiarity with cloud, specifically GCP.
Experience utilizing tools like Spark and understand how to move data, build code-based data pipelines, data transformation, etc.
Development experience with Python and/or Go-lang
Plus: Hotspots and composure experience- how to break queries out—Saves time, performance, and money.
Retail/E-commerce industry would be a plus.
Experience with very large (multi-petabyte size) data lakes.
Experience with Unix (Shell, Scripting) is very helpful.
Development experience with building APIs – ex: REST, etc.—Python and/or Go-Lang
Experience with BigQuery and Composer.
Operational Experience managing a large data lake
Experience with code-based pipelines (ex Spark). Some exposure to Java, Scala, and/or Python is a plus"
Data Engineer,"Job Description
At BGDS, our vision is to untap the economic welfare potential of technology through entrepreneurship. In order to fully realize our vision, we have committed ourselves the mission to provide transparency, openness, collaboration, ease-of-use and insights to technology startup financing so that entrepreneurship thrives globally and founders can develop life-changing technologies.
Job Overview

BGDS is looking for a savvy Data Engineer to join our growing team of data and analytics experts.
As a Data Engineer, you will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for many external sources and professional users. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.
You will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing needs. They must be self-directed and comfortable supporting the data needs of multiple users, systems, and products.
The ideal candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives. We’re in the process of revolutionizing startup financing, and we’re hoping you’ll be part of that experience.

Responsibilities and Duties

Help to design and implement the data repository architecture and a large-scale processing system for BGDS product.
Help to identify, design, and implement processes improvements: optimizing data gathering, data quality, data consolidation, and data delivery. Re-designing infrastructure for greater scalability and stability, etc.
Recommend and sometimes implement ways to improve data reliability, efficiency, and quality.
Help to build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and NoSQL ‘big data’ technologies.
Help to create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Help to keep our data separated and secure across national boundaries through multiple data centers and regions, respecting the data privacy regulation (GDPR and similar).

Work with data and analytics experts to strive for greater functionality in our data systems.
Our approach to supervision is very adaptive, which is to say that we are happy to accommodate a variety of personal styles. We are searching for someone who is an independent contributor, but you will also get the support you need when you need it.
Qualification and Experience
Ability to work in our Austin office 5 days per week
A bachelor’s or higher degree in Computer Science, Physics, Statistics, Informatics, Information Systems or another quantitative field.
3+ years of work experience in software design and development
3+ years in data engineering
Advanced working SQL and CQL knowledge and experience working with relational databases and Cassandra, query authoring (SQL, CQL, KSQL, SparkSQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Discover opportunities for data acquisition.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and work- load management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience with big data tools: Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with data processing engines: Apache Beam, Dataflow, etc.
Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, Python Fireworks, etc.
Experience with GCP cloud services: Compute, Kubernetes, Cloud Functions, BigQuery, Dataproc,
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object development languages: Python, Java, C++, Scala, etc.
Start-up experience is ideal
Salary based on experience - $100K +"
Data Engineer,"Job Description
Unizin is looking for a Senior Data Engineer to expand the data architectures and data services that drive its solutions. The ideal candidate is experienced in building data pipelines, data services, and distributed/concurrent systems. The right candidate will be excited by the prospect of building, optimizing, and even re-designing our company’s data architecture to support our next generation of products and data initiatives.

What you'll do
Build services that extract, transform, and load (ETL) data from a variety of data sources using cloud-based, ""big data"" technologies (e.g., Spark, Hadoop)
Building services that leverage queuing and stream processing to deliver near-real-time data services
Create and maintain optimal data pipeline architecture
Generate test data sets based on established internal and external schemas
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build data analytics services that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics
Make heavy use of Google Cloud offerings to provide timely, accurate, and comprehensive data services to large research universities for analytics and reporting
Participate in a production support rotation with other engineers
What makes you a great fit
You love building automated data pipelines for batch and event data processing
You love wrangling, processing, and analyzing data in streamlined data systems
You love building systems from the ground up
Required skills
Experience in building concurrent, distributed systems
Expert-level knowledge of streaming and batch ETL techniques
Expert-level knowledge of SQL
Expert-level knowledge of relational and NoSQL databases (e.g. Postgres, BigQuery)
Experience with distributed query systems (e.g., Hive, Spark SQL, Presto)
Python, Ansible, Docker, Kubernetes, Git
Skills that will set you apart
Experience building catalog and inventory systems
Experience with RStudio, Jupyter
Experience in translating analytical models in R to production implementations
Apache Airflow
Experience with GitfFlow & GitOps
Benefits
Great team
Cool technology
Mission of high social value*
Flexibility to implement new, custom, and better solutions
Competitive salary
Excellent medical, dental, and optical plans
403b with matching
Open PTO
Free parking
Loaded kitchen
Flexible hours
Powered by JazzHR

9UZqd62Hza"
Data Engineer,"Join SADA as a Data Engineer!

Your Mission

As a Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data issues facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring a combination of batch or streaming data pipelines, data lakes and data warehouses.

You will be recognized as an established contributor by your team. You will contribute design and implementation components for multiple projects. You will work mostly independently with limited oversight. You will also participate in client-facing discussions in areas of expertise.
Pathway to Success

#BeOneStepAhead: At SADA we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage adaptability. This means that not only do our engineers understand that change is inevitable, but they embrace this change to continuously broaden their skills, preparing for future customer needs.

Your success comes from your enthusiasm, insight, and positive impact. You will be given direct feedback quarterly with respect to the scope and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, your collaboration with your peers, and the consultative skill you demonstrate in customer interactions.

As you continue to execute successfully, we will build a personalized development plan together that leads you through the engineering or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with a first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:
Google Professional Data Engineer Certified or able to complete within the first 45 days of employment

Required Qualifications:
Expertise in at least one of the following domain areas:
Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or other customer-facing role

Useful Qualifications:
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Applied experience operationalizing machine learning models on large datasets
Knowledge and understanding of industry trends and new technologies and ability to apply trends to architectural needs
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem

About SADA

Values: We built our core values on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing

Work with the best: SADA has been the largest partner in North America for GCP since 2016 and has been named the 2019 and 2018 Google Cloud Global Partner of the Year. SADA has also been awarded Best Place to Work by Inc. as well as LA Business Journal!

Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, RRSP, professional development reimbursement program as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."
Data Engineer,"Infoblox is looking for a Senior Data Engineer to augment our growing Cyber Security Software Development Team. This growing team supports the Infoblox mission to thwart cybersecurity threats in our customer's networks. This is an opportunity to work closely with data scientists and threat analysts to curate the data that makes this mission possible.

The ideal candidate is a savvy software engineer with experience in data engineering and a solid background in Spark and Python. Preferably you know that countMinSketch is not a children's game. You are comfortable wearing several hats in a small organization with a wide range of responsibilities and have worked in a cloud environment, such as Amazon EMR. You know that Big Data is both a blessing and a curse; without good data engineering, it loses its potential. You are passionate about the nexus between data and computer science-driven to figure out how best to represent and summarize data in a way that informs good decisions and drives new products. When someone says, ""my Spark job failed"", your first question is ""what's the skew?"". Come join our growing Cyber Threat Intelligence team and help us build world-class solutions!

Responsibilities:
Curate very large-scale data from a multitude of sources into appropriate sets for research and development for data scientists, threat analysts, and developers across the company.
Design, test, and implement storage solutions for various consumers of the data.
Design and implement mechanisms to monitor data sources over time for changes using summarization, monitoring, and statistical methods.
Leverage computer science algorithms and constructs, including probabilistic data structures, to distill large data into sources of insight and enable future analytics.
Convert prototypes into production data engineering solutions through disciplined software engineering practices, Spark optimizations, and modern deployment pipelines.
Collaborate on the design, implementation, and deployment of applications with the rest of software engineering.
Support data scientists and threat analysts in building, debugging and deploying Spark applications that best leverage data.
Build and maintain tools for automation, deployment, monitoring, and operations.
Create test plans, test cases, and run tests with automated tools.
Requirements:
5+ years of experience with Python3, and 2+ years experience with Spark. Scala experience is helpful.
5+ years of experience in data engineering, data science, and related data-centric fields using large-scale data environments.
3+ years of experience in using SQL and working with modern relational databases, including MySQL or PostgreSQL
3+ years of experience with developing ETL pipelines and data manipulation scripts
Proficient in Object-Oriented Design and S.O.L.I.D principles.
Strong emphasis on unit testing and code quality.
Proficient with AWS products (EMR S3, Lambda, VPC, EC2, API Gateway, etc).
Preferred Experience:
Very strong Python and PySpark experience.
Very strong back end development experience.
Strong experience with cloud deployments and CI/CD.
Experience with virtualization, containers, and orchestration (Docker, Kubernetes, XEN).
Experience with NoSQL Non-Relational databases (AWS DynamoDB).
Education:
MS or BS in Computer Science or a related field, or equivalent work experience required.
Perks:
Work with a world-class technology team in a rapidly growing company
A career path with opportunities to grow
Discretionary Paid Time Off policy to promote a healthy work/life balance + world-class benefits
And many, many more perks!
It's an exciting time to be at Infoblox. We are the market leader in technology for network control. Our success depends on bright, energetic, talented people who share a passion for excellence in building the next generation of networking technologies-and having fun along the way. Infoblox offers a fast-paced, action-oriented environment. We promote a culture that embraces innovation, change, teamwork, and strong partnerships. Join the winning Infoblox team-our future looks bright, and so will yours. To check out what it's like to be a Bloxer click here.

#LI-AB1"
Data Engineer,"Overview


ProSphere is seeking an experienced Data Engineer to assist in strategizing, designing, continually improving and operating an existing Department of Army Government client’s cloud-based information technology infrastructure to host mission systems, applications, services, and data. This is full-time position that can be located in Austin, TX, National Capital (DC) Region, Redstone, AL, Detroit, MI, Natick, MA, Orlando, FL along with the possibility of telework options. Veterans are encouraged to apply.

Responsibilities


• Works with application and data science teams to support development of custom data solutions. • Capable of supporting rapid iterations of feature engineering with the development of coding solutions to enrich existing data sets. • Experience with traditional, modern, and cloud native database solutions. • Support the database design, development, implementation, information storage and retrieval, data flow and analysis activities. • Translate a set of requirements and data into a usable database schema by creating or recreating ad hoc queries, scripts and macros, updates existing queries, creates new ones to manipulate data into a master file. • Support development of databases, database parser software, database loading software, and database structures that fit into the overall architecture of the system under development.

Qualifications


• Requires a Bachelor’s degree in Computer Science, Information Systems, Engineering, or other Scientific/Technical discipline and • 3 years of related data engineering work experience. • Must meet IAT-I requirements as specified in DoD 8570.01-M (A+ CE, CCNA-Security, CND, Network+ CE, SSCP)• U.S. Citizenship (Government Requirement)• Must have an active Secret Clearance

Qualifications Highly Desired• U.S. Army or Army Future Command experience • Former Military

Knowledge/Skills/Abilities• Well organized• Superior attention to detail• Exceptional multi-tasking skills• Demonstrated and verifiable ability to recruit, develop, support, and maintain high performing teams

Physical Demands


• Ability to sit in an office environment for long periods of time • Typical office environment. Ability to sit and stand for extended periods of time; ability to lift 5-20 lbs

ProSphere offers full-time employees a comprehensive and competitive benefits package including paid vacation, sick leave, holidays, health insurance, life insurance, military leave, training, tuition reimbursement, a wellness program, short- and long-term disability, 401(k) retirement plan with company matches/immediate vesting, commuter benefits, and more.It is ProSphere’s policy to promote equal employment opportunities. All personnel decisions, including, but not limited to, recruiting, hiring, training, promotion, compensation, benefits and termination, are made without regard to race, creed, color, religion, national origin, sex, age, marital status, sexual orientation, gender identity, citizenship status, veteran status, disability or any other characteristic protected by applicable federal, state or local law."
Data Engineer,"Company Founded in Austin, Texas, in 2014, our client has built a powerful digital platform where buyers and service providers can easily connect and transact on a full range of oil field services. Job Description The Senior Data Engineer will be responsible for the design, structure, and maintenance of the data environment. The data practice is still very young within the organization and this individual will be tasked with helping develop it into a more robust environment. The management and organization of data is highly technical and requires advanced skills with computers and proficiency with data-oriented computer languages such as Python, SQL, and XML. A Senior Data Engineer possesses superior analytical skills and is detail-oriented. This Senior Data Engineer will be required to communicate effectively with C-Level and needs to explain complex technical concepts to non-technical staff. Since development of data models and logical workflows is common, a Senior Data Engineer must also exhibit advanced visualization skills, as well as creative problem-solving. Responsibilities Plans, architects, designs, analyses, develops, codes, tests, debugs and documents data analytics platforms to satisfy business requirements for large, complex Data ReservoirData Warehouse, Reporting Analytics development Lead and perform database level tuning and optimization in support of application development teams on an ad-hoc basis. Analyses business and data requirements to support the implementation of an applicationrsquos full functionality Contributes to high level functional design used across all Reporting Analytics applications based on system build and knowledge of business needs Collaborates with fellow team members and keeps the team and other key stakeholders well informed of progress of application business features being developed Create data architecture strategies for each subject area of the enterprise data model. Communicate plans, status and issues to higher management levels. Collaborate with the business and other IT organizations to plan a data strategy. Qualifications Bachelor or Masterrsquos degree in computer science or similar 5+ years of experience with demonstrated knowledge in the design development of data warehouses andor data reservoirdata lakedata mart platform. 4+ years of advanced analytics tools methodologies (Python, R etc.) 4+ years of experience in data ingestion tools and techniques including ETL ELT methodologies. 3+ years experience in Tableau is preferred but other visualization tools such as PowerBI will also be considered. Working proficiency in a selection of software engineering disciplines and demonstrates understanding of overall software skills including business analysis, development, testing, deployment, maintenance and improvement of software. Strong communication skills with demonstrated experience coordinating development cycles and project management. Self-starter that can work alone and as part of a larger internal and external team Works well with others and understands the importance of the team Exceptional data analysis skills and problem solving ability Experience with statistical analysis and predictive modelling skills ndash a plus ----------------------------------------------------------------------- For more information on this job visit httpsrekruiters.comjobs httpswww.google.comurl?qhttpsrekruiters.-UpcPd8ow Rekruiters has been named by business journals as one of the best places to work. We offer benefits such as weekly pay, health insurance, 401k and even profit sharing to our consultants. Corporate httpswww.rekruiters.com httpswww.google.comurl?qhttpswww.rekruiters. ndash Main Site rekruiters.com ndash Twitter httpswww.facebook.comrekruiters httpswww.google.comurl?qhttpswww.facebook.-hlw ndash Facebook ------------------------------------------------------------------------ JOB ID - 6581"
Data Engineer,"H-E-B Digital is seeking new team members
(Partners)! Since our inception, we’ve been investing heavily in our
customers’ digital experience, reinventing how they find inspiration from food,
how they make food decisions, and how they ultimately get food into their
homes. This is an exciting time to join H-E-B Digital, and we’re hiring across
the stack: front-end web and mobile, full-stack, and backend engineering. We’re
using the best available technologies to deliver modern, engaging, reliable,
and scalable experiences to meet the needs of our growing audience. Our digital
solutions are growing in popularity and adoption—like Curbside and Home
Delivery—so you’ll get the opportunity to define the user experience for
millions of customers and hundreds of thousands of Partners. If you’re someone
who enjoys taking on new challenges, working in a rapidly changing environment,
learning new skills, and applying it all to solve large and impactful business
problems, we want you as part of our team.

Our Partners thrive The H-E-B Way. In the Sr. Data Engineer job, that means
you have a…

HEART FOR PEOPLE… you can organize multiple engineers, negotiate
solutions, and provide upward communication

HEAD FOR BUSINESS… you consistently demonstrate and uphold the
standards of codding, infrastructure, and process

PASSION FOR RESULTS… you’re capable of high-velocity contributions
in multiple technical domains
What you’ll do
Work with HEB Digital teams to
provide data solutions for ecommerce, supply chain, store operations,
finance, and marketing reporting and analytics platforms
Contribute to existing data
platforms and implement new technologies
Develop a deep understanding of
HEB’s data and become a domain expert
Ensure data is distributed in a
timely and accurate manner
Make data discoverable and
accessible to business users

Who You
Are
4 years of
data engineering experience
Proficient
with data technologies (e.g. Spark, Kinesis, Kafka, Airflow, Oracle,
PostgreSQL, Redshift, Presto, etc.)
Experienced
with designing and developing ETL data pipelines using tools such as
Airflow, Nifi, or Kafka.
Strong
understanding of SQL and data modeling
Understanding
of Linux, Amazon Web Services (or other cloud platforms), Python, Docker,
and Kubernetes
Experienced
with common software engineering tools (e.g., Git, JIRA, Confluence, or
similar)
Bachelor's
degree in computer science or comparable field or equivalent experience
A proven
understanding and application of computer science fundamentals: data
structures, algorithms, design patterns, and data modeling

What are the Perks?
A robust Benefits
plan with coverage starting Day One
Dental, vision, life,
and other insurance plans; flexible spending accounts; short term / long term
disability coverage
Partner Care Team,
for any time you have healthcare or coverage questions
Telehealth offers
24/7 access to board-certified doctors by phone
Partner Guidance
allows free counselor visits
Funeral leave, jury
duty, and military pay (subject to applicable law)
Maternal / paternal
leave for new parents, including adoptions
10"" off H-E-B brand
products in-store and online
Eligibility to
participate in 401(k)
Opportunity to become
a “Partner-Owner” after 12 months
Who
We Are
H-E-B is one of the largest, independently owned food
retailers in the nation, operating over 400 stores throughout Texas and Mexico,
with annual sales generating over $25 billion
We hire talented
people (109,000 Partners), and give them autonomy to be creative in how they
impact the business
We’re a
Partner-driven company with a Bold Promise – Because People Matter
We embrace Diversity
and Inclusion as core values, and support them with thriving company-wide
programs
We’re a truly
original Texas-based company that created the Spirit of Giving to help Texas
communities every day
Once eligible, our
Partners become Owners in the company. “Partner-owned” means our most important
resources—People—drive the innovation, growth, and success that make H-E-B The
Greatest Retailing Company
04-2019

DASO3232"
Data Engineer,"We are looking for a senior data engineer who will help define the data architecture and transformation workflows for our Data Analytics Platform. Working with the Product Manager and Development Manager, you will help us design our analytics environment and bring fast and reliable Analytics to our internal and external customers. Hypori is a Virtual Mobile Infrastructure platform that enables a secure BYOD system for clients. You should have the experience to manage and build enterprise class software that can be deployed around the world. You can learn more about the Hypori product at: https://hypori.com/

You can take our vision of Agile Analytics development, fill in the gaps and bring it to life.
Investigate and recommend data platforms and tools to support Hypori’s analytics needs
Build data pipelines and transformation workflows
Mentor junior Data Engineers on data flow and quality
Advise on data stewardship and ETL/ELT quality assurance best practices

5+ years of data pipeline and ETL work and tuning data management environments
3+ years of data platform architecture
Experience applying common, data warehousing concepts (Kimball, Inmon, etc)
Experience with reporting and visualization for unstructured and structured data.
Experience with data cleansing and data quality functions.
Nice to Have
Professional Data Engineer Certification.
Experience embedding analytics in automated processes.
You have an interest in assigning a dollar value to our data.
What you’ll get working with us
Day One benefits package of an established company
The perks of an Austin startup such as free snacks, soda, coffee, and a relaxed office
The opportunity to work on cool technology with brilliant people
Breakfast Taco Fridays
Since 2006, Intelligent Waves has remained committed to providing quality engineering, operations, and intelligence solutions to our clients and it all starts with YOU. Intelligent Waves provides a great and generous benefits package to include medical, dental and vision, PTO leave, and life & disability packages. We also invest in our employees' futures by providing a contribution with vesting starting from DAY 1, technical training, tuition bonuses, and much, much more.
Intelligent Waves LLC is an equal opportunity employer. We are committed to providing equal opportunity to all applicants and employees in full compliance with all applicable state and federal laws prohibiting discrimination on the basis of race, color, age, gender, religion, national origin, disability, sexual orientation, and gender identity protected veteran status and individuals with disabilities, or any other class protected by applicable state or federal law.
A Veteran Friendly Organization
#IW
#Hypori"
Data Engineer,"Join SADA as a Sr. Data Engineer!

Your Mission

As a Sr. Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring batch or streaming data pipelines, data lakes and data warehouses.

You will be expected to run point on whole projects, end-to-end, and to mentor less experienced Data Engineers. You will be recognized as an expert within the team and will build a reputation with Google and our customers. You will demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise. You will also participate in early-stage opportunity qualification calls, as well as lead client-facing technical discussions for established projects.
Pathway to Success

#BeOneStepAhead: At SADA we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our engineers know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.

Your success starts by positively impacting the direction of a fast-growing practice with vision and passion. You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions.

As you continue to execute successfully, we will build a customized development plan together that leads you through the engineering or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:
Google Professional Data Engineer Certified or able to complete within the first 45 days of employment

Required Qualifications:
Mastery in at least one of the following domain areas:
Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or customer-facing role

Useful Qualifications:
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Experience operationalizing machine learning models on large datasets
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem

About SADA

Values: We built our core values on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

Make them rave
Be data driven
Be one step ahead
Be a change agent
Do the right thing

Work with the best: SADA has been the largest partner in North America for GCP since 2016 and has been named the 2019 and 2018 Google Cloud Global Partner of the Year. SADA has also been awarded Best Place to Work by Inc. as well as LA Business Journal!

Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match, professional development reimbursement program as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."
Data Engineer,"Position Overview: From software hacking to hardware hacking, we help secure everything from cryptocurrency exchanges and space telescopes to autonomous vehicles and the electric grid. Today, Praetorian is making significant investments in terms of financial and engineering resources to develop a radically new customer experience we call “Security-as-a-Service” to provide customers with a unified, efficient, and data-driven security platform. We are looking to add the right individual to our growing team supporting the next wave of cybersecurity products and solutions.

As part of that investment, Praetorian is seeking a seasoned Data Engineer with a successful track record in data engineering in a hyper growth company setting. You will have the opportunity to work with some of the best security engineers in the world who hail from organizations such as Amazon, CIA, Facebook, Google, Microsoft, NSA, Redhat, Sun Microsystems, and US Air Force. As an Inc. Best Places to Work, Inc. 500 | 5000, Cybersecurity 500, and Austin Fast 50 Award recipient, we are seeking an individual that understands the professional and personal growth attached to this opportunity and who has the corresponding internal drive to maximize it.

To learn more about Praetorian, visit: https://www.praetorian.com/careers

Career opportunity:
Join an industry with massive socio, economic, and political importance in the 21st century
Work alongside some of the best and the brightest minds in the security industry
Leave an indelible mark on a company where individual input has real impact
Be recognized, internally and publicly, for your contributions in a high profile position
Align your career trajectory with a hyper growth company that is on the move
Core responsibilities:
Create pipelines to ingest and maintain complex data sets into Praetorian's data stores for use in machine learning models
Create tools to scour the internet to find important security information and ingest it into Praetorian's infrastructure
Work with data scientists to create and maintain data ontologies for security
Create the roadmap of how to continually evolve the data engineering infrastructure and techniques to improve Praetorian's ability to find security information
Mentor junior data engineers and teach them how to use data engineering techniques to solve real world problems
Communicate complex concepts to team members

Accountable for:
Creation of data engineering pipelines to find and ingest security vulnerabilities
Creation of data engineering tools to help label and validate data

Required qualifications:
At least 8 years experience designing and building data processing/ETL pipelines
At least 8 years experience in Python and Spark or similar technologies
At least 8 years experience with SQL and relational databases
At least 8 years experience parsing flat files
8+ years development experience
Prior track record in a hyper-growth, high-tech company
Bachelor's degree or equivalent practical experience

Desired qualifications:
Experience working with Google Tensorflow
Experience with modern technology stacks
Experience with micro-services architectures
Experience with cloud platforms and SaaS solutions
Experience with agile/scrum development practices
Experience with test driven development, continuous integration, continuous deployment
Experience with Git, JIRA, Confluence
Experience with Google Compute, Firebase, and GKE
Experience with Docker

Desired behaviors:
Relentless restlessness to turn theory into practice and develop production worthy code that solves real-world customer problems
Determination to always learn and get better and never rest on ones laurels
Personable individual who enjoys working in a team-oriented environment
Comfort dealing with ambiguity in an environment where we build the plane as we fly it
Ability to work within constraints and to challenge the status quo
Ability to self-direct work and truly own the position in a hyper-growth environment

Compensation package:
Competitive compensation
Ownership opportunity through employee stock option plan
Health, dental, and vision insurance
4% company 401K matching vested immediately

In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.

We are committed to an inclusive and diverse Praetorian. We are an equal opportunity employer. We do not discriminate based on race, ethnicity, color, ancestry, national origin, religion, sex, sexual orientation, gender identity, disability, veteran status, genetic information, marital status, or any other legally protected status.

We ask that you please include a few paragraphs about yourself and what you are passionate about in your application."
Data Engineer,"Introduction:
We seek a highly skilled Data Engineer to join our team. As a member of our team, you will improve the quality, quantity, and timeliness of data that we provide to candidates and activists through numerous applications. Our applications cover a wide range: voter registration, canvassing, fundraising, and more. Experience with US elections administration or political campaigns is a plus.

If you want to work on disruptive civic technologies that will make a difference in your community and around the country, then we strongly encourage you to apply.
What you will do:
Design, implement, and maintain our data architecture
Define and monitor metrics for our data architecture
Diagnose and resolve deficiencies in data quality and systems performance
Guide and assist team efforts for data collection, data cleansing, and data sharing
Perform exploratory analysis and provide ad-hoc reports
Track, evaluate, and contribute to technology advances across academic, open source, and commercial forums
Collaborate with partner organizations in the progressive ecosystem on tools and data sets that further our goal of registering more voters and electing more Democrats
Participate in political events that promote, train, and/or directly use our systems
About you:
Advanced skills in database programming (every aspect of SQL, plus PL/pgSQL or PL/SQL or T-SQL)
Basic skills in database administration (PostgreSQL preferred, but Oracle or MySQL okay)
Practical experience with database tuning and performance optimization
Solid understanding of RDBMS principles and modern practices
Solid programming skills (especially Java, Python, and/or JavaScript)
Practical experience with a variety of application data workloads (OLTP, OLAP, etc.)
Familiar with a variety of data integration and data warehouse approaches
Familiar with a variety of middleware approaches and tools (ORM, MQ, GraphQL, REST, webhook, etc)
Committed to leveraging data to elect Democrats and empower progressive organizations
Additional Qualifications (desired but not required):
Experience with cloud computing environments (Google Cloud preferred, but AWS or Azure okay)
Experience with ETL and data pipeline systems (AirFlow, Spark, Nifi, Stitch, Talend, or others)
Experience with data visualization, dashboard, and/or reporting tools (Tableau, Jasper, or others)
Experience with mapping/spatial/GIS data (including tools such as PostGIS)Experience with graph data (such as social influence networks)
Knowledge of machine learning (ML) and artificial intelligence (AI) methods
Advanced knowledge of scaling and high-availability techniques for data architectures
Advanced knowledge of cryptography, authentication, authorization, and/or data privacy methods
Advanced knowledge of USA elections administration and/or campaign operations
Advanced knowledge of statistics (including tools such as R)Familiar with web application frameworks (especially Angular, but VueJS or React okay)
About Civitech: Civitech is an Austin, Texas based startup that is applying modern technology-based solutions to civic problems. We are building elegant, usable interfaces that will help people run for public office, make it easier for volunteers to get involved in the civic arena, and offer modern solutions for local governments to better provide services to their constituents.

Civitech simplifies the process of launching and running a campaign, from fundraising to voter outreach. Political campaigns are expensive and inefficient, frequently spending their limited resources reaching out to the same people over and over because the existing software solutions don’t talk to each other. Civitech integrates with the existing software solutions, simplifies the big picture of a campaign, and uses social media and voter registration to reach out to voters who are frequently left out of the process.

To build truly transformational solutions, Civitech staff must reflect the diverse backgrounds and experiences of the voters, campaigns, and progressive organizations we serve. We strongly encourage women, people of color, first-generation Americans, new parents, single parents, people with disabilities, and members of the queer community to apply.

About our Data Team: The Data Team is focused on building the most robust database of information about voting-eligible adults in the United States in the country. Our datasets cover voter registrations, voter history, statistical scores, contact information, campaign goals, campaign actions, mail tracking, web tracking, advanced address & geocode processes, election-related maps, election results, and more.

Classification, Salary, and Benefits: Full-time, competitive salary. Benefits include a company health plan with medical, dental, and vision insurance; four weeks accrued paid vacation time per year; 10 days accrued paid sick time; and a competitive company option plan.

Civitech is not able to provide visa sponsorship at this time.

Civitech provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, disability, genetic information, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state and local laws."
Data Engineer,"Job Description:

Mandatory Skills:
3 years of experience with Big Data technologies such as Hadoop, MapReduce, Kafka, Spark, or Flink.
At least 2 year's experience in JavaScript, Python, or Shell scripting.
At least 2 years of experience in Java programming.
At least 2 years of experience with Linux / Unix operating systems.
At least 2 years' experience with Hortonworks Data Platform or Cloudera.
At least 2 years' experience with Elastic stack.
At least 2 years' experience with Ansible or Chef.
At least 2 years' experience with Full-stack Development.
At least 1-year experience with Java frameworks (e.g. spring, hibernate, struts).
At least 1-year experience with XML and REST API integration.
At least 1-year experience with Nginx or other web application servers.
At least 1 year experience with Relational database design/development (e.g. DB2, MySQL).
At least 1-year experience with NoSQL database design/development (e.g. HBase, MongoDB, Cloudant).
1year experience integrating SAML / Single Sign-On (SSO) / LDAP authentication for applications Ability and interest to learn new tools and technologies.
1-year experience with Agile development / DevOps methodologies and tools (e.g. GitHub | Travis | Jira) is preferred.
Technical and Professional Experience:
Experience with implementing ServiceNow Knowledge and hands-on experience with Cybersecurity technologies.
Experience with Machine Learning and User Behavioral Analytics (UBA).
Experience with a major SIEM (QRadar, Splunk, ArcSight).
Experience integrating with Resilient and/or Remedy platform."
Data Engineer,"Data Engineer - Austin, TX - $120,000-$130,000Job Description:

The Data Engineer will be responsible for supporting the development of a data factory pipeline within a cloud environment. The Data Engineer will work closely with Software Engineers, DevOps, and Data Analysts. They must be comfortable working both independently and as part of the larger data factory team in a complex, fluid environment.

Requirements:

• Data Factory Experience
• Data Pipeline experience
• CI/CD experience
• ETL
• Big Data experience
• SQL
• Cloud Services
• Java or Scala

Day-to-day responsibilities:

• Responsibilities for Data Engineer Develop and maintain optimal data pipeline architecture - including development related to data acquisition and monitoring, data quality, integration, normalization, and analytics development.
• Adhere to good code practices within an agile environment with a DevOps approach to development and implementation.
• Develop/design appropriate orchestration and structures supporting data transformation, metadata, code dependency and workload management.

In order to fast-track your opportunity, please reach out to me ASAP to talk more about this role! The company is looking to fill the position soon and it will not be on the market long. Please note, this is a permanent role. Candidates for this position must be either US citizens or Green Card holders; Visa sponsorship is not provided. My contact information can be found below.

Email: e.riley@nigelfrank.com

LinkedIn: https://www.linkedin.com/in/emma-riley-72028917a/

Job Requirements:
Azure, SQL, ETL, Data"
Data Engineer,"Job Description
Location: Round Rock, TX
Duration: 9 Months
Looking for local candidates only.

Project: Modeling Environment

The Modeling Environment Project objective is to integrate the process, tools, and platform used by our data scientists in the Digital Marketing R&D Lab for purposes of:
Data Acquisition (Hadoop, Spark, SQL, GreenPlum etc.)
Data Engineering & Preparation
Modeling & Machine learning
Source & Version Control
Microservice-enablement of models & delivery in Pivotal Cloud Foundry
Lab Operations (logging, SRs, uptime, dependency management)

Must Have (skillsets Data engineer with CS background. BS required Master’s degree ideal):

All data scientists are:
Using the same general tools (while different languages)
Expert in applied Python and R
Knowledge in Java, JavaScript
Can easily share, reuse and operationalize the predictive models & results
Have shared repositories, version control and security features

Assumptions/Considerations: (like to have):
Collaborate with the Data Science lab on tools and technologies
Experienced with Big data sets Structured and Unstructured

Regards,
Vikas
Vikasy@apninc.com"
Data Engineer,"You must have Strong knowledge Good knowledge on Java/Python/Big Data
You must have good hand on experience in Full Stack Java Engineer with knowledge on Bigdata like Spark, Scala, Hadoop Good hands-on of Java programming in Hadoop, Spark and Scala, SQL.
Knowledge in data warehouse and basics of data analytics
Having extensive experience Build back-end ETL components and solutions using Hive and spark sql,Thorough analysis of documents and propose design approach,Working Proficiency and Expertise on Hive is required.
Strong in any programming language Python/Java/Scala
Good understanding of Hadoop/spark architecture.Strong experience with Data Warehousing concepts and standards. Working experience in handling huge volume of data.
Good knowledge in Working with the team to build, manage, optimize and customize ETL products and solutions applying best practices, Proficiency in SQL"
Data Engineer,"Job DescriptionSUMMARY/GENERAL DESCRIPTION:

The Data Engineer will support the design and development of data workflows, ETL-like processes, SQL queries, and Visualizations of various clinical and non-clinical databases in the Clearsense Data Ecosystem. They must also demonstrate advanced analytical skills, technical and business knowledge and have a strong understanding of how to leverage industry standard tools and methods to solve problems. The Data Engineer will work closely with Software Engineers by providing data mapping and wrangling expertise and Data Scientists by helping to determine and provide data sets needed for analysis. They often wrestle with problems associated with database integration and messy, unstructured data sets. Their ultimate aim is to provide clean, usable data to whomever may require it.

Must have: SQL, scripting languages, ETL tools and Data workflow tools Responsibilities:

Job Requirements:

• KEY RESPONSIBILITIES:
Research opportunities for data acquisition and new uses for existing data
Develop data set processes for data modeling, mining and production
Employ a variety of languages and tools (e.g. scripting languages) to merge data together
Recommend ways to improve data reliability, efficiency and quality
Define and Develop Clearsense Data Governance Policies
Aggregate and analyze various data sets to provide actionable insight
Develop reports, dashboards, and tools for business-users
Perform detailed analysis of Customer data source
Write complex SQL queries across multiple data sources Job Requirements QUALIFICATIONS:
Must have 5+ years within a data management role performing implementation, integration and/or technical development, with a heavy focus on SQL and relational databases
A nice to have is prior use of Data Governance tools and processes
A nice to have background would involve knowledge and experience with healthcare data exchange platforms and data aggregation tools and healthcare interoperability and messaging standards, including but not limited to HL7 2.x, HL7 3.x, HL7 FHIR, IHE integration profiles
A nice to have background would be an understanding of general medical terminology and healthcare clinical code sets such as LOINC, CPT, ICD, RxNorm, etc.
A nice to have background would be a demonstrated advanced knowledge in Healthcare data, HL7 scripting and two or more programming languages, Healthcare operations, process improvement, and application of technology to improve patient outcomes.
A nice to have background would be as a highly skilled and proficient knowledge of and experience with build tools of the electronic medical record, and other clinical systems.
Self-starter, self-motivated, high level of initiative within a fast-paced, constantly evolving data management environment
Result focused, ability to solve complex problems and resolve conflicts in a timely manner
Required: Bachelors degree in Data Informatics, Computer Science, Business or related field."
Data Engineer,"Title Data Engineer Location Jacksonville, FL Duration 3 months Compensation 40.00 - 45.00 per hour Work Requirements , GC Holders or Authorized to Work in the US Overview TekPartners has some of the most sought after Information Technology positions available. As a reputable company in the IT staffing industry, you can trust us to place you in the right position. We currently have an opportunity for a Data Engineer in Jacksonville, FL Qualifications Engineer with light analytical background. Enables the core infrastructure needed by the Business Analysts and Data Scientists to perform analytics. Partners with the enterprise IT to fill the data and tool gaps. Ability to work in a fast paced environment and adapt to changing requirements. Strong troubleshooting skills, able to debug production code with limited guidance. Self-starter, able to work with limited supervision and see tasks through to completion. Proficiency to create and support database technical environments. Excellent customer service skills that build high levels of customer satisfaction for internal and external customers. Excellent verbal and written communication skills to technical and non-technical audiences of various levels in the organization (e.g., executive, management, individual contributors). Willingly shares relevant technical andor industry knowledge and expertise to other resources. Excellent decision-making, problem-solving, team, and time management skills. Is resourceful and proactive in gathering information and sharing ideas. Ability to estimate work effort for project sub-plans or small projects and ensure the project is successfully completed. 3-5 years' experience with MS SQL. 2 years SSIS experience. Responsibilities Ability to build, update, and scale large datasets. Write queries to automate production tasks. Responsible for health and hygiene of non-IT managed data. Consume business requirements and develops the data, database specifications, tables and element attributes to support. Models databases and develops tables, stored procedures, views, and other database objects. Maintains database dictionaries, monitors overall database standards and procedures, and integrates systems through database design. Works closely with other developers to integrate databases with other applications. May provide leadership andor guidance to other technical professionals. Performs other related duties as assigned. Our benefits package includes Comprehensive Medical Benefits Competitive Pay, 401K Retirement Plan And Much More About TekPartners TekPartners is one of the fastest growing private staffing firms in the United States. We are a premier provider of highly qualified IT talent, Workforce Solutions and Business Intelligence Solutions to many enterprise organizations across the nation. As experts in the industry, our team continues to match proven talent to the right job opportunity every day. TekPartners is an Equal Opportunity Employer."
Data Engineer,"The Business

GradTests.com is a leading provider of practice psychometric tests to graduates, students and young professionals.

The Role

The core purpose of the role is to make high quality, high availability, accurate data available for our data analysts and data scientists to do their analysis, derive their insights and build their models. You are the Scotty Pippin to the Michael Jordans. You are the Xavi to the Messis.

You'll do things like:
Ensure our data warehouse is well structured, running smoothly and efficiently for all business intelligence
Set up and maintain various data pipelines used for customer analytics, marketing analytics and product analytics
Skills and experience

Non negotiables:
SQL
Python
Strong knowledge of traditional relational databases - we don't mind which
Some experience with cloud technologies - again we don't mind if it's AWS, GCP or Azure
Experience in any streaming technology
Great experience in using third party APIs at scale
Some web scraping experience
An obsession with data quality
Strong communication skills
Nice to haves:
Experience in working with analysts
Any basic knowledge of advanced analytics techniques
Experience in a visualisation tool like Tableau
Job Types: Full-time, Contract

Salary: $100,000.00 /year

Work Remotely:
Yes"
Data Engineer,"Where good people build rewarding careers.

Think that working in the insurance field cant be exciting, rewarding and challenging? Think again. Youll help us reinvent protection and retirement to improve customers lives. Well help you make an impact with our training and mentoring offerings. Here, youll have the opportunity to expand and apply your skills in ways you never thought possible. And youll have fun doing it. Join a company of individuals with hopes, plans and passions, all using and developing our talents for good, at work and in life.
Job Description
The Big Data Engineer - is accountable for leading a technical team of developers on the Master Data Management (MDM) team to deliver innovative data quality solutions that generate business value, specifically focused on Hadoop-based, non-relational data. This role requires in-depth data experience to translate unique business requirements into technical data quality specifications that ensure data is fit-for-purpose. Engage with strategic partners to optimize the data quality function and identify strategic capabilities to keep up with data and insurance industry trends.

Data Science incorporates techniques across many disciplines including mathematics/statistics, computer programming, data engineering and ETL, software development, and high performance computing with traditional business expertise with the goal of extracting meaning from data to optimize future business decisions. Individuals in this field should be an expert/fluent in several of these disciplines and sufficiently proficient in others to effectively design, build, and deliver end to end predictive analytics products to optimize future decisions. Individual demonstrates sufficient analytic agility to quickly develop new skills across these disciplines as those disciplines evolve. The Big Data Engineer job family is accountable for end to end engineering of data solutions which includes designing and building systems for data storage and analytics that enable Allstate analysts to make better decisions to achieve Allstates goals.

This role is responsible for driving multiple complex tracks of work to deliver Big Data solutions enabling advanced data science and analytics. This includes working with the team on new Big Data systems for analyzing data; the coding & development of advanced analytics solutions to make/optimize business decisions and processes; integrating new tools to improve descriptive, predictive, and prescriptive analytics; and discovery of new technical challenges that can be solved with existing and emerging Big Data hardware and software solutions.

This role contributes to the structured and unstructured Big Data / Data Science tools of Allstate from traditional to emerging analytics technologies and methods. The role is responsible for assisting in the selection and development of other team members.
Key Responsibilities
Executes complex functional work tracks for the team.
Partners with ATSV teams on Big Data efforts.
Partners closely with team members on Big Data solutions for our data science community and analytic users.
Leverages and uses Big Data best practices / lessons learned to develop technical solutions used for descriptive analytics, ETL, predictive modeling, and prescriptive real time decisions analytics
Influence within the team on the effectiveness of Big Data systems to solve their business problems.
Participates in the development of complex technical solutions using Big Data techniques in data & analytics processes.
Supports Innovation; regularly provides new ideas to help people, process, and technology that interact with analytic ecosystem.
Participates in the development of complex prototypes and department applications that integrate Big Data and advanced analytics to make business decisions.
Uses new areas of Big Data technologies, (ingestion, processing, distribution) and research delivery methods that can solve business problems.
Understands the Big Data related problems and requirements to identify the correct technical approach.
Works with key team members to ensure efforts within owned tracks of work will meet their needs.
Drives multiple tracks of work within the research group.
Identifies and develops Big Data sources & techniques to solve business problems.
Co-mingles data sources to lead work on data and problems across departments to drive improved business & technical results through designing, building, and partnering to implement models.
Manages various Big Data analytic tool development projects with midsize teams.
Executes on Big Data requests to improve the accuracy, quality, completeness, speed of data, and decisions made from Big Data analysis.
Uses, learns, teaches, and supports a wide variety of Big Data and Data Science tools to achieve results (i.e., R, ETL Tools, Hadoop, and others).
Uses, learns, teaches, and supports a wide variety of programming languages on Big Data and Data Science work (i.e. Java, C#, Python, and Perl).
Trains more junior engineers.
Job Qualifications
Experience with various data types
Experience with development, management, and manipulation of large, complex datasets
Experience with database & ETL technologies
Demonstrated knowledge of data management competencies and implementation
Proficient at writing SQL queries and verifying the results
Familiar with organizational change management strategies
Strong communication skills, both written and verbal
Excellent time-management and organization skills
Ability to operate in a rapidly changing, high-visibility environment
Desire to learn and ability to learn quickly
Advanced analytical and problem-solving skills
Strong decision-making skills
Ability to draw meaningful conclusions and recommendations

The candidate(s) offered this position will be required to submit to a background investigation, which includes a drug screen.

Good Work. Good Life. Good Hands®.

As a Fortune 100 company and industry leader, we provide a competitive salary but thats just the beginning. Our Total Rewards package also offers benefits like tuition assistance, medical and dental insurance, as well as a robust pension and 401(k). Plus, youll have access to a wide variety of programs to help you balance your work and personal life -- including a generous paid time off policy.

Learn more about life at Allstate. Connect with us on Twitter, Facebook, Instagram and LinkedIn or watch a video.

Allstate generally does not sponsor individuals for employment-based visas for this position.

Effective July 1, 2014, under Indiana House Enrolled Act (HEA) 1242, it is against public policy of the State of Indiana and a discriminatory practice for an employer to discriminate against a prospective employee on the basis of status as a veteran by refusing to employ an applicant on the basis that they are a veteran of the armed forces of the United States, a member of the Indiana National Guard or a member of a reserve component.

For jobs in San Francisco, please click here for information regarding the San Francisco Fair Chance Ordinance.
For jobs in Los Angeles, please click here for information regarding the Los Angeles Fair Chance Initiative for Hiring Ordinance.

To view the EEO is the Law poster click here. This poster provides information concerning the laws and procedures for filing complaints of violations of the laws with the Office of Federal Contract Compliance Programs

To view the FMLA poster, click here. This poster summarizing the major provisions of the Family and Medical Leave Act (FMLA) and telling employees how to file a complaint.

It is the Companys policy to employ the best qualified individuals available for all jobs. Therefore, any discriminatory action taken on account of an employees ancestry, age, color, disability, genetic information, gender, gender identity, gender expression, sexual and reproductive health decision, marital status, medical condition, military or veteran status, national origin, race (include traits historically associated with race, including, but not limited to, hair texture and protective hairstyles), religion (including religious dress), sex, or sexual orientation that adversely affects an employee's terms or conditions of employment is prohibited. This policy applies to all aspects of the employment relationship, including, but not limited to, hiring, training, salary administration, promotion, job assignment, benefits, discipline, and separation of employment."
Data Engineer,"Hi, Hope you are doing well Please let me know if would be interested for the below position and share your updated resume to cmartinamvotech.com Job Title Data Engineer Location Fort Worth, TX Mode Direct Hire Remote till Covid ends. Job Description Specifically, the Data Engineer has a passion for data, and creatively leverages expertise with data tools and programming languages to assess, transform, organize, optimize, and exploit systems and platform data. Further, you will be responsible for generating representative data sets for demonstrations, technology evaluation, systems development, and data science initiatives. You will be expected to help automate processes, and articulate methods to gain data management efficiencies, as well implementing and adopting of Data Engineering Best Practices to include mentoring projects and team members toward success. Senior skill level 4+ years of data engineering, schema design, dimensional data modeling, and or data management experience Proficient with data management tools, such as Python, SQL, Java, and use of Git Demonstrated experience with extracting, cleaning, managing, optimizing, and exploiting large and very complex data sets Experience with best practices for compute, storage, and transfer optimization in processing large volumes of data Active SECRET security clearance is required. Preferred Experience with graph databases and event sourcing models Familiarity with machine learning, artificial intelligence, and or geospatial data analysis Strong interpersonal skills combined with ability to multi-task and maintain flexibility and creativity in a variety of situations Notes Candidates must have strong experience in SQL, Python and Data Modeling Must have secret clearance Warehousing would be preferred Thanks Regards Chris Martin cmartinamvotech.com"
Data Engineer,"Responsibilities Data Extraction, Transformation, Loading are main responsibilities considering enterprise data management for Data Lakes and Data Warehouses. Build Azure Data Factory Pipelines. Implement Azure Cloud Data Warehouses. Work with the Application Development team to Implement Data Strategies, Build Data Flows and Develop Conceptual Data Models. Required skills Experience Experience in Building Data Applications using Azure Data Factory Experience running ETLELT Projects from end to end, understanding required Experience in Azure Data Bricks with Python, Spark mandatory Strong SQL Skills Good Understanding of Azure SQL Data WarehouseDB Excellent in Data Analytical Skills Strong understanding of Data Integration (Validation and Cleaning), familiarity with Complex Data Structures Good to know Data Visualization tool Power BI"
Data Engineer,"Role Developer - Data Engineer Location Irving, TX Type of Hire Full-time Job Details Role Description Analyze and understand data sources APIs bull Design and Develop methods to connect collect data from different data sources bull Design and Develop methods to filtercleanse the data bull Design and Develop SQL , Hive queries, APIs to extract data from the store bull Work closely with data Scientists to ensure the source data is aggregated and cleansed bull Work with product managers to understand the business objectives bull Work with cloud and data architects to define robust architecture in cloud setup pipelines and work flows bull Work with DevOps to build automated data pipelines Total Experience Required bull 4 years 10 of relevant experience bull The candidate should have performed client facing roles and possess excellent communication skills Business Domain knowledge Finance banking systems, Fraud, Payments Required Technical Skills bull Big Data-Hadoop, NoSQL, Hive, Apache Spark bull Python bull Java REST bull GIT and Version Control Personal Skills bull Experienced in managing work with distributed teams bull Experience working in SCRUM methodology bull Proven sense of high accountability and self-drive to take on and see through big challenges bull Confident, takes ownership, willingness to get the job done bull Excellent verbal communications and cross group collaboration skills"
Data Engineer,"The Data Engineer joins a team of engineers, subject matter experts, and data scientists to jointly provide experienced, high-quality Information Technology (IT) engineering solutions. The Data Engineer will work with our clients on their hardest challenges – advancing and enhancing systems supporting the defense of our nation and its partners. Your efforts will be integral to the success of these initiatives. As an NT Concepts team member, you will be expected to be solutions-oriented, innovative, collaborative and agile. Performing as part of, or in support of a SCRUM team, you will be expected to thrive in organized chaos and immediately adopt agile team self-organizing methods and techniques.

Specifically, the Data Engineer has a passion for data, and creatively leverages expertise with data tools and programming languages to assess, transform, organize, optimize, and exploit systems and platform data. Further, you will be responsible for generating representative data sets for demonstrations, technology evaluation, systems development, and data science initiatives. You will be expected to help automate processes, and articulate methods to gain data management efficiencies, as well implementing and adopting of Data Engineering Best Practices to include mentoring projects and team members toward success.

Senior skill level
4+ years of data engineering, schema design, dimensional data modeling, and / or data management experience
Proficient with data management tools, such as Python, SQL, Java, and use of Git
Demonstrated experience with extracting, cleaning, managing, optimizing, and exploiting large and very complex data sets
Experience with best practices for compute, storage, and transfer optimization in processing large volumes of data
Active SECRET security clearance is required.
Preferred:
Experience with graph databases and event sourcing models
Familiarity with machine learning, artificial intelligence, and / or geospatial data analysis
Strong interpersonal skills combined with ability to multi-task and maintain flexibility and creativity in a variety of situations
#CJ
#JT"
Data Engineer,"DATA ENGINEER

Job Description

Simpli.fi is hiring talented and experienced software engineers to join its Data Engineering team.

At Simpli.fi, you will have the opportunity to truly work with Big Data. We have roughly 100 Terabytes of data in our data warehouse and nearly 1 Petabyte in our Hadoop cluster. We handle over 70 billion messages a day funneled through Kafka topics. We integrate with a real time system that processes nearly 3 million queries per second on over 60,000 active campaigns. The Data Engineering team is responsible for moving and transforming massive datasets into valuable and insightful information.

Our Data Engineering team works very closely with all aspects of operational data, both internal and external. We are hiring Data Engineers with the Software Engineering capabilities to not only build data pipelines that efficiently transform and move data across systems, but also to build the next generation of data tools that will enable us to take full advantage of this data. In this role, your work will broadly influence the company's clients and internal analysts.

A career at Simpli.fi offers countless ways to make an impact in a fast-growing organization. This is a full-time position based in our office in Fort Worth, Texas.

Â

Responsibilities
Build data expertise and own data quality for the transfer pipelines that you build to transform and move data to our voluminous Data Warehouse (Flume, Kafka, Spark Streaming, Hadoop, Vertica)
Architect, build and launch new data models that provide intuitive analytics to our customers (Vertica/Star Schema, Looker analytics)
Design and develop new systems and tools to enable clients to optimize and track advertising campaigns (Vertica, Looker, Spark)
Use your expert skills across a number of platforms and tools such as Python, Ruby, SQL, Linux shell scripting, Git, and Chef
Work across multiple teams in high visibility roles and own the solution end-to-end
Provide support for our existing production systems. We use Datadog and PagerDuty for monitoring and alerting.
Requirements
Proficiency building and supporting applications on Linux topology.
Familiarity with OO and FP methodologies and philosophies.
Moderate experience in Big Data ecosystem (Hadoop, Spark, Kafka, etc.)
Proficiency in Ruby or Python development.
Familiarity with column-oriented Big Data systems such as Vertica or Cassandra.
Familiarity with profiling and tuning a SQL execution plan
Familiarity with the JVM. Scala is a definite plus.
Excellent communication skills including the ability to identify and communicate data driven insights.
BS or MS degree in Computer Science, Software Engineering, or a related technical field. We will consider equivalent experience in the industry."
Data Engineer,"Req ID: 89867At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company's growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here.NTT DATA Services currently seeks a Data Engineer to join our team in Irving, Texas (US-TX), United States (US).Role Responsibilities:* Analyze and understand data sources & APIs* Design and Develop methods to connect & collect data from different data sources* Design and Develop methods to filter/cleanse the data* Design and Develop SQL, Hive queries, APIs to extract data from the store* Work closely with data Scientists to ensure the source data is aggregated and cleansed* Work with product managers to understand the business objectives* Work with cloud and data architects to define robust architecture in cloud setup pipelines and workflows* Work with DevOps to build automated data pipelinesBasic Qualifications:* 3+ years of Advanced knowledge of Hadoop ecosystem and Big Data technologies* 3+ years of Hadoop (Cloudera) or Cloud Technologies building pipelines using Spark /Pyspark* 3+ years of experience in programming in Scala and Python* 2+ years Hadoop eco-system (HDFS, MapReduce, Yarn, Hive, Pig, Impala, Spark, Kafka,)* 2+ years ETL tools* 1 year of HTTP and invoking web-APIs* 1 year of NLP and text processingPreferences:* Machine learning engineering* Ab Initio* Work with distributed teams* SCRUM methodologyINDFSThis position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment.About NTT DATA ServicesNTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services.NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more.NTT DATA, Inc. (the ""Company"") is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result."
Data Engineer,"Data Engineer Ideal candidates should have experience with Data Ingestion and Consumption. That is transforming from source raw data, cleansing missing data and outliers and preparing the data ready for analytics processing. Key requirements Understanding of Kafka-Yarn-Spark-HDFS ecosystem for ingestion IS A MUST. In depth knowledge of preparing large scale data analytics for consumption's. Key knowledge on HIVE and query optimization in HIVE Banking experience related to risk management and analysis on Fraud is a plus Career progression must show initial work with Hadoop and moving on to include Kafka and Spark in the latter career Hands on experience with Spark implementation using either Spark in Scala or PySpark"
Data Engineer,"• Minimum 8 years of experience working with Hadoop, Hive,Sqoop, Spark, Scala, Python, Kafka and Sentry* Proven Experience in handling Terabyte and Petabyte of data on Cloudera Cluster* Proven Experience in handling variety of data formats* Experience in building large scale Data Lake Environment* Troubleshooting Hive Performance issues and developing HQL queries* Experience with Spark and PySpark* Experience in implementing CI/CD Process and Job Automation through Autosys* Experience in Hadoop Cluster Administration is a big plus* Experience with integration of data from multiple data sources* Assist Analytics and Data Scientist team and Business Users* Exceptional communication skills and the ability to communicate appropriately at all levels of the organization; this includes written and verbal communications as well as visualizations* The ability to act as liaison conveying information needs of the business to IT and data constraints to the business; applies equal conveyance regarding business strategy and IT strategy, business processes and work flow* Team player able to work effectively at all levels of an organization with the ability to influence others to move toward consensus* Strong situational analysis and decision making abilitiesLI-AG1"
Data Engineer,"Job Description

•
Minimum 8 years of experience working with Hadoop, Hive,Sqoop, Spark, Scala,
Python, Kafka and Sentry
• Proven Experience in handling Terabyte and Petabyte of data on Cloudera
Cluster
• Proven Experience in handling variety of data formats
• Experience in building large scale Data Lake Environment
• Troubleshooting Hive Performance issues and developing HQL queries
• Experience with Spark and PySpark
• Experience in implementing CI/CD Process and Job Automation through
Autosys
• Experience in Hadoop Cluster Administration is a big plus
• Experience with integration of data from multiple data sources
• Assist Analytics and Data
Scientist team and Business Users

• Exceptional communication skills and the ability to communicate
appropriately at all levels of the organization; this includes written and
verbal communications as well as visualizations
• The ability to act as liaison conveying information needs of the business
to IT and data constraints to the business; applies equal conveyance
regarding business strategy and IT strategy, business processes and work flow
• Team player able to work effectively at all levels of an organization
with the ability to influence others to move toward consensus
• Strong situational analysis and decision making abilities
#LI-AG1

Job Function

TECHNOLOGY

Role

Developer

Job Id

158552

Desired Skills

Big Data

Desired Candidate Profile

Qualifications :
BACHELOR OF ENGINEERING"
Data Engineer,"Tachyon Technologies is a Digital Transformation consulting firm that partners with businesses to implement customer-focused business transformation. Aligned with SAP's digital core, Tachyon Technologies collaborates with its clients to transform their business by leveraging existing IT investments and leading-edge digital solutions to positively impact their customers' experience. From initiation through realization, Tachyon Technologies understands what it takes for a consulting partner to be effective and strives to deliver a meaningful solution that exceeds its clients' expectations

JD: Python Lambda, building API integration on AWS platform (with Python as the language) and hands-on developer.

Disclaimer: The above statements are not intended to be a complete statement of job content, rather act as a guide to the essential functions performed by the employee assigned to this classification. Management retains the discretion to add or change the duties of the position at any time"
Data Engineer,"Job Description
Big Data Software Engineer
Location: Arlington/Dullas - (US Citizen Only)

Job Description:
Proficiency in, at least, one modern programming language such as Java, C++, C, Python, or Scala
Strong problem solving skills; adaptability, proactivity and willingness to take ownership and deal with ambiguity
Strong verbal and written communication skills - with both technical and non-technical individuals
Knowledge of Software Design Patterns
Experience analyzing large data sets and developing data driven statistical model.
Experience with Apache Hadoop, Hadoop Distributed File System (HDFS) and Hadoop MapReduce.
Experience with tools from the Hadoop ecosystem such as Spark
Experience with Splunk or similar analytic framework, e.g. ELK
Experience with MongoDB and GridFS..
Experience with RESTful Web Services (Jersey, RESTEasy, or similar).
Experience with natural language processing techniques and text analytics.
Ability to pick up new technologies, quickly review and integrate new technologies

Company Overview:

Node.Digital is an independent Digital Automation & Cognitive Engineering company that integrates best of breed technologies to accelerate business impact.
Our Core Values help us in our mission. They include:

OUR CORE VALUES
*Identifying the~RIGHT PEOPLE~and developing them to their full capabilities**
*Our customer’s “Mission” is our “Mission”. Our~MISSION FIRST~approach is designed to keep our customer fully engaged while becoming their trusted partner**
*We believe in~SIMPLIFYING~complex problems with a relentless focus on agile delivery excellence**
*Our mantra is “~Simple*Secure*Speed~”in delivery of innovative services and solutions**"
Data Engineer,"Job Title: Big Data Engineer

Location: Irving, TX

Duration: Full Time

Technical & Functional Skills :

Minimum 8 years of experience working with Hadoop, Hive,Sqoop, Spark, Scala, Python, Kafka and Sentry
Proven Experience in handling Terabyte and Petabyte of data on Cloudera Cluster
Proven Experience in handling variety of data formats
Experience in building large scale Data Lake Environment
Troubleshooting Hive Performance issues and developing HQL queries
Experience with Spark and PySpark
Experience in implementing CI/CD Process and Job Automation through Autosys
Experience in Hadoop Cluster Administration is a big plus
Experience with integration of data from multiple data sources
Assist Analytics and Data Scientist team and Business Users
Exceptional communication skills and the ability to communicate appropriately at all levels of the organization; this includes written and verbal communications as well as visualizations
The ability to act as liaison conveying information needs of the business to IT and data constraints to the business; applies equal conveyance regarding business strategy and IT strategy, business processes and work flow
Team player able to work effectively at all levels of an organization with the ability to influence others to move toward consensus
Strong situational analysis and decision making abilities"
Data Engineer,"Client- City Bank
Location- Irving, TX
FullTime Role

• Minimum 8 years of experience working with Hadoop, Hive,Sqoop, Spark, Scala, Python, Kafka and Sentry
• Proven Experience in handling Terabyte and Petabyte of data on Cloudera Cluster
• Proven Experience in handling variety of data formats
• Experience in building large scale Data Lake Environment
• Troubleshooting Hive Performance issues and developing HQL queries
• Experience with Spark and PySpark
• Experience in implementing CI/CD Process and Job Automation through Autosys
• Experience in Hadoop Cluster Administration is a big plus
• Experience with integration of data from multiple data sources
• Assist Analytics and Data Scientist team and Business Users

• Exceptional communication skills and the ability to communicate appropriately at all levels of the organization; this includes written and verbal communications as well as visualizations
• The ability to act as liaison conveying information needs of the business to IT and data constraints to the business; applies equal conveyance regarding business strategy and IT strategy, business processes and work flow
• Team player able to work effectively at all levels of an organization with the ability to influence others to move toward consensus
• Strong situational analysis and decision making abilities

Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company."
